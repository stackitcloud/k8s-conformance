  I0111 11:19:00.858607      23 e2e.go:117] Starting e2e run "44be952d-0456-4382-9b38-ff2850af2e84" on Ginkgo node 1
  Jan 11 11:19:00.936: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1704971940 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Jan 11 11:19:01.402: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:19:01.405: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Jan 11 11:19:01.611: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Jan 11 11:19:01.639: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-flannel' (0 seconds elapsed)
  Jan 11 11:19:01.639: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm' (0 seconds elapsed)
  Jan 11 11:19:01.639: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm64' (0 seconds elapsed)
  Jan 11 11:19:01.639: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-ppc64le' (0 seconds elapsed)
  Jan 11 11:19:01.639: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-s390x' (0 seconds elapsed)
  Jan 11 11:19:01.639: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  Jan 11 11:19:01.639: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
  Jan 11 11:19:01.639: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'vsphere-cloud-controller-manager' (0 seconds elapsed)
  Jan 11 11:19:01.639: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'vsphere-csi-node' (0 seconds elapsed)
  Jan 11 11:19:01.639: INFO: e2e test version: v1.27.5
  Jan 11 11:19:01.643: INFO: kube-apiserver version: v1.27.5
  Jan 11 11:19:01.643: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:19:01.672: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.270 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 01/11/24 11:19:02.298
  Jan 11 11:19:02.298: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 11:19:02.3
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:19:02.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:19:02.347
  STEP: Creating projection with secret that has name projected-secret-test-76047073-e5dd-4cbf-8496-c6df95074f03 @ 01/11/24 11:19:02.354
  STEP: Creating a pod to test consume secrets @ 01/11/24 11:19:02.368
  STEP: Saw pod success @ 01/11/24 11:19:06.421
  Jan 11 11:19:06.434: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-secrets-3ea8a1f0-5f16-4025-b3d6-dd76c93c93c3 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 01/11/24 11:19:06.495
  Jan 11 11:19:06.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9732" for this suite. @ 01/11/24 11:19:06.571
• [4.314 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 01/11/24 11:19:06.614
  Jan 11 11:19:06.614: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/11/24 11:19:06.615
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:19:06.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:19:06.669
  STEP: set up a multi version CRD @ 01/11/24 11:19:06.679
  Jan 11 11:19:06.681: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: mark a version not serverd @ 01/11/24 11:19:19.899
  STEP: check the unserved version gets removed @ 01/11/24 11:19:19.958
  STEP: check the other version is not changed @ 01/11/24 11:19:23.68
  Jan 11 11:19:31.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2485" for this suite. @ 01/11/24 11:19:31.161
• [24.566 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 01/11/24 11:19:31.185
  Jan 11 11:19:31.185: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename deployment @ 01/11/24 11:19:31.187
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:19:31.248
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:19:31.257
  STEP: creating a Deployment @ 01/11/24 11:19:31.277
  STEP: waiting for Deployment to be created @ 01/11/24 11:19:31.296
  STEP: waiting for all Replicas to be Ready @ 01/11/24 11:19:31.303
  Jan 11 11:19:31.309: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 11 11:19:31.310: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 11 11:19:31.377: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 11 11:19:31.377: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 11 11:19:31.427: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 11 11:19:31.427: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 11 11:19:31.607: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 11 11:19:31.607: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 11 11:19:32.929: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jan 11 11:19:32.929: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jan 11 11:19:33.033: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 01/11/24 11:19:33.033
  W0111 11:19:33.049884      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jan 11 11:19:33.054: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 01/11/24 11:19:33.054
  Jan 11 11:19:33.059: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 0
  Jan 11 11:19:33.060: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 0
  Jan 11 11:19:33.060: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 0
  Jan 11 11:19:33.060: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 0
  Jan 11 11:19:33.061: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 0
  Jan 11 11:19:33.061: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 0
  Jan 11 11:19:33.061: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 0
  Jan 11 11:19:33.061: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 0
  Jan 11 11:19:33.061: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 1
  Jan 11 11:19:33.061: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 1
  Jan 11 11:19:33.062: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2
  Jan 11 11:19:33.062: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2
  Jan 11 11:19:33.062: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2
  Jan 11 11:19:33.062: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2
  Jan 11 11:19:33.091: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2
  Jan 11 11:19:33.091: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2
  Jan 11 11:19:33.190: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2
  Jan 11 11:19:33.190: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2
  Jan 11 11:19:33.244: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2
  Jan 11 11:19:33.244: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2
  Jan 11 11:19:33.293: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 1
  Jan 11 11:19:33.293: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 1
  Jan 11 11:19:35.177: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2
  Jan 11 11:19:35.177: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2
  Jan 11 11:19:35.233: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 1
  STEP: listing Deployments @ 01/11/24 11:19:35.233
  Jan 11 11:19:35.245: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 01/11/24 11:19:35.245
  Jan 11 11:19:35.274: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 01/11/24 11:19:35.274
  Jan 11 11:19:35.295: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 11 11:19:35.316: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 11 11:19:35.389: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 11 11:19:35.442: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 11 11:19:35.467: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 11 11:19:37.030: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 11 11:19:37.111: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 11 11:19:37.197: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 11 11:19:37.292: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 11 11:19:39.005: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 01/11/24 11:19:39.073
  STEP: fetching the DeploymentStatus @ 01/11/24 11:19:39.096
  Jan 11 11:19:39.120: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 1
  Jan 11 11:19:39.120: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 1
  Jan 11 11:19:39.120: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 1
  Jan 11 11:19:39.120: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 1
  Jan 11 11:19:39.121: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 1
  Jan 11 11:19:39.121: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2
  Jan 11 11:19:39.121: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 3
  Jan 11 11:19:39.121: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2
  Jan 11 11:19:39.121: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 2
  Jan 11 11:19:39.121: INFO: observed Deployment test-deployment in namespace deployment-7446 with ReadyReplicas 3
  STEP: deleting the Deployment @ 01/11/24 11:19:39.122
  Jan 11 11:19:39.153: INFO: observed event type MODIFIED
  Jan 11 11:19:39.154: INFO: observed event type MODIFIED
  Jan 11 11:19:39.154: INFO: observed event type MODIFIED
  Jan 11 11:19:39.154: INFO: observed event type MODIFIED
  Jan 11 11:19:39.155: INFO: observed event type MODIFIED
  Jan 11 11:19:39.155: INFO: observed event type MODIFIED
  Jan 11 11:19:39.155: INFO: observed event type MODIFIED
  Jan 11 11:19:39.155: INFO: observed event type MODIFIED
  Jan 11 11:19:39.155: INFO: observed event type MODIFIED
  Jan 11 11:19:39.157: INFO: observed event type MODIFIED
  Jan 11 11:19:39.157: INFO: observed event type MODIFIED
  Jan 11 11:19:39.179: INFO: Log out all the ReplicaSets if there is no deployment created
  Jan 11 11:19:39.201: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-7446  4c32da2c-ed07-4540-a004-24631da527b4 187147745 3 2024-01-11 11:19:31 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 7047368e-80f2-4a69-b0b1-0a9c85e42750 0xc0040510a7 0xc0040510a8}] [] [{kube-controller-manager Update apps/v1 2024-01-11 11:19:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7047368e-80f2-4a69-b0b1-0a9c85e42750\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-11 11:19:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004051130 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jan 11 11:19:39.223: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-7446  cd8db938-485b-4a9f-b0ab-f194d9a936be 187147843 4 2024-01-11 11:19:33 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 7047368e-80f2-4a69-b0b1-0a9c85e42750 0xc0040511a7 0xc0040511a8}] [] [{kube-controller-manager Update apps/v1 2024-01-11 11:19:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7047368e-80f2-4a69-b0b1-0a9c85e42750\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-11 11:19:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004051230 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jan 11 11:19:39.249: INFO: pod: "test-deployment-5b5dcbcd95-dk9zf":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-dk9zf test-deployment-5b5dcbcd95- deployment-7446  e30583a0-1f71-4f38-8f11-d92906342377 187147838 0 2024-01-11 11:19:35 +0000 UTC 2024-01-11 11:19:39 +0000 UTC 0xc0041ac4d0 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 cd8db938-485b-4a9f-b0ab-f194d9a936be 0xc0041ac507 0xc0041ac508}] [] [{kube-controller-manager Update v1 2024-01-11 11:19:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cd8db938-485b-4a9f-b0ab-f194d9a936be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 11:19:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.69.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cdmx7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cdmx7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:19:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:19:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:19:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:19:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.202,PodIP:10.233.69.146,StartTime:2024-01-11 11:19:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-11 11:19:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c,ContainerID:containerd://f17825d02ce778540917d00371749dd0374fc452750dba417bf5ad4118bf8f6f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.69.146,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jan 11 11:19:39.250: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-7446  6ff80575-481a-4ff0-a4b4-291706bd7a47 187147834 2 2024-01-11 11:19:35 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 7047368e-80f2-4a69-b0b1-0a9c85e42750 0xc0040512a7 0xc0040512a8}] [] [{kube-controller-manager Update apps/v1 2024-01-11 11:19:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7047368e-80f2-4a69-b0b1-0a9c85e42750\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-11 11:19:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004051330 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Jan 11 11:19:39.268: INFO: pod: "test-deployment-6fc78d85c6-h9w77":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-h9w77 test-deployment-6fc78d85c6- deployment-7446  fd79a906-de79-4d80-920a-1eef64d22848 187147849 0 2024-01-11 11:19:37 +0000 UTC 2024-01-11 11:19:40 +0000 UTC 0xc004051800 map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 6ff80575-481a-4ff0-a4b4-291706bd7a47 0xc004051837 0xc004051838}] [] [{kube-controller-manager Update v1 2024-01-11 11:19:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ff80575-481a-4ff0-a4b4-291706bd7a47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 11:19:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.69.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-whd6j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-whd6j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:19:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:19:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:19:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:19:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.202,PodIP:10.233.69.147,StartTime:2024-01-11 11:19:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-11 11:19:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://82e85f94928057f4f0bbd03bc9c8fac207a4072797dcc7ae752056cf0b7bd2f7,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.69.147,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jan 11 11:19:39.268: INFO: pod: "test-deployment-6fc78d85c6-llg8x":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-llg8x test-deployment-6fc78d85c6- deployment-7446  f8fc3a3e-ec16-426d-aa9d-bbbc78964b93 187147850 0 2024-01-11 11:19:35 +0000 UTC 2024-01-11 11:19:40 +0000 UTC 0xc004051a00 map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 6ff80575-481a-4ff0-a4b4-291706bd7a47 0xc004051a37 0xc004051a38}] [] [{kube-controller-manager Update v1 2024-01-11 11:19:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ff80575-481a-4ff0-a4b4-291706bd7a47\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 11:19:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pbf9q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pbf9q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:19:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:19:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:19:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:19:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:10.233.68.26,StartTime:2024-01-11 11:19:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-11 11:19:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8c95abdce62c268c378abcef8191c2328addf257fcd2a8dbeb77006408efdb19,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.26,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jan 11 11:19:39.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7446" for this suite. @ 01/11/24 11:19:39.301
• [8.138 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 01/11/24 11:19:39.325
  Jan 11 11:19:39.325: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename webhook @ 01/11/24 11:19:39.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:19:39.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:19:39.41
  STEP: Setting up server cert @ 01/11/24 11:19:39.547
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/11/24 11:19:40.262
  STEP: Deploying the webhook pod @ 01/11/24 11:19:40.288
  STEP: Wait for the deployment to be ready @ 01/11/24 11:19:40.317
  Jan 11 11:19:40.336: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 01/11/24 11:19:42.358
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 11:19:42.38
  Jan 11 11:19:43.380: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 01/11/24 11:19:43.389
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 01/11/24 11:19:43.436
  STEP: Creating a dummy validating-webhook-configuration object @ 01/11/24 11:19:43.471
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 01/11/24 11:19:43.495
  STEP: Creating a dummy mutating-webhook-configuration object @ 01/11/24 11:19:43.515
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 01/11/24 11:19:43.546
  Jan 11 11:19:43.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2458" for this suite. @ 01/11/24 11:19:43.826
  STEP: Destroying namespace "webhook-markers-854" for this suite. @ 01/11/24 11:19:43.842
• [4.534 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:305
  STEP: Creating a kubernetes client @ 01/11/24 11:19:43.861
  Jan 11 11:19:43.861: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename daemonsets @ 01/11/24 11:19:43.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:19:43.907
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:19:43.916
  STEP: Creating a simple DaemonSet "daemon-set" @ 01/11/24 11:19:43.98
  STEP: Check that daemon pods launch on every node of the cluster. @ 01/11/24 11:19:43.99
  Jan 11 11:19:44.005: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:19:44.005: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:19:44.005: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:19:44.015: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 11:19:44.015: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  Jan 11 11:19:45.032: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:19:45.032: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:19:45.032: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:19:45.051: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 11 11:19:45.052: INFO: Node env1-test-worker-1 is running 0 daemon pod, expected 1
  Jan 11 11:19:46.031: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:19:46.031: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:19:46.031: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:19:46.040: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jan 11 11:19:46.040: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 01/11/24 11:19:46.049
  Jan 11 11:19:46.101: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:19:46.101: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:19:46.101: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:19:46.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jan 11 11:19:46.115: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 01/11/24 11:19:46.116
  STEP: Deleting DaemonSet "daemon-set" @ 01/11/24 11:19:47.153
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7055, will wait for the garbage collector to delete the pods @ 01/11/24 11:19:47.153
  Jan 11 11:19:47.233: INFO: Deleting DaemonSet.extensions daemon-set took: 19.041549ms
  Jan 11 11:19:47.334: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.706391ms
  Jan 11 11:19:50.351: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 11:19:50.351: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jan 11 11:19:50.367: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"187148099"},"items":null}

  Jan 11 11:19:50.380: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"187148099"},"items":null}

  Jan 11 11:19:50.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7055" for this suite. @ 01/11/24 11:19:50.481
• [6.658 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 01/11/24 11:19:50.52
  Jan 11 11:19:50.520: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename custom-resource-definition @ 01/11/24 11:19:50.523
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:19:50.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:19:50.6
  Jan 11 11:19:50.608: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:20:52.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6616" for this suite. @ 01/11/24 11:20:52.939
• [62.434 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 01/11/24 11:20:52.957
  Jan 11 11:20:52.957: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 11:20:52.959
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:20:53.009
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:20:53.018
  STEP: Creating a pod to test downward API volume plugin @ 01/11/24 11:20:53.024
  STEP: Saw pod success @ 01/11/24 11:20:57.165
  Jan 11 11:20:57.227: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-2702b5a4-cdd9-4dc8-8dc7-9494d8f99220 container client-container: <nil>
  STEP: delete the pod @ 01/11/24 11:20:57.337
  Jan 11 11:20:57.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5562" for this suite. @ 01/11/24 11:20:57.402
• [4.466 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 01/11/24 11:20:57.422
  Jan 11 11:20:57.423: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 11:20:57.424
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:20:57.457
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:20:57.462
  STEP: Creating a pod to test downward API volume plugin @ 01/11/24 11:20:57.467
  STEP: Saw pod success @ 01/11/24 11:21:01.526
  Jan 11 11:21:01.536: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-4fcbe561-eef3-4957-9303-673b86a574f5 container client-container: <nil>
  STEP: delete the pod @ 01/11/24 11:21:01.555
  Jan 11 11:21:01.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3074" for this suite. @ 01/11/24 11:21:01.625
• [4.234 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 01/11/24 11:21:01.657
  Jan 11 11:21:01.658: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename configmap @ 01/11/24 11:21:01.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:21:01.695
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:21:01.702
  STEP: Creating configMap with name configmap-test-volume-b49c8b76-ec78-4b93-a42c-379941d506e4 @ 01/11/24 11:21:01.708
  STEP: Creating a pod to test consume configMaps @ 01/11/24 11:21:01.723
  STEP: Saw pod success @ 01/11/24 11:21:05.798
  Jan 11 11:21:05.805: INFO: Trying to get logs from node env1-test-worker-1 pod pod-configmaps-74d47148-1d5e-40ea-b19c-7128e148f5db container agnhost-container: <nil>
  STEP: delete the pod @ 01/11/24 11:21:05.822
  Jan 11 11:21:05.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9269" for this suite. @ 01/11/24 11:21:05.869
• [4.225 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 01/11/24 11:21:05.886
  Jan 11 11:21:05.886: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename downward-api @ 01/11/24 11:21:05.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:21:05.938
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:21:05.943
  STEP: Creating a pod to test downward API volume plugin @ 01/11/24 11:21:05.948
  STEP: Saw pod success @ 01/11/24 11:21:10.02
  Jan 11 11:21:10.034: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-ae147a09-ff3e-4397-ac5f-3a286d527072 container client-container: <nil>
  STEP: delete the pod @ 01/11/24 11:21:10.053
  Jan 11 11:21:10.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7396" for this suite. @ 01/11/24 11:21:10.101
• [4.238 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 01/11/24 11:21:10.129
  Jan 11 11:21:10.129: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 11:21:10.131
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:21:10.193
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:21:10.2
  STEP: Creating a pod to test downward API volume plugin @ 01/11/24 11:21:10.207
  STEP: Saw pod success @ 01/11/24 11:21:14.267
  Jan 11 11:21:14.277: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-cad08a61-7f86-4dd4-a1b5-c843402b23c8 container client-container: <nil>
  STEP: delete the pod @ 01/11/24 11:21:14.309
  Jan 11 11:21:14.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6342" for this suite. @ 01/11/24 11:21:14.379
• [4.268 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 01/11/24 11:21:14.398
  Jan 11 11:21:14.398: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename replicaset @ 01/11/24 11:21:14.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:21:14.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:21:14.453
  STEP: Create a ReplicaSet @ 01/11/24 11:21:14.461
  STEP: Verify that the required pods have come up @ 01/11/24 11:21:14.476
  Jan 11 11:21:14.491: INFO: Pod name sample-pod: Found 0 pods out of 3
  Jan 11 11:21:19.509: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 01/11/24 11:21:19.509
  Jan 11 11:21:19.517: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 01/11/24 11:21:19.517
  STEP: DeleteCollection of the ReplicaSets @ 01/11/24 11:21:19.528
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 01/11/24 11:21:19.556
  Jan 11 11:21:19.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9106" for this suite. @ 01/11/24 11:21:19.596
• [5.222 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 01/11/24 11:21:19.626
  Jan 11 11:21:19.626: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 11:21:19.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:21:19.728
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:21:19.738
  STEP: Creating projection with secret that has name projected-secret-test-map-25c8422f-d7cf-4f28-991d-dbcf7dc1116f @ 01/11/24 11:21:19.747
  STEP: Creating a pod to test consume secrets @ 01/11/24 11:21:19.765
  STEP: Saw pod success @ 01/11/24 11:21:23.86
  Jan 11 11:21:23.871: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-secrets-1d56178e-bde6-4938-9381-984169097393 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 01/11/24 11:21:23.891
  Jan 11 11:21:23.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-160" for this suite. @ 01/11/24 11:21:23.943
• [4.337 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 01/11/24 11:21:23.971
  Jan 11 11:21:23.972: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubelet-test @ 01/11/24 11:21:23.973
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:21:24.027
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:21:24.04
  Jan 11 11:21:28.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1721" for this suite. @ 01/11/24 11:21:28.113
• [4.157 seconds]
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 01/11/24 11:21:28.13
  Jan 11 11:21:28.130: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename security-context-test @ 01/11/24 11:21:28.134
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:21:28.173
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:21:28.18
  Jan 11 11:21:32.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-626" for this suite. @ 01/11/24 11:21:32.254
• [4.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 01/11/24 11:21:32.277
  Jan 11 11:21:32.277: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename csiinlinevolumes @ 01/11/24 11:21:32.279
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:21:32.318
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:21:32.324
  STEP: creating @ 01/11/24 11:21:32.33
  STEP: getting @ 01/11/24 11:21:32.4
  STEP: listing in namespace @ 01/11/24 11:21:32.428
  STEP: patching @ 01/11/24 11:21:32.435
  STEP: deleting @ 01/11/24 11:21:32.451
  Jan 11 11:21:32.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-1366" for this suite. @ 01/11/24 11:21:32.513
• [0.277 seconds]
------------------------------
SSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 01/11/24 11:21:32.554
  Jan 11 11:21:32.554: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename downward-api @ 01/11/24 11:21:32.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:21:32.604
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:21:32.61
  STEP: Creating a pod to test downward api env vars @ 01/11/24 11:21:32.616
  STEP: Saw pod success @ 01/11/24 11:21:36.68
  Jan 11 11:21:36.693: INFO: Trying to get logs from node env1-test-worker-2 pod downward-api-0a510610-e790-48ae-873f-d9541a68b932 container dapi-container: <nil>
  STEP: delete the pod @ 01/11/24 11:21:36.752
  Jan 11 11:21:36.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9323" for this suite. @ 01/11/24 11:21:36.814
• [4.281 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 01/11/24 11:21:36.845
  Jan 11 11:21:36.845: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename watch @ 01/11/24 11:21:36.847
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:21:36.92
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:21:36.931
  STEP: getting a starting resourceVersion @ 01/11/24 11:21:36.945
  STEP: starting a background goroutine to produce watch events @ 01/11/24 11:21:36.957
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 01/11/24 11:21:36.957
  Jan 11 11:21:39.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-336" for this suite. @ 01/11/24 11:21:39.716
• [2.963 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 01/11/24 11:21:39.809
  Jan 11 11:21:39.809: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename webhook @ 01/11/24 11:21:39.81
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:21:39.937
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:21:39.943
  STEP: Setting up server cert @ 01/11/24 11:21:40.015
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/11/24 11:21:40.855
  STEP: Deploying the webhook pod @ 01/11/24 11:21:40.88
  STEP: Wait for the deployment to be ready @ 01/11/24 11:21:40.923
  Jan 11 11:21:40.949: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  Jan 11 11:21:42.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 21, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 21, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 21, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 21, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:21:45.007: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 21, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 21, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 21, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 21, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:21:47.004: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 21, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 21, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 21, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 21, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:21:49.004: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 21, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 21, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 21, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 21, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:21:51.010: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 21, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 21, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 21, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 21, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 01/11/24 11:21:53.008
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 11:21:53.049
  Jan 11 11:21:54.050: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 01/11/24 11:21:54.06
  STEP: Registering slow webhook via the AdmissionRegistration API @ 01/11/24 11:21:54.06
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 01/11/24 11:21:54.097
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 01/11/24 11:21:55.121
  STEP: Registering slow webhook via the AdmissionRegistration API @ 01/11/24 11:21:55.121
  STEP: Having no error when timeout is longer than webhook latency @ 01/11/24 11:21:56.19
  STEP: Registering slow webhook via the AdmissionRegistration API @ 01/11/24 11:21:56.19
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 01/11/24 11:22:01.292
  STEP: Registering slow webhook via the AdmissionRegistration API @ 01/11/24 11:22:01.292
  Jan 11 11:22:06.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3151" for this suite. @ 01/11/24 11:22:06.799
  STEP: Destroying namespace "webhook-markers-489" for this suite. @ 01/11/24 11:22:06.829
• [27.050 seconds]
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 01/11/24 11:22:06.859
  Jan 11 11:22:06.859: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename svcaccounts @ 01/11/24 11:22:06.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:22:06.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:22:06.933
  STEP: creating a ServiceAccount @ 01/11/24 11:22:06.942
  STEP: watching for the ServiceAccount to be added @ 01/11/24 11:22:06.961
  STEP: patching the ServiceAccount @ 01/11/24 11:22:06.986
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 01/11/24 11:22:07.024
  STEP: deleting the ServiceAccount @ 01/11/24 11:22:07.037
  Jan 11 11:22:07.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1936" for this suite. @ 01/11/24 11:22:07.102
• [0.260 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 01/11/24 11:22:07.127
  Jan 11 11:22:07.127: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename resourcequota @ 01/11/24 11:22:07.13
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:22:07.173
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:22:07.18
  STEP: Counting existing ResourceQuota @ 01/11/24 11:22:07.186
  STEP: Creating a ResourceQuota @ 01/11/24 11:22:12.209
  STEP: Ensuring resource quota status is calculated @ 01/11/24 11:22:12.227
  STEP: Creating a Pod that fits quota @ 01/11/24 11:22:14.237
  STEP: Ensuring ResourceQuota status captures the pod usage @ 01/11/24 11:22:14.283
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 01/11/24 11:22:16.295
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 01/11/24 11:22:16.306
  STEP: Ensuring a pod cannot update its resource requirements @ 01/11/24 11:22:16.315
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 01/11/24 11:22:16.334
  STEP: Deleting the pod @ 01/11/24 11:22:18.354
  STEP: Ensuring resource quota status released the pod usage @ 01/11/24 11:22:18.402
  Jan 11 11:22:20.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7650" for this suite. @ 01/11/24 11:22:20.434
• [13.325 seconds]
------------------------------
S
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 01/11/24 11:22:20.453
  Jan 11 11:22:20.453: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename security-context @ 01/11/24 11:22:20.455
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:22:20.518
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:22:20.546
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 01/11/24 11:22:20.553
  STEP: Saw pod success @ 01/11/24 11:22:24.617
  Jan 11 11:22:24.627: INFO: Trying to get logs from node env1-test-worker-2 pod security-context-e475ed1f-b426-4f5d-a2b1-424386e6dca9 container test-container: <nil>
  STEP: delete the pod @ 01/11/24 11:22:24.645
  Jan 11 11:22:24.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-2538" for this suite. @ 01/11/24 11:22:24.708
• [4.271 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 01/11/24 11:22:24.732
  Jan 11 11:22:24.732: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename svcaccounts @ 01/11/24 11:22:24.734
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:22:24.782
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:22:24.79
  STEP: reading a file in the container @ 01/11/24 11:22:26.863
  Jan 11 11:22:26.863: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7080 pod-service-account-63189ab4-7a19-471b-9512-018fcc5b0147 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 01/11/24 11:22:27.247
  Jan 11 11:22:27.247: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7080 pod-service-account-63189ab4-7a19-471b-9512-018fcc5b0147 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 01/11/24 11:22:27.576
  Jan 11 11:22:27.577: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7080 pod-service-account-63189ab4-7a19-471b-9512-018fcc5b0147 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Jan 11 11:22:27.904: INFO: Got root ca configmap in namespace "svcaccounts-7080"
  Jan 11 11:22:27.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7080" for this suite. @ 01/11/24 11:22:27.922
• [3.204 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 01/11/24 11:22:27.941
  Jan 11 11:22:27.941: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename services @ 01/11/24 11:22:27.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:22:27.984
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:22:27.991
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-3020 @ 01/11/24 11:22:27.999
  STEP: changing the ExternalName service to type=ClusterIP @ 01/11/24 11:22:28.023
  STEP: creating replication controller externalname-service in namespace services-3020 @ 01/11/24 11:22:28.087
  I0111 11:22:28.118077      23 runners.go:194] Created replication controller with name: externalname-service, namespace: services-3020, replica count: 2
  I0111 11:22:31.170418      23 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 11 11:22:31.170: INFO: Creating new exec pod
  Jan 11 11:22:34.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-3020 exec execpodxq8sd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jan 11 11:22:34.625: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jan 11 11:22:34.626: INFO: stdout: ""
  Jan 11 11:22:35.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-3020 exec execpodxq8sd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jan 11 11:22:35.963: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jan 11 11:22:35.963: INFO: stdout: "externalname-service-7bmrj"
  Jan 11 11:22:35.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-3020 exec execpodxq8sd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.57.138 80'
  Jan 11 11:22:36.286: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.57.138 80\nConnection to 10.233.57.138 80 port [tcp/http] succeeded!\n"
  Jan 11 11:22:36.286: INFO: stdout: "externalname-service-crf58"
  Jan 11 11:22:36.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 11 11:22:36.302: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-3020" for this suite. @ 01/11/24 11:22:36.391
• [8.478 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 01/11/24 11:22:36.421
  Jan 11 11:22:36.421: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename replication-controller @ 01/11/24 11:22:36.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:22:36.48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:22:36.492
  Jan 11 11:22:36.501: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 01/11/24 11:22:37.558
  STEP: Checking rc "condition-test" has the desired failure condition set @ 01/11/24 11:22:37.584
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 01/11/24 11:22:38.611
  Jan 11 11:22:38.639: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 01/11/24 11:22:38.639
  Jan 11 11:22:39.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2127" for this suite. @ 01/11/24 11:22:39.71
• [3.307 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 01/11/24 11:22:39.728
  Jan 11 11:22:39.728: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename resourcequota @ 01/11/24 11:22:39.731
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:22:39.817
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:22:39.829
  STEP: Creating a ResourceQuota @ 01/11/24 11:22:39.839
  STEP: Getting a ResourceQuota @ 01/11/24 11:22:39.863
  STEP: Updating a ResourceQuota @ 01/11/24 11:22:39.874
  STEP: Verifying a ResourceQuota was modified @ 01/11/24 11:22:39.904
  STEP: Deleting a ResourceQuota @ 01/11/24 11:22:39.916
  STEP: Verifying the deleted ResourceQuota @ 01/11/24 11:22:39.936
  Jan 11 11:22:39.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2935" for this suite. @ 01/11/24 11:22:39.979
• [0.276 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 01/11/24 11:22:40.006
  Jan 11 11:22:40.006: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename custom-resource-definition @ 01/11/24 11:22:40.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:22:40.063
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:22:40.085
  STEP: fetching the /apis discovery document @ 01/11/24 11:22:40.097
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 01/11/24 11:22:40.1
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 01/11/24 11:22:40.1
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 01/11/24 11:22:40.1
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 01/11/24 11:22:40.103
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 01/11/24 11:22:40.103
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 01/11/24 11:22:40.105
  Jan 11 11:22:40.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-53" for this suite. @ 01/11/24 11:22:40.119
• [0.147 seconds]
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 01/11/24 11:22:40.153
  Jan 11 11:22:40.153: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename namespaces @ 01/11/24 11:22:40.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:22:40.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:22:40.221
  STEP: Creating a test namespace @ 01/11/24 11:22:40.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:22:40.282
  STEP: Creating a pod in the namespace @ 01/11/24 11:22:40.293
  STEP: Waiting for the pod to have running status @ 01/11/24 11:22:40.325
  STEP: Deleting the namespace @ 01/11/24 11:22:42.353
  STEP: Waiting for the namespace to be removed. @ 01/11/24 11:22:42.371
  STEP: Recreating the namespace @ 01/11/24 11:22:55.381
  STEP: Verifying there are no pods in the namespace @ 01/11/24 11:22:55.433
  Jan 11 11:22:55.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3899" for this suite. @ 01/11/24 11:22:55.461
  STEP: Destroying namespace "nsdeletetest-1462" for this suite. @ 01/11/24 11:22:55.481
  Jan 11 11:22:55.493: INFO: Namespace nsdeletetest-1462 was already deleted
  STEP: Destroying namespace "nsdeletetest-4877" for this suite. @ 01/11/24 11:22:55.493
• [15.357 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 01/11/24 11:22:55.512
  Jan 11 11:22:55.512: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename statefulset @ 01/11/24 11:22:55.514
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:22:55.582
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:22:55.593
  STEP: Creating service test in namespace statefulset-970 @ 01/11/24 11:22:55.6
  STEP: Looking for a node to schedule stateful set and pod @ 01/11/24 11:22:55.614
  STEP: Creating pod with conflicting port in namespace statefulset-970 @ 01/11/24 11:22:55.635
  STEP: Waiting until pod test-pod will start running in namespace statefulset-970 @ 01/11/24 11:22:55.662
  STEP: Creating statefulset with conflicting port in namespace statefulset-970 @ 01/11/24 11:22:57.703
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-970 @ 01/11/24 11:22:57.721
  Jan 11 11:22:57.783: INFO: Observed stateful pod in namespace: statefulset-970, name: ss-0, uid: ab099195-30d1-48d6-aae7-ce5f6394e385, status phase: Pending. Waiting for statefulset controller to delete.
  Jan 11 11:22:57.821: INFO: Observed stateful pod in namespace: statefulset-970, name: ss-0, uid: ab099195-30d1-48d6-aae7-ce5f6394e385, status phase: Failed. Waiting for statefulset controller to delete.
  Jan 11 11:22:57.864: INFO: Observed stateful pod in namespace: statefulset-970, name: ss-0, uid: ab099195-30d1-48d6-aae7-ce5f6394e385, status phase: Failed. Waiting for statefulset controller to delete.
  Jan 11 11:22:57.876: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-970
  STEP: Removing pod with conflicting port in namespace statefulset-970 @ 01/11/24 11:22:57.876
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-970 and will be in running state @ 01/11/24 11:22:57.973
  Jan 11 11:23:00.005: INFO: Deleting all statefulset in ns statefulset-970
  Jan 11 11:23:00.014: INFO: Scaling statefulset ss to 0
  Jan 11 11:23:10.066: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 11 11:23:10.075: INFO: Deleting statefulset ss
  Jan 11 11:23:10.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-970" for this suite. @ 01/11/24 11:23:10.158
• [14.668 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 01/11/24 11:23:10.186
  Jan 11 11:23:10.186: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename webhook @ 01/11/24 11:23:10.189
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:23:10.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:23:10.264
  STEP: Setting up server cert @ 01/11/24 11:23:10.357
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/11/24 11:23:11.847
  STEP: Deploying the webhook pod @ 01/11/24 11:23:11.869
  STEP: Wait for the deployment to be ready @ 01/11/24 11:23:11.914
  Jan 11 11:23:11.952: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/11/24 11:23:13.983
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 11:23:14.004
  Jan 11 11:23:15.005: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jan 11 11:23:15.012: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4992-crds.webhook.example.com via the AdmissionRegistration API @ 01/11/24 11:23:20.557
  STEP: Creating a custom resource while v1 is storage version @ 01/11/24 11:23:20.599
  STEP: Patching Custom Resource Definition to set v2 as storage @ 01/11/24 11:23:22.731
  STEP: Patching the custom resource while v2 is storage version @ 01/11/24 11:23:22.76
  Jan 11 11:23:22.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2705" for this suite. @ 01/11/24 11:23:23.608
  STEP: Destroying namespace "webhook-markers-7453" for this suite. @ 01/11/24 11:23:23.621
• [13.452 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 01/11/24 11:23:23.638
  Jan 11 11:23:23.638: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename resourcequota @ 01/11/24 11:23:23.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:23:23.678
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:23:23.692
  STEP: Creating a ResourceQuota with terminating scope @ 01/11/24 11:23:23.703
  STEP: Ensuring ResourceQuota status is calculated @ 01/11/24 11:23:23.73
  STEP: Creating a ResourceQuota with not terminating scope @ 01/11/24 11:23:25.739
  STEP: Ensuring ResourceQuota status is calculated @ 01/11/24 11:23:25.764
  STEP: Creating a long running pod @ 01/11/24 11:23:27.773
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 01/11/24 11:23:27.813
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 01/11/24 11:23:29.829
  STEP: Deleting the pod @ 01/11/24 11:23:31.843
  STEP: Ensuring resource quota status released the pod usage @ 01/11/24 11:23:31.885
  STEP: Creating a terminating pod @ 01/11/24 11:23:33.898
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 01/11/24 11:23:33.92
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 01/11/24 11:23:35.93
  STEP: Deleting the pod @ 01/11/24 11:23:37.943
  STEP: Ensuring resource quota status released the pod usage @ 01/11/24 11:23:37.985
  Jan 11 11:23:39.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1989" for this suite. @ 01/11/24 11:23:40.007
• [16.394 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 01/11/24 11:23:40.043
  Jan 11 11:23:40.043: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename crd-webhook @ 01/11/24 11:23:40.045
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:23:40.081
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:23:40.089
  STEP: Setting up server cert @ 01/11/24 11:23:40.094
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 01/11/24 11:23:40.705
  STEP: Deploying the custom resource conversion webhook pod @ 01/11/24 11:23:40.72
  STEP: Wait for the deployment to be ready @ 01/11/24 11:23:40.756
  Jan 11 11:23:40.784: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/11/24 11:23:42.821
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 11:23:42.862
  Jan 11 11:23:43.863: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jan 11 11:23:43.874: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Creating a v1 custom resource @ 01/11/24 11:23:51.636
  STEP: Create a v2 custom resource @ 01/11/24 11:23:51.696
  STEP: List CRs in v1 @ 01/11/24 11:23:51.712
  STEP: List CRs in v2 @ 01/11/24 11:23:51.738
  Jan 11 11:23:51.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-9843" for this suite. @ 01/11/24 11:23:52.644
• [12.629 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 01/11/24 11:23:52.677
  Jan 11 11:23:52.677: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubectl @ 01/11/24 11:23:52.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:23:52.805
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:23:52.818
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 01/11/24 11:23:52.833
  Jan 11 11:23:52.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-6266 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Jan 11 11:23:53.049: INFO: stderr: ""
  Jan 11 11:23:53.050: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 01/11/24 11:23:53.05
  Jan 11 11:23:53.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-6266 delete pods e2e-test-httpd-pod'
  Jan 11 11:23:55.483: INFO: stderr: ""
  Jan 11 11:23:55.483: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jan 11 11:23:55.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6266" for this suite. @ 01/11/24 11:23:55.499
• [2.847 seconds]
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 01/11/24 11:23:55.524
  Jan 11 11:23:55.524: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename secrets @ 01/11/24 11:23:55.526
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:23:55.585
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:23:55.591
  Jan 11 11:23:55.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8091" for this suite. @ 01/11/24 11:23:55.759
• [0.258 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 01/11/24 11:23:55.785
  Jan 11 11:23:55.785: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-runtime @ 01/11/24 11:23:55.786
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:23:55.84
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:23:55.849
  STEP: create the container @ 01/11/24 11:23:55.856
  W0111 11:23:55.881821      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 01/11/24 11:23:55.882
  STEP: get the container status @ 01/11/24 11:23:58.929
  STEP: the container should be terminated @ 01/11/24 11:23:58.945
  STEP: the termination message should be set @ 01/11/24 11:23:58.946
  Jan 11 11:23:58.946: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 01/11/24 11:23:58.946
  Jan 11 11:23:58.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5138" for this suite. @ 01/11/24 11:23:59.01
• [3.248 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 01/11/24 11:23:59.035
  Jan 11 11:23:59.035: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename resourcequota @ 01/11/24 11:23:59.036
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:23:59.078
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:23:59.085
  STEP: Discovering how many secrets are in namespace by default @ 01/11/24 11:23:59.09
  STEP: Counting existing ResourceQuota @ 01/11/24 11:24:04.1
  STEP: Creating a ResourceQuota @ 01/11/24 11:24:09.112
  STEP: Ensuring resource quota status is calculated @ 01/11/24 11:24:09.146
  STEP: Creating a Secret @ 01/11/24 11:24:11.159
  STEP: Ensuring resource quota status captures secret creation @ 01/11/24 11:24:11.19
  STEP: Deleting a secret @ 01/11/24 11:24:13.202
  STEP: Ensuring resource quota status released usage @ 01/11/24 11:24:13.221
  Jan 11 11:24:15.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-326" for this suite. @ 01/11/24 11:24:15.25
• [16.235 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 01/11/24 11:24:15.273
  Jan 11 11:24:15.273: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 11:24:15.274
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:24:15.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:24:15.332
  STEP: Creating the pod @ 01/11/24 11:24:15.342
  Jan 11 11:24:17.995: INFO: Successfully updated pod "labelsupdatea200481b-38b9-45ca-81e8-45e899a4c483"
  Jan 11 11:24:20.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6332" for this suite. @ 01/11/24 11:24:20.097
• [4.849 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 01/11/24 11:24:20.133
  Jan 11 11:24:20.133: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename configmap @ 01/11/24 11:24:20.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:24:20.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:24:20.205
  STEP: Creating configMap with name configmap-test-upd-ebdd77c1-c0f7-4064-a58f-a71fd11ec3ce @ 01/11/24 11:24:20.227
  STEP: Creating the pod @ 01/11/24 11:24:20.236
  STEP: Waiting for pod with text data @ 01/11/24 11:24:22.279
  STEP: Waiting for pod with binary data @ 01/11/24 11:24:22.321
  Jan 11 11:24:22.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1924" for this suite. @ 01/11/24 11:24:22.352
• [2.241 seconds]
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 01/11/24 11:24:22.374
  Jan 11 11:24:22.374: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename var-expansion @ 01/11/24 11:24:22.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:24:22.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:24:22.425
  STEP: Creating a pod to test substitution in container's command @ 01/11/24 11:24:22.432
  STEP: Saw pod success @ 01/11/24 11:24:26.491
  Jan 11 11:24:26.501: INFO: Trying to get logs from node env1-test-worker-1 pod var-expansion-4e0ca6e5-2955-4f1b-8a0b-a3743bb6fa4a container dapi-container: <nil>
  STEP: delete the pod @ 01/11/24 11:24:26.52
  Jan 11 11:24:26.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4949" for this suite. @ 01/11/24 11:24:26.583
• [4.229 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 01/11/24 11:24:26.605
  Jan 11 11:24:26.605: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-runtime @ 01/11/24 11:24:26.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:24:26.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:24:26.691
  STEP: create the container @ 01/11/24 11:24:26.705
  W0111 11:24:26.752575      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 01/11/24 11:24:26.752
  STEP: get the container status @ 01/11/24 11:24:30.814
  STEP: the container should be terminated @ 01/11/24 11:24:30.825
  STEP: the termination message should be set @ 01/11/24 11:24:30.825
  Jan 11 11:24:30.825: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 01/11/24 11:24:30.825
  Jan 11 11:24:30.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5135" for this suite. @ 01/11/24 11:24:30.916
• [4.343 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 01/11/24 11:24:30.95
  Jan 11 11:24:30.950: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename deployment @ 01/11/24 11:24:30.956
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:24:31.006
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:24:31.013
  Jan 11 11:24:31.050: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  Jan 11 11:24:36.065: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 01/11/24 11:24:36.065
  Jan 11 11:24:36.065: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 01/11/24 11:24:36.099
  Jan 11 11:24:36.140: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7720  9cfef078-48fc-459f-bc05-9b1400a50e8f 187150611 1 2024-01-11 11:24:36 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2024-01-11 11:24:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007439a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Jan 11 11:24:36.168: INFO: New ReplicaSet "test-cleanup-deployment-68b75d69f8" of Deployment "test-cleanup-deployment":
  &ReplicaSet{ObjectMeta:{test-cleanup-deployment-68b75d69f8  deployment-7720  34a40fda-90d8-478e-b230-5a8f3a473c76 187150613 1 2024-01-11 11:24:36 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 9cfef078-48fc-459f-bc05-9b1400a50e8f 0xc006de8857 0xc006de8858}] [] [{kube-controller-manager Update apps/v1 2024-01-11 11:24:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9cfef078-48fc-459f-bc05-9b1400a50e8f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 68b75d69f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006de88e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 11 11:24:36.169: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Jan 11 11:24:36.170: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7720  2e2dff67-2845-446b-b7be-8ccfd3e1df49 187150612 1 2024-01-11 11:24:31 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 9cfef078-48fc-459f-bc05-9b1400a50e8f 0xc006de8727 0xc006de8728}] [] [{e2e.test Update apps/v1 2024-01-11 11:24:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-11 11:24:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2024-01-11 11:24:36 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"9cfef078-48fc-459f-bc05-9b1400a50e8f\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006de87e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jan 11 11:24:36.188: INFO: Pod "test-cleanup-controller-bnl2r" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-bnl2r test-cleanup-controller- deployment-7720  4c1414d4-05de-4840-8c5c-be097ae93ab0 187150585 0 2024-01-11 11:24:31 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 2e2dff67-2845-446b-b7be-8ccfd3e1df49 0xc0050fdd77 0xc0050fdd78}] [] [{kube-controller-manager Update v1 2024-01-11 11:24:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e2dff67-2845-446b-b7be-8ccfd3e1df49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 11:24:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zcgcb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zcgcb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:24:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:24:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:24:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:24:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:10.233.68.51,StartTime:2024-01-11 11:24:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-11 11:24:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://df79bae56e74b8b15f6deb37160a3ef7af59a3d5aeea41a0199dbb88516aecb7,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.51,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 11:24:36.190: INFO: Pod "test-cleanup-deployment-68b75d69f8-mk4k5" is not available:
  &Pod{ObjectMeta:{test-cleanup-deployment-68b75d69f8-mk4k5 test-cleanup-deployment-68b75d69f8- deployment-7720  7c9eab4b-6a69-40f3-b633-93161b1e9f1c 187150614 0 2024-01-11 11:24:36 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-68b75d69f8 34a40fda-90d8-478e-b230-5a8f3a473c76 0xc0050fdf67 0xc0050fdf68}] [] [{kube-controller-manager Update v1 2024-01-11 11:24:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"34a40fda-90d8-478e-b230-5a8f3a473c76\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4qfvz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4qfvz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 11:24:36.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7720" for this suite. @ 01/11/24 11:24:36.229
• [5.312 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 01/11/24 11:24:36.263
  Jan 11 11:24:36.263: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubelet-test @ 01/11/24 11:24:36.265
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:24:36.321
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:24:36.331
  Jan 11 11:24:38.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-9345" for this suite. @ 01/11/24 11:24:38.445
• [2.203 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 01/11/24 11:24:38.473
  Jan 11 11:24:38.473: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename events @ 01/11/24 11:24:38.475
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:24:38.541
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:24:38.549
  STEP: creating a test event @ 01/11/24 11:24:38.556
  STEP: listing events in all namespaces @ 01/11/24 11:24:38.589
  STEP: listing events in test namespace @ 01/11/24 11:24:38.625
  STEP: listing events with field selection filtering on source @ 01/11/24 11:24:38.633
  STEP: listing events with field selection filtering on reportingController @ 01/11/24 11:24:38.641
  STEP: getting the test event @ 01/11/24 11:24:38.649
  STEP: patching the test event @ 01/11/24 11:24:38.656
  STEP: getting the test event @ 01/11/24 11:24:38.673
  STEP: updating the test event @ 01/11/24 11:24:38.685
  STEP: getting the test event @ 01/11/24 11:24:38.711
  STEP: deleting the test event @ 01/11/24 11:24:38.721
  STEP: listing events in all namespaces @ 01/11/24 11:24:38.742
  STEP: listing events in test namespace @ 01/11/24 11:24:38.752
  Jan 11 11:24:38.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-700" for this suite. @ 01/11/24 11:24:38.778
• [0.327 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 01/11/24 11:24:38.809
  Jan 11 11:24:38.809: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename tables @ 01/11/24 11:24:38.812
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:24:38.855
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:24:38.862
  Jan 11 11:24:38.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-6235" for this suite. @ 01/11/24 11:24:38.896
• [0.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 01/11/24 11:24:38.924
  Jan 11 11:24:38.924: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename services @ 01/11/24 11:24:38.926
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:24:38.977
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:24:38.984
  STEP: creating service multi-endpoint-test in namespace services-2229 @ 01/11/24 11:24:38.99
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2229 to expose endpoints map[] @ 01/11/24 11:24:39.027
  Jan 11 11:24:39.039: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  Jan 11 11:24:40.056: INFO: successfully validated that service multi-endpoint-test in namespace services-2229 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-2229 @ 01/11/24 11:24:40.057
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2229 to expose endpoints map[pod1:[100]] @ 01/11/24 11:24:42.105
  Jan 11 11:24:42.145: INFO: successfully validated that service multi-endpoint-test in namespace services-2229 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-2229 @ 01/11/24 11:24:42.145
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2229 to expose endpoints map[pod1:[100] pod2:[101]] @ 01/11/24 11:24:44.2
  Jan 11 11:24:44.261: INFO: successfully validated that service multi-endpoint-test in namespace services-2229 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 01/11/24 11:24:44.261
  Jan 11 11:24:44.261: INFO: Creating new exec pod
  Jan 11 11:24:47.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2229 exec execpod5lxwf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Jan 11 11:24:47.705: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Jan 11 11:24:47.705: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 11 11:24:47.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2229 exec execpod5lxwf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.9.107 80'
  Jan 11 11:24:48.022: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.9.107 80\nConnection to 10.233.9.107 80 port [tcp/http] succeeded!\n"
  Jan 11 11:24:48.022: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 11 11:24:48.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2229 exec execpod5lxwf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Jan 11 11:24:48.329: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Jan 11 11:24:48.329: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 11 11:24:48.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2229 exec execpod5lxwf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.9.107 81'
  Jan 11 11:24:48.656: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.9.107 81\nConnection to 10.233.9.107 81 port [tcp/*] succeeded!\n"
  Jan 11 11:24:48.656: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-2229 @ 01/11/24 11:24:48.656
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2229 to expose endpoints map[pod2:[101]] @ 01/11/24 11:24:48.677
  Jan 11 11:24:48.720: INFO: successfully validated that service multi-endpoint-test in namespace services-2229 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-2229 @ 01/11/24 11:24:48.72
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2229 to expose endpoints map[] @ 01/11/24 11:24:48.76
  Jan 11 11:24:49.805: INFO: successfully validated that service multi-endpoint-test in namespace services-2229 exposes endpoints map[]
  Jan 11 11:24:49.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2229" for this suite. @ 01/11/24 11:24:49.883
• [10.986 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 01/11/24 11:24:49.914
  Jan 11 11:24:49.914: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename replication-controller @ 01/11/24 11:24:49.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:24:49.997
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:24:50.005
  STEP: Creating replication controller my-hostname-basic-8feb778e-2d6c-46cc-9510-9f970beb8deb @ 01/11/24 11:24:50.015
  Jan 11 11:24:50.068: INFO: Pod name my-hostname-basic-8feb778e-2d6c-46cc-9510-9f970beb8deb: Found 0 pods out of 1
  Jan 11 11:24:55.092: INFO: Pod name my-hostname-basic-8feb778e-2d6c-46cc-9510-9f970beb8deb: Found 1 pods out of 1
  Jan 11 11:24:55.092: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-8feb778e-2d6c-46cc-9510-9f970beb8deb" are running
  Jan 11 11:24:55.106: INFO: Pod "my-hostname-basic-8feb778e-2d6c-46cc-9510-9f970beb8deb-hjcqc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-11 11:24:50 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-11 11:24:51 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-11 11:24:51 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-11 11:24:50 +0000 UTC Reason: Message:}])
  Jan 11 11:24:55.106: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 01/11/24 11:24:55.106
  Jan 11 11:24:55.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4671" for this suite. @ 01/11/24 11:24:55.161
• [5.267 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 01/11/24 11:24:55.194
  Jan 11 11:24:55.195: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename services @ 01/11/24 11:24:55.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:24:55.252
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:24:55.261
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-2512 @ 01/11/24 11:24:55.268
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 01/11/24 11:24:55.327
  STEP: creating service externalsvc in namespace services-2512 @ 01/11/24 11:24:55.327
  STEP: creating replication controller externalsvc in namespace services-2512 @ 01/11/24 11:24:55.377
  I0111 11:24:55.411484      23 runners.go:194] Created replication controller with name: externalsvc, namespace: services-2512, replica count: 2
  I0111 11:24:58.462460      23 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 01/11/24 11:24:58.477
  Jan 11 11:24:58.563: INFO: Creating new exec pod
  Jan 11 11:25:00.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2512 exec execpod586wt -- /bin/sh -x -c nslookup nodeport-service.services-2512.svc.cluster.local'
  Jan 11 11:25:01.103: INFO: stderr: "+ nslookup nodeport-service.services-2512.svc.cluster.local\n"
  Jan 11 11:25:01.103: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-2512.svc.cluster.local\tcanonical name = externalsvc.services-2512.svc.cluster.local.\nName:\texternalsvc.services-2512.svc.cluster.local\nAddress: 10.233.59.178\n\n"
  Jan 11 11:25:01.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-2512, will wait for the garbage collector to delete the pods @ 01/11/24 11:25:01.15
  Jan 11 11:25:01.243: INFO: Deleting ReplicationController externalsvc took: 23.025835ms
  Jan 11 11:25:01.343: INFO: Terminating ReplicationController externalsvc pods took: 100.369989ms
  Jan 11 11:25:03.619: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-2512" for this suite. @ 01/11/24 11:25:03.657
• [8.485 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 01/11/24 11:25:03.682
  Jan 11 11:25:03.682: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename security-context-test @ 01/11/24 11:25:03.685
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:25:03.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:25:03.741
  Jan 11 11:25:07.823: INFO: Got logs for pod "busybox-privileged-false-b66f212e-60fc-4746-9202-d5637a2aee13": "ip: RTNETLINK answers: Operation not permitted\n"
  Jan 11 11:25:07.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-7040" for this suite. @ 01/11/24 11:25:07.837
• [4.177 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 01/11/24 11:25:07.867
  Jan 11 11:25:07.867: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 01/11/24 11:25:07.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:25:07.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:25:07.928
  STEP: Setting up the test @ 01/11/24 11:25:07.935
  STEP: Creating hostNetwork=false pod @ 01/11/24 11:25:07.935
  STEP: Creating hostNetwork=true pod @ 01/11/24 11:25:09.987
  STEP: Running the test @ 01/11/24 11:25:12.029
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 01/11/24 11:25:12.029
  Jan 11 11:25:12.029: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7210 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 11:25:12.029: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:25:12.030: INFO: ExecWithOptions: Clientset creation
  Jan 11 11:25:12.030: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7210/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jan 11 11:25:12.200: INFO: Exec stderr: ""
  Jan 11 11:25:12.200: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7210 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 11:25:12.200: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:25:12.201: INFO: ExecWithOptions: Clientset creation
  Jan 11 11:25:12.202: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7210/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jan 11 11:25:12.362: INFO: Exec stderr: ""
  Jan 11 11:25:12.362: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7210 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 11:25:12.362: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:25:12.363: INFO: ExecWithOptions: Clientset creation
  Jan 11 11:25:12.363: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7210/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jan 11 11:25:12.507: INFO: Exec stderr: ""
  Jan 11 11:25:12.507: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7210 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 11:25:12.507: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:25:12.509: INFO: ExecWithOptions: Clientset creation
  Jan 11 11:25:12.509: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7210/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jan 11 11:25:12.657: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 01/11/24 11:25:12.659
  Jan 11 11:25:12.659: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7210 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 11:25:12.659: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:25:12.660: INFO: ExecWithOptions: Clientset creation
  Jan 11 11:25:12.661: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7210/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jan 11 11:25:12.794: INFO: Exec stderr: ""
  Jan 11 11:25:12.794: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7210 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 11:25:12.794: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:25:12.797: INFO: ExecWithOptions: Clientset creation
  Jan 11 11:25:12.797: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7210/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jan 11 11:25:12.947: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 01/11/24 11:25:12.947
  Jan 11 11:25:12.947: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7210 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 11:25:12.947: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:25:12.949: INFO: ExecWithOptions: Clientset creation
  Jan 11 11:25:12.949: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7210/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jan 11 11:25:13.089: INFO: Exec stderr: ""
  Jan 11 11:25:13.089: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7210 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 11:25:13.089: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:25:13.090: INFO: ExecWithOptions: Clientset creation
  Jan 11 11:25:13.091: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7210/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jan 11 11:25:13.249: INFO: Exec stderr: ""
  Jan 11 11:25:13.249: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7210 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 11:25:13.250: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:25:13.251: INFO: ExecWithOptions: Clientset creation
  Jan 11 11:25:13.251: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7210/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jan 11 11:25:13.420: INFO: Exec stderr: ""
  Jan 11 11:25:13.421: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7210 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 11:25:13.421: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:25:13.422: INFO: ExecWithOptions: Clientset creation
  Jan 11 11:25:13.423: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7210/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jan 11 11:25:13.616: INFO: Exec stderr: ""
  Jan 11 11:25:13.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-7210" for this suite. @ 01/11/24 11:25:13.635
• [5.811 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 01/11/24 11:25:13.679
  Jan 11 11:25:13.679: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename emptydir @ 01/11/24 11:25:13.681
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:25:13.73
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:25:13.738
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 01/11/24 11:25:13.744
  STEP: Saw pod success @ 01/11/24 11:25:17.805
  Jan 11 11:25:17.818: INFO: Trying to get logs from node env1-test-worker-1 pod pod-6e93d231-5ace-483c-854f-d5ddfe9ee42e container test-container: <nil>
  STEP: delete the pod @ 01/11/24 11:25:17.835
  Jan 11 11:25:17.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-576" for this suite. @ 01/11/24 11:25:17.903
• [4.242 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 01/11/24 11:25:17.928
  Jan 11 11:25:17.929: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 11:25:17.93
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:25:17.972
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:25:17.981
  STEP: Creating projection with secret that has name projected-secret-test-c2343f8a-a20f-4f7b-962b-8302defadced @ 01/11/24 11:25:17.99
  STEP: Creating a pod to test consume secrets @ 01/11/24 11:25:18.008
  STEP: Saw pod success @ 01/11/24 11:25:22.085
  Jan 11 11:25:22.096: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-secrets-6fe136f2-b8db-49f8-aed8-ce92486306c0 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 01/11/24 11:25:22.118
  Jan 11 11:25:22.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4203" for this suite. @ 01/11/24 11:25:22.16
• [4.246 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 01/11/24 11:25:22.175
  Jan 11 11:25:22.175: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 11:25:22.177
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:25:22.211
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:25:22.217
  STEP: Creating the pod @ 01/11/24 11:25:22.223
  Jan 11 11:25:24.834: INFO: Successfully updated pod "annotationupdate93118f73-3674-4d0b-8cf6-3611ae97d64c"
  Jan 11 11:25:26.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3457" for this suite. @ 01/11/24 11:25:26.906
• [4.753 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 01/11/24 11:25:26.93
  Jan 11 11:25:26.930: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename field-validation @ 01/11/24 11:25:26.933
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:25:26.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:25:26.993
  STEP: apply creating a deployment @ 01/11/24 11:25:27.001
  Jan 11 11:25:27.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-380" for this suite. @ 01/11/24 11:25:27.062
• [0.149 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 01/11/24 11:25:27.082
  Jan 11 11:25:27.082: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename downward-api @ 01/11/24 11:25:27.084
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:25:27.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:25:27.137
  STEP: Creating a pod to test downward API volume plugin @ 01/11/24 11:25:27.145
  STEP: Saw pod success @ 01/11/24 11:25:31.199
  Jan 11 11:25:31.209: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-ee47ff26-edb6-42b3-92de-1320f94ac7ed container client-container: <nil>
  STEP: delete the pod @ 01/11/24 11:25:31.233
  Jan 11 11:25:31.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6791" for this suite. @ 01/11/24 11:25:31.315
• [4.257 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 01/11/24 11:25:31.343
  Jan 11 11:25:31.343: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename subpath @ 01/11/24 11:25:31.345
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:25:31.409
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:25:31.417
  STEP: Setting up data @ 01/11/24 11:25:31.425
  STEP: Creating pod pod-subpath-test-projected-mb6x @ 01/11/24 11:25:31.454
  STEP: Creating a pod to test atomic-volume-subpath @ 01/11/24 11:25:31.454
  STEP: Saw pod success @ 01/11/24 11:25:55.679
  Jan 11 11:25:55.688: INFO: Trying to get logs from node env1-test-worker-2 pod pod-subpath-test-projected-mb6x container test-container-subpath-projected-mb6x: <nil>
  STEP: delete the pod @ 01/11/24 11:25:55.713
  STEP: Deleting pod pod-subpath-test-projected-mb6x @ 01/11/24 11:25:55.756
  Jan 11 11:25:55.756: INFO: Deleting pod "pod-subpath-test-projected-mb6x" in namespace "subpath-2163"
  Jan 11 11:25:55.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-2163" for this suite. @ 01/11/24 11:25:55.787
• [24.463 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 01/11/24 11:25:55.811
  Jan 11 11:25:55.811: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename statefulset @ 01/11/24 11:25:55.813
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:25:55.857
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:25:55.863
  STEP: Creating service test in namespace statefulset-2952 @ 01/11/24 11:25:55.869
  STEP: Creating stateful set ss in namespace statefulset-2952 @ 01/11/24 11:25:55.888
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2952 @ 01/11/24 11:25:55.904
  Jan 11 11:25:55.918: INFO: Found 0 stateful pods, waiting for 1
  Jan 11 11:26:05.929: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 01/11/24 11:26:05.929
  Jan 11 11:26:05.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-2952 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 11 11:26:06.289: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 11 11:26:06.290: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 11 11:26:06.290: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 11 11:26:06.306: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Jan 11 11:26:16.329: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jan 11 11:26:16.329: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 11 11:26:16.395: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
  Jan 11 11:26:16.395: INFO: ss-0  env1-test-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:25:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:25:55 +0000 UTC  }]
  Jan 11 11:26:16.395: INFO: 
  Jan 11 11:26:16.395: INFO: StatefulSet ss has not reached scale 3, at 1
  Jan 11 11:26:17.409: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981476377s
  Jan 11 11:26:18.423: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.966089989s
  Jan 11 11:26:19.434: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.952925764s
  Jan 11 11:26:20.445: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.942592887s
  Jan 11 11:26:21.457: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.930847928s
  Jan 11 11:26:22.471: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.919616633s
  Jan 11 11:26:23.483: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.904695335s
  Jan 11 11:26:24.492: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.892764907s
  Jan 11 11:26:25.513: INFO: Verifying statefulset ss doesn't scale past 3 for another 883.950192ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2952 @ 01/11/24 11:26:26.514
  Jan 11 11:26:26.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-2952 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 11 11:26:26.877: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 11 11:26:26.877: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 11 11:26:26.877: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 11 11:26:26.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-2952 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 11 11:26:27.208: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jan 11 11:26:27.208: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 11 11:26:27.208: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 11 11:26:27.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-2952 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 11 11:26:27.532: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jan 11 11:26:27.532: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 11 11:26:27.532: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 11 11:26:27.542: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 11 11:26:27.542: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jan 11 11:26:27.542: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 01/11/24 11:26:27.542
  Jan 11 11:26:27.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-2952 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 11 11:26:27.820: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 11 11:26:27.820: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 11 11:26:27.820: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 11 11:26:27.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-2952 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 11 11:26:28.073: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 11 11:26:28.073: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 11 11:26:28.073: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 11 11:26:28.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-2952 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 11 11:26:28.362: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 11 11:26:28.362: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 11 11:26:28.362: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 11 11:26:28.362: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 11 11:26:28.371: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  Jan 11 11:26:38.389: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jan 11 11:26:38.389: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jan 11 11:26:38.389: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jan 11 11:26:38.429: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
  Jan 11 11:26:38.429: INFO: ss-0  env1-test-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:25:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:25:55 +0000 UTC  }]
  Jan 11 11:26:38.429: INFO: ss-1  env1-test-worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:16 +0000 UTC  }]
  Jan 11 11:26:38.429: INFO: ss-2  env1-test-worker-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:16 +0000 UTC  }]
  Jan 11 11:26:38.429: INFO: 
  Jan 11 11:26:38.429: INFO: StatefulSet ss has not reached scale 0, at 3
  Jan 11 11:26:39.449: INFO: POD   NODE                PHASE      GRACE  CONDITIONS
  Jan 11 11:26:39.450: INFO: ss-1  env1-test-worker-0  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:16 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:28 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:28 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:16 +0000 UTC  }]
  Jan 11 11:26:39.450: INFO: ss-2  env1-test-worker-2  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:16 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:28 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:28 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:26:16 +0000 UTC  }]
  Jan 11 11:26:39.450: INFO: 
  Jan 11 11:26:39.450: INFO: StatefulSet ss has not reached scale 0, at 2
  Jan 11 11:26:40.461: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.959870193s
  Jan 11 11:26:41.474: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.948400869s
  Jan 11 11:26:42.485: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.935762392s
  Jan 11 11:26:43.493: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.924571237s
  Jan 11 11:26:44.500: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.916944328s
  Jan 11 11:26:45.510: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.909055281s
  Jan 11 11:26:46.521: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.899647486s
  Jan 11 11:26:47.537: INFO: Verifying statefulset ss doesn't scale past 0 for another 888.785839ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2952 @ 01/11/24 11:26:48.537
  Jan 11 11:26:48.554: INFO: Scaling statefulset ss to 0
  Jan 11 11:26:48.603: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 11 11:26:48.612: INFO: Deleting all statefulset in ns statefulset-2952
  Jan 11 11:26:48.623: INFO: Scaling statefulset ss to 0
  Jan 11 11:26:48.655: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 11 11:26:48.663: INFO: Deleting statefulset ss
  Jan 11 11:26:48.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2952" for this suite. @ 01/11/24 11:26:48.751
• [52.957 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:622
  STEP: Creating a kubernetes client @ 01/11/24 11:26:48.771
  Jan 11 11:26:48.771: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename field-validation @ 01/11/24 11:26:48.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:26:48.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:26:48.822
  Jan 11 11:26:48.829: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  W0111 11:26:56.522683      23 warnings.go:70] unknown field "alpha"
  W0111 11:26:56.522747      23 warnings.go:70] unknown field "beta"
  W0111 11:26:56.522767      23 warnings.go:70] unknown field "delta"
  W0111 11:26:56.522785      23 warnings.go:70] unknown field "epsilon"
  W0111 11:26:56.522801      23 warnings.go:70] unknown field "gamma"
  Jan 11 11:26:57.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5574" for this suite. @ 01/11/24 11:26:57.262
• [8.548 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 01/11/24 11:26:57.321
  Jan 11 11:26:57.321: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename pods @ 01/11/24 11:26:57.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:26:57.382
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:26:57.387
  STEP: creating the pod @ 01/11/24 11:26:57.396
  STEP: submitting the pod to kubernetes @ 01/11/24 11:26:57.396
  STEP: verifying QOS class is set on the pod @ 01/11/24 11:26:57.449
  Jan 11 11:26:57.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3539" for this suite. @ 01/11/24 11:26:57.486
• [0.177 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 01/11/24 11:26:57.498
  Jan 11 11:26:57.498: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename var-expansion @ 01/11/24 11:26:57.5
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:26:57.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:26:57.561
  STEP: Creating a pod to test env composition @ 01/11/24 11:26:57.569
  STEP: Saw pod success @ 01/11/24 11:27:01.658
  Jan 11 11:27:01.668: INFO: Trying to get logs from node env1-test-worker-2 pod var-expansion-297abd61-c728-44d3-9928-71f7c2f7463c container dapi-container: <nil>
  STEP: delete the pod @ 01/11/24 11:27:01.692
  Jan 11 11:27:01.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9868" for this suite. @ 01/11/24 11:27:01.745
• [4.265 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 01/11/24 11:27:01.767
  Jan 11 11:27:01.767: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/11/24 11:27:01.769
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:27:01.816
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:27:01.824
  Jan 11 11:27:01.830: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 01/11/24 11:27:10.618
  Jan 11 11:27:10.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-1254 --namespace=crd-publish-openapi-1254 create -f -'
  Jan 11 11:27:13.524: INFO: stderr: ""
  Jan 11 11:27:13.524: INFO: stdout: "e2e-test-crd-publish-openapi-9934-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jan 11 11:27:13.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-1254 --namespace=crd-publish-openapi-1254 delete e2e-test-crd-publish-openapi-9934-crds test-cr'
  Jan 11 11:27:13.740: INFO: stderr: ""
  Jan 11 11:27:13.740: INFO: stdout: "e2e-test-crd-publish-openapi-9934-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Jan 11 11:27:13.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-1254 --namespace=crd-publish-openapi-1254 apply -f -'
  Jan 11 11:27:16.469: INFO: stderr: ""
  Jan 11 11:27:16.469: INFO: stdout: "e2e-test-crd-publish-openapi-9934-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jan 11 11:27:16.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-1254 --namespace=crd-publish-openapi-1254 delete e2e-test-crd-publish-openapi-9934-crds test-cr'
  Jan 11 11:27:16.673: INFO: stderr: ""
  Jan 11 11:27:16.673: INFO: stdout: "e2e-test-crd-publish-openapi-9934-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 01/11/24 11:27:16.673
  Jan 11 11:27:16.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-1254 explain e2e-test-crd-publish-openapi-9934-crds'
  Jan 11 11:27:19.463: INFO: stderr: ""
  Jan 11 11:27:19.463: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-9934-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  Jan 11 11:27:22.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1254" for this suite. @ 01/11/24 11:27:22.799
• [21.051 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 01/11/24 11:27:22.818
  Jan 11 11:27:22.818: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename replicaset @ 01/11/24 11:27:22.821
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:27:22.861
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:27:22.87
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 01/11/24 11:27:22.876
  Jan 11 11:27:22.904: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jan 11 11:27:27.916: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 01/11/24 11:27:27.916
  STEP: getting scale subresource @ 01/11/24 11:27:27.916
  STEP: updating a scale subresource @ 01/11/24 11:27:27.935
  STEP: verifying the replicaset Spec.Replicas was modified @ 01/11/24 11:27:27.953
  STEP: Patch a scale subresource @ 01/11/24 11:27:27.981
  Jan 11 11:27:28.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7517" for this suite. @ 01/11/24 11:27:28.091
• [5.323 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 01/11/24 11:27:28.142
  Jan 11 11:27:28.142: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-probe @ 01/11/24 11:27:28.145
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:27:28.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:27:28.226
  STEP: Creating pod test-grpc-bbb0ccf2-87b0-4953-bc70-75f11fc64b49 in namespace container-probe-7945 @ 01/11/24 11:27:28.233
  Jan 11 11:27:30.286: INFO: Started pod test-grpc-bbb0ccf2-87b0-4953-bc70-75f11fc64b49 in namespace container-probe-7945
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/11/24 11:27:30.287
  Jan 11 11:27:30.299: INFO: Initial restart count of pod test-grpc-bbb0ccf2-87b0-4953-bc70-75f11fc64b49 is 0
  Jan 11 11:31:31.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/11/24 11:31:31.839
  STEP: Destroying namespace "container-probe-7945" for this suite. @ 01/11/24 11:31:31.866
• [243.754 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 01/11/24 11:31:31.9
  Jan 11 11:31:31.901: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename runtimeclass @ 01/11/24 11:31:31.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:31:31.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:31:31.977
  Jan 11 11:31:34.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-4595" for this suite. @ 01/11/24 11:31:34.099
• [2.234 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 01/11/24 11:31:34.136
  Jan 11 11:31:34.136: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename watch @ 01/11/24 11:31:34.138
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:31:34.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:31:34.211
  STEP: creating a watch on configmaps with label A @ 01/11/24 11:31:34.224
  STEP: creating a watch on configmaps with label B @ 01/11/24 11:31:34.227
  STEP: creating a watch on configmaps with label A or B @ 01/11/24 11:31:34.231
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 01/11/24 11:31:34.237
  Jan 11 11:31:34.248: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9249  f80e1aec-f1e9-4e06-a134-801f6016f1b6 187153187 0 2024-01-11 11:31:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-11 11:31:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 11 11:31:34.248: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9249  f80e1aec-f1e9-4e06-a134-801f6016f1b6 187153187 0 2024-01-11 11:31:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-11 11:31:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 01/11/24 11:31:34.249
  Jan 11 11:31:34.269: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9249  f80e1aec-f1e9-4e06-a134-801f6016f1b6 187153188 0 2024-01-11 11:31:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-11 11:31:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 11 11:31:34.269: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9249  f80e1aec-f1e9-4e06-a134-801f6016f1b6 187153188 0 2024-01-11 11:31:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-11 11:31:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 01/11/24 11:31:34.269
  Jan 11 11:31:34.289: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9249  f80e1aec-f1e9-4e06-a134-801f6016f1b6 187153189 0 2024-01-11 11:31:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-11 11:31:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 11 11:31:34.289: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9249  f80e1aec-f1e9-4e06-a134-801f6016f1b6 187153189 0 2024-01-11 11:31:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-11 11:31:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 01/11/24 11:31:34.289
  Jan 11 11:31:34.307: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9249  f80e1aec-f1e9-4e06-a134-801f6016f1b6 187153190 0 2024-01-11 11:31:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-11 11:31:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 11 11:31:34.308: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9249  f80e1aec-f1e9-4e06-a134-801f6016f1b6 187153190 0 2024-01-11 11:31:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-11 11:31:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 01/11/24 11:31:34.308
  Jan 11 11:31:34.320: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9249  6dfdbea6-7e1c-4d9b-81c3-fd6e5c9bb5a8 187153191 0 2024-01-11 11:31:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-11 11:31:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 11 11:31:34.321: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9249  6dfdbea6-7e1c-4d9b-81c3-fd6e5c9bb5a8 187153191 0 2024-01-11 11:31:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-11 11:31:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 01/11/24 11:31:44.324
  Jan 11 11:31:44.344: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9249  6dfdbea6-7e1c-4d9b-81c3-fd6e5c9bb5a8 187153251 0 2024-01-11 11:31:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-11 11:31:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 11 11:31:44.345: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9249  6dfdbea6-7e1c-4d9b-81c3-fd6e5c9bb5a8 187153251 0 2024-01-11 11:31:34 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-11 11:31:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 11 11:31:54.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9249" for this suite. @ 01/11/24 11:31:54.365
• [20.249 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 01/11/24 11:31:54.387
  Jan 11 11:31:54.387: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename resourcequota @ 01/11/24 11:31:54.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:31:54.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:31:54.44
  STEP: Creating a ResourceQuota with best effort scope @ 01/11/24 11:31:54.445
  STEP: Ensuring ResourceQuota status is calculated @ 01/11/24 11:31:54.454
  STEP: Creating a ResourceQuota with not best effort scope @ 01/11/24 11:31:56.465
  STEP: Ensuring ResourceQuota status is calculated @ 01/11/24 11:31:56.487
  STEP: Creating a best-effort pod @ 01/11/24 11:31:58.496
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 01/11/24 11:31:58.537
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 01/11/24 11:32:00.554
  STEP: Deleting the pod @ 01/11/24 11:32:02.561
  STEP: Ensuring resource quota status released the pod usage @ 01/11/24 11:32:02.598
  STEP: Creating a not best-effort pod @ 01/11/24 11:32:04.613
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 01/11/24 11:32:04.646
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 01/11/24 11:32:06.654
  STEP: Deleting the pod @ 01/11/24 11:32:08.664
  STEP: Ensuring resource quota status released the pod usage @ 01/11/24 11:32:08.704
  Jan 11 11:32:10.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-651" for this suite. @ 01/11/24 11:32:10.731
• [16.370 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 01/11/24 11:32:10.759
  Jan 11 11:32:10.759: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename sched-pred @ 01/11/24 11:32:10.762
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:32:10.82
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:32:10.827
  Jan 11 11:32:10.835: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jan 11 11:32:10.872: INFO: Waiting for terminating namespaces to be deleted...
  Jan 11 11:32:10.884: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-0 before test
  Jan 11 11:32:10.934: INFO: kube-flannel-r8g5h from kube-system started at 2024-01-09 16:24:44 +0000 UTC (1 container statuses recorded)
  Jan 11 11:32:10.934: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 11 11:32:10.934: INFO: kube-proxy-c8shb from kube-system started at 2024-01-11 00:56:00 +0000 UTC (1 container statuses recorded)
  Jan 11 11:32:10.934: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 11 11:32:10.934: INFO: metrics-server-6b7574f5b-jmbtm from kube-system started at 2024-01-09 16:32:13 +0000 UTC (1 container statuses recorded)
  Jan 11 11:32:10.934: INFO: 	Container metrics-server ready: true, restart count 0
  Jan 11 11:32:10.934: INFO: nginx-proxy-env1-test-worker-0 from kube-system started at 2024-01-09 16:31:48 +0000 UTC (1 container statuses recorded)
  Jan 11 11:32:10.934: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 11 11:32:10.934: INFO: nodelocaldns-vkvkp from kube-system started at 2024-01-09 15:52:20 +0000 UTC (1 container statuses recorded)
  Jan 11 11:32:10.934: INFO: 	Container node-cache ready: true, restart count 0
  Jan 11 11:32:10.934: INFO: vsphere-csi-node-5gf9n from kube-system started at 2024-01-11 01:05:49 +0000 UTC (3 container statuses recorded)
  Jan 11 11:32:10.934: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 11 11:32:10.934: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 11 11:32:10.934: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 11 11:32:10.934: INFO: sonobuoy-systemd-logs-daemon-set-7b26ca18c17c40c9-cqtts from sonobuoy started at 2024-01-11 11:18:31 +0000 UTC (2 container statuses recorded)
  Jan 11 11:32:10.934: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 11 11:32:10.934: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 11 11:32:10.934: INFO: traefik-ingress-g4tjs from traefik-ingress started at 2024-01-10 14:39:42 +0000 UTC (1 container statuses recorded)
  Jan 11 11:32:10.934: INFO: 	Container traefik-ingress ready: true, restart count 0
  Jan 11 11:32:10.934: INFO: velero-794b84894f-nwdwf from velero started at 2024-01-10 13:26:26 +0000 UTC (1 container statuses recorded)
  Jan 11 11:32:10.934: INFO: 	Container velero ready: true, restart count 0
  Jan 11 11:32:10.934: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-1 before test
  Jan 11 11:32:10.965: INFO: kube-flannel-jxf5s from kube-system started at 2024-01-09 16:24:09 +0000 UTC (1 container statuses recorded)
  Jan 11 11:32:10.965: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 11 11:32:10.965: INFO: kube-proxy-sdfjc from kube-system started at 2024-01-11 00:56:00 +0000 UTC (1 container statuses recorded)
  Jan 11 11:32:10.965: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 11 11:32:10.965: INFO: nginx-proxy-env1-test-worker-1 from kube-system started at 2024-01-09 16:38:31 +0000 UTC (1 container statuses recorded)
  Jan 11 11:32:10.965: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 11 11:32:10.965: INFO: nodelocaldns-7qx4w from kube-system started at 2024-01-09 15:52:12 +0000 UTC (1 container statuses recorded)
  Jan 11 11:32:10.965: INFO: 	Container node-cache ready: true, restart count 0
  Jan 11 11:32:10.965: INFO: vsphere-csi-node-pwx5m from kube-system started at 2024-01-11 01:05:49 +0000 UTC (3 container statuses recorded)
  Jan 11 11:32:10.965: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 11 11:32:10.965: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 11 11:32:10.965: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 11 11:32:10.965: INFO: sonobuoy from sonobuoy started at 2024-01-11 11:18:29 +0000 UTC (1 container statuses recorded)
  Jan 11 11:32:10.965: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jan 11 11:32:10.965: INFO: sonobuoy-systemd-logs-daemon-set-7b26ca18c17c40c9-zn5jg from sonobuoy started at 2024-01-11 11:18:31 +0000 UTC (2 container statuses recorded)
  Jan 11 11:32:10.965: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 11 11:32:10.966: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 11 11:32:10.966: INFO: traefik-ingress-kdxzt from traefik-ingress started at 2024-01-11 02:25:50 +0000 UTC (1 container statuses recorded)
  Jan 11 11:32:10.966: INFO: 	Container traefik-ingress ready: true, restart count 0
  Jan 11 11:32:10.966: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-2 before test
  Jan 11 11:32:11.001: INFO: kube-flannel-hlhld from kube-system started at 2024-01-11 00:55:50 +0000 UTC (1 container statuses recorded)
  Jan 11 11:32:11.001: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 11 11:32:11.001: INFO: kube-proxy-s78x8 from kube-system started at 2024-01-11 00:56:00 +0000 UTC (1 container statuses recorded)
  Jan 11 11:32:11.001: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 11 11:32:11.001: INFO: nginx-proxy-env1-test-worker-2 from kube-system started at 2024-01-11 01:03:57 +0000 UTC (1 container statuses recorded)
  Jan 11 11:32:11.001: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 11 11:32:11.001: INFO: nodelocaldns-49gbl from kube-system started at 2024-01-11 00:55:50 +0000 UTC (1 container statuses recorded)
  Jan 11 11:32:11.001: INFO: 	Container node-cache ready: true, restart count 0
  Jan 11 11:32:11.001: INFO: vsphere-csi-node-75mjp from kube-system started at 2024-01-11 01:05:49 +0000 UTC (3 container statuses recorded)
  Jan 11 11:32:11.001: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 11 11:32:11.001: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 11 11:32:11.001: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 11 11:32:11.001: INFO: sonobuoy-e2e-job-4f4795256cf94e3a from sonobuoy started at 2024-01-11 11:18:30 +0000 UTC (2 container statuses recorded)
  Jan 11 11:32:11.001: INFO: 	Container e2e ready: true, restart count 0
  Jan 11 11:32:11.001: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 11 11:32:11.001: INFO: sonobuoy-systemd-logs-daemon-set-7b26ca18c17c40c9-7pxz5 from sonobuoy started at 2024-01-11 11:18:31 +0000 UTC (2 container statuses recorded)
  Jan 11 11:32:11.002: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 11 11:32:11.002: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 11 11:32:11.002: INFO: traefik-ingress-mplg7 from traefik-ingress started at 2024-01-11 09:01:54 +0000 UTC (1 container statuses recorded)
  Jan 11 11:32:11.002: INFO: 	Container traefik-ingress ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 01/11/24 11:32:11.002
  STEP: Explicitly delete pod here to free the resource it takes. @ 01/11/24 11:32:13.058
  STEP: Trying to apply a random label on the found node. @ 01/11/24 11:32:13.088
  STEP: verifying the node has the label kubernetes.io/e2e-e12ecab3-27fc-4d1e-8dc1-a63f98dde424 42 @ 01/11/24 11:32:13.141
  STEP: Trying to relaunch the pod, now with labels. @ 01/11/24 11:32:13.151
  STEP: removing the label kubernetes.io/e2e-e12ecab3-27fc-4d1e-8dc1-a63f98dde424 off the node env1-test-worker-1 @ 01/11/24 11:32:15.195
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-e12ecab3-27fc-4d1e-8dc1-a63f98dde424 @ 01/11/24 11:32:15.256
  Jan 11 11:32:15.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-6533" for this suite. @ 01/11/24 11:32:15.283
• [4.540 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 01/11/24 11:32:15.306
  Jan 11 11:32:15.307: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename lease-test @ 01/11/24 11:32:15.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:32:15.359
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:32:15.366
  Jan 11 11:32:15.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-2131" for this suite. @ 01/11/24 11:32:15.596
• [0.311 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 01/11/24 11:32:15.618
  Jan 11 11:32:15.618: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename emptydir @ 01/11/24 11:32:15.62
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:32:15.668
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:32:15.675
  STEP: Creating a pod to test emptydir volume type on node default medium @ 01/11/24 11:32:15.683
  STEP: Saw pod success @ 01/11/24 11:32:19.758
  Jan 11 11:32:19.769: INFO: Trying to get logs from node env1-test-worker-1 pod pod-bdc5acd7-b3c6-42f7-9cfe-5bcd0cd2f586 container test-container: <nil>
  STEP: delete the pod @ 01/11/24 11:32:19.818
  Jan 11 11:32:19.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3075" for this suite. @ 01/11/24 11:32:19.865
• [4.264 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 01/11/24 11:32:19.883
  Jan 11 11:32:19.883: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename downward-api @ 01/11/24 11:32:19.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:32:19.919
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:32:19.927
  STEP: Creating a pod to test downward api env vars @ 01/11/24 11:32:19.935
  STEP: Saw pod success @ 01/11/24 11:32:24.039
  Jan 11 11:32:24.069: INFO: Trying to get logs from node env1-test-worker-2 pod downward-api-c7735c60-a9ca-49a6-bf46-82505d928003 container dapi-container: <nil>
  STEP: delete the pod @ 01/11/24 11:32:24.134
  Jan 11 11:32:24.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9664" for this suite. @ 01/11/24 11:32:24.196
• [4.334 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 01/11/24 11:32:24.219
  Jan 11 11:32:24.219: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubectl @ 01/11/24 11:32:24.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:32:24.262
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:32:24.269
  STEP: creating Agnhost RC @ 01/11/24 11:32:24.278
  Jan 11 11:32:24.278: INFO: namespace kubectl-9038
  Jan 11 11:32:24.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-9038 create -f -'
  Jan 11 11:32:27.678: INFO: stderr: ""
  Jan 11 11:32:27.678: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 01/11/24 11:32:27.679
  Jan 11 11:32:28.687: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 11 11:32:28.687: INFO: Found 0 / 1
  Jan 11 11:32:29.687: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 11 11:32:29.687: INFO: Found 1 / 1
  Jan 11 11:32:29.687: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jan 11 11:32:29.695: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 11 11:32:29.695: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jan 11 11:32:29.695: INFO: wait on agnhost-primary startup in kubectl-9038 
  Jan 11 11:32:29.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-9038 logs agnhost-primary-5q2c2 agnhost-primary'
  Jan 11 11:32:29.874: INFO: stderr: ""
  Jan 11 11:32:29.874: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 01/11/24 11:32:29.874
  Jan 11 11:32:29.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-9038 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Jan 11 11:32:30.035: INFO: stderr: ""
  Jan 11 11:32:30.035: INFO: stdout: "service/rm2 exposed\n"
  Jan 11 11:32:30.050: INFO: Service rm2 in namespace kubectl-9038 found.
  STEP: exposing service @ 01/11/24 11:32:32.063
  Jan 11 11:32:32.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-9038 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Jan 11 11:32:32.194: INFO: stderr: ""
  Jan 11 11:32:32.194: INFO: stdout: "service/rm3 exposed\n"
  Jan 11 11:32:32.204: INFO: Service rm3 in namespace kubectl-9038 found.
  Jan 11 11:32:34.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9038" for this suite. @ 01/11/24 11:32:34.225
• [10.016 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 01/11/24 11:32:34.238
  Jan 11 11:32:34.238: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename webhook @ 01/11/24 11:32:34.24
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:32:34.268
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:32:34.274
  STEP: Setting up server cert @ 01/11/24 11:32:34.327
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/11/24 11:32:34.868
  STEP: Deploying the webhook pod @ 01/11/24 11:32:34.883
  STEP: Wait for the deployment to be ready @ 01/11/24 11:32:34.903
  Jan 11 11:32:34.927: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/11/24 11:32:36.949
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 11:32:36.974
  Jan 11 11:32:37.974: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jan 11 11:32:37.981: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 01/11/24 11:32:43.504
  STEP: Creating a custom resource that should be denied by the webhook @ 01/11/24 11:32:43.564
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 01/11/24 11:32:45.614
  STEP: Updating the custom resource with disallowed data should be denied @ 01/11/24 11:32:45.628
  STEP: Deleting the custom resource should be denied @ 01/11/24 11:32:45.645
  STEP: Remove the offending key and value from the custom resource data @ 01/11/24 11:32:45.659
  STEP: Deleting the updated custom resource should be successful @ 01/11/24 11:32:45.682
  Jan 11 11:32:45.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9340" for this suite. @ 01/11/24 11:32:46.382
  STEP: Destroying namespace "webhook-markers-23" for this suite. @ 01/11/24 11:32:46.397
• [12.190 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 01/11/24 11:32:46.429
  Jan 11 11:32:46.429: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename var-expansion @ 01/11/24 11:32:46.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:32:46.513
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:32:46.522
  STEP: Creating a pod to test substitution in volume subpath @ 01/11/24 11:32:46.53
  STEP: Saw pod success @ 01/11/24 11:32:50.597
  Jan 11 11:32:50.604: INFO: Trying to get logs from node env1-test-worker-1 pod var-expansion-1c365f52-a2a3-4efb-995f-738d57dc004c container dapi-container: <nil>
  STEP: delete the pod @ 01/11/24 11:32:50.623
  Jan 11 11:32:50.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2490" for this suite. @ 01/11/24 11:32:50.694
• [4.285 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 01/11/24 11:32:50.715
  Jan 11 11:32:50.715: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename webhook @ 01/11/24 11:32:50.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:32:50.76
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:32:50.768
  STEP: Setting up server cert @ 01/11/24 11:32:50.825
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/11/24 11:32:51.692
  STEP: Deploying the webhook pod @ 01/11/24 11:32:51.714
  STEP: Wait for the deployment to be ready @ 01/11/24 11:32:51.749
  Jan 11 11:32:51.796: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/11/24 11:32:53.824
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 11:32:53.846
  Jan 11 11:32:54.846: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 01/11/24 11:32:54.855
  STEP: create a pod that should be updated by the webhook @ 01/11/24 11:32:54.891
  Jan 11 11:32:54.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6581" for this suite. @ 01/11/24 11:32:55.123
  STEP: Destroying namespace "webhook-markers-616" for this suite. @ 01/11/24 11:32:55.144
• [4.450 seconds]
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 01/11/24 11:32:55.166
  Jan 11 11:32:55.166: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename configmap @ 01/11/24 11:32:55.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:32:55.225
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:32:55.232
  STEP: Creating configMap configmap-6262/configmap-test-12f472a8-9716-4670-8aa3-e4312aaca037 @ 01/11/24 11:32:55.239
  STEP: Creating a pod to test consume configMaps @ 01/11/24 11:32:55.249
  STEP: Saw pod success @ 01/11/24 11:32:59.299
  Jan 11 11:32:59.307: INFO: Trying to get logs from node env1-test-worker-2 pod pod-configmaps-4171a67c-034f-498a-87c3-2a6b43ba3006 container env-test: <nil>
  STEP: delete the pod @ 01/11/24 11:32:59.324
  Jan 11 11:32:59.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6262" for this suite. @ 01/11/24 11:32:59.386
• [4.239 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 01/11/24 11:32:59.414
  Jan 11 11:32:59.414: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename csiinlinevolumes @ 01/11/24 11:32:59.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:32:59.457
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:32:59.464
  STEP: creating @ 01/11/24 11:32:59.47
  STEP: getting @ 01/11/24 11:32:59.544
  STEP: listing @ 01/11/24 11:32:59.556
  STEP: deleting @ 01/11/24 11:32:59.563
  Jan 11 11:32:59.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-9215" for this suite. @ 01/11/24 11:32:59.635
• [0.242 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 01/11/24 11:32:59.658
  Jan 11 11:32:59.658: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename pod-network-test @ 01/11/24 11:32:59.66
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:32:59.695
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:32:59.702
  STEP: Performing setup for networking test in namespace pod-network-test-6697 @ 01/11/24 11:32:59.708
  STEP: creating a selector @ 01/11/24 11:32:59.708
  STEP: Creating the service pods in kubernetes @ 01/11/24 11:32:59.709
  Jan 11 11:32:59.709: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 01/11/24 11:33:11.959
  Jan 11 11:33:14.015: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jan 11 11:33:14.016: INFO: Breadth first check of 10.233.67.25 on host 10.61.1.200...
  Jan 11 11:33:14.027: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.69.168:9080/dial?request=hostname&protocol=udp&host=10.233.67.25&port=8081&tries=1'] Namespace:pod-network-test-6697 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 11:33:14.027: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:33:14.029: INFO: ExecWithOptions: Clientset creation
  Jan 11 11:33:14.029: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6697/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.69.168%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.67.25%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jan 11 11:33:14.201: INFO: Waiting for responses: map[]
  Jan 11 11:33:14.201: INFO: reached 10.233.67.25 after 0/1 tries
  Jan 11 11:33:14.201: INFO: Breadth first check of 10.233.68.72 on host 10.61.1.201...
  Jan 11 11:33:14.211: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.69.168:9080/dial?request=hostname&protocol=udp&host=10.233.68.72&port=8081&tries=1'] Namespace:pod-network-test-6697 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 11:33:14.211: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:33:14.212: INFO: ExecWithOptions: Clientset creation
  Jan 11 11:33:14.213: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6697/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.69.168%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.68.72%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jan 11 11:33:14.369: INFO: Waiting for responses: map[]
  Jan 11 11:33:14.369: INFO: reached 10.233.68.72 after 0/1 tries
  Jan 11 11:33:14.369: INFO: Breadth first check of 10.233.69.167 on host 10.61.1.202...
  Jan 11 11:33:14.378: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.69.168:9080/dial?request=hostname&protocol=udp&host=10.233.69.167&port=8081&tries=1'] Namespace:pod-network-test-6697 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 11:33:14.379: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:33:14.380: INFO: ExecWithOptions: Clientset creation
  Jan 11 11:33:14.380: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6697/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.69.168%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.69.167%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jan 11 11:33:14.540: INFO: Waiting for responses: map[]
  Jan 11 11:33:14.540: INFO: reached 10.233.69.167 after 0/1 tries
  Jan 11 11:33:14.540: INFO: Going to retry 0 out of 3 pods....
  Jan 11 11:33:14.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-6697" for this suite. @ 01/11/24 11:33:14.557
• [14.920 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 01/11/24 11:33:14.579
  Jan 11 11:33:14.579: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename gc @ 01/11/24 11:33:14.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:33:14.618
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:33:14.626
  STEP: create the rc1 @ 01/11/24 11:33:14.651
  STEP: create the rc2 @ 01/11/24 11:33:14.664
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 01/11/24 11:33:20.712
  STEP: delete the rc simpletest-rc-to-be-deleted @ 01/11/24 11:33:22.755
  STEP: wait for the rc to be deleted @ 01/11/24 11:33:22.811
  Jan 11 11:33:27.847: INFO: 73 pods remaining
  Jan 11 11:33:27.847: INFO: 73 pods has nil DeletionTimestamp
  Jan 11 11:33:27.847: INFO: 
  STEP: Gathering metrics @ 01/11/24 11:33:32.842
  Jan 11 11:33:33.198: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jan 11 11:33:33.200: INFO: Deleting pod "simpletest-rc-to-be-deleted-2n6f9" in namespace "gc-1575"
  Jan 11 11:33:33.236: INFO: Deleting pod "simpletest-rc-to-be-deleted-4885n" in namespace "gc-1575"
  Jan 11 11:33:33.286: INFO: Deleting pod "simpletest-rc-to-be-deleted-4dks7" in namespace "gc-1575"
  Jan 11 11:33:33.361: INFO: Deleting pod "simpletest-rc-to-be-deleted-4gjz4" in namespace "gc-1575"
  Jan 11 11:33:33.393: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mjs6" in namespace "gc-1575"
  Jan 11 11:33:33.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zpwp" in namespace "gc-1575"
  Jan 11 11:33:33.458: INFO: Deleting pod "simpletest-rc-to-be-deleted-5bdbz" in namespace "gc-1575"
  Jan 11 11:33:33.523: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dzgj" in namespace "gc-1575"
  Jan 11 11:33:33.610: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qvpx" in namespace "gc-1575"
  Jan 11 11:33:33.695: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wbvp" in namespace "gc-1575"
  Jan 11 11:33:33.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-64f9j" in namespace "gc-1575"
  Jan 11 11:33:33.815: INFO: Deleting pod "simpletest-rc-to-be-deleted-6d98f" in namespace "gc-1575"
  Jan 11 11:33:33.858: INFO: Deleting pod "simpletest-rc-to-be-deleted-6g4nb" in namespace "gc-1575"
  Jan 11 11:33:33.904: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jvlr" in namespace "gc-1575"
  Jan 11 11:33:33.977: INFO: Deleting pod "simpletest-rc-to-be-deleted-6n9ww" in namespace "gc-1575"
  Jan 11 11:33:34.034: INFO: Deleting pod "simpletest-rc-to-be-deleted-6nslc" in namespace "gc-1575"
  Jan 11 11:33:34.094: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tvxk" in namespace "gc-1575"
  Jan 11 11:33:34.155: INFO: Deleting pod "simpletest-rc-to-be-deleted-729rs" in namespace "gc-1575"
  Jan 11 11:33:34.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-76zpj" in namespace "gc-1575"
  Jan 11 11:33:34.298: INFO: Deleting pod "simpletest-rc-to-be-deleted-78c8k" in namespace "gc-1575"
  Jan 11 11:33:34.392: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bncl" in namespace "gc-1575"
  Jan 11 11:33:34.511: INFO: Deleting pod "simpletest-rc-to-be-deleted-8v46d" in namespace "gc-1575"
  Jan 11 11:33:34.611: INFO: Deleting pod "simpletest-rc-to-be-deleted-98trs" in namespace "gc-1575"
  Jan 11 11:33:34.728: INFO: Deleting pod "simpletest-rc-to-be-deleted-9k7j6" in namespace "gc-1575"
  Jan 11 11:33:34.822: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rp7v" in namespace "gc-1575"
  Jan 11 11:33:34.894: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdbsr" in namespace "gc-1575"
  Jan 11 11:33:34.971: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdhbb" in namespace "gc-1575"
  Jan 11 11:33:35.069: INFO: Deleting pod "simpletest-rc-to-be-deleted-btmvd" in namespace "gc-1575"
  Jan 11 11:33:35.161: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4qps" in namespace "gc-1575"
  Jan 11 11:33:35.264: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7ccf" in namespace "gc-1575"
  Jan 11 11:33:35.323: INFO: Deleting pod "simpletest-rc-to-be-deleted-ch86z" in namespace "gc-1575"
  Jan 11 11:33:35.386: INFO: Deleting pod "simpletest-rc-to-be-deleted-cht9w" in namespace "gc-1575"
  Jan 11 11:33:35.455: INFO: Deleting pod "simpletest-rc-to-be-deleted-cspvj" in namespace "gc-1575"
  Jan 11 11:33:35.512: INFO: Deleting pod "simpletest-rc-to-be-deleted-czbn7" in namespace "gc-1575"
  Jan 11 11:33:35.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqgp2" in namespace "gc-1575"
  Jan 11 11:33:35.660: INFO: Deleting pod "simpletest-rc-to-be-deleted-fchxv" in namespace "gc-1575"
  Jan 11 11:33:35.699: INFO: Deleting pod "simpletest-rc-to-be-deleted-flxm5" in namespace "gc-1575"
  Jan 11 11:33:35.740: INFO: Deleting pod "simpletest-rc-to-be-deleted-hsxwc" in namespace "gc-1575"
  Jan 11 11:33:35.820: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwk4l" in namespace "gc-1575"
  Jan 11 11:33:35.883: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5mwq" in namespace "gc-1575"
  Jan 11 11:33:35.964: INFO: Deleting pod "simpletest-rc-to-be-deleted-k54ld" in namespace "gc-1575"
  Jan 11 11:33:36.048: INFO: Deleting pod "simpletest-rc-to-be-deleted-k6rwc" in namespace "gc-1575"
  Jan 11 11:33:36.114: INFO: Deleting pod "simpletest-rc-to-be-deleted-kcqgv" in namespace "gc-1575"
  Jan 11 11:33:36.181: INFO: Deleting pod "simpletest-rc-to-be-deleted-kf7t8" in namespace "gc-1575"
  Jan 11 11:33:36.279: INFO: Deleting pod "simpletest-rc-to-be-deleted-kjhll" in namespace "gc-1575"
  Jan 11 11:33:36.345: INFO: Deleting pod "simpletest-rc-to-be-deleted-knsc5" in namespace "gc-1575"
  Jan 11 11:33:36.442: INFO: Deleting pod "simpletest-rc-to-be-deleted-knzlj" in namespace "gc-1575"
  Jan 11 11:33:36.496: INFO: Deleting pod "simpletest-rc-to-be-deleted-kpxs7" in namespace "gc-1575"
  Jan 11 11:33:36.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-krwl6" in namespace "gc-1575"
  Jan 11 11:33:36.641: INFO: Deleting pod "simpletest-rc-to-be-deleted-ktjqw" in namespace "gc-1575"
  Jan 11 11:33:36.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1575" for this suite. @ 01/11/24 11:33:36.782
• [22.239 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 01/11/24 11:33:36.821
  Jan 11 11:33:36.822: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-probe @ 01/11/24 11:33:36.824
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:33:36.944
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:33:36.957
  STEP: Creating pod liveness-949caa70-96af-4c0b-8fdb-4b113f831bea in namespace container-probe-1846 @ 01/11/24 11:33:36.965
  Jan 11 11:33:39.028: INFO: Started pod liveness-949caa70-96af-4c0b-8fdb-4b113f831bea in namespace container-probe-1846
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/11/24 11:33:39.028
  Jan 11 11:33:39.036: INFO: Initial restart count of pod liveness-949caa70-96af-4c0b-8fdb-4b113f831bea is 0
  Jan 11 11:33:59.212: INFO: Restart count of pod container-probe-1846/liveness-949caa70-96af-4c0b-8fdb-4b113f831bea is now 1 (20.175216229s elapsed)
  Jan 11 11:34:19.345: INFO: Restart count of pod container-probe-1846/liveness-949caa70-96af-4c0b-8fdb-4b113f831bea is now 2 (40.308488862s elapsed)
  Jan 11 11:34:39.484: INFO: Restart count of pod container-probe-1846/liveness-949caa70-96af-4c0b-8fdb-4b113f831bea is now 3 (1m0.447485357s elapsed)
  Jan 11 11:34:59.663: INFO: Restart count of pod container-probe-1846/liveness-949caa70-96af-4c0b-8fdb-4b113f831bea is now 4 (1m20.626410107s elapsed)
  Jan 11 11:36:06.023: INFO: Restart count of pod container-probe-1846/liveness-949caa70-96af-4c0b-8fdb-4b113f831bea is now 5 (2m26.98675187s elapsed)
  Jan 11 11:36:06.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/11/24 11:36:06.035
  STEP: Destroying namespace "container-probe-1846" for this suite. @ 01/11/24 11:36:06.061
• [149.255 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 01/11/24 11:36:06.077
  Jan 11 11:36:06.077: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 11:36:06.079
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:36:06.114
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:36:06.123
  STEP: Creating configMap with name cm-test-opt-del-ce48212f-d70d-421f-956a-8b3d4a46ed1d @ 01/11/24 11:36:06.159
  STEP: Creating configMap with name cm-test-opt-upd-527a535e-e0f2-45d7-9876-cd340ef5de24 @ 01/11/24 11:36:06.182
  STEP: Creating the pod @ 01/11/24 11:36:06.194
  STEP: Deleting configmap cm-test-opt-del-ce48212f-d70d-421f-956a-8b3d4a46ed1d @ 01/11/24 11:36:10.365
  STEP: Updating configmap cm-test-opt-upd-527a535e-e0f2-45d7-9876-cd340ef5de24 @ 01/11/24 11:36:10.412
  STEP: Creating configMap with name cm-test-opt-create-67946370-e349-4129-b664-c5c68e1681ee @ 01/11/24 11:36:10.43
  STEP: waiting to observe update in volume @ 01/11/24 11:36:10.446
  Jan 11 11:37:21.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9638" for this suite. @ 01/11/24 11:37:21.584
• [75.528 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 01/11/24 11:37:21.608
  Jan 11 11:37:21.608: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename podtemplate @ 01/11/24 11:37:21.61
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:37:21.657
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:37:21.664
  Jan 11 11:37:21.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-8340" for this suite. @ 01/11/24 11:37:21.784
• [0.199 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 01/11/24 11:37:21.814
  Jan 11 11:37:21.814: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 01/11/24 11:37:21.816
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:37:21.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:37:21.872
  STEP: create the container to handle the HTTPGet hook request. @ 01/11/24 11:37:21.894
  STEP: create the pod with lifecycle hook @ 01/11/24 11:37:23.95
  STEP: delete the pod with lifecycle hook @ 01/11/24 11:37:25.997
  STEP: check prestop hook @ 01/11/24 11:37:30.056
  Jan 11 11:37:30.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-5529" for this suite. @ 01/11/24 11:37:30.1
• [8.312 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 01/11/24 11:37:30.126
  Jan 11 11:37:30.127: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-probe @ 01/11/24 11:37:30.128
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:37:30.171
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:37:30.18
  STEP: Creating pod liveness-e60b2d65-4616-4f07-a49e-8ad052db1e38 in namespace container-probe-6496 @ 01/11/24 11:37:30.188
  Jan 11 11:37:32.236: INFO: Started pod liveness-e60b2d65-4616-4f07-a49e-8ad052db1e38 in namespace container-probe-6496
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/11/24 11:37:32.236
  Jan 11 11:37:32.242: INFO: Initial restart count of pod liveness-e60b2d65-4616-4f07-a49e-8ad052db1e38 is 0
  Jan 11 11:41:33.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/11/24 11:41:33.713
  STEP: Destroying namespace "container-probe-6496" for this suite. @ 01/11/24 11:41:33.752
• [243.649 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 01/11/24 11:41:33.777
  Jan 11 11:41:33.778: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename emptydir @ 01/11/24 11:41:33.78
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:41:33.848
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:41:33.856
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 01/11/24 11:41:33.863
  STEP: Saw pod success @ 01/11/24 11:41:37.936
  Jan 11 11:41:37.947: INFO: Trying to get logs from node env1-test-worker-1 pod pod-df73a0fc-f4e9-4427-8673-b0727e272e7c container test-container: <nil>
  STEP: delete the pod @ 01/11/24 11:41:37.998
  Jan 11 11:41:38.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5928" for this suite. @ 01/11/24 11:41:38.083
• [4.322 seconds]
------------------------------
S
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 01/11/24 11:41:38.101
  Jan 11 11:41:38.101: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename downward-api @ 01/11/24 11:41:38.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:41:38.145
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:41:38.153
  STEP: Creating the pod @ 01/11/24 11:41:38.16
  Jan 11 11:41:40.800: INFO: Successfully updated pod "labelsupdateb9d641e2-e3c4-4c53-8de6-d5d998ef9c9b"
  Jan 11 11:41:42.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-823" for this suite. @ 01/11/24 11:41:42.852
• [4.769 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 01/11/24 11:41:42.872
  Jan 11 11:41:42.872: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename resourcequota @ 01/11/24 11:41:42.875
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:41:42.915
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:41:42.922
  STEP: Counting existing ResourceQuota @ 01/11/24 11:41:42.928
  STEP: Creating a ResourceQuota @ 01/11/24 11:41:47.942
  STEP: Ensuring resource quota status is calculated @ 01/11/24 11:41:47.965
  Jan 11 11:41:49.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-685" for this suite. @ 01/11/24 11:41:50
• [7.154 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 01/11/24 11:41:50.039
  Jan 11 11:41:50.039: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename svcaccounts @ 01/11/24 11:41:50.041
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:41:50.094
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:41:50.102
  Jan 11 11:41:50.116: INFO: Got root ca configmap in namespace "svcaccounts-1179"
  Jan 11 11:41:50.132: INFO: Deleted root ca configmap in namespace "svcaccounts-1179"
  STEP: waiting for a new root ca configmap created @ 01/11/24 11:41:50.633
  Jan 11 11:41:50.643: INFO: Recreated root ca configmap in namespace "svcaccounts-1179"
  Jan 11 11:41:50.655: INFO: Updated root ca configmap in namespace "svcaccounts-1179"
  STEP: waiting for the root ca configmap reconciled @ 01/11/24 11:41:51.156
  Jan 11 11:41:51.167: INFO: Reconciled root ca configmap in namespace "svcaccounts-1179"
  Jan 11 11:41:51.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1179" for this suite. @ 01/11/24 11:41:51.185
• [1.168 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 01/11/24 11:41:51.21
  Jan 11 11:41:51.210: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 11:41:51.212
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:41:51.256
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:41:51.264
  STEP: Creating configMap with name projected-configmap-test-volume-map-978370bc-a586-4eb9-b5e1-384efb41f25f @ 01/11/24 11:41:51.271
  STEP: Creating a pod to test consume configMaps @ 01/11/24 11:41:51.286
  STEP: Saw pod success @ 01/11/24 11:41:55.354
  Jan 11 11:41:55.365: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-configmaps-9fe0e64a-68cc-4ea0-982f-2bb4de50e5de container agnhost-container: <nil>
  STEP: delete the pod @ 01/11/24 11:41:55.39
  Jan 11 11:41:55.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3183" for this suite. @ 01/11/24 11:41:55.458
• [4.275 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 01/11/24 11:41:55.503
  Jan 11 11:41:55.503: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename webhook @ 01/11/24 11:41:55.504
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:41:55.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:41:55.573
  STEP: Setting up server cert @ 01/11/24 11:41:55.646
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/11/24 11:41:57.029
  STEP: Deploying the webhook pod @ 01/11/24 11:41:57.059
  STEP: Wait for the deployment to be ready @ 01/11/24 11:41:57.097
  Jan 11 11:41:57.151: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  Jan 11 11:41:59.181: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:42:01.193: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:42:03.189: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:42:05.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:42:07.189: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 41, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 01/11/24 11:42:09.194
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 11:42:09.227
  Jan 11 11:42:10.228: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 01/11/24 11:42:10.235
  STEP: create a namespace for the webhook @ 01/11/24 11:42:10.271
  STEP: create a configmap should be unconditionally rejected by the webhook @ 01/11/24 11:42:10.315
  Jan 11 11:42:10.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3780" for this suite. @ 01/11/24 11:42:10.497
  STEP: Destroying namespace "webhook-markers-2300" for this suite. @ 01/11/24 11:42:10.513
  STEP: Destroying namespace "fail-closed-namespace-7195" for this suite. @ 01/11/24 11:42:10.537
• [15.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 01/11/24 11:42:10.554
  Jan 11 11:42:10.554: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename runtimeclass @ 01/11/24 11:42:10.557
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:42:10.598
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:42:10.605
  STEP: getting /apis @ 01/11/24 11:42:10.615
  STEP: getting /apis/node.k8s.io @ 01/11/24 11:42:10.637
  STEP: getting /apis/node.k8s.io/v1 @ 01/11/24 11:42:10.64
  STEP: creating @ 01/11/24 11:42:10.644
  STEP: watching @ 01/11/24 11:42:10.683
  Jan 11 11:42:10.684: INFO: starting watch
  STEP: getting @ 01/11/24 11:42:10.699
  STEP: listing @ 01/11/24 11:42:10.705
  STEP: patching @ 01/11/24 11:42:10.712
  STEP: updating @ 01/11/24 11:42:10.72
  Jan 11 11:42:10.733: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 01/11/24 11:42:10.734
  STEP: deleting a collection @ 01/11/24 11:42:10.761
  Jan 11 11:42:10.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3177" for this suite. @ 01/11/24 11:42:10.811
• [0.275 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 01/11/24 11:42:10.834
  Jan 11 11:42:10.834: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename watch @ 01/11/24 11:42:10.836
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:42:10.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:42:10.886
  STEP: creating a new configmap @ 01/11/24 11:42:10.89
  STEP: modifying the configmap once @ 01/11/24 11:42:10.897
  STEP: modifying the configmap a second time @ 01/11/24 11:42:10.909
  STEP: deleting the configmap @ 01/11/24 11:42:10.927
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 01/11/24 11:42:10.937
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 01/11/24 11:42:10.94
  Jan 11 11:42:10.940: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8784  431bccd9-249c-45eb-9afd-784166d6c22a 187158505 0 2024-01-11 11:42:10 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2024-01-11 11:42:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 11 11:42:10.941: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8784  431bccd9-249c-45eb-9afd-784166d6c22a 187158506 0 2024-01-11 11:42:10 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2024-01-11 11:42:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 11 11:42:10.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8784" for this suite. @ 01/11/24 11:42:10.952
• [0.137 seconds]
------------------------------
SS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 01/11/24 11:42:10.975
  Jan 11 11:42:10.975: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename security-context-test @ 01/11/24 11:42:10.977
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:42:11.006
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:42:11.012
  Jan 11 11:42:15.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-4108" for this suite. @ 01/11/24 11:42:15.097
• [4.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 01/11/24 11:42:15.136
  Jan 11 11:42:15.136: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename custom-resource-definition @ 01/11/24 11:42:15.138
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:42:15.185
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:42:15.193
  Jan 11 11:42:15.200: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:42:21.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8507" for this suite. @ 01/11/24 11:42:21.374
• [6.273 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 01/11/24 11:42:21.413
  Jan 11 11:42:21.413: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-runtime @ 01/11/24 11:42:21.415
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:42:21.491
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:42:21.504
  STEP: create the container @ 01/11/24 11:42:21.562
  W0111 11:42:21.588945      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 01/11/24 11:42:21.589
  STEP: get the container status @ 01/11/24 11:42:24.636
  STEP: the container should be terminated @ 01/11/24 11:42:24.645
  STEP: the termination message should be set @ 01/11/24 11:42:24.645
  Jan 11 11:42:24.645: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 01/11/24 11:42:24.645
  Jan 11 11:42:24.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6404" for this suite. @ 01/11/24 11:42:24.705
• [3.308 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 01/11/24 11:42:24.726
  Jan 11 11:42:24.726: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename disruption @ 01/11/24 11:42:24.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:42:24.79
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:42:24.796
  STEP: Waiting for the pdb to be processed @ 01/11/24 11:42:24.818
  STEP: Updating PodDisruptionBudget status @ 01/11/24 11:42:26.836
  STEP: Waiting for all pods to be running @ 01/11/24 11:42:26.869
  Jan 11 11:42:26.878: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 01/11/24 11:42:28.89
  STEP: Waiting for the pdb to be processed @ 01/11/24 11:42:28.926
  STEP: Patching PodDisruptionBudget status @ 01/11/24 11:42:28.957
  STEP: Waiting for the pdb to be processed @ 01/11/24 11:42:28.983
  Jan 11 11:42:28.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2687" for this suite. @ 01/11/24 11:42:29.014
• [4.309 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 01/11/24 11:42:29.045
  Jan 11 11:42:29.045: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename secrets @ 01/11/24 11:42:29.048
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:42:29.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:42:29.104
  STEP: Creating secret with name secret-test-c78be417-a206-47f3-821a-7894b774ff72 @ 01/11/24 11:42:29.174
  STEP: Creating a pod to test consume secrets @ 01/11/24 11:42:29.186
  STEP: Saw pod success @ 01/11/24 11:42:33.252
  Jan 11 11:42:33.260: INFO: Trying to get logs from node env1-test-worker-1 pod pod-secrets-55db9e6c-7266-4113-80cf-53db24dfd537 container secret-volume-test: <nil>
  STEP: delete the pod @ 01/11/24 11:42:33.275
  Jan 11 11:42:33.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-994" for this suite. @ 01/11/24 11:42:33.328
  STEP: Destroying namespace "secret-namespace-8057" for this suite. @ 01/11/24 11:42:33.355
• [4.325 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 01/11/24 11:42:33.386
  Jan 11 11:42:33.386: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 11:42:33.39
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:42:33.427
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:42:33.434
  STEP: Creating a pod to test downward API volume plugin @ 01/11/24 11:42:33.441
  STEP: Saw pod success @ 01/11/24 11:42:37.498
  Jan 11 11:42:37.508: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-25c0e64c-4f2b-4c7d-a6aa-f504bdb9b905 container client-container: <nil>
  STEP: delete the pod @ 01/11/24 11:42:37.531
  Jan 11 11:42:37.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7440" for this suite. @ 01/11/24 11:42:37.595
• [4.229 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 01/11/24 11:42:37.616
  Jan 11 11:42:37.616: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename gc @ 01/11/24 11:42:37.618
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:42:37.665
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:42:37.67
  STEP: create the deployment @ 01/11/24 11:42:37.676
  W0111 11:42:37.686148      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 01/11/24 11:42:37.686
  STEP: delete the deployment @ 01/11/24 11:42:38.216
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 01/11/24 11:42:38.237
  STEP: Gathering metrics @ 01/11/24 11:42:38.803
  Jan 11 11:42:39.113: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jan 11 11:42:39.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5229" for this suite. @ 01/11/24 11:42:39.142
• [1.571 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 01/11/24 11:42:39.221
  Jan 11 11:42:39.221: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/11/24 11:42:39.224
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:42:39.286
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:42:39.292
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 01/11/24 11:42:39.298
  Jan 11 11:42:39.299: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 01/11/24 11:42:59.483
  Jan 11 11:42:59.485: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:43:08.976: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:43:30.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9909" for this suite. @ 01/11/24 11:43:30.266
• [51.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 01/11/24 11:43:30.299
  Jan 11 11:43:30.299: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/11/24 11:43:30.302
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:43:30.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:43:30.365
  Jan 11 11:43:30.376: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 01/11/24 11:43:40.055
  Jan 11 11:43:40.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-9931 --namespace=crd-publish-openapi-9931 create -f -'
  Jan 11 11:43:42.728: INFO: stderr: ""
  Jan 11 11:43:42.728: INFO: stdout: "e2e-test-crd-publish-openapi-4855-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jan 11 11:43:42.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-9931 --namespace=crd-publish-openapi-9931 delete e2e-test-crd-publish-openapi-4855-crds test-cr'
  Jan 11 11:43:42.946: INFO: stderr: ""
  Jan 11 11:43:42.946: INFO: stdout: "e2e-test-crd-publish-openapi-4855-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Jan 11 11:43:42.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-9931 --namespace=crd-publish-openapi-9931 apply -f -'
  Jan 11 11:43:45.357: INFO: stderr: ""
  Jan 11 11:43:45.357: INFO: stdout: "e2e-test-crd-publish-openapi-4855-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jan 11 11:43:45.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-9931 --namespace=crd-publish-openapi-9931 delete e2e-test-crd-publish-openapi-4855-crds test-cr'
  Jan 11 11:43:45.557: INFO: stderr: ""
  Jan 11 11:43:45.557: INFO: stdout: "e2e-test-crd-publish-openapi-4855-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 01/11/24 11:43:45.557
  Jan 11 11:43:45.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-9931 explain e2e-test-crd-publish-openapi-4855-crds'
  Jan 11 11:43:47.855: INFO: stderr: ""
  Jan 11 11:43:47.855: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-4855-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Jan 11 11:43:52.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9931" for this suite. @ 01/11/24 11:43:52.062
• [21.780 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 01/11/24 11:43:52.08
  Jan 11 11:43:52.080: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename var-expansion @ 01/11/24 11:43:52.082
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:43:52.127
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:43:52.133
  STEP: Creating a pod to test substitution in container's args @ 01/11/24 11:43:52.137
  STEP: Saw pod success @ 01/11/24 11:43:56.183
  Jan 11 11:43:56.192: INFO: Trying to get logs from node env1-test-worker-1 pod var-expansion-0a1588a6-2da4-4a88-a273-0c9051d0bd0a container dapi-container: <nil>
  STEP: delete the pod @ 01/11/24 11:43:56.226
  Jan 11 11:43:56.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1122" for this suite. @ 01/11/24 11:43:56.269
• [4.203 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 01/11/24 11:43:56.286
  Jan 11 11:43:56.286: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename svcaccounts @ 01/11/24 11:43:56.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:43:56.327
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:43:56.337
  STEP: Creating ServiceAccount "e2e-sa-wz8pc"  @ 01/11/24 11:43:56.343
  Jan 11 11:43:56.355: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-wz8pc"  @ 01/11/24 11:43:56.355
  Jan 11 11:43:56.384: INFO: AutomountServiceAccountToken: true
  Jan 11 11:43:56.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4832" for this suite. @ 01/11/24 11:43:56.402
• [0.137 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 01/11/24 11:43:56.424
  Jan 11 11:43:56.424: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename dns @ 01/11/24 11:43:56.426
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:43:56.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:43:56.463
  STEP: Creating a test headless service @ 01/11/24 11:43:56.47
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7966.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7966.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 01/11/24 11:43:56.485
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7966.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7966.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 01/11/24 11:43:56.485
  STEP: creating a pod to probe DNS @ 01/11/24 11:43:56.485
  STEP: submitting the pod to kubernetes @ 01/11/24 11:43:56.485
  STEP: retrieving the pod @ 01/11/24 11:43:58.535
  STEP: looking for the results for each expected name from probers @ 01/11/24 11:43:58.542
  Jan 11 11:43:58.582: INFO: DNS probes using dns-7966/dns-test-805257bf-f462-4522-933c-7e3da744b47e succeeded

  Jan 11 11:43:58.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/11/24 11:43:58.592
  STEP: deleting the test headless service @ 01/11/24 11:43:58.657
  STEP: Destroying namespace "dns-7966" for this suite. @ 01/11/24 11:43:58.723
• [2.316 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 01/11/24 11:43:58.742
  Jan 11 11:43:58.742: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename resourcequota @ 01/11/24 11:43:58.745
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:43:58.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:43:58.803
  STEP: Counting existing ResourceQuota @ 01/11/24 11:43:58.812
  STEP: Creating a ResourceQuota @ 01/11/24 11:44:03.822
  STEP: Ensuring resource quota status is calculated @ 01/11/24 11:44:03.837
  STEP: Creating a ReplicationController @ 01/11/24 11:44:05.847
  STEP: Ensuring resource quota status captures replication controller creation @ 01/11/24 11:44:05.881
  STEP: Deleting a ReplicationController @ 01/11/24 11:44:07.891
  STEP: Ensuring resource quota status released usage @ 01/11/24 11:44:07.913
  Jan 11 11:44:09.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3943" for this suite. @ 01/11/24 11:44:09.94
• [11.217 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 01/11/24 11:44:09.964
  Jan 11 11:44:09.964: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename containers @ 01/11/24 11:44:09.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:44:10.005
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:44:10.014
  STEP: Creating a pod to test override command @ 01/11/24 11:44:10.022
  STEP: Saw pod success @ 01/11/24 11:44:14.081
  Jan 11 11:44:14.091: INFO: Trying to get logs from node env1-test-worker-1 pod client-containers-df14488f-366e-4ba6-80d3-f98e7a6169f2 container agnhost-container: <nil>
  STEP: delete the pod @ 01/11/24 11:44:14.107
  Jan 11 11:44:14.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-4323" for this suite. @ 01/11/24 11:44:14.172
• [4.227 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 01/11/24 11:44:14.193
  Jan 11 11:44:14.193: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename services @ 01/11/24 11:44:14.195
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:44:14.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:44:14.253
  STEP: creating service in namespace services-5971 @ 01/11/24 11:44:14.259
  STEP: creating service affinity-nodeport-transition in namespace services-5971 @ 01/11/24 11:44:14.259
  STEP: creating replication controller affinity-nodeport-transition in namespace services-5971 @ 01/11/24 11:44:14.294
  I0111 11:44:14.313018      23 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-5971, replica count: 3
  I0111 11:44:17.364519      23 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 11 11:44:17.410: INFO: Creating new exec pod
  Jan 11 11:44:20.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-5971 exec execpod-affinitytwbkh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Jan 11 11:44:20.839: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Jan 11 11:44:20.839: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 11 11:44:20.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-5971 exec execpod-affinitytwbkh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.2.187 80'
  Jan 11 11:44:21.214: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.2.187 80\nConnection to 10.233.2.187 80 port [tcp/http] succeeded!\n"
  Jan 11 11:44:21.214: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 11 11:44:21.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-5971 exec execpod-affinitytwbkh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.202 30756'
  Jan 11 11:44:21.553: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.61.1.202 30756\nConnection to 10.61.1.202 30756 port [tcp/*] succeeded!\n"
  Jan 11 11:44:21.554: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 11 11:44:21.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-5971 exec execpod-affinitytwbkh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.201 30756'
  Jan 11 11:44:21.866: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.61.1.201 30756\nConnection to 10.61.1.201 30756 port [tcp/*] succeeded!\n"
  Jan 11 11:44:21.867: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 11 11:44:21.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-5971 exec execpod-affinitytwbkh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.61.1.200:30756/ ; done'
  Jan 11 11:44:31.499: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n"
  Jan 11 11:44:31.500: INFO: stdout: "\naffinity-nodeport-transition-9g674\naffinity-nodeport-transition-ww2gk\n\naffinity-nodeport-transition-9g674\naffinity-nodeport-transition-ww2gk\n\naffinity-nodeport-transition-9g674\naffinity-nodeport-transition-ww2gk\n\naffinity-nodeport-transition-9g674\naffinity-nodeport-transition-ww2gk\n\naffinity-nodeport-transition-9g674\naffinity-nodeport-transition-ww2gk\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-9g674"
  Jan 11 11:44:31.501: INFO: Received response from host: affinity-nodeport-transition-9g674
  Jan 11 11:44:31.501: INFO: Received response from host: affinity-nodeport-transition-ww2gk
  Jan 11 11:44:31.501: INFO: Received response from host: affinity-nodeport-transition-9g674
  Jan 11 11:44:31.501: INFO: Received response from host: affinity-nodeport-transition-ww2gk
  Jan 11 11:44:31.501: INFO: Received response from host: affinity-nodeport-transition-9g674
  Jan 11 11:44:31.501: INFO: Received response from host: affinity-nodeport-transition-ww2gk
  Jan 11 11:44:31.501: INFO: Received response from host: affinity-nodeport-transition-9g674
  Jan 11 11:44:31.501: INFO: Received response from host: affinity-nodeport-transition-ww2gk
  Jan 11 11:44:31.501: INFO: Received response from host: affinity-nodeport-transition-9g674
  Jan 11 11:44:31.502: INFO: Received response from host: affinity-nodeport-transition-ww2gk
  Jan 11 11:44:31.502: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:44:31.502: INFO: Received response from host: affinity-nodeport-transition-9g674
  Jan 11 11:45:01.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-5971 exec execpod-affinitytwbkh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.61.1.200:30756/ ; done'
  Jan 11 11:45:01.975: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n"
  Jan 11 11:45:01.975: INFO: stdout: "\naffinity-nodeport-transition-ww2gk\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-9g674\naffinity-nodeport-transition-ww2gk\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-9g674\naffinity-nodeport-transition-ww2gk\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-9g674\naffinity-nodeport-transition-ww2gk\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-9g674\naffinity-nodeport-transition-ww2gk\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-9g674\naffinity-nodeport-transition-ww2gk"
  Jan 11 11:45:01.975: INFO: Received response from host: affinity-nodeport-transition-ww2gk
  Jan 11 11:45:01.975: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:01.975: INFO: Received response from host: affinity-nodeport-transition-9g674
  Jan 11 11:45:01.975: INFO: Received response from host: affinity-nodeport-transition-ww2gk
  Jan 11 11:45:01.975: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:01.975: INFO: Received response from host: affinity-nodeport-transition-9g674
  Jan 11 11:45:01.975: INFO: Received response from host: affinity-nodeport-transition-ww2gk
  Jan 11 11:45:01.975: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:01.975: INFO: Received response from host: affinity-nodeport-transition-9g674
  Jan 11 11:45:01.975: INFO: Received response from host: affinity-nodeport-transition-ww2gk
  Jan 11 11:45:01.975: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:01.975: INFO: Received response from host: affinity-nodeport-transition-9g674
  Jan 11 11:45:01.975: INFO: Received response from host: affinity-nodeport-transition-ww2gk
  Jan 11 11:45:01.975: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:01.975: INFO: Received response from host: affinity-nodeport-transition-9g674
  Jan 11 11:45:01.975: INFO: Received response from host: affinity-nodeport-transition-ww2gk
  Jan 11 11:45:02.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-5971 exec execpod-affinitytwbkh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.61.1.200:30756/ ; done'
  Jan 11 11:45:02.451: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30756/\n"
  Jan 11 11:45:02.451: INFO: stdout: "\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-xv8r2\naffinity-nodeport-transition-xv8r2"
  Jan 11 11:45:02.451: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:02.451: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:02.451: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:02.451: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:02.451: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:02.451: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:02.451: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:02.451: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:02.451: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:02.451: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:02.451: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:02.451: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:02.451: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:02.451: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:02.451: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:02.451: INFO: Received response from host: affinity-nodeport-transition-xv8r2
  Jan 11 11:45:02.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 11 11:45:02.461: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5971, will wait for the garbage collector to delete the pods @ 01/11/24 11:45:02.533
  Jan 11 11:45:02.616: INFO: Deleting ReplicationController affinity-nodeport-transition took: 22.366408ms
  Jan 11 11:45:02.718: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.498534ms
  STEP: Destroying namespace "services-5971" for this suite. @ 01/11/24 11:45:05.135
• [50.964 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 01/11/24 11:45:05.162
  Jan 11 11:45:05.163: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename dns @ 01/11/24 11:45:05.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:45:05.221
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:45:05.229
  STEP: Creating a test headless service @ 01/11/24 11:45:05.236
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4570.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4570.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4570.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4570.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4570.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4570.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4570.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4570.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4570.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4570.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4570.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4570.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 76.37.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.37.76_udp@PTR;check="$$(dig +tcp +noall +answer +search 76.37.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.37.76_tcp@PTR;sleep 1; done
   @ 01/11/24 11:45:05.318
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4570.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4570.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4570.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4570.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4570.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4570.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4570.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4570.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4570.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4570.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4570.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4570.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 76.37.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.37.76_udp@PTR;check="$$(dig +tcp +noall +answer +search 76.37.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.37.76_tcp@PTR;sleep 1; done
   @ 01/11/24 11:45:05.318
  STEP: creating a pod to probe DNS @ 01/11/24 11:45:05.318
  STEP: submitting the pod to kubernetes @ 01/11/24 11:45:05.318
  STEP: retrieving the pod @ 01/11/24 11:45:07.423
  STEP: looking for the results for each expected name from probers @ 01/11/24 11:45:07.43
  Jan 11 11:45:07.442: INFO: Unable to read wheezy_udp@dns-test-service.dns-4570.svc.cluster.local from pod dns-4570/dns-test-a066b4e6-5f8c-4418-945d-fd8f786a1cba: the server could not find the requested resource (get pods dns-test-a066b4e6-5f8c-4418-945d-fd8f786a1cba)
  Jan 11 11:45:07.450: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4570.svc.cluster.local from pod dns-4570/dns-test-a066b4e6-5f8c-4418-945d-fd8f786a1cba: the server could not find the requested resource (get pods dns-test-a066b4e6-5f8c-4418-945d-fd8f786a1cba)
  Jan 11 11:45:07.459: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4570.svc.cluster.local from pod dns-4570/dns-test-a066b4e6-5f8c-4418-945d-fd8f786a1cba: the server could not find the requested resource (get pods dns-test-a066b4e6-5f8c-4418-945d-fd8f786a1cba)
  Jan 11 11:45:07.469: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4570.svc.cluster.local from pod dns-4570/dns-test-a066b4e6-5f8c-4418-945d-fd8f786a1cba: the server could not find the requested resource (get pods dns-test-a066b4e6-5f8c-4418-945d-fd8f786a1cba)
  Jan 11 11:45:07.525: INFO: Unable to read jessie_udp@dns-test-service.dns-4570.svc.cluster.local from pod dns-4570/dns-test-a066b4e6-5f8c-4418-945d-fd8f786a1cba: the server could not find the requested resource (get pods dns-test-a066b4e6-5f8c-4418-945d-fd8f786a1cba)
  Jan 11 11:45:07.534: INFO: Unable to read jessie_tcp@dns-test-service.dns-4570.svc.cluster.local from pod dns-4570/dns-test-a066b4e6-5f8c-4418-945d-fd8f786a1cba: the server could not find the requested resource (get pods dns-test-a066b4e6-5f8c-4418-945d-fd8f786a1cba)
  Jan 11 11:45:07.542: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4570.svc.cluster.local from pod dns-4570/dns-test-a066b4e6-5f8c-4418-945d-fd8f786a1cba: the server could not find the requested resource (get pods dns-test-a066b4e6-5f8c-4418-945d-fd8f786a1cba)
  Jan 11 11:45:07.550: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4570.svc.cluster.local from pod dns-4570/dns-test-a066b4e6-5f8c-4418-945d-fd8f786a1cba: the server could not find the requested resource (get pods dns-test-a066b4e6-5f8c-4418-945d-fd8f786a1cba)
  Jan 11 11:45:07.592: INFO: Lookups using dns-4570/dns-test-a066b4e6-5f8c-4418-945d-fd8f786a1cba failed for: [wheezy_udp@dns-test-service.dns-4570.svc.cluster.local wheezy_tcp@dns-test-service.dns-4570.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4570.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4570.svc.cluster.local jessie_udp@dns-test-service.dns-4570.svc.cluster.local jessie_tcp@dns-test-service.dns-4570.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4570.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4570.svc.cluster.local]

  Jan 11 11:45:12.730: INFO: DNS probes using dns-4570/dns-test-a066b4e6-5f8c-4418-945d-fd8f786a1cba succeeded

  Jan 11 11:45:12.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/11/24 11:45:12.748
  STEP: deleting the test service @ 01/11/24 11:45:12.782
  STEP: deleting the test headless service @ 01/11/24 11:45:12.854
  STEP: Destroying namespace "dns-4570" for this suite. @ 01/11/24 11:45:12.881
• [7.738 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 01/11/24 11:45:12.901
  Jan 11 11:45:12.901: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename statefulset @ 01/11/24 11:45:12.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:45:12.962
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:45:12.97
  STEP: Creating service test in namespace statefulset-3003 @ 01/11/24 11:45:12.98
  Jan 11 11:45:13.017: INFO: Found 0 stateful pods, waiting for 1
  Jan 11 11:45:23.032: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 01/11/24 11:45:23.045
  W0111 11:45:23.058828      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jan 11 11:45:23.071: INFO: Found 1 stateful pods, waiting for 2
  Jan 11 11:45:33.084: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=false
  Jan 11 11:45:43.084: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 11 11:45:43.084: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 01/11/24 11:45:43.102
  STEP: Delete all of the StatefulSets @ 01/11/24 11:45:43.109
  STEP: Verify that StatefulSets have been deleted @ 01/11/24 11:45:43.129
  Jan 11 11:45:43.139: INFO: Deleting all statefulset in ns statefulset-3003
  Jan 11 11:45:43.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3003" for this suite. @ 01/11/24 11:45:43.212
• [30.330 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:96
  STEP: Creating a kubernetes client @ 01/11/24 11:45:43.235
  Jan 11 11:45:43.236: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename aggregator @ 01/11/24 11:45:43.238
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:45:43.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:45:43.303
  Jan 11 11:45:43.310: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Registering the sample API server. @ 01/11/24 11:45:43.312
  Jan 11 11:45:44.543: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Jan 11 11:45:44.623: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  Jan 11 11:45:46.765: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:45:48.775: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:45:50.776: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:45:52.775: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:45:54.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:45:56.850: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:45:58.776: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:46:00.778: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:46:02.776: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:46:04.779: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:46:06.779: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 45, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:46:08.942: INFO: Waited 149.555252ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 01/11/24 11:46:09.055
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 01/11/24 11:46:09.069
  STEP: List APIServices @ 01/11/24 11:46:09.082
  Jan 11 11:46:09.106: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 01/11/24 11:46:09.106
  Jan 11 11:46:09.135: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 01/11/24 11:46:09.135
  Jan 11 11:46:09.161: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2024, time.January, 11, 11, 46, 8, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 01/11/24 11:46:09.161
  Jan 11 11:46:09.169: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2024-01-11 11:46:08 +0000 UTC Passed all checks passed}
  Jan 11 11:46:09.169: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 11 11:46:09.170: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 01/11/24 11:46:09.17
  Jan 11 11:46:09.195: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1274607144" @ 01/11/24 11:46:09.195
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 01/11/24 11:46:09.219
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 01/11/24 11:46:09.234
  STEP: Patch APIService Status @ 01/11/24 11:46:09.241
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 01/11/24 11:46:09.261
  Jan 11 11:46:09.271: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2024-01-11 11:46:08 +0000 UTC Passed all checks passed}
  Jan 11 11:46:09.271: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 11 11:46:09.271: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Jan 11 11:46:09.271: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 01/11/24 11:46:09.271
  STEP: Confirm that the generated APIService has been deleted @ 01/11/24 11:46:09.282
  Jan 11 11:46:09.282: INFO: Requesting list of APIServices to confirm quantity
  Jan 11 11:46:09.305: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Jan 11 11:46:09.305: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Jan 11 11:46:09.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-6210" for this suite. @ 01/11/24 11:46:09.886
• [26.679 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 01/11/24 11:46:09.915
  Jan 11 11:46:09.915: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename resourcequota @ 01/11/24 11:46:09.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:46:09.982
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:46:09.991
  STEP: Counting existing ResourceQuota @ 01/11/24 11:46:09.997
  STEP: Creating a ResourceQuota @ 01/11/24 11:46:15.007
  STEP: Ensuring resource quota status is calculated @ 01/11/24 11:46:15.034
  STEP: Creating a Service @ 01/11/24 11:46:17.046
  STEP: Creating a NodePort Service @ 01/11/24 11:46:17.085
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 01/11/24 11:46:17.142
  STEP: Ensuring resource quota status captures service creation @ 01/11/24 11:46:17.198
  STEP: Deleting Services @ 01/11/24 11:46:19.207
  STEP: Ensuring resource quota status released usage @ 01/11/24 11:46:19.327
  Jan 11 11:46:21.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3383" for this suite. @ 01/11/24 11:46:21.372
• [11.480 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 01/11/24 11:46:21.397
  Jan 11 11:46:21.397: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename configmap @ 01/11/24 11:46:21.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:46:21.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:46:21.45
  STEP: Creating configMap with name configmap-test-volume-map-231cd98d-2296-41ce-9c6d-df2ac2138f7a @ 01/11/24 11:46:21.456
  STEP: Creating a pod to test consume configMaps @ 01/11/24 11:46:21.474
  STEP: Saw pod success @ 01/11/24 11:46:25.552
  Jan 11 11:46:25.562: INFO: Trying to get logs from node env1-test-worker-1 pod pod-configmaps-9523c0b2-5e54-476e-b358-1e1a539225c6 container agnhost-container: <nil>
  STEP: delete the pod @ 01/11/24 11:46:25.604
  Jan 11 11:46:25.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4559" for this suite. @ 01/11/24 11:46:25.679
• [4.298 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 01/11/24 11:46:25.699
  Jan 11 11:46:25.699: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 01/11/24 11:46:25.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:46:25.744
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:46:25.751
  STEP: creating a target pod @ 01/11/24 11:46:25.758
  STEP: adding an ephemeral container @ 01/11/24 11:46:27.815
  STEP: checking pod container endpoints @ 01/11/24 11:46:29.855
  Jan 11 11:46:29.855: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4542 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 11:46:29.855: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:46:29.856: INFO: ExecWithOptions: Clientset creation
  Jan 11 11:46:29.856: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-4542/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Jan 11 11:46:30.001: INFO: Exec stderr: ""
  Jan 11 11:46:30.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-4542" for this suite. @ 01/11/24 11:46:30.036
• [4.355 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 01/11/24 11:46:30.058
  Jan 11 11:46:30.058: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubectl @ 01/11/24 11:46:30.061
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:46:30.105
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:46:30.112
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 01/11/24 11:46:30.119
  Jan 11 11:46:30.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-2336 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jan 11 11:46:30.334: INFO: stderr: ""
  Jan 11 11:46:30.335: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 01/11/24 11:46:30.335
  Jan 11 11:46:30.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-2336 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Jan 11 11:46:30.550: INFO: stderr: ""
  Jan 11 11:46:30.550: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 01/11/24 11:46:30.55
  Jan 11 11:46:30.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-2336 delete pods e2e-test-httpd-pod'
  Jan 11 11:46:33.281: INFO: stderr: ""
  Jan 11 11:46:33.281: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jan 11 11:46:33.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2336" for this suite. @ 01/11/24 11:46:33.297
• [3.254 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 01/11/24 11:46:33.313
  Jan 11 11:46:33.313: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename cronjob @ 01/11/24 11:46:33.315
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:46:33.356
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:46:33.365
  STEP: Creating a cronjob @ 01/11/24 11:46:33.372
  STEP: Ensuring more than one job is running at a time @ 01/11/24 11:46:33.388
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 01/11/24 11:48:01.398
  STEP: Removing cronjob @ 01/11/24 11:48:01.417
  Jan 11 11:48:01.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-1413" for this suite. @ 01/11/24 11:48:01.457
• [88.177 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 01/11/24 11:48:01.495
  Jan 11 11:48:01.495: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename secrets @ 01/11/24 11:48:01.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:48:01.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:48:01.614
  STEP: Creating secret with name secret-test-77269e02-b6fe-42f0-b53c-c7d4e3d20e70 @ 01/11/24 11:48:01.621
  STEP: Creating a pod to test consume secrets @ 01/11/24 11:48:01.671
  STEP: Saw pod success @ 01/11/24 11:48:05.727
  Jan 11 11:48:05.734: INFO: Trying to get logs from node env1-test-worker-2 pod pod-secrets-065b4bb3-f5c4-4757-afd5-10b29f361527 container secret-volume-test: <nil>
  STEP: delete the pod @ 01/11/24 11:48:05.776
  Jan 11 11:48:05.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3677" for this suite. @ 01/11/24 11:48:05.846
• [4.378 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 01/11/24 11:48:05.877
  Jan 11 11:48:05.877: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename webhook @ 01/11/24 11:48:05.878
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:48:05.948
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:48:05.956
  STEP: Setting up server cert @ 01/11/24 11:48:06.02
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/11/24 11:48:07.754
  STEP: Deploying the webhook pod @ 01/11/24 11:48:07.777
  STEP: Wait for the deployment to be ready @ 01/11/24 11:48:07.815
  Jan 11 11:48:07.855: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/11/24 11:48:09.886
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 11:48:09.931
  Jan 11 11:48:10.931: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 01/11/24 11:48:10.942
  STEP: Creating a custom resource definition that should be denied by the webhook @ 01/11/24 11:48:10.996
  Jan 11 11:48:10.996: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:48:11.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7725" for this suite. @ 01/11/24 11:48:11.269
  STEP: Destroying namespace "webhook-markers-2142" for this suite. @ 01/11/24 11:48:11.301
• [5.478 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 01/11/24 11:48:11.361
  Jan 11 11:48:11.361: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename pods @ 01/11/24 11:48:11.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:48:11.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:48:11.428
  STEP: Saw pod success @ 01/11/24 11:48:17.695
  Jan 11 11:48:17.711: INFO: Trying to get logs from node env1-test-worker-1 pod client-envvars-0a5788f9-a5c3-4e64-a026-ba8f76ff0acc container env3cont: <nil>
  STEP: delete the pod @ 01/11/24 11:48:17.822
  Jan 11 11:48:17.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4947" for this suite. @ 01/11/24 11:48:17.887
• [6.556 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 01/11/24 11:48:17.92
  Jan 11 11:48:17.920: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename gc @ 01/11/24 11:48:17.921
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:48:17.979
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:48:17.986
  STEP: create the rc @ 01/11/24 11:48:18.01
  W0111 11:48:18.049147      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 01/11/24 11:48:24.059
  STEP: wait for the rc to be deleted @ 01/11/24 11:48:24.071
  Jan 11 11:48:25.108: INFO: 84 pods remaining
  Jan 11 11:48:25.108: INFO: 83 pods has nil DeletionTimestamp
  Jan 11 11:48:25.108: INFO: 
  Jan 11 11:48:26.091: INFO: 68 pods remaining
  Jan 11 11:48:26.091: INFO: 68 pods has nil DeletionTimestamp
  Jan 11 11:48:26.091: INFO: 
  Jan 11 11:48:27.092: INFO: 59 pods remaining
  Jan 11 11:48:27.092: INFO: 59 pods has nil DeletionTimestamp
  Jan 11 11:48:27.092: INFO: 
  Jan 11 11:48:28.099: INFO: 43 pods remaining
  Jan 11 11:48:28.099: INFO: 42 pods has nil DeletionTimestamp
  Jan 11 11:48:28.099: INFO: 
  Jan 11 11:48:29.091: INFO: 29 pods remaining
  Jan 11 11:48:29.091: INFO: 29 pods has nil DeletionTimestamp
  Jan 11 11:48:29.091: INFO: 
  Jan 11 11:48:30.103: INFO: 19 pods remaining
  Jan 11 11:48:30.103: INFO: 19 pods has nil DeletionTimestamp
  Jan 11 11:48:30.103: INFO: 
  STEP: Gathering metrics @ 01/11/24 11:48:31.089
  Jan 11 11:48:31.317: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jan 11 11:48:31.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3037" for this suite. @ 01/11/24 11:48:31.335
• [13.443 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 01/11/24 11:48:31.363
  Jan 11 11:48:31.363: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename pod-network-test @ 01/11/24 11:48:31.364
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:48:31.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:48:31.427
  STEP: Performing setup for networking test in namespace pod-network-test-3240 @ 01/11/24 11:48:31.433
  STEP: creating a selector @ 01/11/24 11:48:31.433
  STEP: Creating the service pods in kubernetes @ 01/11/24 11:48:31.434
  Jan 11 11:48:31.434: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 01/11/24 11:48:53.772
  Jan 11 11:48:55.856: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jan 11 11:48:55.856: INFO: Breadth first check of 10.233.67.91 on host 10.61.1.200...
  Jan 11 11:48:55.867: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.171:9080/dial?request=hostname&protocol=http&host=10.233.67.91&port=8083&tries=1'] Namespace:pod-network-test-3240 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 11:48:55.867: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:48:55.869: INFO: ExecWithOptions: Clientset creation
  Jan 11 11:48:55.869: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3240/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.171%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.67.91%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jan 11 11:48:56.041: INFO: Waiting for responses: map[]
  Jan 11 11:48:56.041: INFO: reached 10.233.67.91 after 0/1 tries
  Jan 11 11:48:56.041: INFO: Breadth first check of 10.233.68.170 on host 10.61.1.201...
  Jan 11 11:48:56.055: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.171:9080/dial?request=hostname&protocol=http&host=10.233.68.170&port=8083&tries=1'] Namespace:pod-network-test-3240 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 11:48:56.055: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:48:56.057: INFO: ExecWithOptions: Clientset creation
  Jan 11 11:48:56.057: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3240/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.171%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.68.170%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jan 11 11:48:56.217: INFO: Waiting for responses: map[]
  Jan 11 11:48:56.217: INFO: reached 10.233.68.170 after 0/1 tries
  Jan 11 11:48:56.217: INFO: Breadth first check of 10.233.69.246 on host 10.61.1.202...
  Jan 11 11:48:56.226: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.171:9080/dial?request=hostname&protocol=http&host=10.233.69.246&port=8083&tries=1'] Namespace:pod-network-test-3240 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 11:48:56.226: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:48:56.227: INFO: ExecWithOptions: Clientset creation
  Jan 11 11:48:56.227: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3240/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.171%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.69.246%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jan 11 11:48:56.390: INFO: Waiting for responses: map[]
  Jan 11 11:48:56.390: INFO: reached 10.233.69.246 after 0/1 tries
  Jan 11 11:48:56.390: INFO: Going to retry 0 out of 3 pods....
  Jan 11 11:48:56.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-3240" for this suite. @ 01/11/24 11:48:56.409
• [25.064 seconds]
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 01/11/24 11:48:56.429
  Jan 11 11:48:56.429: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename statefulset @ 01/11/24 11:48:56.432
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:48:56.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:48:56.495
  STEP: Creating service test in namespace statefulset-2367 @ 01/11/24 11:48:56.503
  STEP: Creating statefulset ss in namespace statefulset-2367 @ 01/11/24 11:48:56.536
  Jan 11 11:48:56.576: INFO: Found 0 stateful pods, waiting for 1
  Jan 11 11:49:06.588: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 01/11/24 11:49:06.606
  STEP: Getting /status @ 01/11/24 11:49:06.624
  Jan 11 11:49:06.636: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 01/11/24 11:49:06.636
  Jan 11 11:49:06.666: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 01/11/24 11:49:06.667
  Jan 11 11:49:06.673: INFO: Observed &StatefulSet event: ADDED
  Jan 11 11:49:06.673: INFO: Found Statefulset ss in namespace statefulset-2367 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 11 11:49:06.674: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 01/11/24 11:49:06.674
  Jan 11 11:49:06.674: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jan 11 11:49:06.695: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 01/11/24 11:49:06.696
  Jan 11 11:49:06.703: INFO: Observed &StatefulSet event: ADDED
  Jan 11 11:49:06.703: INFO: Deleting all statefulset in ns statefulset-2367
  Jan 11 11:49:06.717: INFO: Scaling statefulset ss to 0
  Jan 11 11:49:16.773: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 11 11:49:16.791: INFO: Deleting statefulset ss
  Jan 11 11:49:16.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2367" for this suite. @ 01/11/24 11:49:16.855
• [20.446 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 01/11/24 11:49:16.88
  Jan 11 11:49:16.881: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename webhook @ 01/11/24 11:49:16.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:49:16.972
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:49:16.98
  STEP: Setting up server cert @ 01/11/24 11:49:17.041
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/11/24 11:49:18.315
  STEP: Deploying the webhook pod @ 01/11/24 11:49:18.338
  STEP: Wait for the deployment to be ready @ 01/11/24 11:49:18.378
  Jan 11 11:49:18.399: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/11/24 11:49:20.423
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 11:49:20.457
  Jan 11 11:49:21.457: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 01/11/24 11:49:21.467
  Jan 11 11:49:21.520: INFO: Waiting for webhook configuration to be ready...
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 01/11/24 11:49:21.658
  STEP: Creating a configMap that should not be mutated @ 01/11/24 11:49:21.674
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 01/11/24 11:49:21.698
  STEP: Creating a configMap that should be mutated @ 01/11/24 11:49:21.714
  Jan 11 11:49:21.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6134" for this suite. @ 01/11/24 11:49:22.203
  STEP: Destroying namespace "webhook-markers-2670" for this suite. @ 01/11/24 11:49:22.242
• [5.388 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 01/11/24 11:49:22.275
  Jan 11 11:49:22.275: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-runtime @ 01/11/24 11:49:22.278
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:49:22.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:49:22.355
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 01/11/24 11:49:22.393
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 01/11/24 11:49:39.616
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 01/11/24 11:49:39.628
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 01/11/24 11:49:39.648
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 01/11/24 11:49:39.648
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 01/11/24 11:49:39.705
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 01/11/24 11:49:42.751
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 01/11/24 11:49:43.769
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 01/11/24 11:49:43.806
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 01/11/24 11:49:43.806
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 01/11/24 11:49:43.874
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 01/11/24 11:49:44.906
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 01/11/24 11:49:46.948
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 01/11/24 11:49:46.969
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 01/11/24 11:49:46.97
  Jan 11 11:49:47.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3068" for this suite. @ 01/11/24 11:49:47.067
• [24.807 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 01/11/24 11:49:47.082
  Jan 11 11:49:47.083: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename custom-resource-definition @ 01/11/24 11:49:47.084
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:49:47.136
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:49:47.147
  Jan 11 11:49:47.155: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:49:52.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4961" for this suite. @ 01/11/24 11:49:52.87
• [5.806 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 01/11/24 11:49:52.891
  Jan 11 11:49:52.891: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 01/11/24 11:49:52.892
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:49:53.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:49:53.022
  STEP: create the container to handle the HTTPGet hook request. @ 01/11/24 11:49:53.062
  STEP: create the pod with lifecycle hook @ 01/11/24 11:49:55.132
  STEP: check poststart hook @ 01/11/24 11:50:13.266
  STEP: delete the pod with lifecycle hook @ 01/11/24 11:50:13.304
  Jan 11 11:50:15.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-6877" for this suite. @ 01/11/24 11:50:15.357
• [22.483 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 01/11/24 11:50:15.377
  Jan 11 11:50:15.377: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/11/24 11:50:15.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:50:15.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:50:15.429
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 01/11/24 11:50:15.437
  Jan 11 11:50:15.437: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:50:25.306: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:50:46.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6300" for this suite. @ 01/11/24 11:50:46.558
• [31.214 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 01/11/24 11:50:46.593
  Jan 11 11:50:46.593: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename field-validation @ 01/11/24 11:50:46.595
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:50:46.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:50:46.642
  STEP: apply creating a deployment @ 01/11/24 11:50:46.659
  Jan 11 11:50:46.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9538" for this suite. @ 01/11/24 11:50:46.722
• [0.143 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 01/11/24 11:50:46.747
  Jan 11 11:50:46.747: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename services @ 01/11/24 11:50:46.749
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:50:46.799
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:50:46.806
  STEP: fetching services @ 01/11/24 11:50:46.814
  Jan 11 11:50:46.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9622" for this suite. @ 01/11/24 11:50:46.852
• [0.127 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 01/11/24 11:50:46.879
  Jan 11 11:50:46.879: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename svcaccounts @ 01/11/24 11:50:46.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:50:46.918
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:50:46.929
  Jan 11 11:50:46.994: INFO: created pod
  STEP: Saw pod success @ 01/11/24 11:50:51.035
  Jan 11 11:51:21.035: INFO: polling logs
  Jan 11 11:51:21.076: INFO: Pod logs: 
  I0111 11:50:47.813042       1 log.go:198] OK: Got token
  I0111 11:50:47.813133       1 log.go:198] validating with in-cluster discovery
  I0111 11:50:47.813922       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0111 11:50:47.813998       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-3626:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1704974447, NotBefore:1704973847, IssuedAt:1704973847, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3626", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b1747f54-0ec6-4081-8146-57fd3d8e6b4b"}}}
  I0111 11:50:47.844593       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0111 11:50:47.859823       1 log.go:198] OK: Validated signature on JWT
  I0111 11:50:47.860258       1 log.go:198] OK: Got valid claims from token!
  I0111 11:50:47.860330       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-3626:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1704974447, NotBefore:1704973847, IssuedAt:1704973847, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3626", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b1747f54-0ec6-4081-8146-57fd3d8e6b4b"}}}

  Jan 11 11:51:21.076: INFO: completed pod
  Jan 11 11:51:21.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3626" for this suite. @ 01/11/24 11:51:21.107
• [34.244 seconds]
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 01/11/24 11:51:21.124
  Jan 11 11:51:21.124: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 11:51:21.125
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:51:21.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:51:21.17
  STEP: Creating secret with name projected-secret-test-7edb7c60-b181-4e2a-a12a-2f7ad72f6e00 @ 01/11/24 11:51:21.178
  STEP: Creating a pod to test consume secrets @ 01/11/24 11:51:21.197
  STEP: Saw pod success @ 01/11/24 11:51:25.257
  Jan 11 11:51:25.266: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-secrets-b06c59e1-29c2-4ec3-83de-4643d7901b24 container secret-volume-test: <nil>
  STEP: delete the pod @ 01/11/24 11:51:25.291
  Jan 11 11:51:25.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-158" for this suite. @ 01/11/24 11:51:25.348
• [4.243 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 01/11/24 11:51:25.367
  Jan 11 11:51:25.367: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-probe @ 01/11/24 11:51:25.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:51:25.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:51:25.415
  Jan 11 11:51:57.677: INFO: Container started at 2024-01-11 11:51:26 +0000 UTC, pod became ready at 2024-01-11 11:51:55 +0000 UTC
  Jan 11 11:51:57.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-9598" for this suite. @ 01/11/24 11:51:57.696
• [32.347 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 01/11/24 11:51:57.716
  Jan 11 11:51:57.717: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubectl @ 01/11/24 11:51:57.718
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:51:57.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:51:57.771
  STEP: creating a replication controller @ 01/11/24 11:51:57.78
  Jan 11 11:51:57.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 create -f -'
  Jan 11 11:52:00.719: INFO: stderr: ""
  Jan 11 11:52:00.719: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 01/11/24 11:52:00.719
  Jan 11 11:52:00.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 11 11:52:00.923: INFO: stderr: ""
  Jan 11 11:52:00.923: INFO: stdout: "update-demo-nautilus-gfmlc update-demo-nautilus-rvhks "
  Jan 11 11:52:00.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods update-demo-nautilus-gfmlc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 11 11:52:01.085: INFO: stderr: ""
  Jan 11 11:52:01.086: INFO: stdout: ""
  Jan 11 11:52:01.086: INFO: update-demo-nautilus-gfmlc is created but not running
  Jan 11 11:52:06.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 11 11:52:06.243: INFO: stderr: ""
  Jan 11 11:52:06.243: INFO: stdout: "update-demo-nautilus-gfmlc update-demo-nautilus-rvhks "
  Jan 11 11:52:06.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods update-demo-nautilus-gfmlc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 11 11:52:06.368: INFO: stderr: ""
  Jan 11 11:52:06.368: INFO: stdout: "true"
  Jan 11 11:52:06.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods update-demo-nautilus-gfmlc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 11 11:52:06.530: INFO: stderr: ""
  Jan 11 11:52:06.530: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 11 11:52:06.530: INFO: validating pod update-demo-nautilus-gfmlc
  Jan 11 11:52:06.545: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 11 11:52:06.546: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 11 11:52:06.546: INFO: update-demo-nautilus-gfmlc is verified up and running
  Jan 11 11:52:06.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods update-demo-nautilus-rvhks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 11 11:52:06.717: INFO: stderr: ""
  Jan 11 11:52:06.717: INFO: stdout: "true"
  Jan 11 11:52:06.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods update-demo-nautilus-rvhks -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 11 11:52:06.903: INFO: stderr: ""
  Jan 11 11:52:06.903: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 11 11:52:06.903: INFO: validating pod update-demo-nautilus-rvhks
  Jan 11 11:52:06.921: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 11 11:52:06.921: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 11 11:52:06.921: INFO: update-demo-nautilus-rvhks is verified up and running
  STEP: scaling down the replication controller @ 01/11/24 11:52:06.921
  Jan 11 11:52:06.924: INFO: scanned /root for discovery docs: <nil>
  Jan 11 11:52:06.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  Jan 11 11:52:08.175: INFO: stderr: ""
  Jan 11 11:52:08.175: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 01/11/24 11:52:08.175
  Jan 11 11:52:08.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 11 11:52:08.359: INFO: stderr: ""
  Jan 11 11:52:08.359: INFO: stdout: "update-demo-nautilus-gfmlc update-demo-nautilus-rvhks "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 01/11/24 11:52:08.359
  Jan 11 11:52:13.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 11 11:52:13.556: INFO: stderr: ""
  Jan 11 11:52:13.556: INFO: stdout: "update-demo-nautilus-rvhks "
  Jan 11 11:52:13.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods update-demo-nautilus-rvhks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 11 11:52:13.731: INFO: stderr: ""
  Jan 11 11:52:13.731: INFO: stdout: "true"
  Jan 11 11:52:13.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods update-demo-nautilus-rvhks -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 11 11:52:13.917: INFO: stderr: ""
  Jan 11 11:52:13.917: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 11 11:52:13.917: INFO: validating pod update-demo-nautilus-rvhks
  Jan 11 11:52:13.945: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 11 11:52:13.946: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 11 11:52:13.946: INFO: update-demo-nautilus-rvhks is verified up and running
  STEP: scaling up the replication controller @ 01/11/24 11:52:13.946
  Jan 11 11:52:13.950: INFO: scanned /root for discovery docs: <nil>
  Jan 11 11:52:13.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  Jan 11 11:52:15.175: INFO: stderr: ""
  Jan 11 11:52:15.175: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 01/11/24 11:52:15.175
  Jan 11 11:52:15.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 11 11:52:15.370: INFO: stderr: ""
  Jan 11 11:52:15.370: INFO: stdout: "update-demo-nautilus-pmclf update-demo-nautilus-rvhks "
  Jan 11 11:52:15.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods update-demo-nautilus-pmclf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 11 11:52:15.556: INFO: stderr: ""
  Jan 11 11:52:15.556: INFO: stdout: ""
  Jan 11 11:52:15.556: INFO: update-demo-nautilus-pmclf is created but not running
  Jan 11 11:52:20.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 11 11:52:20.689: INFO: stderr: ""
  Jan 11 11:52:20.689: INFO: stdout: "update-demo-nautilus-pmclf update-demo-nautilus-rvhks "
  Jan 11 11:52:20.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods update-demo-nautilus-pmclf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 11 11:52:20.878: INFO: stderr: ""
  Jan 11 11:52:20.878: INFO: stdout: "true"
  Jan 11 11:52:20.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods update-demo-nautilus-pmclf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 11 11:52:20.998: INFO: stderr: ""
  Jan 11 11:52:20.998: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 11 11:52:20.998: INFO: validating pod update-demo-nautilus-pmclf
  Jan 11 11:52:21.010: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 11 11:52:21.010: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 11 11:52:21.010: INFO: update-demo-nautilus-pmclf is verified up and running
  Jan 11 11:52:21.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods update-demo-nautilus-rvhks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 11 11:52:21.158: INFO: stderr: ""
  Jan 11 11:52:21.158: INFO: stdout: "true"
  Jan 11 11:52:21.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods update-demo-nautilus-rvhks -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 11 11:52:21.349: INFO: stderr: ""
  Jan 11 11:52:21.349: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 11 11:52:21.349: INFO: validating pod update-demo-nautilus-rvhks
  Jan 11 11:52:21.361: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 11 11:52:21.362: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 11 11:52:21.362: INFO: update-demo-nautilus-rvhks is verified up and running
  STEP: using delete to clean up resources @ 01/11/24 11:52:21.362
  Jan 11 11:52:21.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 delete --grace-period=0 --force -f -'
  Jan 11 11:52:21.602: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 11 11:52:21.602: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jan 11 11:52:21.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get rc,svc -l name=update-demo --no-headers'
  Jan 11 11:52:21.799: INFO: stderr: "No resources found in kubectl-8940 namespace.\n"
  Jan 11 11:52:21.799: INFO: stdout: ""
  Jan 11 11:52:21.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8940 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jan 11 11:52:22.007: INFO: stderr: ""
  Jan 11 11:52:22.007: INFO: stdout: ""
  Jan 11 11:52:22.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8940" for this suite. @ 01/11/24 11:52:22.024
• [24.328 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 01/11/24 11:52:22.046
  Jan 11 11:52:22.046: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename downward-api @ 01/11/24 11:52:22.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:52:22.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:52:22.091
  STEP: Creating a pod to test downward API volume plugin @ 01/11/24 11:52:22.101
  STEP: Saw pod success @ 01/11/24 11:52:26.157
  Jan 11 11:52:26.167: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-bad3699b-5c73-4b07-a956-106edf134bcb container client-container: <nil>
  STEP: delete the pod @ 01/11/24 11:52:26.185
  Jan 11 11:52:26.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-503" for this suite. @ 01/11/24 11:52:26.238
• [4.210 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 01/11/24 11:52:26.256
  Jan 11 11:52:26.257: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename resourcequota @ 01/11/24 11:52:26.258
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:52:26.3
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:52:26.309
  STEP: Creating a ResourceQuota @ 01/11/24 11:52:26.316
  STEP: Getting a ResourceQuota @ 01/11/24 11:52:26.332
  STEP: Listing all ResourceQuotas with LabelSelector @ 01/11/24 11:52:26.352
  STEP: Patching the ResourceQuota @ 01/11/24 11:52:26.359
  STEP: Deleting a Collection of ResourceQuotas @ 01/11/24 11:52:26.372
  STEP: Verifying the deleted ResourceQuota @ 01/11/24 11:52:26.391
  Jan 11 11:52:26.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8949" for this suite. @ 01/11/24 11:52:26.412
• [0.170 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 01/11/24 11:52:26.427
  Jan 11 11:52:26.428: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubelet-test @ 01/11/24 11:52:26.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:52:26.469
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:52:26.476
  Jan 11 11:52:26.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6446" for this suite. @ 01/11/24 11:52:26.583
• [0.176 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 01/11/24 11:52:26.605
  Jan 11 11:52:26.605: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 11:52:26.607
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:52:26.645
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:52:26.651
  STEP: Creating configMap with name projected-configmap-test-volume-30590d50-1cd6-4de9-bcfc-19703278a498 @ 01/11/24 11:52:26.657
  STEP: Creating a pod to test consume configMaps @ 01/11/24 11:52:26.669
  STEP: Saw pod success @ 01/11/24 11:52:30.722
  Jan 11 11:52:30.730: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-configmaps-c4d0a7cc-c916-4bce-bd1c-388a8cafe780 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 01/11/24 11:52:30.743
  Jan 11 11:52:30.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-693" for this suite. @ 01/11/24 11:52:30.8
• [4.211 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 01/11/24 11:52:30.82
  Jan 11 11:52:30.820: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename proxy @ 01/11/24 11:52:30.821
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:52:30.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:52:30.86
  STEP: starting an echo server on multiple ports @ 01/11/24 11:52:30.896
  STEP: creating replication controller proxy-service-tmtsp in namespace proxy-4194 @ 01/11/24 11:52:30.896
  I0111 11:52:30.923737      23 runners.go:194] Created replication controller with name: proxy-service-tmtsp, namespace: proxy-4194, replica count: 1
  I0111 11:52:31.975409      23 runners.go:194] proxy-service-tmtsp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0111 11:52:32.976007      23 runners.go:194] proxy-service-tmtsp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 11 11:52:32.986: INFO: setup took 2.118284641s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 01/11/24 11:52:32.986
  Jan 11 11:52:33.001: INFO: (0) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 13.744514ms)
  Jan 11 11:52:33.004: INFO: (0) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 17.916466ms)
  Jan 11 11:52:33.008: INFO: (0) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 22.258204ms)
  Jan 11 11:52:33.011: INFO: (0) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 24.292487ms)
  Jan 11 11:52:33.013: INFO: (0) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 26.06713ms)
  Jan 11 11:52:33.018: INFO: (0) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 30.382145ms)
  Jan 11 11:52:33.018: INFO: (0) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 30.530692ms)
  Jan 11 11:52:33.027: INFO: (0) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 39.640377ms)
  Jan 11 11:52:33.029: INFO: (0) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 41.473658ms)
  Jan 11 11:52:33.030: INFO: (0) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 42.987013ms)
  Jan 11 11:52:33.030: INFO: (0) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 42.659931ms)
  Jan 11 11:52:33.031: INFO: (0) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 44.395571ms)
  Jan 11 11:52:33.032: INFO: (0) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 45.394329ms)
  Jan 11 11:52:33.032: INFO: (0) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 45.019856ms)
  Jan 11 11:52:33.032: INFO: (0) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 45.132736ms)
  Jan 11 11:52:33.041: INFO: (0) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 54.673691ms)
  Jan 11 11:52:33.055: INFO: (1) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 12.848067ms)
  Jan 11 11:52:33.056: INFO: (1) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 14.062967ms)
  Jan 11 11:52:33.057: INFO: (1) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 13.741848ms)
  Jan 11 11:52:33.059: INFO: (1) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 17.904467ms)
  Jan 11 11:52:33.065: INFO: (1) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 22.021395ms)
  Jan 11 11:52:33.065: INFO: (1) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 21.113421ms)
  Jan 11 11:52:33.067: INFO: (1) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 23.325809ms)
  Jan 11 11:52:33.069: INFO: (1) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 24.909219ms)
  Jan 11 11:52:33.070: INFO: (1) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 26.178341ms)
  Jan 11 11:52:33.070: INFO: (1) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 26.541068ms)
  Jan 11 11:52:33.070: INFO: (1) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 25.966106ms)
  Jan 11 11:52:33.071: INFO: (1) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 28.400265ms)
  Jan 11 11:52:33.072: INFO: (1) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 29.271082ms)
  Jan 11 11:52:33.072: INFO: (1) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 28.011931ms)
  Jan 11 11:52:33.073: INFO: (1) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 30.40523ms)
  Jan 11 11:52:33.081: INFO: (1) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 37.91349ms)
  Jan 11 11:52:33.096: INFO: (2) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 13.142252ms)
  Jan 11 11:52:33.096: INFO: (2) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 12.917384ms)
  Jan 11 11:52:33.100: INFO: (2) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 17.236848ms)
  Jan 11 11:52:33.101: INFO: (2) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 17.645608ms)
  Jan 11 11:52:33.101: INFO: (2) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 18.930094ms)
  Jan 11 11:52:33.101: INFO: (2) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 17.892037ms)
  Jan 11 11:52:33.101: INFO: (2) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 18.32262ms)
  Jan 11 11:52:33.102: INFO: (2) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 19.036939ms)
  Jan 11 11:52:33.107: INFO: (2) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 24.794783ms)
  Jan 11 11:52:33.107: INFO: (2) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 25.324691ms)
  Jan 11 11:52:33.107: INFO: (2) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 23.40819ms)
  Jan 11 11:52:33.110: INFO: (2) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 26.995674ms)
  Jan 11 11:52:33.110: INFO: (2) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 26.653955ms)
  Jan 11 11:52:33.110: INFO: (2) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 27.649382ms)
  Jan 11 11:52:33.110: INFO: (2) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 27.012984ms)
  Jan 11 11:52:33.116: INFO: (2) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 33.804318ms)
  Jan 11 11:52:33.130: INFO: (3) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 14.206654ms)
  Jan 11 11:52:33.132: INFO: (3) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 16.261727ms)
  Jan 11 11:52:33.133: INFO: (3) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 16.68547ms)
  Jan 11 11:52:33.140: INFO: (3) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 23.132954ms)
  Jan 11 11:52:33.140: INFO: (3) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 23.298092ms)
  Jan 11 11:52:33.145: INFO: (3) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 28.89257ms)
  Jan 11 11:52:33.146: INFO: (3) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 29.785462ms)
  Jan 11 11:52:33.147: INFO: (3) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 30.617319ms)
  Jan 11 11:52:33.147: INFO: (3) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 30.680103ms)
  Jan 11 11:52:33.149: INFO: (3) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 32.505257ms)
  Jan 11 11:52:33.149: INFO: (3) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 32.310022ms)
  Jan 11 11:52:33.153: INFO: (3) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 36.843482ms)
  Jan 11 11:52:33.153: INFO: (3) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 37.06661ms)
  Jan 11 11:52:33.153: INFO: (3) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 37.01878ms)
  Jan 11 11:52:33.154: INFO: (3) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 37.680399ms)
  Jan 11 11:52:33.155: INFO: (3) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 38.094365ms)
  Jan 11 11:52:33.171: INFO: (4) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 16.196546ms)
  Jan 11 11:52:33.172: INFO: (4) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 15.227462ms)
  Jan 11 11:52:33.172: INFO: (4) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 16.047705ms)
  Jan 11 11:52:33.173: INFO: (4) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 15.79143ms)
  Jan 11 11:52:33.173: INFO: (4) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 17.239232ms)
  Jan 11 11:52:33.173: INFO: (4) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 17.023139ms)
  Jan 11 11:52:33.173: INFO: (4) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 16.923195ms)
  Jan 11 11:52:33.176: INFO: (4) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 19.097942ms)
  Jan 11 11:52:33.178: INFO: (4) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 21.772107ms)
  Jan 11 11:52:33.179: INFO: (4) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 22.425531ms)
  Jan 11 11:52:33.180: INFO: (4) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 23.77509ms)
  Jan 11 11:52:33.180: INFO: (4) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 23.424043ms)
  Jan 11 11:52:33.180: INFO: (4) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 24.032264ms)
  Jan 11 11:52:33.180: INFO: (4) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 24.857175ms)
  Jan 11 11:52:33.181: INFO: (4) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 24.65198ms)
  Jan 11 11:52:33.182: INFO: (4) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 25.062003ms)
  Jan 11 11:52:33.194: INFO: (5) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 12.442447ms)
  Jan 11 11:52:33.202: INFO: (5) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 18.182949ms)
  Jan 11 11:52:33.202: INFO: (5) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 18.187069ms)
  Jan 11 11:52:33.202: INFO: (5) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 19.019559ms)
  Jan 11 11:52:33.202: INFO: (5) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 19.938705ms)
  Jan 11 11:52:33.202: INFO: (5) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 18.180122ms)
  Jan 11 11:52:33.206: INFO: (5) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 22.712475ms)
  Jan 11 11:52:33.206: INFO: (5) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 22.701013ms)
  Jan 11 11:52:33.206: INFO: (5) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 22.047003ms)
  Jan 11 11:52:33.207: INFO: (5) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 23.381873ms)
  Jan 11 11:52:33.207: INFO: (5) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 25.075486ms)
  Jan 11 11:52:33.208: INFO: (5) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 24.859017ms)
  Jan 11 11:52:33.210: INFO: (5) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 27.759268ms)
  Jan 11 11:52:33.213: INFO: (5) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 29.477844ms)
  Jan 11 11:52:33.214: INFO: (5) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 30.758804ms)
  Jan 11 11:52:33.215: INFO: (5) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 31.362123ms)
  Jan 11 11:52:33.243: INFO: (6) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 24.822427ms)
  Jan 11 11:52:33.243: INFO: (6) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 24.850031ms)
  Jan 11 11:52:33.243: INFO: (6) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 23.485664ms)
  Jan 11 11:52:33.243: INFO: (6) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 23.600731ms)
  Jan 11 11:52:33.243: INFO: (6) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 23.727556ms)
  Jan 11 11:52:33.243: INFO: (6) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 23.866567ms)
  Jan 11 11:52:33.243: INFO: (6) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 25.392588ms)
  Jan 11 11:52:33.243: INFO: (6) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 25.273898ms)
  Jan 11 11:52:33.243: INFO: (6) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 25.56709ms)
  Jan 11 11:52:33.243: INFO: (6) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 26.321856ms)
  Jan 11 11:52:33.243: INFO: (6) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 25.906435ms)
  Jan 11 11:52:33.251: INFO: (6) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 33.076135ms)
  Jan 11 11:52:33.255: INFO: (6) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 38.097388ms)
  Jan 11 11:52:33.255: INFO: (6) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 38.317349ms)
  Jan 11 11:52:33.257: INFO: (6) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 39.382312ms)
  Jan 11 11:52:33.257: INFO: (6) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 38.633145ms)
  Jan 11 11:52:33.269: INFO: (7) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 10.965429ms)
  Jan 11 11:52:33.270: INFO: (7) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 12.1831ms)
  Jan 11 11:52:33.272: INFO: (7) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 14.691395ms)
  Jan 11 11:52:33.275: INFO: (7) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 17.215078ms)
  Jan 11 11:52:33.280: INFO: (7) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 21.337037ms)
  Jan 11 11:52:33.280: INFO: (7) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 21.864394ms)
  Jan 11 11:52:33.280: INFO: (7) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 21.855051ms)
  Jan 11 11:52:33.291: INFO: (7) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 32.918567ms)
  Jan 11 11:52:33.294: INFO: (7) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 35.788394ms)
  Jan 11 11:52:33.294: INFO: (7) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 36.046882ms)
  Jan 11 11:52:33.294: INFO: (7) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 35.630182ms)
  Jan 11 11:52:33.294: INFO: (7) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 36.015372ms)
  Jan 11 11:52:33.294: INFO: (7) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 35.708255ms)
  Jan 11 11:52:33.294: INFO: (7) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 36.434749ms)
  Jan 11 11:52:33.295: INFO: (7) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 37.001875ms)
  Jan 11 11:52:33.296: INFO: (7) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 37.661128ms)
  Jan 11 11:52:33.312: INFO: (8) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 15.582299ms)
  Jan 11 11:52:33.314: INFO: (8) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 16.943251ms)
  Jan 11 11:52:33.314: INFO: (8) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 17.112074ms)
  Jan 11 11:52:33.316: INFO: (8) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 19.628183ms)
  Jan 11 11:52:33.320: INFO: (8) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 22.670113ms)
  Jan 11 11:52:33.321: INFO: (8) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 23.512668ms)
  Jan 11 11:52:33.321: INFO: (8) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 23.85135ms)
  Jan 11 11:52:33.325: INFO: (8) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 27.913882ms)
  Jan 11 11:52:33.328: INFO: (8) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 31.043793ms)
  Jan 11 11:52:33.329: INFO: (8) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 31.963115ms)
  Jan 11 11:52:33.330: INFO: (8) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 33.726197ms)
  Jan 11 11:52:33.330: INFO: (8) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 32.668196ms)
  Jan 11 11:52:33.330: INFO: (8) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 33.040898ms)
  Jan 11 11:52:33.334: INFO: (8) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 36.59405ms)
  Jan 11 11:52:33.335: INFO: (8) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 37.467017ms)
  Jan 11 11:52:33.335: INFO: (8) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 38.055933ms)
  Jan 11 11:52:33.353: INFO: (9) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 16.797631ms)
  Jan 11 11:52:33.354: INFO: (9) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 17.717373ms)
  Jan 11 11:52:33.356: INFO: (9) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 19.355127ms)
  Jan 11 11:52:33.360: INFO: (9) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 23.152443ms)
  Jan 11 11:52:33.360: INFO: (9) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 24.40187ms)
  Jan 11 11:52:33.360: INFO: (9) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 23.978778ms)
  Jan 11 11:52:33.365: INFO: (9) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 28.552872ms)
  Jan 11 11:52:33.369: INFO: (9) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 33.549193ms)
  Jan 11 11:52:33.376: INFO: (9) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 39.446655ms)
  Jan 11 11:52:33.376: INFO: (9) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 39.880448ms)
  Jan 11 11:52:33.376: INFO: (9) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 40.516037ms)
  Jan 11 11:52:33.377: INFO: (9) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 40.269862ms)
  Jan 11 11:52:33.377: INFO: (9) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 40.512629ms)
  Jan 11 11:52:33.378: INFO: (9) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 42.017971ms)
  Jan 11 11:52:33.379: INFO: (9) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 42.829603ms)
  Jan 11 11:52:33.381: INFO: (9) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 45.080888ms)
  Jan 11 11:52:33.397: INFO: (10) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 15.219512ms)
  Jan 11 11:52:33.398: INFO: (10) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 15.451793ms)
  Jan 11 11:52:33.398: INFO: (10) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 16.622413ms)
  Jan 11 11:52:33.398: INFO: (10) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 16.145993ms)
  Jan 11 11:52:33.400: INFO: (10) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 18.640147ms)
  Jan 11 11:52:33.402: INFO: (10) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 20.411518ms)
  Jan 11 11:52:33.408: INFO: (10) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 26.057766ms)
  Jan 11 11:52:33.412: INFO: (10) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 30.03277ms)
  Jan 11 11:52:33.413: INFO: (10) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 31.298374ms)
  Jan 11 11:52:33.414: INFO: (10) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 31.856218ms)
  Jan 11 11:52:33.415: INFO: (10) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 33.507634ms)
  Jan 11 11:52:33.416: INFO: (10) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 34.210541ms)
  Jan 11 11:52:33.417: INFO: (10) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 35.71279ms)
  Jan 11 11:52:33.418: INFO: (10) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 36.274189ms)
  Jan 11 11:52:33.419: INFO: (10) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 36.86749ms)
  Jan 11 11:52:33.426: INFO: (10) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 44.398537ms)
  Jan 11 11:52:33.443: INFO: (11) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 16.488642ms)
  Jan 11 11:52:33.453: INFO: (11) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 25.726492ms)
  Jan 11 11:52:33.453: INFO: (11) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 26.064485ms)
  Jan 11 11:52:33.454: INFO: (11) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 26.412944ms)
  Jan 11 11:52:33.455: INFO: (11) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 28.092805ms)
  Jan 11 11:52:33.455: INFO: (11) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 28.427786ms)
  Jan 11 11:52:33.468: INFO: (11) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 40.431762ms)
  Jan 11 11:52:33.468: INFO: (11) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 40.201579ms)
  Jan 11 11:52:33.468: INFO: (11) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 40.658585ms)
  Jan 11 11:52:33.469: INFO: (11) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 41.532481ms)
  Jan 11 11:52:33.469: INFO: (11) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 41.666274ms)
  Jan 11 11:52:33.469: INFO: (11) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 42.092028ms)
  Jan 11 11:52:33.471: INFO: (11) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 44.348664ms)
  Jan 11 11:52:33.474: INFO: (11) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 46.520883ms)
  Jan 11 11:52:33.480: INFO: (11) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 52.342062ms)
  Jan 11 11:52:33.481: INFO: (11) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 54.036886ms)
  Jan 11 11:52:33.500: INFO: (12) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 17.875313ms)
  Jan 11 11:52:33.501: INFO: (12) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 19.18058ms)
  Jan 11 11:52:33.506: INFO: (12) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 24.096836ms)
  Jan 11 11:52:33.507: INFO: (12) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 24.677979ms)
  Jan 11 11:52:33.507: INFO: (12) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 25.332944ms)
  Jan 11 11:52:33.511: INFO: (12) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 29.731379ms)
  Jan 11 11:52:33.514: INFO: (12) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 32.000091ms)
  Jan 11 11:52:33.518: INFO: (12) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 36.199686ms)
  Jan 11 11:52:33.519: INFO: (12) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 36.9536ms)
  Jan 11 11:52:33.520: INFO: (12) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 38.035577ms)
  Jan 11 11:52:33.521: INFO: (12) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 38.636566ms)
  Jan 11 11:52:33.521: INFO: (12) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 38.697882ms)
  Jan 11 11:52:33.521: INFO: (12) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 38.970752ms)
  Jan 11 11:52:33.522: INFO: (12) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 39.400593ms)
  Jan 11 11:52:33.522: INFO: (12) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 39.66477ms)
  Jan 11 11:52:33.537: INFO: (12) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 54.88002ms)
  Jan 11 11:52:33.567: INFO: (13) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 29.489993ms)
  Jan 11 11:52:33.573: INFO: (13) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 35.056305ms)
  Jan 11 11:52:33.576: INFO: (13) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 38.550602ms)
  Jan 11 11:52:33.582: INFO: (13) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 43.996016ms)
  Jan 11 11:52:33.584: INFO: (13) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 45.991888ms)
  Jan 11 11:52:33.590: INFO: (13) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 52.102886ms)
  Jan 11 11:52:33.590: INFO: (13) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 52.148471ms)
  Jan 11 11:52:33.591: INFO: (13) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 53.456786ms)
  Jan 11 11:52:33.592: INFO: (13) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 54.434209ms)
  Jan 11 11:52:33.592: INFO: (13) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 54.85962ms)
  Jan 11 11:52:33.602: INFO: (13) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 63.685158ms)
  Jan 11 11:52:33.605: INFO: (13) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 67.316568ms)
  Jan 11 11:52:33.605: INFO: (13) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 67.541786ms)
  Jan 11 11:52:33.605: INFO: (13) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 67.528037ms)
  Jan 11 11:52:33.606: INFO: (13) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 68.26695ms)
  Jan 11 11:52:33.609: INFO: (13) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 71.664435ms)
  Jan 11 11:52:33.630: INFO: (14) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 19.527414ms)
  Jan 11 11:52:33.633: INFO: (14) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 22.81206ms)
  Jan 11 11:52:33.636: INFO: (14) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 25.905376ms)
  Jan 11 11:52:33.644: INFO: (14) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 33.348381ms)
  Jan 11 11:52:33.646: INFO: (14) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 35.908887ms)
  Jan 11 11:52:33.647: INFO: (14) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 36.80915ms)
  Jan 11 11:52:33.649: INFO: (14) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 39.586622ms)
  Jan 11 11:52:33.650: INFO: (14) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 39.55313ms)
  Jan 11 11:52:33.650: INFO: (14) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 39.379679ms)
  Jan 11 11:52:33.650: INFO: (14) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 39.817803ms)
  Jan 11 11:52:33.650: INFO: (14) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 39.735189ms)
  Jan 11 11:52:33.655: INFO: (14) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 44.341626ms)
  Jan 11 11:52:33.658: INFO: (14) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 47.701202ms)
  Jan 11 11:52:33.660: INFO: (14) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 49.953587ms)
  Jan 11 11:52:33.661: INFO: (14) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 51.252243ms)
  Jan 11 11:52:33.662: INFO: (14) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 51.330633ms)
  Jan 11 11:52:33.681: INFO: (15) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 19.623509ms)
  Jan 11 11:52:33.681: INFO: (15) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 19.36242ms)
  Jan 11 11:52:33.689: INFO: (15) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 26.664589ms)
  Jan 11 11:52:33.690: INFO: (15) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 28.281339ms)
  Jan 11 11:52:33.691: INFO: (15) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 27.841447ms)
  Jan 11 11:52:33.691: INFO: (15) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 27.973957ms)
  Jan 11 11:52:33.691: INFO: (15) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 28.54444ms)
  Jan 11 11:52:33.691: INFO: (15) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 28.975097ms)
  Jan 11 11:52:33.692: INFO: (15) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 29.606626ms)
  Jan 11 11:52:33.693: INFO: (15) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 30.37137ms)
  Jan 11 11:52:33.694: INFO: (15) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 31.700755ms)
  Jan 11 11:52:33.695: INFO: (15) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 32.65907ms)
  Jan 11 11:52:33.698: INFO: (15) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 35.55023ms)
  Jan 11 11:52:33.700: INFO: (15) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 37.024055ms)
  Jan 11 11:52:33.701: INFO: (15) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 38.463524ms)
  Jan 11 11:52:33.705: INFO: (15) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 42.907914ms)
  Jan 11 11:52:33.720: INFO: (16) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 14.740737ms)
  Jan 11 11:52:33.725: INFO: (16) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 19.732332ms)
  Jan 11 11:52:33.730: INFO: (16) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 25.410568ms)
  Jan 11 11:52:33.731: INFO: (16) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 25.315629ms)
  Jan 11 11:52:33.731: INFO: (16) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 25.45804ms)
  Jan 11 11:52:33.731: INFO: (16) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 25.617145ms)
  Jan 11 11:52:33.732: INFO: (16) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 26.4338ms)
  Jan 11 11:52:33.732: INFO: (16) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 26.376825ms)
  Jan 11 11:52:33.732: INFO: (16) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 27.044852ms)
  Jan 11 11:52:33.739: INFO: (16) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 33.396741ms)
  Jan 11 11:52:33.742: INFO: (16) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 36.542209ms)
  Jan 11 11:52:33.745: INFO: (16) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 39.236672ms)
  Jan 11 11:52:33.745: INFO: (16) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 39.3714ms)
  Jan 11 11:52:33.748: INFO: (16) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 42.260333ms)
  Jan 11 11:52:33.750: INFO: (16) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 43.969258ms)
  Jan 11 11:52:33.750: INFO: (16) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 44.553421ms)
  Jan 11 11:52:33.768: INFO: (17) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 16.711307ms)
  Jan 11 11:52:33.773: INFO: (17) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 22.599861ms)
  Jan 11 11:52:33.773: INFO: (17) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 22.374117ms)
  Jan 11 11:52:33.785: INFO: (17) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 34.330779ms)
  Jan 11 11:52:33.785: INFO: (17) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 34.123836ms)
  Jan 11 11:52:33.789: INFO: (17) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 37.445534ms)
  Jan 11 11:52:33.790: INFO: (17) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 38.349634ms)
  Jan 11 11:52:33.790: INFO: (17) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 38.663124ms)
  Jan 11 11:52:33.792: INFO: (17) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 40.674321ms)
  Jan 11 11:52:33.792: INFO: (17) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 41.056067ms)
  Jan 11 11:52:33.794: INFO: (17) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 43.219642ms)
  Jan 11 11:52:33.795: INFO: (17) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 43.7807ms)
  Jan 11 11:52:33.795: INFO: (17) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 43.947325ms)
  Jan 11 11:52:33.795: INFO: (17) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 44.020521ms)
  Jan 11 11:52:33.796: INFO: (17) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 44.709076ms)
  Jan 11 11:52:33.794: INFO: (17) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 43.046064ms)
  Jan 11 11:52:33.852: INFO: (18) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 54.021299ms)
  Jan 11 11:52:33.852: INFO: (18) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 54.358251ms)
  Jan 11 11:52:33.852: INFO: (18) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 54.488318ms)
  Jan 11 11:52:33.852: INFO: (18) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 54.178474ms)
  Jan 11 11:52:33.852: INFO: (18) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 54.189148ms)
  Jan 11 11:52:33.852: INFO: (18) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 54.669554ms)
  Jan 11 11:52:33.852: INFO: (18) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 54.381688ms)
  Jan 11 11:52:33.866: INFO: (18) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 68.90113ms)
  Jan 11 11:52:33.866: INFO: (18) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 68.132606ms)
  Jan 11 11:52:33.866: INFO: (18) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 68.190706ms)
  Jan 11 11:52:33.867: INFO: (18) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 68.304071ms)
  Jan 11 11:52:33.867: INFO: (18) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 69.295593ms)
  Jan 11 11:52:33.867: INFO: (18) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 69.696758ms)
  Jan 11 11:52:33.869: INFO: (18) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 70.806624ms)
  Jan 11 11:52:33.870: INFO: (18) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 71.628002ms)
  Jan 11 11:52:33.879: INFO: (18) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 81.056095ms)
  Jan 11 11:52:33.912: INFO: (19) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz/proxy/rewriteme">test</a> (200; 31.950677ms)
  Jan 11 11:52:33.912: INFO: (19) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:162/proxy/: bar (200; 31.845966ms)
  Jan 11 11:52:33.918: INFO: (19) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:160/proxy/: foo (200; 38.50592ms)
  Jan 11 11:52:33.919: INFO: (19) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:160/proxy/: foo (200; 39.407464ms)
  Jan 11 11:52:33.920: INFO: (19) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:443/proxy/tlsrewritem... (200; 40.338981ms)
  Jan 11 11:52:33.920: INFO: (19) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:460/proxy/: tls baz (200; 40.563775ms)
  Jan 11 11:52:33.922: INFO: (19) /api/v1/namespaces/proxy-4194/pods/https:proxy-service-tmtsp-65npz:462/proxy/: tls qux (200; 42.256967ms)
  Jan 11 11:52:33.924: INFO: (19) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:1080/proxy/rewriteme">... (200; 44.83117ms)
  Jan 11 11:52:33.925: INFO: (19) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname1/proxy/: foo (200; 45.684159ms)
  Jan 11 11:52:33.926: INFO: (19) /api/v1/namespaces/proxy-4194/services/proxy-service-tmtsp:portname2/proxy/: bar (200; 46.596646ms)
  Jan 11 11:52:33.927: INFO: (19) /api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/: <a href="/api/v1/namespaces/proxy-4194/pods/proxy-service-tmtsp-65npz:1080/proxy/rewriteme">test<... (200; 47.832472ms)
  Jan 11 11:52:33.929: INFO: (19) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname1/proxy/: tls baz (200; 49.273389ms)
  Jan 11 11:52:33.929: INFO: (19) /api/v1/namespaces/proxy-4194/pods/http:proxy-service-tmtsp-65npz:162/proxy/: bar (200; 49.151171ms)
  Jan 11 11:52:33.931: INFO: (19) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname1/proxy/: foo (200; 51.103276ms)
  Jan 11 11:52:33.932: INFO: (19) /api/v1/namespaces/proxy-4194/services/https:proxy-service-tmtsp:tlsportname2/proxy/: tls qux (200; 52.076415ms)
  Jan 11 11:52:33.937: INFO: (19) /api/v1/namespaces/proxy-4194/services/http:proxy-service-tmtsp:portname2/proxy/: bar (200; 57.189984ms)
  Jan 11 11:52:33.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-tmtsp in namespace proxy-4194, will wait for the garbage collector to delete the pods @ 01/11/24 11:52:33.954
  Jan 11 11:52:34.035: INFO: Deleting ReplicationController proxy-service-tmtsp took: 14.527227ms
  Jan 11 11:52:34.136: INFO: Terminating ReplicationController proxy-service-tmtsp pods took: 100.794558ms
  STEP: Destroying namespace "proxy-4194" for this suite. @ 01/11/24 11:52:36.937
• [6.137 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 01/11/24 11:52:36.971
  Jan 11 11:52:36.972: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename deployment @ 01/11/24 11:52:36.974
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:52:37.022
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:52:37.029
  Jan 11 11:52:37.038: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Jan 11 11:52:37.072: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jan 11 11:52:42.086: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 01/11/24 11:52:42.086
  Jan 11 11:52:42.086: INFO: Creating deployment "test-rolling-update-deployment"
  Jan 11 11:52:42.107: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Jan 11 11:52:42.160: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  Jan 11 11:52:44.183: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Jan 11 11:52:44.192: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Jan 11 11:52:44.214: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5309  b812a2c4-d69e-4960-a996-624cc11650cd 187164678 1 2024-01-11 11:52:42 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2024-01-11 11:52:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-11 11:52:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008b262d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-11 11:52:42 +0000 UTC,LastTransitionTime:2024-01-11 11:52:42 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2024-01-11 11:52:43 +0000 UTC,LastTransitionTime:2024-01-11 11:52:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jan 11 11:52:44.224: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-5309  83ae34e0-2435-4edd-a9ad-4c67b0c751e6 187164668 1 2024-01-11 11:52:42 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment b812a2c4-d69e-4960-a996-624cc11650cd 0xc008a23027 0xc008a23028}] [] [{kube-controller-manager Update apps/v1 2024-01-11 11:52:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b812a2c4-d69e-4960-a996-624cc11650cd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-11 11:52:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008a230d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jan 11 11:52:44.224: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Jan 11 11:52:44.224: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5309  383bde10-8eb1-464f-83b1-4b48571f9926 187164677 2 2024-01-11 11:52:37 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment b812a2c4-d69e-4960-a996-624cc11650cd 0xc008a22ef7 0xc008a22ef8}] [] [{e2e.test Update apps/v1 2024-01-11 11:52:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-11 11:52:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b812a2c4-d69e-4960-a996-624cc11650cd\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2024-01-11 11:52:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc008a22fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 11 11:52:44.233: INFO: Pod "test-rolling-update-deployment-656d657cd8-gfszp" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-gfszp test-rolling-update-deployment-656d657cd8- deployment-5309  9e02b20c-cff0-4062-97cc-6af5d6f56ed9 187164667 0 2024-01-11 11:52:42 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 83ae34e0-2435-4edd-a9ad-4c67b0c751e6 0xc008909907 0xc008909908}] [] [{kube-controller-manager Update v1 2024-01-11 11:52:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"83ae34e0-2435-4edd-a9ad-4c67b0c751e6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 11:52:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.69.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dlrnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dlrnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:52:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:52:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:52:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 11:52:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.202,PodIP:10.233.69.250,StartTime:2024-01-11 11:52:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-11 11:52:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://e636dab5436514160b5319300fa07af7a4c183f25ce690b74b5d7461f35aae37,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.69.250,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 11:52:44.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5309" for this suite. @ 01/11/24 11:52:44.246
• [7.290 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 01/11/24 11:52:44.262
  Jan 11 11:52:44.262: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename resourcequota @ 01/11/24 11:52:44.264
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:52:44.29
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:52:44.296
  STEP: Creating resourceQuota "e2e-rq-status-nzw48" @ 01/11/24 11:52:44.312
  Jan 11 11:52:44.337: INFO: Resource quota "e2e-rq-status-nzw48" reports spec: hard cpu limit of 500m
  Jan 11 11:52:44.337: INFO: Resource quota "e2e-rq-status-nzw48" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-nzw48" /status @ 01/11/24 11:52:44.337
  STEP: Confirm /status for "e2e-rq-status-nzw48" resourceQuota via watch @ 01/11/24 11:52:44.359
  Jan 11 11:52:44.363: INFO: observed resourceQuota "e2e-rq-status-nzw48" in namespace "resourcequota-3963" with hard status: v1.ResourceList(nil)
  Jan 11 11:52:44.363: INFO: Found resourceQuota "e2e-rq-status-nzw48" in namespace "resourcequota-3963" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jan 11 11:52:44.363: INFO: ResourceQuota "e2e-rq-status-nzw48" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 01/11/24 11:52:44.373
  Jan 11 11:52:44.385: INFO: Resource quota "e2e-rq-status-nzw48" reports spec: hard cpu limit of 1
  Jan 11 11:52:44.385: INFO: Resource quota "e2e-rq-status-nzw48" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-nzw48" /status @ 01/11/24 11:52:44.385
  STEP: Confirm /status for "e2e-rq-status-nzw48" resourceQuota via watch @ 01/11/24 11:52:44.406
  Jan 11 11:52:44.410: INFO: observed resourceQuota "e2e-rq-status-nzw48" in namespace "resourcequota-3963" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jan 11 11:52:44.410: INFO: Found resourceQuota "e2e-rq-status-nzw48" in namespace "resourcequota-3963" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Jan 11 11:52:44.411: INFO: ResourceQuota "e2e-rq-status-nzw48" /status was patched
  STEP: Get "e2e-rq-status-nzw48" /status @ 01/11/24 11:52:44.411
  Jan 11 11:52:44.419: INFO: Resourcequota "e2e-rq-status-nzw48" reports status: hard cpu of 1
  Jan 11 11:52:44.419: INFO: Resourcequota "e2e-rq-status-nzw48" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-nzw48" /status before checking Spec is unchanged @ 01/11/24 11:52:44.428
  Jan 11 11:52:44.445: INFO: Resourcequota "e2e-rq-status-nzw48" reports status: hard cpu of 2
  Jan 11 11:52:44.446: INFO: Resourcequota "e2e-rq-status-nzw48" reports status: hard memory of 2Gi
  Jan 11 11:52:44.449: INFO: Found resourceQuota "e2e-rq-status-nzw48" in namespace "resourcequota-3963" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  Jan 11 11:55:09.465: INFO: ResourceQuota "e2e-rq-status-nzw48" Spec was unchanged and /status reset
  Jan 11 11:55:09.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3963" for this suite. @ 01/11/24 11:55:09.476
• [145.230 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 01/11/24 11:55:09.495
  Jan 11 11:55:09.495: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename podtemplate @ 01/11/24 11:55:09.497
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:55:09.535
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:55:09.539
  STEP: Create a pod template @ 01/11/24 11:55:09.546
  STEP: Replace a pod template @ 01/11/24 11:55:09.556
  Jan 11 11:55:09.585: INFO: Found updated podtemplate annotation: "true"

  Jan 11 11:55:09.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-5080" for this suite. @ 01/11/24 11:55:09.6
• [0.124 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 01/11/24 11:55:09.619
  Jan 11 11:55:09.619: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename emptydir @ 01/11/24 11:55:09.621
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:55:09.671
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:55:09.677
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 01/11/24 11:55:09.683
  STEP: Saw pod success @ 01/11/24 11:55:13.73
  Jan 11 11:55:13.738: INFO: Trying to get logs from node env1-test-worker-1 pod pod-979997a2-25f3-402e-9769-234224529878 container test-container: <nil>
  STEP: delete the pod @ 01/11/24 11:55:13.782
  Jan 11 11:55:13.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3228" for this suite. @ 01/11/24 11:55:13.838
• [4.240 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 01/11/24 11:55:13.866
  Jan 11 11:55:13.866: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 01/11/24 11:55:13.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:55:13.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:55:13.913
  STEP: create the container to handle the HTTPGet hook request. @ 01/11/24 11:55:13.939
  STEP: create the pod with lifecycle hook @ 01/11/24 11:55:16.005
  STEP: check poststart hook @ 01/11/24 11:55:18.068
  STEP: delete the pod with lifecycle hook @ 01/11/24 11:55:18.114
  Jan 11 11:55:20.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3820" for this suite. @ 01/11/24 11:55:20.168
• [6.319 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 01/11/24 11:55:20.185
  Jan 11 11:55:20.185: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename server-version @ 01/11/24 11:55:20.187
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:55:20.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:55:20.232
  STEP: Request ServerVersion @ 01/11/24 11:55:20.237
  STEP: Confirm major version @ 01/11/24 11:55:20.239
  Jan 11 11:55:20.239: INFO: Major version: 1
  STEP: Confirm minor version @ 01/11/24 11:55:20.239
  Jan 11 11:55:20.239: INFO: cleanMinorVersion: 27
  Jan 11 11:55:20.239: INFO: Minor version: 27
  Jan 11 11:55:20.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-2086" for this suite. @ 01/11/24 11:55:20.251
• [0.085 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 01/11/24 11:55:20.272
  Jan 11 11:55:20.272: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/11/24 11:55:20.274
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:55:20.318
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:55:20.326
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 01/11/24 11:55:20.332
  Jan 11 11:55:20.333: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:55:30.037: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 11:55:50.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-985" for this suite. @ 01/11/24 11:55:50.482
• [30.229 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 01/11/24 11:55:50.522
  Jan 11 11:55:50.522: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 11:55:50.525
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:55:50.579
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:55:50.588
  STEP: Creating configMap with name projected-configmap-test-volume-map-03dced14-e43a-4a66-849a-eff6e0aa8144 @ 01/11/24 11:55:50.594
  STEP: Creating a pod to test consume configMaps @ 01/11/24 11:55:50.606
  STEP: Saw pod success @ 01/11/24 11:55:54.666
  Jan 11 11:55:54.676: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-configmaps-704c9ca1-0988-4c02-b5e0-8f0e4f12643a container agnhost-container: <nil>
  STEP: delete the pod @ 01/11/24 11:55:54.696
  Jan 11 11:55:54.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4115" for this suite. @ 01/11/24 11:55:54.753
• [4.260 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:875
  STEP: Creating a kubernetes client @ 01/11/24 11:55:54.789
  Jan 11 11:55:54.789: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename daemonsets @ 01/11/24 11:55:54.791
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:55:54.824
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:55:54.833
  STEP: Creating simple DaemonSet "daemon-set" @ 01/11/24 11:55:54.902
  STEP: Check that daemon pods launch on every node of the cluster. @ 01/11/24 11:55:54.912
  Jan 11 11:55:54.933: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:55:54.933: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:55:54.933: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:55:54.949: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 11:55:54.949: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  Jan 11 11:55:55.969: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:55:55.969: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:55:55.969: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:55:55.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 11:55:55.986: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  Jan 11 11:55:56.966: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:55:56.966: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:55:56.966: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:55:56.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jan 11 11:55:56.982: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 01/11/24 11:55:56.989
  Jan 11 11:55:56.998: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 01/11/24 11:55:56.998
  Jan 11 11:55:57.022: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 01/11/24 11:55:57.023
  Jan 11 11:55:57.027: INFO: Observed &DaemonSet event: ADDED
  Jan 11 11:55:57.028: INFO: Observed &DaemonSet event: MODIFIED
  Jan 11 11:55:57.028: INFO: Observed &DaemonSet event: MODIFIED
  Jan 11 11:55:57.029: INFO: Observed &DaemonSet event: MODIFIED
  Jan 11 11:55:57.029: INFO: Observed &DaemonSet event: MODIFIED
  Jan 11 11:55:57.030: INFO: Observed &DaemonSet event: MODIFIED
  Jan 11 11:55:57.030: INFO: Found daemon set daemon-set in namespace daemonsets-2863 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jan 11 11:55:57.030: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 01/11/24 11:55:57.03
  STEP: watching for the daemon set status to be patched @ 01/11/24 11:55:57.044
  Jan 11 11:55:57.053: INFO: Observed &DaemonSet event: ADDED
  Jan 11 11:55:57.054: INFO: Observed &DaemonSet event: MODIFIED
  Jan 11 11:55:57.055: INFO: Observed &DaemonSet event: MODIFIED
  Jan 11 11:55:57.056: INFO: Observed &DaemonSet event: MODIFIED
  Jan 11 11:55:57.057: INFO: Observed &DaemonSet event: MODIFIED
  Jan 11 11:55:57.058: INFO: Observed &DaemonSet event: MODIFIED
  Jan 11 11:55:57.058: INFO: Observed daemon set daemon-set in namespace daemonsets-2863 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jan 11 11:55:57.059: INFO: Observed &DaemonSet event: MODIFIED
  Jan 11 11:55:57.059: INFO: Found daemon set daemon-set in namespace daemonsets-2863 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Jan 11 11:55:57.059: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 01/11/24 11:55:57.068
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2863, will wait for the garbage collector to delete the pods @ 01/11/24 11:55:57.068
  Jan 11 11:55:57.144: INFO: Deleting DaemonSet.extensions daemon-set took: 19.13449ms
  Jan 11 11:55:57.345: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.664238ms
  Jan 11 11:55:58.855: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 11:55:58.855: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jan 11 11:55:58.860: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"187165707"},"items":null}

  Jan 11 11:55:58.868: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"187165707"},"items":null}

  Jan 11 11:55:58.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2863" for this suite. @ 01/11/24 11:55:58.927
• [4.157 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 01/11/24 11:55:58.948
  Jan 11 11:55:58.948: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename secrets @ 01/11/24 11:55:58.95
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:55:58.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:55:58.999
  STEP: Creating secret with name s-test-opt-del-d1d6695c-213c-422d-8eae-439df878de41 @ 01/11/24 11:55:59.017
  STEP: Creating secret with name s-test-opt-upd-7739e8fa-3762-4ab4-aa54-b449fd2ddb86 @ 01/11/24 11:55:59.03
  STEP: Creating the pod @ 01/11/24 11:55:59.045
  STEP: Deleting secret s-test-opt-del-d1d6695c-213c-422d-8eae-439df878de41 @ 01/11/24 11:56:01.15
  STEP: Updating secret s-test-opt-upd-7739e8fa-3762-4ab4-aa54-b449fd2ddb86 @ 01/11/24 11:56:01.164
  STEP: Creating secret with name s-test-opt-create-7929e078-ec23-4956-8eb4-aebe02dfd61d @ 01/11/24 11:56:01.175
  STEP: waiting to observe update in volume @ 01/11/24 11:56:01.187
  Jan 11 11:57:10.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1200" for this suite. @ 01/11/24 11:57:10.082
• [71.160 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 01/11/24 11:57:10.108
  Jan 11 11:57:10.108: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename emptydir @ 01/11/24 11:57:10.111
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:57:10.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:57:10.168
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 01/11/24 11:57:10.178
  STEP: Saw pod success @ 01/11/24 11:57:14.246
  Jan 11 11:57:14.260: INFO: Trying to get logs from node env1-test-worker-2 pod pod-99f32f18-001d-4257-8a76-be84e09a70ab container test-container: <nil>
  STEP: delete the pod @ 01/11/24 11:57:14.308
  Jan 11 11:57:14.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6622" for this suite. @ 01/11/24 11:57:14.368
• [4.278 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 01/11/24 11:57:14.39
  Jan 11 11:57:14.391: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename subpath @ 01/11/24 11:57:14.393
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:57:14.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:57:14.425
  STEP: Setting up data @ 01/11/24 11:57:14.432
  STEP: Creating pod pod-subpath-test-secret-m6lm @ 01/11/24 11:57:14.464
  STEP: Creating a pod to test atomic-volume-subpath @ 01/11/24 11:57:14.464
  STEP: Saw pod success @ 01/11/24 11:57:38.641
  Jan 11 11:57:38.650: INFO: Trying to get logs from node env1-test-worker-2 pod pod-subpath-test-secret-m6lm container test-container-subpath-secret-m6lm: <nil>
  STEP: delete the pod @ 01/11/24 11:57:38.676
  STEP: Deleting pod pod-subpath-test-secret-m6lm @ 01/11/24 11:57:38.726
  Jan 11 11:57:38.726: INFO: Deleting pod "pod-subpath-test-secret-m6lm" in namespace "subpath-7104"
  Jan 11 11:57:38.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7104" for this suite. @ 01/11/24 11:57:38.747
• [24.375 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 01/11/24 11:57:38.788
  Jan 11 11:57:38.788: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename replication-controller @ 01/11/24 11:57:38.791
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:57:38.853
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:57:38.862
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 01/11/24 11:57:38.869
  STEP: When a replication controller with a matching selector is created @ 01/11/24 11:57:40.928
  STEP: Then the orphan pod is adopted @ 01/11/24 11:57:40.941
  Jan 11 11:57:41.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2793" for this suite. @ 01/11/24 11:57:41.973
• [3.199 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 01/11/24 11:57:41.988
  Jan 11 11:57:41.988: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename init-container @ 01/11/24 11:57:41.99
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:57:42.016
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:57:42.021
  STEP: creating the pod @ 01/11/24 11:57:42.026
  Jan 11 11:57:42.026: INFO: PodSpec: initContainers in spec.initContainers
  Jan 11 11:58:22.174: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-205b9223-a26f-4810-aae0-043b182f39b3", GenerateName:"", Namespace:"init-container-7855", SelfLink:"", UID:"4e98c506-e695-4683-9742-916d5d361ca0", ResourceVersion:"187166476", Generation:0, CreationTimestamp:time.Date(2024, time.January, 11, 11, 57, 42, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"26783116"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.January, 11, 11, 57, 42, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00700e2b8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.January, 11, 11, 58, 22, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00700e2e8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-9w24h", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc008bca340), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-9w24h", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-9w24h", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-9w24h", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc007d18578), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"env1-test-worker-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00093e3f0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc007d185f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc007d18610)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc007d18618), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc007d1861c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc007aae0b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 11, 11, 57, 42, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 11, 11, 57, 42, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 11, 11, 57, 42, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 11, 11, 57, 42, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.61.1.202", PodIP:"10.233.69.3", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.69.3"}}, StartTime:time.Date(2024, time.January, 11, 11, 57, 42, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00093e4d0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00093e540)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://65934912584167b125cb8ee12e489cf5f58f3943d0bd671bb3a997a6856a1bc9", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc008bca3c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc008bca3a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc007d1869f), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Jan 11 11:58:22.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-7855" for this suite. @ 01/11/24 11:58:22.195
• [40.229 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 01/11/24 11:58:22.219
  Jan 11 11:58:22.219: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubectl @ 01/11/24 11:58:22.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:58:22.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:58:22.281
  STEP: validating cluster-info @ 01/11/24 11:58:22.289
  Jan 11 11:58:22.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-3306 cluster-info'
  Jan 11 11:58:22.466: INFO: stderr: ""
  Jan 11 11:58:22.466: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Jan 11 11:58:22.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3306" for this suite. @ 01/11/24 11:58:22.48
• [0.291 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 01/11/24 11:58:22.511
  Jan 11 11:58:22.511: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename csistoragecapacity @ 01/11/24 11:58:22.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:58:22.55
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:58:22.555
  STEP: getting /apis @ 01/11/24 11:58:22.561
  STEP: getting /apis/storage.k8s.io @ 01/11/24 11:58:22.595
  STEP: getting /apis/storage.k8s.io/v1 @ 01/11/24 11:58:22.598
  STEP: creating @ 01/11/24 11:58:22.601
  STEP: watching @ 01/11/24 11:58:22.646
  Jan 11 11:58:22.646: INFO: starting watch
  STEP: getting @ 01/11/24 11:58:22.668
  STEP: listing in namespace @ 01/11/24 11:58:22.675
  STEP: listing across namespaces @ 01/11/24 11:58:22.683
  STEP: patching @ 01/11/24 11:58:22.699
  STEP: updating @ 01/11/24 11:58:22.717
  Jan 11 11:58:22.732: INFO: waiting for watch events with expected annotations in namespace
  Jan 11 11:58:22.732: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 01/11/24 11:58:22.733
  STEP: deleting a collection @ 01/11/24 11:58:22.769
  Jan 11 11:58:22.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-642" for this suite. @ 01/11/24 11:58:22.823
• [0.325 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 01/11/24 11:58:22.837
  Jan 11 11:58:22.837: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename webhook @ 01/11/24 11:58:22.84
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:58:22.873
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:58:22.879
  STEP: Setting up server cert @ 01/11/24 11:58:22.933
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/11/24 11:58:24.272
  STEP: Deploying the webhook pod @ 01/11/24 11:58:24.291
  STEP: Wait for the deployment to be ready @ 01/11/24 11:58:24.329
  Jan 11 11:58:24.362: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  Jan 11 11:58:26.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:58:28.403: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:58:30.401: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:58:32.401: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 11 11:58:34.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 11, 58, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 01/11/24 11:58:36.397
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 11:58:36.438
  Jan 11 11:58:37.439: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 01/11/24 11:58:37.449
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 01/11/24 11:58:37.482
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 01/11/24 11:58:37.509
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 01/11/24 11:58:37.543
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 01/11/24 11:58:37.578
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 01/11/24 11:58:37.596
  Jan 11 11:58:37.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4355" for this suite. @ 01/11/24 11:58:37.887
  STEP: Destroying namespace "webhook-markers-2117" for this suite. @ 01/11/24 11:58:37.921
• [15.129 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 01/11/24 11:58:37.968
  Jan 11 11:58:37.968: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename downward-api @ 01/11/24 11:58:37.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:58:38.057
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:58:38.064
  STEP: Creating a pod to test downward API volume plugin @ 01/11/24 11:58:38.073
  STEP: Saw pod success @ 01/11/24 11:58:42.144
  Jan 11 11:58:42.153: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-49abc2b2-1688-42e3-8be3-258abbda6ca0 container client-container: <nil>
  STEP: delete the pod @ 01/11/24 11:58:42.197
  Jan 11 11:58:42.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3762" for this suite. @ 01/11/24 11:58:42.259
• [4.310 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 01/11/24 11:58:42.279
  Jan 11 11:58:42.280: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename pods @ 01/11/24 11:58:42.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:58:42.324
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:58:42.331
  STEP: creating a Pod with a static label @ 01/11/24 11:58:42.359
  STEP: watching for Pod to be ready @ 01/11/24 11:58:42.384
  Jan 11 11:58:42.391: INFO: observed Pod pod-test in namespace pods-403 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Jan 11 11:58:42.396: INFO: observed Pod pod-test in namespace pods-403 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:58:42 +0000 UTC  }]
  Jan 11 11:58:42.430: INFO: observed Pod pod-test in namespace pods-403 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:58:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:58:42 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:58:42 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:58:42 +0000 UTC  }]
  Jan 11 11:58:44.398: INFO: Found Pod pod-test in namespace pods-403 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:58:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:58:44 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:58:44 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-11 11:58:42 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 01/11/24 11:58:44.411
  STEP: getting the Pod and ensuring that it's patched @ 01/11/24 11:58:44.436
  STEP: replacing the Pod's status Ready condition to False @ 01/11/24 11:58:44.445
  STEP: check the Pod again to ensure its Ready conditions are False @ 01/11/24 11:58:44.481
  STEP: deleting the Pod via a Collection with a LabelSelector @ 01/11/24 11:58:44.482
  STEP: watching for the Pod to be deleted @ 01/11/24 11:58:44.506
  Jan 11 11:58:44.510: INFO: observed event type MODIFIED
  Jan 11 11:58:46.407: INFO: observed event type MODIFIED
  Jan 11 11:58:46.660: INFO: observed event type MODIFIED
  Jan 11 11:58:47.421: INFO: observed event type MODIFIED
  Jan 11 11:58:47.458: INFO: observed event type MODIFIED
  Jan 11 11:58:47.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-403" for this suite. @ 01/11/24 11:58:47.52
• [5.261 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:836
  STEP: Creating a kubernetes client @ 01/11/24 11:58:47.542
  Jan 11 11:58:47.542: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename daemonsets @ 01/11/24 11:58:47.543
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:58:47.605
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:58:47.617
  STEP: Creating simple DaemonSet "daemon-set" @ 01/11/24 11:58:47.698
  STEP: Check that daemon pods launch on every node of the cluster. @ 01/11/24 11:58:47.724
  Jan 11 11:58:47.738: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:58:47.738: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:58:47.738: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:58:47.753: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 11:58:47.753: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  Jan 11 11:58:48.768: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:58:48.768: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:58:48.768: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:58:48.781: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 11:58:48.781: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  Jan 11 11:58:49.772: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:58:49.772: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:58:49.772: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 11:58:49.781: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jan 11 11:58:49.781: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 01/11/24 11:58:49.79
  STEP: DeleteCollection of the DaemonSets @ 01/11/24 11:58:49.801
  STEP: Verify that ReplicaSets have been deleted @ 01/11/24 11:58:49.841
  Jan 11 11:58:49.926: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"187166797"},"items":null}

  Jan 11 11:58:49.938: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"187166798"},"items":[{"metadata":{"name":"daemon-set-jtl9w","generateName":"daemon-set-","namespace":"daemonsets-6480","uid":"fab0a256-33d3-465f-ae58-254154011757","resourceVersion":"187166797","creationTimestamp":"2024-01-11T11:58:47Z","deletionTimestamp":"2024-01-11T11:59:19Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"bd2c46ec-382c-4d26-b64c-8f82bf72406b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-01-11T11:58:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd2c46ec-382c-4d26-b64c-8f82bf72406b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-01-11T11:58:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.69.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-fklpb","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-fklpb","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"env1-test-worker-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["env1-test-worker-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-11T11:58:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-11T11:58:49Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-11T11:58:49Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-11T11:58:47Z"}],"hostIP":"10.61.1.202","podIP":"10.233.69.4","podIPs":[{"ip":"10.233.69.4"}],"startTime":"2024-01-11T11:58:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-01-11T11:58:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://99ff867b1d60c7e6268ce82814b889e44d30148fbfdc767fa82bae025b29ab65","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-ndwhg","generateName":"daemon-set-","namespace":"daemonsets-6480","uid":"49358221-5722-4770-8f37-6bb7f2c37588","resourceVersion":"187166796","creationTimestamp":"2024-01-11T11:58:47Z","deletionTimestamp":"2024-01-11T11:59:19Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"bd2c46ec-382c-4d26-b64c-8f82bf72406b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-01-11T11:58:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd2c46ec-382c-4d26-b64c-8f82bf72406b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-01-11T11:58:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-2x97p","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-2x97p","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"env1-test-worker-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["env1-test-worker-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-11T11:58:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-11T11:58:49Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-11T11:58:49Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-11T11:58:47Z"}],"hostIP":"10.61.1.200","podIP":"10.233.67.93","podIPs":[{"ip":"10.233.67.93"}],"startTime":"2024-01-11T11:58:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-01-11T11:58:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://cb28b864a66732da44e6cb3cacc492a832c0cb1264f813ce7b4779b13196cc1e","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qtwrd","generateName":"daemon-set-","namespace":"daemonsets-6480","uid":"b11245da-6c5b-436f-b63a-8a990aac78d9","resourceVersion":"187166795","creationTimestamp":"2024-01-11T11:58:47Z","deletionTimestamp":"2024-01-11T11:59:19Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"bd2c46ec-382c-4d26-b64c-8f82bf72406b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-01-11T11:58:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd2c46ec-382c-4d26-b64c-8f82bf72406b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-01-11T11:58:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ktpnl","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ktpnl","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"env1-test-worker-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["env1-test-worker-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-11T11:58:47Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-11T11:58:49Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-11T11:58:49Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-11T11:58:47Z"}],"hostIP":"10.61.1.201","podIP":"10.233.68.195","podIPs":[{"ip":"10.233.68.195"}],"startTime":"2024-01-11T11:58:47Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-01-11T11:58:48Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://99f59aef6d3250fcdbb086391bbd34f6a0bbf1118a39077546f31afdedb9848e","started":true}],"qosClass":"BestEffort"}}]}

  Jan 11 11:58:50.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6480" for this suite. @ 01/11/24 11:58:50.021
• [2.497 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 01/11/24 11:58:50.048
  Jan 11 11:58:50.049: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-probe @ 01/11/24 11:58:50.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:58:50.087
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:58:50.093
  STEP: Creating pod liveness-3b3f88e0-9fe5-479b-950f-47ba48e958d3 in namespace container-probe-1838 @ 01/11/24 11:58:50.1
  Jan 11 11:58:52.157: INFO: Started pod liveness-3b3f88e0-9fe5-479b-950f-47ba48e958d3 in namespace container-probe-1838
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/11/24 11:58:52.157
  Jan 11 11:58:52.165: INFO: Initial restart count of pod liveness-3b3f88e0-9fe5-479b-950f-47ba48e958d3 is 0
  Jan 11 11:59:12.281: INFO: Restart count of pod container-probe-1838/liveness-3b3f88e0-9fe5-479b-950f-47ba48e958d3 is now 1 (20.115905995s elapsed)
  Jan 11 11:59:12.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/11/24 11:59:12.295
  STEP: Destroying namespace "container-probe-1838" for this suite. @ 01/11/24 11:59:12.329
• [22.298 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 01/11/24 11:59:12.347
  Jan 11 11:59:12.347: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename subpath @ 01/11/24 11:59:12.349
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:59:12.382
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:59:12.39
  STEP: Setting up data @ 01/11/24 11:59:12.396
  STEP: Creating pod pod-subpath-test-configmap-74rb @ 01/11/24 11:59:12.42
  STEP: Creating a pod to test atomic-volume-subpath @ 01/11/24 11:59:12.42
  STEP: Saw pod success @ 01/11/24 11:59:36.593
  Jan 11 11:59:36.600: INFO: Trying to get logs from node env1-test-worker-1 pod pod-subpath-test-configmap-74rb container test-container-subpath-configmap-74rb: <nil>
  STEP: delete the pod @ 01/11/24 11:59:36.621
  STEP: Deleting pod pod-subpath-test-configmap-74rb @ 01/11/24 11:59:36.671
  Jan 11 11:59:36.671: INFO: Deleting pod "pod-subpath-test-configmap-74rb" in namespace "subpath-6515"
  Jan 11 11:59:36.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-6515" for this suite. @ 01/11/24 11:59:36.693
• [24.360 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 01/11/24 11:59:36.714
  Jan 11 11:59:36.714: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename events @ 01/11/24 11:59:36.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:59:36.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:59:36.768
  STEP: Create set of events @ 01/11/24 11:59:36.774
  STEP: get a list of Events with a label in the current namespace @ 01/11/24 11:59:36.823
  STEP: delete a list of events @ 01/11/24 11:59:36.838
  Jan 11 11:59:36.839: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 01/11/24 11:59:36.921
  Jan 11 11:59:36.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5824" for this suite. @ 01/11/24 11:59:36.957
• [0.258 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 01/11/24 11:59:36.976
  Jan 11 11:59:36.976: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename svcaccounts @ 01/11/24 11:59:36.978
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:59:37.008
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:59:37.015
  Jan 11 11:59:37.062: INFO: created pod pod-service-account-defaultsa
  Jan 11 11:59:37.063: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Jan 11 11:59:37.074: INFO: created pod pod-service-account-mountsa
  Jan 11 11:59:37.074: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Jan 11 11:59:37.099: INFO: created pod pod-service-account-nomountsa
  Jan 11 11:59:37.099: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Jan 11 11:59:37.129: INFO: created pod pod-service-account-defaultsa-mountspec
  Jan 11 11:59:37.129: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Jan 11 11:59:37.154: INFO: created pod pod-service-account-mountsa-mountspec
  Jan 11 11:59:37.155: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Jan 11 11:59:37.187: INFO: created pod pod-service-account-nomountsa-mountspec
  Jan 11 11:59:37.187: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Jan 11 11:59:37.222: INFO: created pod pod-service-account-defaultsa-nomountspec
  Jan 11 11:59:37.222: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Jan 11 11:59:37.264: INFO: created pod pod-service-account-mountsa-nomountspec
  Jan 11 11:59:37.264: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Jan 11 11:59:37.292: INFO: created pod pod-service-account-nomountsa-nomountspec
  Jan 11 11:59:37.292: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Jan 11 11:59:37.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5373" for this suite. @ 01/11/24 11:59:37.333
• [0.393 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 01/11/24 11:59:37.37
  Jan 11 11:59:37.370: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 01/11/24 11:59:37.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:59:37.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:59:37.426
  STEP: create the container to handle the HTTPGet hook request. @ 01/11/24 11:59:37.446
  STEP: create the pod with lifecycle hook @ 01/11/24 11:59:39.511
  STEP: delete the pod with lifecycle hook @ 01/11/24 11:59:41.551
  STEP: check prestop hook @ 01/11/24 11:59:43.614
  Jan 11 11:59:43.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9245" for this suite. @ 01/11/24 11:59:43.692
• [6.344 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 01/11/24 11:59:43.717
  Jan 11 11:59:43.717: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename sysctl @ 01/11/24 11:59:43.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:59:43.764
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:59:43.771
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 01/11/24 11:59:43.777
  STEP: Watching for error events or started pod @ 01/11/24 11:59:43.8
  STEP: Waiting for pod completion @ 01/11/24 11:59:45.819
  STEP: Checking that the pod succeeded @ 01/11/24 11:59:47.849
  STEP: Getting logs from the pod @ 01/11/24 11:59:47.849
  STEP: Checking that the sysctl is actually updated @ 01/11/24 11:59:47.866
  Jan 11 11:59:47.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-581" for this suite. @ 01/11/24 11:59:47.885
• [4.183 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 01/11/24 11:59:47.914
  Jan 11 11:59:47.914: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename webhook @ 01/11/24 11:59:47.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:59:47.957
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:59:47.967
  STEP: Setting up server cert @ 01/11/24 11:59:48.061
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/11/24 11:59:49.273
  STEP: Deploying the webhook pod @ 01/11/24 11:59:49.295
  STEP: Wait for the deployment to be ready @ 01/11/24 11:59:49.329
  Jan 11 11:59:49.359: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/11/24 11:59:51.395
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 11:59:51.433
  Jan 11 11:59:52.434: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 01/11/24 11:59:52.443
  STEP: create a configmap that should be updated by the webhook @ 01/11/24 11:59:52.491
  Jan 11 11:59:52.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1073" for this suite. @ 01/11/24 11:59:52.695
  STEP: Destroying namespace "webhook-markers-3322" for this suite. @ 01/11/24 11:59:52.731
• [4.832 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 01/11/24 11:59:52.752
  Jan 11 11:59:52.752: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename subpath @ 01/11/24 11:59:52.753
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 11:59:52.795
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 11:59:52.809
  STEP: Setting up data @ 01/11/24 11:59:52.815
  STEP: Creating pod pod-subpath-test-configmap-sxhj @ 01/11/24 11:59:52.854
  STEP: Creating a pod to test atomic-volume-subpath @ 01/11/24 11:59:52.854
  STEP: Saw pod success @ 01/11/24 12:00:17.035
  Jan 11 12:00:17.044: INFO: Trying to get logs from node env1-test-worker-2 pod pod-subpath-test-configmap-sxhj container test-container-subpath-configmap-sxhj: <nil>
  STEP: delete the pod @ 01/11/24 12:00:17.082
  STEP: Deleting pod pod-subpath-test-configmap-sxhj @ 01/11/24 12:00:17.118
  Jan 11 12:00:17.118: INFO: Deleting pod "pod-subpath-test-configmap-sxhj" in namespace "subpath-4416"
  Jan 11 12:00:17.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4416" for this suite. @ 01/11/24 12:00:17.145
• [24.414 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 01/11/24 12:00:17.166
  Jan 11 12:00:17.166: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename secrets @ 01/11/24 12:00:17.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:00:17.205
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:00:17.213
  STEP: creating a secret @ 01/11/24 12:00:17.221
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 01/11/24 12:00:17.236
  STEP: patching the secret @ 01/11/24 12:00:17.282
  STEP: deleting the secret using a LabelSelector @ 01/11/24 12:00:17.301
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 01/11/24 12:00:17.326
  Jan 11 12:00:17.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4979" for this suite. @ 01/11/24 12:00:17.393
• [0.244 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 01/11/24 12:00:17.412
  Jan 11 12:00:17.412: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename disruption @ 01/11/24 12:00:17.414
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:00:17.448
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:00:17.454
  STEP: creating the pdb @ 01/11/24 12:00:17.46
  STEP: Waiting for the pdb to be processed @ 01/11/24 12:00:17.479
  STEP: updating the pdb @ 01/11/24 12:00:19.499
  STEP: Waiting for the pdb to be processed @ 01/11/24 12:00:19.521
  STEP: patching the pdb @ 01/11/24 12:00:19.538
  STEP: Waiting for the pdb to be processed @ 01/11/24 12:00:19.581
  STEP: Waiting for the pdb to be deleted @ 01/11/24 12:00:21.617
  Jan 11 12:00:21.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-8738" for this suite. @ 01/11/24 12:00:21.651
• [4.252 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 01/11/24 12:00:21.665
  Jan 11 12:00:21.665: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename emptydir @ 01/11/24 12:00:21.666
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:00:21.696
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:00:21.701
  STEP: Creating Pod @ 01/11/24 12:00:21.707
  STEP: Reading file content from the nginx-container @ 01/11/24 12:00:23.748
  Jan 11 12:00:23.749: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4281 PodName:pod-sharedvolume-16b70f07-0ad1-4384-8240-f056720f9f2e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 12:00:23.750: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 12:00:23.752: INFO: ExecWithOptions: Clientset creation
  Jan 11 12:00:23.753: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-4281/pods/pod-sharedvolume-16b70f07-0ad1-4384-8240-f056720f9f2e/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Jan 11 12:00:23.879: INFO: Exec stderr: ""
  Jan 11 12:00:23.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4281" for this suite. @ 01/11/24 12:00:23.89
• [2.237 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 01/11/24 12:00:23.902
  Jan 11 12:00:23.902: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename gc @ 01/11/24 12:00:23.904
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:00:23.972
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:00:23.977
  STEP: create the deployment @ 01/11/24 12:00:23.985
  W0111 12:00:24.000860      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 01/11/24 12:00:24
  STEP: delete the deployment @ 01/11/24 12:00:24.022
  STEP: wait for all rs to be garbage collected @ 01/11/24 12:00:24.08
  STEP: expected 0 pods, got 1 pods @ 01/11/24 12:00:24.102
  STEP: Gathering metrics @ 01/11/24 12:00:24.696
  Jan 11 12:00:24.914: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jan 11 12:00:24.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1148" for this suite. @ 01/11/24 12:00:24.94
• [1.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 01/11/24 12:00:24.959
  Jan 11 12:00:24.959: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 12:00:24.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:00:24.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:00:24.995
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-0ae1b5ad-c7d5-4a26-a64a-4e908b76513b @ 01/11/24 12:00:25.011
  STEP: Creating the pod @ 01/11/24 12:00:25.019
  STEP: Updating configmap projected-configmap-test-upd-0ae1b5ad-c7d5-4a26-a64a-4e908b76513b @ 01/11/24 12:00:27.101
  STEP: waiting to observe update in volume @ 01/11/24 12:00:27.118
  Jan 11 12:00:29.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8059" for this suite. @ 01/11/24 12:00:29.168
• [4.226 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 01/11/24 12:00:29.192
  Jan 11 12:00:29.192: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename emptydir @ 01/11/24 12:00:29.193
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:00:29.233
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:00:29.239
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 01/11/24 12:00:29.246
  STEP: Saw pod success @ 01/11/24 12:00:33.305
  Jan 11 12:00:33.313: INFO: Trying to get logs from node env1-test-worker-2 pod pod-8e0997e7-eee9-429f-a218-c477409349f5 container test-container: <nil>
  STEP: delete the pod @ 01/11/24 12:00:33.338
  Jan 11 12:00:33.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9849" for this suite. @ 01/11/24 12:00:33.41
• [4.242 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 01/11/24 12:00:33.44
  Jan 11 12:00:33.441: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename services @ 01/11/24 12:00:33.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:00:33.484
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:00:33.49
  STEP: creating service nodeport-test with type=NodePort in namespace services-6042 @ 01/11/24 12:00:33.496
  STEP: creating replication controller nodeport-test in namespace services-6042 @ 01/11/24 12:00:33.58
  I0111 12:00:33.617560      23 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-6042, replica count: 2
  I0111 12:00:36.668860      23 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 11 12:00:36.668: INFO: Creating new exec pod
  Jan 11 12:00:39.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-6042 exec execpodw8ms8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jan 11 12:00:40.072: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jan 11 12:00:40.072: INFO: stdout: "nodeport-test-gzlwq"
  Jan 11 12:00:40.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-6042 exec execpodw8ms8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.9.246 80'
  Jan 11 12:00:40.390: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.9.246 80\nConnection to 10.233.9.246 80 port [tcp/http] succeeded!\n"
  Jan 11 12:00:40.391: INFO: stdout: ""
  Jan 11 12:00:41.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-6042 exec execpodw8ms8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.9.246 80'
  Jan 11 12:00:41.715: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.9.246 80\nConnection to 10.233.9.246 80 port [tcp/http] succeeded!\n"
  Jan 11 12:00:41.715: INFO: stdout: "nodeport-test-gzlwq"
  Jan 11 12:00:41.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-6042 exec execpodw8ms8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.202 30234'
  Jan 11 12:00:41.992: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.61.1.202 30234\nConnection to 10.61.1.202 30234 port [tcp/*] succeeded!\n"
  Jan 11 12:00:41.992: INFO: stdout: ""
  Jan 11 12:00:42.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-6042 exec execpodw8ms8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.202 30234'
  Jan 11 12:00:43.298: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.61.1.202 30234\nConnection to 10.61.1.202 30234 port [tcp/*] succeeded!\n"
  Jan 11 12:00:43.298: INFO: stdout: "nodeport-test-2vkwj"
  Jan 11 12:00:43.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-6042 exec execpodw8ms8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.200 30234'
  Jan 11 12:00:43.592: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.61.1.200 30234\nConnection to 10.61.1.200 30234 port [tcp/*] succeeded!\n"
  Jan 11 12:00:43.592: INFO: stdout: ""
  Jan 11 12:00:44.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-6042 exec execpodw8ms8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.200 30234'
  Jan 11 12:00:44.896: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.61.1.200 30234\nConnection to 10.61.1.200 30234 port [tcp/*] succeeded!\n"
  Jan 11 12:00:44.896: INFO: stdout: "nodeport-test-2vkwj"
  Jan 11 12:00:44.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6042" for this suite. @ 01/11/24 12:00:44.911
• [11.492 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 01/11/24 12:00:44.933
  Jan 11 12:00:44.933: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename configmap @ 01/11/24 12:00:44.935
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:00:44.972
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:00:44.979
  STEP: Creating configMap with name configmap-test-volume-map-394f463d-61e7-47cb-acf8-82e4f75a580b @ 01/11/24 12:00:44.986
  STEP: Creating a pod to test consume configMaps @ 01/11/24 12:00:44.996
  STEP: Saw pod success @ 01/11/24 12:00:49.048
  Jan 11 12:00:49.065: INFO: Trying to get logs from node env1-test-worker-1 pod pod-configmaps-f2d128f0-46e0-469a-aaa9-63474f60bc25 container agnhost-container: <nil>
  STEP: delete the pod @ 01/11/24 12:00:49.085
  Jan 11 12:00:49.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-221" for this suite. @ 01/11/24 12:00:49.14
• [4.223 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 01/11/24 12:00:49.156
  Jan 11 12:00:49.156: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 12:00:49.158
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:00:49.19
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:00:49.196
  STEP: Creating configMap with name projected-configmap-test-volume-2c145675-788b-4add-b928-f3f3ccb4bcc4 @ 01/11/24 12:00:49.201
  STEP: Creating a pod to test consume configMaps @ 01/11/24 12:00:49.213
  STEP: Saw pod success @ 01/11/24 12:00:53.269
  Jan 11 12:00:53.284: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-configmaps-9cb3296f-d65d-4f14-8789-c0b0ec2a6d85 container agnhost-container: <nil>
  STEP: delete the pod @ 01/11/24 12:00:53.316
  Jan 11 12:00:53.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-56" for this suite. @ 01/11/24 12:00:53.409
• [4.273 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 01/11/24 12:00:53.437
  Jan 11 12:00:53.437: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubectl-logs @ 01/11/24 12:00:53.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:00:53.471
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:00:53.485
  STEP: creating an pod @ 01/11/24 12:00:53.493
  Jan 11 12:00:53.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-logs-9461 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Jan 11 12:00:53.683: INFO: stderr: ""
  Jan 11 12:00:53.683: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 01/11/24 12:00:53.683
  Jan 11 12:00:53.683: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  Jan 11 12:00:55.704: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 01/11/24 12:00:55.704
  Jan 11 12:00:55.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-logs-9461 logs logs-generator logs-generator'
  Jan 11 12:00:55.864: INFO: stderr: ""
  Jan 11 12:00:55.864: INFO: stdout: "I0111 12:00:54.507030       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/7v9 243\nI0111 12:00:54.707496       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/29rj 286\nI0111 12:00:54.907988       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/snk5 302\nI0111 12:00:55.107152       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/4nl 423\nI0111 12:00:55.307608       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/l7j4 301\nI0111 12:00:55.508040       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/7gbf 292\nI0111 12:00:55.707457       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/lgr 207\n"
  STEP: limiting log lines @ 01/11/24 12:00:55.864
  Jan 11 12:00:55.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-logs-9461 logs logs-generator logs-generator --tail=1'
  Jan 11 12:00:56.067: INFO: stderr: ""
  Jan 11 12:00:56.067: INFO: stdout: "I0111 12:00:55.907927       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/hpv9 588\n"
  Jan 11 12:00:56.067: INFO: got output "I0111 12:00:55.907927       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/hpv9 588\n"
  STEP: limiting log bytes @ 01/11/24 12:00:56.067
  Jan 11 12:00:56.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-logs-9461 logs logs-generator logs-generator --limit-bytes=1'
  Jan 11 12:00:56.244: INFO: stderr: ""
  Jan 11 12:00:56.244: INFO: stdout: "I"
  Jan 11 12:00:56.244: INFO: got output "I"
  STEP: exposing timestamps @ 01/11/24 12:00:56.244
  Jan 11 12:00:56.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-logs-9461 logs logs-generator logs-generator --tail=1 --timestamps'
  Jan 11 12:00:56.506: INFO: stderr: ""
  Jan 11 12:00:56.507: INFO: stdout: "2024-01-11T12:00:56.308043834Z I0111 12:00:56.307735       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/dj97 476\n"
  Jan 11 12:00:56.507: INFO: got output "2024-01-11T12:00:56.308043834Z I0111 12:00:56.307735       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/dj97 476\n"
  STEP: restricting to a time range @ 01/11/24 12:00:56.507
  Jan 11 12:00:59.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-logs-9461 logs logs-generator logs-generator --since=1s'
  Jan 11 12:00:59.200: INFO: stderr: ""
  Jan 11 12:00:59.200: INFO: stdout: "I0111 12:00:58.307651       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/dtz 388\nI0111 12:00:58.508188       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/l47 570\nI0111 12:00:58.707674       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/69cp 449\nI0111 12:00:58.907154       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/vf4h 490\nI0111 12:00:59.107254       1 logs_generator.go:76] 23 GET /api/v1/namespaces/kube-system/pods/28zk 233\n"
  Jan 11 12:00:59.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-logs-9461 logs logs-generator logs-generator --since=24h'
  Jan 11 12:00:59.408: INFO: stderr: ""
  Jan 11 12:00:59.408: INFO: stdout: "I0111 12:00:54.507030       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/7v9 243\nI0111 12:00:54.707496       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/29rj 286\nI0111 12:00:54.907988       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/snk5 302\nI0111 12:00:55.107152       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/4nl 423\nI0111 12:00:55.307608       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/l7j4 301\nI0111 12:00:55.508040       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/7gbf 292\nI0111 12:00:55.707457       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/lgr 207\nI0111 12:00:55.907927       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/hpv9 588\nI0111 12:00:56.107180       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/fjt6 542\nI0111 12:00:56.307735       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/dj97 476\nI0111 12:00:56.507174       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/k8v 551\nI0111 12:00:56.707493       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/8t9n 565\nI0111 12:00:56.908041       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/j5lz 213\nI0111 12:00:57.107629       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/xjl 496\nI0111 12:00:57.308068       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/bc97 330\nI0111 12:00:57.507685       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/ph8v 431\nI0111 12:00:57.708110       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/kjq 482\nI0111 12:00:57.907539       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/cxpr 228\nI0111 12:00:58.108063       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/wn4 411\nI0111 12:00:58.307651       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/dtz 388\nI0111 12:00:58.508188       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/l47 570\nI0111 12:00:58.707674       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/69cp 449\nI0111 12:00:58.907154       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/vf4h 490\nI0111 12:00:59.107254       1 logs_generator.go:76] 23 GET /api/v1/namespaces/kube-system/pods/28zk 233\nI0111 12:00:59.307795       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/4cgg 450\n"
  Jan 11 12:00:59.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-logs-9461 delete pod logs-generator'
  Jan 11 12:01:00.188: INFO: stderr: ""
  Jan 11 12:01:00.188: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Jan 11 12:01:00.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-9461" for this suite. @ 01/11/24 12:01:00.205
• [6.788 seconds]
------------------------------
S
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 01/11/24 12:01:00.226
  Jan 11 12:01:00.226: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename prestop @ 01/11/24 12:01:00.227
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:01:00.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:01:00.282
  STEP: Creating server pod server in namespace prestop-7359 @ 01/11/24 12:01:00.294
  STEP: Waiting for pods to come up. @ 01/11/24 12:01:00.325
  STEP: Creating tester pod tester in namespace prestop-7359 @ 01/11/24 12:01:02.364
  STEP: Deleting pre-stop pod @ 01/11/24 12:01:04.395
  Jan 11 12:01:09.434: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Jan 11 12:01:09.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 01/11/24 12:01:09.452
  STEP: Destroying namespace "prestop-7359" for this suite. @ 01/11/24 12:01:09.493
• [9.290 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 01/11/24 12:01:09.517
  Jan 11 12:01:09.517: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename resourcequota @ 01/11/24 12:01:09.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:01:09.557
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:01:09.571
  STEP: Counting existing ResourceQuota @ 01/11/24 12:01:26.594
  STEP: Creating a ResourceQuota @ 01/11/24 12:01:31.602
  STEP: Ensuring resource quota status is calculated @ 01/11/24 12:01:31.611
  STEP: Creating a ConfigMap @ 01/11/24 12:01:33.622
  STEP: Ensuring resource quota status captures configMap creation @ 01/11/24 12:01:33.674
  STEP: Deleting a ConfigMap @ 01/11/24 12:01:35.686
  STEP: Ensuring resource quota status released usage @ 01/11/24 12:01:35.712
  Jan 11 12:01:37.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2436" for this suite. @ 01/11/24 12:01:37.738
• [28.240 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 01/11/24 12:01:37.76
  Jan 11 12:01:37.760: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename init-container @ 01/11/24 12:01:37.762
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:01:37.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:01:37.811
  STEP: creating the pod @ 01/11/24 12:01:37.817
  Jan 11 12:01:37.818: INFO: PodSpec: initContainers in spec.initContainers
  Jan 11 12:01:41.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-8976" for this suite. @ 01/11/24 12:01:41.422
• [3.690 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 01/11/24 12:01:41.455
  Jan 11 12:01:41.455: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename discovery @ 01/11/24 12:01:41.457
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:01:41.516
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:01:41.525
  STEP: Setting up server cert @ 01/11/24 12:01:41.534
  Jan 11 12:01:42.622: INFO: Checking APIGroup: apiregistration.k8s.io
  Jan 11 12:01:42.624: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Jan 11 12:01:42.625: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Jan 11 12:01:42.625: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Jan 11 12:01:42.625: INFO: Checking APIGroup: apps
  Jan 11 12:01:42.627: INFO: PreferredVersion.GroupVersion: apps/v1
  Jan 11 12:01:42.627: INFO: Versions found [{apps/v1 v1}]
  Jan 11 12:01:42.627: INFO: apps/v1 matches apps/v1
  Jan 11 12:01:42.627: INFO: Checking APIGroup: events.k8s.io
  Jan 11 12:01:42.629: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Jan 11 12:01:42.630: INFO: Versions found [{events.k8s.io/v1 v1}]
  Jan 11 12:01:42.630: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Jan 11 12:01:42.630: INFO: Checking APIGroup: authentication.k8s.io
  Jan 11 12:01:42.632: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Jan 11 12:01:42.632: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Jan 11 12:01:42.632: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Jan 11 12:01:42.632: INFO: Checking APIGroup: authorization.k8s.io
  Jan 11 12:01:42.634: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Jan 11 12:01:42.634: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Jan 11 12:01:42.634: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Jan 11 12:01:42.634: INFO: Checking APIGroup: autoscaling
  Jan 11 12:01:42.637: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Jan 11 12:01:42.637: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Jan 11 12:01:42.637: INFO: autoscaling/v2 matches autoscaling/v2
  Jan 11 12:01:42.637: INFO: Checking APIGroup: batch
  Jan 11 12:01:42.640: INFO: PreferredVersion.GroupVersion: batch/v1
  Jan 11 12:01:42.640: INFO: Versions found [{batch/v1 v1}]
  Jan 11 12:01:42.640: INFO: batch/v1 matches batch/v1
  Jan 11 12:01:42.640: INFO: Checking APIGroup: certificates.k8s.io
  Jan 11 12:01:42.642: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Jan 11 12:01:42.642: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Jan 11 12:01:42.642: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Jan 11 12:01:42.642: INFO: Checking APIGroup: networking.k8s.io
  Jan 11 12:01:42.644: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Jan 11 12:01:42.644: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Jan 11 12:01:42.645: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Jan 11 12:01:42.645: INFO: Checking APIGroup: policy
  Jan 11 12:01:42.646: INFO: PreferredVersion.GroupVersion: policy/v1
  Jan 11 12:01:42.646: INFO: Versions found [{policy/v1 v1}]
  Jan 11 12:01:42.646: INFO: policy/v1 matches policy/v1
  Jan 11 12:01:42.646: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Jan 11 12:01:42.648: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Jan 11 12:01:42.648: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Jan 11 12:01:42.649: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Jan 11 12:01:42.649: INFO: Checking APIGroup: storage.k8s.io
  Jan 11 12:01:42.653: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Jan 11 12:01:42.653: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Jan 11 12:01:42.653: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Jan 11 12:01:42.653: INFO: Checking APIGroup: admissionregistration.k8s.io
  Jan 11 12:01:42.654: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Jan 11 12:01:42.655: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Jan 11 12:01:42.656: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Jan 11 12:01:42.656: INFO: Checking APIGroup: apiextensions.k8s.io
  Jan 11 12:01:42.659: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Jan 11 12:01:42.659: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Jan 11 12:01:42.660: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Jan 11 12:01:42.661: INFO: Checking APIGroup: scheduling.k8s.io
  Jan 11 12:01:42.664: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Jan 11 12:01:42.664: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Jan 11 12:01:42.664: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Jan 11 12:01:42.664: INFO: Checking APIGroup: coordination.k8s.io
  Jan 11 12:01:42.666: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Jan 11 12:01:42.667: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Jan 11 12:01:42.667: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Jan 11 12:01:42.667: INFO: Checking APIGroup: node.k8s.io
  Jan 11 12:01:42.670: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Jan 11 12:01:42.670: INFO: Versions found [{node.k8s.io/v1 v1}]
  Jan 11 12:01:42.671: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Jan 11 12:01:42.671: INFO: Checking APIGroup: discovery.k8s.io
  Jan 11 12:01:42.674: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Jan 11 12:01:42.674: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Jan 11 12:01:42.674: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Jan 11 12:01:42.674: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Jan 11 12:01:42.677: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Jan 11 12:01:42.677: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Jan 11 12:01:42.677: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Jan 11 12:01:42.678: INFO: Checking APIGroup: monitoring.coreos.com
  Jan 11 12:01:42.681: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
  Jan 11 12:01:42.681: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
  Jan 11 12:01:42.681: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
  Jan 11 12:01:42.681: INFO: Checking APIGroup: velero.io
  Jan 11 12:01:42.683: INFO: PreferredVersion.GroupVersion: velero.io/v1
  Jan 11 12:01:42.684: INFO: Versions found [{velero.io/v1 v1}]
  Jan 11 12:01:42.684: INFO: velero.io/v1 matches velero.io/v1
  Jan 11 12:01:42.685: INFO: Checking APIGroup: cns.vmware.com
  Jan 11 12:01:42.688: INFO: PreferredVersion.GroupVersion: cns.vmware.com/v1alpha1
  Jan 11 12:01:42.688: INFO: Versions found [{cns.vmware.com/v1alpha1 v1alpha1}]
  Jan 11 12:01:42.689: INFO: cns.vmware.com/v1alpha1 matches cns.vmware.com/v1alpha1
  Jan 11 12:01:42.689: INFO: Checking APIGroup: traefik.containo.us
  Jan 11 12:01:42.692: INFO: PreferredVersion.GroupVersion: traefik.containo.us/v1alpha1
  Jan 11 12:01:42.692: INFO: Versions found [{traefik.containo.us/v1alpha1 v1alpha1}]
  Jan 11 12:01:42.692: INFO: traefik.containo.us/v1alpha1 matches traefik.containo.us/v1alpha1
  Jan 11 12:01:42.692: INFO: Checking APIGroup: metrics.k8s.io
  Jan 11 12:01:42.694: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  Jan 11 12:01:42.694: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  Jan 11 12:01:42.694: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  Jan 11 12:01:42.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-9918" for this suite. @ 01/11/24 12:01:42.711
• [1.275 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 01/11/24 12:01:42.731
  Jan 11 12:01:42.732: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename runtimeclass @ 01/11/24 12:01:42.733
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:01:42.757
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:01:42.765
  Jan 11 12:01:44.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8109" for this suite. @ 01/11/24 12:01:44.869
• [2.154 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 01/11/24 12:01:44.887
  Jan 11 12:01:44.887: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename pods @ 01/11/24 12:01:44.89
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:01:44.923
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:01:44.93
  STEP: Create a pod @ 01/11/24 12:01:44.937
  STEP: patching /status @ 01/11/24 12:01:46.989
  Jan 11 12:01:47.012: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Jan 11 12:01:47.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9980" for this suite. @ 01/11/24 12:01:47.034
• [2.181 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 01/11/24 12:01:47.069
  Jan 11 12:01:47.069: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename namespaces @ 01/11/24 12:01:47.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:01:47.125
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:01:47.133
  STEP: creating a Namespace @ 01/11/24 12:01:47.146
  STEP: patching the Namespace @ 01/11/24 12:01:47.191
  STEP: get the Namespace and ensuring it has the label @ 01/11/24 12:01:47.215
  Jan 11 12:01:47.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3505" for this suite. @ 01/11/24 12:01:47.264
  STEP: Destroying namespace "nspatchtest-89f1a329-ff02-40cb-a86f-9dfe868db208-3227" for this suite. @ 01/11/24 12:01:47.287
• [0.238 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:443
  STEP: Creating a kubernetes client @ 01/11/24 12:01:47.31
  Jan 11 12:01:47.310: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename daemonsets @ 01/11/24 12:01:47.312
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:01:47.381
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:01:47.391
  Jan 11 12:01:47.505: INFO: Create a RollingUpdate DaemonSet
  Jan 11 12:01:47.532: INFO: Check that daemon pods launch on every node of the cluster
  Jan 11 12:01:47.571: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:47.571: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:47.572: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:47.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 12:01:47.594: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  Jan 11 12:01:48.610: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:48.610: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:48.610: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:48.620: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 12:01:48.620: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  Jan 11 12:01:49.611: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:49.611: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:49.612: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:49.622: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jan 11 12:01:49.622: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Jan 11 12:01:49.622: INFO: Update the DaemonSet to trigger a rollout
  Jan 11 12:01:49.678: INFO: Updating DaemonSet daemon-set
  Jan 11 12:01:51.734: INFO: Roll back the DaemonSet before rollout is complete
  Jan 11 12:01:51.756: INFO: Updating DaemonSet daemon-set
  Jan 11 12:01:51.756: INFO: Make sure DaemonSet rollback is complete
  Jan 11 12:01:51.773: INFO: Wrong image for pod: daemon-set-rnszj. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Jan 11 12:01:51.773: INFO: Pod daemon-set-rnszj is not available
  Jan 11 12:01:51.791: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:51.792: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:51.793: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:52.824: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:52.825: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:52.826: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:53.817: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:53.817: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:53.817: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:54.815: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:54.817: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:54.817: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:55.825: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:55.825: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:55.825: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:56.809: INFO: Pod daemon-set-zzkwp is not available
  Jan 11 12:01:56.827: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:56.827: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:01:56.827: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 01/11/24 12:01:56.856
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8461, will wait for the garbage collector to delete the pods @ 01/11/24 12:01:56.857
  Jan 11 12:01:56.961: INFO: Deleting DaemonSet.extensions daemon-set took: 32.715354ms
  Jan 11 12:01:57.161: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.772023ms
  Jan 11 12:01:59.373: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 12:01:59.373: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jan 11 12:01:59.381: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"187168528"},"items":null}

  Jan 11 12:01:59.389: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"187168528"},"items":null}

  Jan 11 12:01:59.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8461" for this suite. @ 01/11/24 12:01:59.457
• [12.165 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 01/11/24 12:01:59.482
  Jan 11 12:01:59.482: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename dns @ 01/11/24 12:01:59.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:01:59.526
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:01:59.533
  STEP: Creating a test headless service @ 01/11/24 12:01:59.539
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9573 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9573;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9573 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9573;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9573.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9573.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9573.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9573.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9573.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9573.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9573.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9573.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9573.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9573.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9573.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9573.svc;check="$$(dig +notcp +noall +answer +search 161.40.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.40.161_udp@PTR;check="$$(dig +tcp +noall +answer +search 161.40.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.40.161_tcp@PTR;sleep 1; done
   @ 01/11/24 12:01:59.61
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9573 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9573;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9573 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9573;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9573.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9573.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9573.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9573.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9573.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9573.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9573.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9573.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9573.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9573.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9573.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9573.svc;check="$$(dig +notcp +noall +answer +search 161.40.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.40.161_udp@PTR;check="$$(dig +tcp +noall +answer +search 161.40.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.40.161_tcp@PTR;sleep 1; done
   @ 01/11/24 12:01:59.61
  STEP: creating a pod to probe DNS @ 01/11/24 12:01:59.611
  STEP: submitting the pod to kubernetes @ 01/11/24 12:01:59.611
  STEP: retrieving the pod @ 01/11/24 12:02:01.679
  STEP: looking for the results for each expected name from probers @ 01/11/24 12:02:01.688
  Jan 11 12:02:01.703: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9573/dns-test-489f704e-9567-4f76-af88-09328361fe6a: the server could not find the requested resource (get pods dns-test-489f704e-9567-4f76-af88-09328361fe6a)
  Jan 11 12:02:01.715: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9573/dns-test-489f704e-9567-4f76-af88-09328361fe6a: the server could not find the requested resource (get pods dns-test-489f704e-9567-4f76-af88-09328361fe6a)
  Jan 11 12:02:01.727: INFO: Unable to read wheezy_udp@dns-test-service.dns-9573 from pod dns-9573/dns-test-489f704e-9567-4f76-af88-09328361fe6a: the server could not find the requested resource (get pods dns-test-489f704e-9567-4f76-af88-09328361fe6a)
  Jan 11 12:02:01.736: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9573 from pod dns-9573/dns-test-489f704e-9567-4f76-af88-09328361fe6a: the server could not find the requested resource (get pods dns-test-489f704e-9567-4f76-af88-09328361fe6a)
  Jan 11 12:02:01.746: INFO: Unable to read wheezy_udp@dns-test-service.dns-9573.svc from pod dns-9573/dns-test-489f704e-9567-4f76-af88-09328361fe6a: the server could not find the requested resource (get pods dns-test-489f704e-9567-4f76-af88-09328361fe6a)
  Jan 11 12:02:01.755: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9573.svc from pod dns-9573/dns-test-489f704e-9567-4f76-af88-09328361fe6a: the server could not find the requested resource (get pods dns-test-489f704e-9567-4f76-af88-09328361fe6a)
  Jan 11 12:02:01.763: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9573.svc from pod dns-9573/dns-test-489f704e-9567-4f76-af88-09328361fe6a: the server could not find the requested resource (get pods dns-test-489f704e-9567-4f76-af88-09328361fe6a)
  Jan 11 12:02:01.786: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9573.svc from pod dns-9573/dns-test-489f704e-9567-4f76-af88-09328361fe6a: the server could not find the requested resource (get pods dns-test-489f704e-9567-4f76-af88-09328361fe6a)
  Jan 11 12:02:01.840: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9573/dns-test-489f704e-9567-4f76-af88-09328361fe6a: the server could not find the requested resource (get pods dns-test-489f704e-9567-4f76-af88-09328361fe6a)
  Jan 11 12:02:01.851: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9573/dns-test-489f704e-9567-4f76-af88-09328361fe6a: the server could not find the requested resource (get pods dns-test-489f704e-9567-4f76-af88-09328361fe6a)
  Jan 11 12:02:01.864: INFO: Unable to read jessie_udp@dns-test-service.dns-9573 from pod dns-9573/dns-test-489f704e-9567-4f76-af88-09328361fe6a: the server could not find the requested resource (get pods dns-test-489f704e-9567-4f76-af88-09328361fe6a)
  Jan 11 12:02:01.873: INFO: Unable to read jessie_tcp@dns-test-service.dns-9573 from pod dns-9573/dns-test-489f704e-9567-4f76-af88-09328361fe6a: the server could not find the requested resource (get pods dns-test-489f704e-9567-4f76-af88-09328361fe6a)
  Jan 11 12:02:01.900: INFO: Unable to read jessie_udp@dns-test-service.dns-9573.svc from pod dns-9573/dns-test-489f704e-9567-4f76-af88-09328361fe6a: the server could not find the requested resource (get pods dns-test-489f704e-9567-4f76-af88-09328361fe6a)
  Jan 11 12:02:01.916: INFO: Unable to read jessie_tcp@dns-test-service.dns-9573.svc from pod dns-9573/dns-test-489f704e-9567-4f76-af88-09328361fe6a: the server could not find the requested resource (get pods dns-test-489f704e-9567-4f76-af88-09328361fe6a)
  Jan 11 12:02:01.932: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9573.svc from pod dns-9573/dns-test-489f704e-9567-4f76-af88-09328361fe6a: the server could not find the requested resource (get pods dns-test-489f704e-9567-4f76-af88-09328361fe6a)
  Jan 11 12:02:01.941: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9573.svc from pod dns-9573/dns-test-489f704e-9567-4f76-af88-09328361fe6a: the server could not find the requested resource (get pods dns-test-489f704e-9567-4f76-af88-09328361fe6a)
  Jan 11 12:02:01.979: INFO: Lookups using dns-9573/dns-test-489f704e-9567-4f76-af88-09328361fe6a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9573 wheezy_tcp@dns-test-service.dns-9573 wheezy_udp@dns-test-service.dns-9573.svc wheezy_tcp@dns-test-service.dns-9573.svc wheezy_udp@_http._tcp.dns-test-service.dns-9573.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9573.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9573 jessie_tcp@dns-test-service.dns-9573 jessie_udp@dns-test-service.dns-9573.svc jessie_tcp@dns-test-service.dns-9573.svc jessie_udp@_http._tcp.dns-test-service.dns-9573.svc jessie_tcp@_http._tcp.dns-test-service.dns-9573.svc]

  Jan 11 12:02:07.245: INFO: DNS probes using dns-9573/dns-test-489f704e-9567-4f76-af88-09328361fe6a succeeded

  Jan 11 12:02:07.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/11/24 12:02:07.262
  STEP: deleting the test service @ 01/11/24 12:02:07.315
  STEP: deleting the test headless service @ 01/11/24 12:02:07.412
  STEP: Destroying namespace "dns-9573" for this suite. @ 01/11/24 12:02:07.468
• [8.021 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 01/11/24 12:02:07.506
  Jan 11 12:02:07.506: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename job @ 01/11/24 12:02:07.508
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:02:07.563
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:02:07.582
  STEP: Creating a job @ 01/11/24 12:02:07.592
  STEP: Ensuring job reaches completions @ 01/11/24 12:02:07.614
  Jan 11 12:02:19.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6186" for this suite. @ 01/11/24 12:02:19.654
• [12.165 seconds]
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 01/11/24 12:02:19.671
  Jan 11 12:02:19.671: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename containers @ 01/11/24 12:02:19.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:02:19.708
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:02:19.716
  STEP: Creating a pod to test override all @ 01/11/24 12:02:19.724
  STEP: Saw pod success @ 01/11/24 12:02:23.829
  Jan 11 12:02:23.836: INFO: Trying to get logs from node env1-test-worker-1 pod client-containers-ee99c719-8da6-4c34-ac37-c65d61accf1c container agnhost-container: <nil>
  STEP: delete the pod @ 01/11/24 12:02:23.86
  Jan 11 12:02:23.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8916" for this suite. @ 01/11/24 12:02:23.915
• [4.265 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 01/11/24 12:02:23.94
  Jan 11 12:02:23.940: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-probe @ 01/11/24 12:02:23.942
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:02:23.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:02:24.002
  STEP: Creating pod busybox-ecb694cb-fe98-4ede-a912-b3fe95a26c46 in namespace container-probe-4441 @ 01/11/24 12:02:24.007
  Jan 11 12:02:26.056: INFO: Started pod busybox-ecb694cb-fe98-4ede-a912-b3fe95a26c46 in namespace container-probe-4441
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/11/24 12:02:26.056
  Jan 11 12:02:26.066: INFO: Initial restart count of pod busybox-ecb694cb-fe98-4ede-a912-b3fe95a26c46 is 0
  Jan 11 12:03:16.351: INFO: Restart count of pod container-probe-4441/busybox-ecb694cb-fe98-4ede-a912-b3fe95a26c46 is now 1 (50.284850039s elapsed)
  Jan 11 12:03:16.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/11/24 12:03:16.367
  STEP: Destroying namespace "container-probe-4441" for this suite. @ 01/11/24 12:03:16.398
• [52.477 seconds]
------------------------------
SSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 01/11/24 12:03:16.417
  Jan 11 12:03:16.417: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename secrets @ 01/11/24 12:03:16.419
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:03:16.489
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:03:16.495
  STEP: Creating projection with secret that has name secret-emptykey-test-e5f23de2-a29b-42a5-bbe4-cc2174f3956f @ 01/11/24 12:03:16.501
  Jan 11 12:03:16.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4460" for this suite. @ 01/11/24 12:03:16.521
• [0.131 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 01/11/24 12:03:16.549
  Jan 11 12:03:16.550: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename configmap @ 01/11/24 12:03:16.551
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:03:16.593
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:03:16.602
  STEP: Creating configMap with name configmap-test-volume-b41c663f-10ee-4af6-af20-0fb27116c16a @ 01/11/24 12:03:16.619
  STEP: Creating a pod to test consume configMaps @ 01/11/24 12:03:16.632
  STEP: Saw pod success @ 01/11/24 12:03:20.693
  Jan 11 12:03:20.711: INFO: Trying to get logs from node env1-test-worker-1 pod pod-configmaps-aae47f7c-07a3-47c8-b78b-1d481373e2e4 container agnhost-container: <nil>
  STEP: delete the pod @ 01/11/24 12:03:20.733
  Jan 11 12:03:20.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9794" for this suite. @ 01/11/24 12:03:20.792
• [4.261 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 01/11/24 12:03:20.817
  Jan 11 12:03:20.817: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename emptydir @ 01/11/24 12:03:20.819
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:03:20.851
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:03:20.861
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 01/11/24 12:03:20.87
  STEP: Saw pod success @ 01/11/24 12:03:24.937
  Jan 11 12:03:24.955: INFO: Trying to get logs from node env1-test-worker-1 pod pod-08efd706-017b-4f20-80e2-6ef09bac6143 container test-container: <nil>
  STEP: delete the pod @ 01/11/24 12:03:24.99
  Jan 11 12:03:25.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2892" for this suite. @ 01/11/24 12:03:25.054
• [4.252 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 01/11/24 12:03:25.073
  Jan 11 12:03:25.073: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename cronjob @ 01/11/24 12:03:25.075
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:03:25.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:03:25.134
  STEP: Creating a ForbidConcurrent cronjob @ 01/11/24 12:03:25.142
  STEP: Ensuring a job is scheduled @ 01/11/24 12:03:25.169
  STEP: Ensuring exactly one is scheduled @ 01/11/24 12:04:01.18
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 01/11/24 12:04:01.189
  STEP: Ensuring no more jobs are scheduled @ 01/11/24 12:04:01.201
  STEP: Removing cronjob @ 01/11/24 12:09:01.222
  Jan 11 12:09:01.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-1294" for this suite. @ 01/11/24 12:09:01.261
• [336.208 seconds]
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 01/11/24 12:09:01.282
  Jan 11 12:09:01.282: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename crd-watch @ 01/11/24 12:09:01.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:09:01.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:09:01.387
  Jan 11 12:09:01.398: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Creating first CR  @ 01/11/24 12:09:09.037
  Jan 11 12:09:09.118: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-11T12:09:09Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-11T12:09:09Z]] name:name1 resourceVersion:187170655 uid:a99d130c-f70b-43ba-9995-f38f393a076d] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Creating second CR @ 01/11/24 12:09:19.12
  Jan 11 12:09:19.144: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-11T12:09:19Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-11T12:09:19Z]] name:name2 resourceVersion:187170697 uid:a2c75aa3-e59c-4e2f-a445-b03e2a85e26d] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying first CR @ 01/11/24 12:09:29.145
  Jan 11 12:09:29.161: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-11T12:09:09Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-11T12:09:29Z]] name:name1 resourceVersion:187170738 uid:a99d130c-f70b-43ba-9995-f38f393a076d] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying second CR @ 01/11/24 12:09:39.161
  Jan 11 12:09:39.187: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-11T12:09:19Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-11T12:09:39Z]] name:name2 resourceVersion:187170780 uid:a2c75aa3-e59c-4e2f-a445-b03e2a85e26d] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting first CR @ 01/11/24 12:09:49.19
  Jan 11 12:09:49.211: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-11T12:09:09Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-11T12:09:29Z]] name:name1 resourceVersion:187170823 uid:a99d130c-f70b-43ba-9995-f38f393a076d] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting second CR @ 01/11/24 12:09:59.212
  Jan 11 12:09:59.239: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-11T12:09:19Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-11T12:09:39Z]] name:name2 resourceVersion:187170864 uid:a2c75aa3-e59c-4e2f-a445-b03e2a85e26d] num:map[num1:9223372036854775807 num2:1000000]]}
  Jan 11 12:10:09.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-9830" for this suite. @ 01/11/24 12:10:09.799
• [68.536 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 01/11/24 12:10:09.821
  Jan 11 12:10:09.821: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename var-expansion @ 01/11/24 12:10:09.824
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:10:09.856
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:10:09.865
  STEP: creating the pod @ 01/11/24 12:10:09.873
  STEP: waiting for pod running @ 01/11/24 12:10:09.904
  STEP: creating a file in subpath @ 01/11/24 12:10:11.946
  Jan 11 12:10:11.955: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2875 PodName:var-expansion-3f848e1e-fbe2-4305-b372-9e844d63b738 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 12:10:11.955: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 12:10:11.956: INFO: ExecWithOptions: Clientset creation
  Jan 11 12:10:11.956: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-2875/pods/var-expansion-3f848e1e-fbe2-4305-b372-9e844d63b738/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 01/11/24 12:10:12.104
  Jan 11 12:10:12.113: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2875 PodName:var-expansion-3f848e1e-fbe2-4305-b372-9e844d63b738 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 12:10:12.113: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 12:10:12.114: INFO: ExecWithOptions: Clientset creation
  Jan 11 12:10:12.114: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-2875/pods/var-expansion-3f848e1e-fbe2-4305-b372-9e844d63b738/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 01/11/24 12:10:12.246
  Jan 11 12:10:12.784: INFO: Successfully updated pod "var-expansion-3f848e1e-fbe2-4305-b372-9e844d63b738"
  STEP: waiting for annotated pod running @ 01/11/24 12:10:12.784
  STEP: deleting the pod gracefully @ 01/11/24 12:10:12.791
  Jan 11 12:10:12.791: INFO: Deleting pod "var-expansion-3f848e1e-fbe2-4305-b372-9e844d63b738" in namespace "var-expansion-2875"
  Jan 11 12:10:12.809: INFO: Wait up to 5m0s for pod "var-expansion-3f848e1e-fbe2-4305-b372-9e844d63b738" to be fully deleted
  Jan 11 12:10:45.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2875" for this suite. @ 01/11/24 12:10:45.06
• [35.255 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 01/11/24 12:10:45.078
  Jan 11 12:10:45.078: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 12:10:45.08
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:10:45.124
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:10:45.131
  STEP: Creating configMap with name projected-configmap-test-volume-949c6bc9-30b6-40d9-b209-3d849b21ac88 @ 01/11/24 12:10:45.137
  STEP: Creating a pod to test consume configMaps @ 01/11/24 12:10:45.151
  STEP: Saw pod success @ 01/11/24 12:10:49.228
  Jan 11 12:10:49.235: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-configmaps-fdd597f9-0a9f-403a-8dc4-8255e305b87c container agnhost-container: <nil>
  STEP: delete the pod @ 01/11/24 12:10:49.285
  Jan 11 12:10:49.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5156" for this suite. @ 01/11/24 12:10:49.359
• [4.297 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 01/11/24 12:10:49.376
  Jan 11 12:10:49.376: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename services @ 01/11/24 12:10:49.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:10:49.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:10:49.424
  Jan 11 12:10:49.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7329" for this suite. @ 01/11/24 12:10:49.45
• [0.091 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 01/11/24 12:10:49.473
  Jan 11 12:10:49.473: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename statefulset @ 01/11/24 12:10:49.475
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:10:49.521
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:10:49.528
  STEP: Creating service test in namespace statefulset-4622 @ 01/11/24 12:10:49.535
  STEP: Creating a new StatefulSet @ 01/11/24 12:10:49.559
  Jan 11 12:10:49.609: INFO: Found 0 stateful pods, waiting for 3
  Jan 11 12:10:59.630: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 11 12:10:59.630: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jan 11 12:10:59.630: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 01/11/24 12:10:59.684
  Jan 11 12:10:59.725: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 01/11/24 12:10:59.725
  STEP: Not applying an update when the partition is greater than the number of replicas @ 01/11/24 12:11:09.769
  STEP: Performing a canary update @ 01/11/24 12:11:09.769
  Jan 11 12:11:09.801: INFO: Updating stateful set ss2
  Jan 11 12:11:09.839: INFO: Waiting for Pod statefulset-4622/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 01/11/24 12:11:19.868
  Jan 11 12:11:20.067: INFO: Found 1 stateful pods, waiting for 3
  Jan 11 12:11:30.082: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 11 12:11:30.082: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jan 11 12:11:30.082: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 01/11/24 12:11:30.104
  Jan 11 12:11:30.147: INFO: Updating stateful set ss2
  Jan 11 12:11:30.199: INFO: Waiting for Pod statefulset-4622/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jan 11 12:11:40.269: INFO: Updating stateful set ss2
  Jan 11 12:11:40.308: INFO: Waiting for StatefulSet statefulset-4622/ss2 to complete update
  Jan 11 12:11:40.308: INFO: Waiting for Pod statefulset-4622/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jan 11 12:11:50.329: INFO: Deleting all statefulset in ns statefulset-4622
  Jan 11 12:11:50.336: INFO: Scaling statefulset ss2 to 0
  Jan 11 12:12:00.402: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 11 12:12:00.413: INFO: Deleting statefulset ss2
  Jan 11 12:12:00.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4622" for this suite. @ 01/11/24 12:12:00.496
• [71.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 01/11/24 12:12:00.523
  Jan 11 12:12:00.523: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename webhook @ 01/11/24 12:12:00.525
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:12:00.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:12:00.565
  STEP: Setting up server cert @ 01/11/24 12:12:00.621
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/11/24 12:12:01.414
  STEP: Deploying the webhook pod @ 01/11/24 12:12:01.456
  STEP: Wait for the deployment to be ready @ 01/11/24 12:12:01.507
  Jan 11 12:12:01.542: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 01/11/24 12:12:03.582
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 12:12:03.611
  Jan 11 12:12:04.611: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 01/11/24 12:12:04.781
  STEP: Creating a configMap that should be mutated @ 01/11/24 12:12:04.818
  STEP: Deleting the collection of validation webhooks @ 01/11/24 12:12:04.894
  STEP: Creating a configMap that should not be mutated @ 01/11/24 12:12:05.082
  Jan 11 12:12:05.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8379" for this suite. @ 01/11/24 12:12:05.307
  STEP: Destroying namespace "webhook-markers-3964" for this suite. @ 01/11/24 12:12:05.357
• [4.868 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 01/11/24 12:12:05.392
  Jan 11 12:12:05.392: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename secrets @ 01/11/24 12:12:05.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:12:05.446
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:12:05.456
  STEP: Creating secret with name secret-test-map-f51df368-2279-4738-8c27-0a911441dd99 @ 01/11/24 12:12:05.464
  STEP: Creating a pod to test consume secrets @ 01/11/24 12:12:05.473
  STEP: Saw pod success @ 01/11/24 12:12:09.542
  Jan 11 12:12:09.551: INFO: Trying to get logs from node env1-test-worker-1 pod pod-secrets-f61016fd-3ef7-4af2-90f6-01a290ff0d5c container secret-volume-test: <nil>
  STEP: delete the pod @ 01/11/24 12:12:09.573
  Jan 11 12:12:09.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4397" for this suite. @ 01/11/24 12:12:09.664
• [4.294 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 01/11/24 12:12:09.702
  Jan 11 12:12:09.702: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename cronjob @ 01/11/24 12:12:09.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:12:09.755
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:12:09.762
  STEP: Creating a cronjob @ 01/11/24 12:12:09.769
  STEP: creating @ 01/11/24 12:12:09.769
  STEP: getting @ 01/11/24 12:12:09.784
  STEP: listing @ 01/11/24 12:12:09.792
  STEP: watching @ 01/11/24 12:12:09.8
  Jan 11 12:12:09.801: INFO: starting watch
  STEP: cluster-wide listing @ 01/11/24 12:12:09.804
  STEP: cluster-wide watching @ 01/11/24 12:12:09.813
  Jan 11 12:12:09.813: INFO: starting watch
  STEP: patching @ 01/11/24 12:12:09.816
  STEP: updating @ 01/11/24 12:12:09.849
  Jan 11 12:12:09.869: INFO: waiting for watch events with expected annotations
  Jan 11 12:12:09.869: INFO: saw patched and updated annotations
  STEP: patching /status @ 01/11/24 12:12:09.869
  STEP: updating /status @ 01/11/24 12:12:09.889
  STEP: get /status @ 01/11/24 12:12:09.91
  STEP: deleting @ 01/11/24 12:12:09.918
  STEP: deleting a collection @ 01/11/24 12:12:09.961
  Jan 11 12:12:09.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7877" for this suite. @ 01/11/24 12:12:10.017
• [0.335 seconds]
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 01/11/24 12:12:10.037
  Jan 11 12:12:10.037: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubectl @ 01/11/24 12:12:10.039
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:12:10.074
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:12:10.081
  STEP: creating a replication controller @ 01/11/24 12:12:10.09
  Jan 11 12:12:10.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1469 create -f -'
  Jan 11 12:12:13.401: INFO: stderr: ""
  Jan 11 12:12:13.401: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 01/11/24 12:12:13.401
  Jan 11 12:12:13.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1469 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 11 12:12:13.618: INFO: stderr: ""
  Jan 11 12:12:13.618: INFO: stdout: "update-demo-nautilus-8c4g8 update-demo-nautilus-94sbb "
  Jan 11 12:12:13.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1469 get pods update-demo-nautilus-8c4g8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 11 12:12:13.821: INFO: stderr: ""
  Jan 11 12:12:13.821: INFO: stdout: ""
  Jan 11 12:12:13.821: INFO: update-demo-nautilus-8c4g8 is created but not running
  Jan 11 12:12:18.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1469 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 11 12:12:19.011: INFO: stderr: ""
  Jan 11 12:12:19.011: INFO: stdout: "update-demo-nautilus-8c4g8 update-demo-nautilus-94sbb "
  Jan 11 12:12:19.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1469 get pods update-demo-nautilus-8c4g8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 11 12:12:19.197: INFO: stderr: ""
  Jan 11 12:12:19.197: INFO: stdout: "true"
  Jan 11 12:12:19.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1469 get pods update-demo-nautilus-8c4g8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 11 12:12:19.386: INFO: stderr: ""
  Jan 11 12:12:19.386: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 11 12:12:19.386: INFO: validating pod update-demo-nautilus-8c4g8
  Jan 11 12:12:19.402: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 11 12:12:19.402: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 11 12:12:19.402: INFO: update-demo-nautilus-8c4g8 is verified up and running
  Jan 11 12:12:19.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1469 get pods update-demo-nautilus-94sbb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 11 12:12:19.576: INFO: stderr: ""
  Jan 11 12:12:19.576: INFO: stdout: "true"
  Jan 11 12:12:19.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1469 get pods update-demo-nautilus-94sbb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 11 12:12:19.771: INFO: stderr: ""
  Jan 11 12:12:19.772: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 11 12:12:19.772: INFO: validating pod update-demo-nautilus-94sbb
  Jan 11 12:12:19.792: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 11 12:12:19.792: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 11 12:12:19.792: INFO: update-demo-nautilus-94sbb is verified up and running
  STEP: using delete to clean up resources @ 01/11/24 12:12:19.793
  Jan 11 12:12:19.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1469 delete --grace-period=0 --force -f -'
  Jan 11 12:12:19.992: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 11 12:12:19.992: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jan 11 12:12:19.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1469 get rc,svc -l name=update-demo --no-headers'
  Jan 11 12:12:20.201: INFO: stderr: "No resources found in kubectl-1469 namespace.\n"
  Jan 11 12:12:20.202: INFO: stdout: ""
  Jan 11 12:12:20.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1469 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jan 11 12:12:20.391: INFO: stderr: ""
  Jan 11 12:12:20.391: INFO: stdout: ""
  Jan 11 12:12:20.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1469" for this suite. @ 01/11/24 12:12:20.406
• [10.405 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:205
  STEP: Creating a kubernetes client @ 01/11/24 12:12:20.443
  Jan 11 12:12:20.443: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename daemonsets @ 01/11/24 12:12:20.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:12:20.49
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:12:20.496
  Jan 11 12:12:20.561: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 01/11/24 12:12:20.582
  Jan 11 12:12:20.591: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 12:12:20.591: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 01/11/24 12:12:20.591
  Jan 11 12:12:20.702: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 12:12:20.703: INFO: Node env1-test-worker-2 is running 0 daemon pod, expected 1
  Jan 11 12:12:21.714: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 12:12:21.715: INFO: Node env1-test-worker-2 is running 0 daemon pod, expected 1
  Jan 11 12:12:22.716: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 11 12:12:22.718: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 01/11/24 12:12:22.733
  Jan 11 12:12:22.802: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 11 12:12:22.802: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  Jan 11 12:12:23.812: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 12:12:23.813: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 01/11/24 12:12:23.813
  Jan 11 12:12:23.856: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 12:12:23.856: INFO: Node env1-test-worker-2 is running 0 daemon pod, expected 1
  Jan 11 12:12:24.867: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 12:12:24.867: INFO: Node env1-test-worker-2 is running 0 daemon pod, expected 1
  Jan 11 12:12:25.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 12:12:25.866: INFO: Node env1-test-worker-2 is running 0 daemon pod, expected 1
  Jan 11 12:12:26.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 11 12:12:26.866: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 01/11/24 12:12:26.896
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1077, will wait for the garbage collector to delete the pods @ 01/11/24 12:12:26.897
  Jan 11 12:12:26.997: INFO: Deleting DaemonSet.extensions daemon-set took: 41.371034ms
  Jan 11 12:12:27.098: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.846665ms
  Jan 11 12:12:28.808: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 12:12:28.808: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jan 11 12:12:28.815: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"187172087"},"items":null}

  Jan 11 12:12:28.823: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"187172087"},"items":null}

  Jan 11 12:12:28.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1077" for this suite. @ 01/11/24 12:12:28.945
• [8.519 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 01/11/24 12:12:28.976
  Jan 11 12:12:28.977: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename disruption @ 01/11/24 12:12:28.978
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:12:29.025
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:12:29.062
  STEP: Waiting for the pdb to be processed @ 01/11/24 12:12:29.106
  STEP: Waiting for all pods to be running @ 01/11/24 12:12:31.21
  Jan 11 12:12:31.236: INFO: running pods: 0 < 3
  Jan 11 12:12:33.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-3818" for this suite. @ 01/11/24 12:12:33.287
• [4.334 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 01/11/24 12:12:33.316
  Jan 11 12:12:33.316: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename configmap @ 01/11/24 12:12:33.318
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:12:33.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:12:33.372
  STEP: Creating configMap with name configmap-test-volume-4943c341-c39a-4074-9ea3-0ce3b8b7d179 @ 01/11/24 12:12:33.378
  STEP: Creating a pod to test consume configMaps @ 01/11/24 12:12:33.394
  STEP: Saw pod success @ 01/11/24 12:12:37.453
  Jan 11 12:12:37.461: INFO: Trying to get logs from node env1-test-worker-1 pod pod-configmaps-a6cdf286-62ac-4bdc-ae9d-48c4fed9ec88 container agnhost-container: <nil>
  STEP: delete the pod @ 01/11/24 12:12:37.478
  Jan 11 12:12:37.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2658" for this suite. @ 01/11/24 12:12:37.539
• [4.245 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 01/11/24 12:12:37.562
  Jan 11 12:12:37.562: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename namespaces @ 01/11/24 12:12:37.564
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:12:37.608
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:12:37.614
  STEP: Updating Namespace "namespaces-5968" @ 01/11/24 12:12:37.621
  Jan 11 12:12:37.657: INFO: Namespace "namespaces-5968" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"44be952d-0456-4382-9b38-ff2850af2e84", "kubernetes.io/metadata.name":"namespaces-5968", "namespaces-5968":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Jan 11 12:12:37.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5968" for this suite. @ 01/11/24 12:12:37.673
• [0.135 seconds]
------------------------------
S
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 01/11/24 12:12:37.698
  Jan 11 12:12:37.698: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename dns @ 01/11/24 12:12:37.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:12:37.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:12:37.742
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 01/11/24 12:12:37.749
  Jan 11 12:12:37.771: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2981  a99032bc-6c64-436c-aa86-c3bc5419aed6 187172205 0 2024-01-11 12:12:37 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2024-01-11 12:12:37 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p9jb2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p9jb2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 01/11/24 12:12:39.794
  Jan 11 12:12:39.794: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2981 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 12:12:39.795: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 12:12:39.797: INFO: ExecWithOptions: Clientset creation
  Jan 11 12:12:39.797: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-2981/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 01/11/24 12:12:40.061
  Jan 11 12:12:40.061: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2981 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 12:12:40.061: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 12:12:40.062: INFO: ExecWithOptions: Clientset creation
  Jan 11 12:12:40.062: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-2981/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jan 11 12:12:40.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 11 12:12:40.295: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-2981" for this suite. @ 01/11/24 12:12:40.354
• [2.688 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 01/11/24 12:12:40.386
  Jan 11 12:12:40.386: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename dns @ 01/11/24 12:12:40.388
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:12:40.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:12:40.439
  STEP: Creating a test externalName service @ 01/11/24 12:12:40.449
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7482.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7482.svc.cluster.local; sleep 1; done
   @ 01/11/24 12:12:40.483
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7482.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7482.svc.cluster.local; sleep 1; done
   @ 01/11/24 12:12:40.483
  STEP: creating a pod to probe DNS @ 01/11/24 12:12:40.483
  STEP: submitting the pod to kubernetes @ 01/11/24 12:12:40.483
  STEP: retrieving the pod @ 01/11/24 12:12:42.526
  STEP: looking for the results for each expected name from probers @ 01/11/24 12:12:42.536
  Jan 11 12:12:42.561: INFO: DNS probes using dns-test-1e7810f1-2859-4cd2-8ec1-695331431aac succeeded

  STEP: changing the externalName to bar.example.com @ 01/11/24 12:12:42.561
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7482.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7482.svc.cluster.local; sleep 1; done
   @ 01/11/24 12:12:42.614
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7482.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7482.svc.cluster.local; sleep 1; done
   @ 01/11/24 12:12:42.614
  STEP: creating a second pod to probe DNS @ 01/11/24 12:12:42.614
  STEP: submitting the pod to kubernetes @ 01/11/24 12:12:42.615
  STEP: retrieving the pod @ 01/11/24 12:12:46.687
  STEP: looking for the results for each expected name from probers @ 01/11/24 12:12:46.701
  Jan 11 12:12:46.715: INFO: File wheezy_udp@dns-test-service-3.dns-7482.svc.cluster.local from pod  dns-7482/dns-test-4b353e03-ce91-4b0b-bbfd-6cb48316ef9b contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jan 11 12:12:46.726: INFO: File jessie_udp@dns-test-service-3.dns-7482.svc.cluster.local from pod  dns-7482/dns-test-4b353e03-ce91-4b0b-bbfd-6cb48316ef9b contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jan 11 12:12:46.726: INFO: Lookups using dns-7482/dns-test-4b353e03-ce91-4b0b-bbfd-6cb48316ef9b failed for: [wheezy_udp@dns-test-service-3.dns-7482.svc.cluster.local jessie_udp@dns-test-service-3.dns-7482.svc.cluster.local]

  Jan 11 12:12:51.750: INFO: DNS probes using dns-test-4b353e03-ce91-4b0b-bbfd-6cb48316ef9b succeeded

  STEP: changing the service to type=ClusterIP @ 01/11/24 12:12:51.751
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7482.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7482.svc.cluster.local; sleep 1; done
   @ 01/11/24 12:12:51.788
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7482.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7482.svc.cluster.local; sleep 1; done
   @ 01/11/24 12:12:51.789
  STEP: creating a third pod to probe DNS @ 01/11/24 12:12:51.789
  STEP: submitting the pod to kubernetes @ 01/11/24 12:12:51.806
  STEP: retrieving the pod @ 01/11/24 12:12:53.857
  STEP: looking for the results for each expected name from probers @ 01/11/24 12:12:53.867
  Jan 11 12:12:53.892: INFO: DNS probes using dns-test-7cfcf537-0d9c-4b99-93ca-9de53fc7d131 succeeded

  Jan 11 12:12:53.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/11/24 12:12:53.908
  STEP: deleting the pod @ 01/11/24 12:12:53.946
  STEP: deleting the pod @ 01/11/24 12:12:53.984
  STEP: deleting the test externalName service @ 01/11/24 12:12:54.028
  STEP: Destroying namespace "dns-7482" for this suite. @ 01/11/24 12:12:54.086
• [13.727 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 01/11/24 12:12:54.119
  Jan 11 12:12:54.119: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename sched-preemption @ 01/11/24 12:12:54.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:12:54.181
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:12:54.191
  Jan 11 12:12:54.274: INFO: Waiting up to 1m0s for all nodes to be ready
  Jan 11 12:13:54.398: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 01/11/24 12:13:54.406
  Jan 11 12:13:54.508: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jan 11 12:13:54.526: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jan 11 12:13:54.618: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jan 11 12:13:54.643: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jan 11 12:13:54.734: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jan 11 12:13:54.766: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 01/11/24 12:13:54.766
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 01/11/24 12:13:56.865
  Jan 11 12:14:00.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-8286" for this suite. @ 01/11/24 12:14:01.161
• [67.067 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 01/11/24 12:14:01.193
  Jan 11 12:14:01.193: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename pods @ 01/11/24 12:14:01.195
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:14:01.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:14:01.254
  STEP: creating the pod @ 01/11/24 12:14:01.264
  STEP: submitting the pod to kubernetes @ 01/11/24 12:14:01.265
  W0111 12:14:01.289332      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: verifying the pod is in kubernetes @ 01/11/24 12:14:03.325
  STEP: updating the pod @ 01/11/24 12:14:03.338
  Jan 11 12:14:03.870: INFO: Successfully updated pod "pod-update-activedeadlineseconds-db73f6de-80f9-40f1-b320-816599fdf2c0"
  Jan 11 12:14:09.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3053" for this suite. @ 01/11/24 12:14:09.936
• [8.774 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 01/11/24 12:14:09.985
  Jan 11 12:14:09.986: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename field-validation @ 01/11/24 12:14:09.988
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:14:10.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:14:10.079
  Jan 11 12:14:10.085: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  W0111 12:14:18.060315      23 warnings.go:70] unknown field "alpha"
  W0111 12:14:18.060396      23 warnings.go:70] unknown field "beta"
  W0111 12:14:18.060408      23 warnings.go:70] unknown field "delta"
  W0111 12:14:18.060446      23 warnings.go:70] unknown field "epsilon"
  W0111 12:14:18.060458      23 warnings.go:70] unknown field "gamma"
  Jan 11 12:14:18.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2781" for this suite. @ 01/11/24 12:14:19.023
• [9.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 01/11/24 12:14:19.05
  Jan 11 12:14:19.050: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename downward-api @ 01/11/24 12:14:19.052
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:14:19.214
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:14:19.225
  STEP: Creating the pod @ 01/11/24 12:14:19.256
  Jan 11 12:14:21.915: INFO: Successfully updated pod "annotationupdate0dd00ef6-9733-4ffd-9d04-76f1820fdeee"
  Jan 11 12:14:23.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3604" for this suite. @ 01/11/24 12:14:23.971
• [4.938 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 01/11/24 12:14:23.99
  Jan 11 12:14:23.990: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubectl @ 01/11/24 12:14:23.992
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:14:24.023
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:14:24.029
  STEP: validating api versions @ 01/11/24 12:14:24.036
  Jan 11 12:14:24.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1324 api-versions'
  Jan 11 12:14:24.223: INFO: stderr: ""
  Jan 11 12:14:24.223: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncns.vmware.com/v1alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\ntraefik.containo.us/v1alpha1\nv1\nvelero.io/v1\n"
  Jan 11 12:14:24.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1324" for this suite. @ 01/11/24 12:14:24.244
• [0.288 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 01/11/24 12:14:24.285
  Jan 11 12:14:24.285: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename conformance-tests @ 01/11/24 12:14:24.287
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:14:24.35
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:14:24.367
  STEP: Getting node addresses @ 01/11/24 12:14:24.373
  Jan 11 12:14:24.374: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Jan 11 12:14:24.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-9995" for this suite. @ 01/11/24 12:14:24.412
• [0.156 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 01/11/24 12:14:24.454
  Jan 11 12:14:24.455: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubectl @ 01/11/24 12:14:24.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:14:24.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:14:24.504
  STEP: creating the pod @ 01/11/24 12:14:24.511
  Jan 11 12:14:24.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-3596 create -f -'
  Jan 11 12:14:25.895: INFO: stderr: ""
  Jan 11 12:14:25.896: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 01/11/24 12:14:27.925
  Jan 11 12:14:27.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-3596 label pods pause testing-label=testing-label-value'
  Jan 11 12:14:28.120: INFO: stderr: ""
  Jan 11 12:14:28.120: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 01/11/24 12:14:28.12
  Jan 11 12:14:28.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-3596 get pod pause -L testing-label'
  Jan 11 12:14:28.283: INFO: stderr: ""
  Jan 11 12:14:28.283: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 01/11/24 12:14:28.283
  Jan 11 12:14:28.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-3596 label pods pause testing-label-'
  Jan 11 12:14:28.483: INFO: stderr: ""
  Jan 11 12:14:28.483: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 01/11/24 12:14:28.483
  Jan 11 12:14:28.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-3596 get pod pause -L testing-label'
  Jan 11 12:14:28.672: INFO: stderr: ""
  Jan 11 12:14:28.672: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 01/11/24 12:14:28.673
  Jan 11 12:14:28.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-3596 delete --grace-period=0 --force -f -'
  Jan 11 12:14:28.886: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 11 12:14:28.886: INFO: stdout: "pod \"pause\" force deleted\n"
  Jan 11 12:14:28.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-3596 get rc,svc -l name=pause --no-headers'
  Jan 11 12:14:29.100: INFO: stderr: "No resources found in kubectl-3596 namespace.\n"
  Jan 11 12:14:29.100: INFO: stdout: ""
  Jan 11 12:14:29.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-3596 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jan 11 12:14:29.255: INFO: stderr: ""
  Jan 11 12:14:29.255: INFO: stdout: ""
  Jan 11 12:14:29.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3596" for this suite. @ 01/11/24 12:14:29.265
• [4.842 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 01/11/24 12:14:29.297
  Jan 11 12:14:29.297: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename job @ 01/11/24 12:14:29.298
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:14:29.347
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:14:29.354
  STEP: Creating Indexed job @ 01/11/24 12:14:29.367
  STEP: Ensuring job reaches completions @ 01/11/24 12:14:29.39
  STEP: Ensuring pods with index for job exist @ 01/11/24 12:14:37.406
  Jan 11 12:14:37.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2026" for this suite. @ 01/11/24 12:14:37.447
• [8.173 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 01/11/24 12:14:37.471
  Jan 11 12:14:37.471: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename crd-webhook @ 01/11/24 12:14:37.473
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:14:37.514
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:14:37.521
  STEP: Setting up server cert @ 01/11/24 12:14:37.528
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 01/11/24 12:14:38.41
  STEP: Deploying the custom resource conversion webhook pod @ 01/11/24 12:14:38.429
  STEP: Wait for the deployment to be ready @ 01/11/24 12:14:38.457
  Jan 11 12:14:38.493: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/11/24 12:14:40.519
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 12:14:40.543
  Jan 11 12:14:41.543: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jan 11 12:14:41.558: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Creating a v1 custom resource @ 01/11/24 12:14:49.293
  STEP: v2 custom resource should be converted @ 01/11/24 12:14:49.305
  Jan 11 12:14:49.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-6368" for this suite. @ 01/11/24 12:14:50.083
• [12.632 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 01/11/24 12:14:50.103
  Jan 11 12:14:50.104: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-probe @ 01/11/24 12:14:50.106
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:14:50.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:14:50.155
  Jan 11 12:15:50.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-5336" for this suite. @ 01/11/24 12:15:50.216
• [60.128 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 01/11/24 12:15:50.232
  Jan 11 12:15:50.232: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename emptydir @ 01/11/24 12:15:50.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:15:50.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:15:50.284
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 01/11/24 12:15:50.294
  STEP: Saw pod success @ 01/11/24 12:15:54.345
  Jan 11 12:15:54.356: INFO: Trying to get logs from node env1-test-worker-1 pod pod-4e58d412-d74c-4998-af8b-575623184c26 container test-container: <nil>
  STEP: delete the pod @ 01/11/24 12:15:54.395
  Jan 11 12:15:54.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7550" for this suite. @ 01/11/24 12:15:54.456
• [4.240 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 01/11/24 12:15:54.474
  Jan 11 12:15:54.474: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 12:15:54.476
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:15:54.516
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:15:54.522
  STEP: Creating a pod to test downward API volume plugin @ 01/11/24 12:15:54.528
  STEP: Saw pod success @ 01/11/24 12:15:58.582
  Jan 11 12:15:58.592: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-e5dae6c9-792f-47a5-b9ae-fbb913afcd1b container client-container: <nil>
  STEP: delete the pod @ 01/11/24 12:15:58.607
  Jan 11 12:15:58.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9722" for this suite. @ 01/11/24 12:15:58.658
• [4.203 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 01/11/24 12:15:58.681
  Jan 11 12:15:58.681: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename namespaces @ 01/11/24 12:15:58.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:15:58.717
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:15:58.723
  STEP: Read namespace status @ 01/11/24 12:15:58.73
  Jan 11 12:15:58.738: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 01/11/24 12:15:58.739
  Jan 11 12:15:58.752: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 01/11/24 12:15:58.752
  Jan 11 12:15:58.779: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Jan 11 12:15:58.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-2545" for this suite. @ 01/11/24 12:15:58.8
• [0.155 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 01/11/24 12:15:58.842
  Jan 11 12:15:58.842: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename ingress @ 01/11/24 12:15:58.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:15:58.873
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:15:58.881
  STEP: getting /apis @ 01/11/24 12:15:58.888
  STEP: getting /apis/networking.k8s.io @ 01/11/24 12:15:58.899
  STEP: getting /apis/networking.k8s.iov1 @ 01/11/24 12:15:58.902
  STEP: creating @ 01/11/24 12:15:58.905
  STEP: getting @ 01/11/24 12:15:58.939
  STEP: listing @ 01/11/24 12:15:58.947
  STEP: watching @ 01/11/24 12:15:58.956
  Jan 11 12:15:58.956: INFO: starting watch
  STEP: cluster-wide listing @ 01/11/24 12:15:58.959
  STEP: cluster-wide watching @ 01/11/24 12:15:58.966
  Jan 11 12:15:58.967: INFO: starting watch
  STEP: patching @ 01/11/24 12:15:58.969
  STEP: updating @ 01/11/24 12:15:58.98
  Jan 11 12:15:58.996: INFO: waiting for watch events with expected annotations
  Jan 11 12:15:58.997: INFO: saw patched and updated annotations
  STEP: patching /status @ 01/11/24 12:15:58.998
  STEP: updating /status @ 01/11/24 12:15:59.01
  STEP: get /status @ 01/11/24 12:15:59.028
  STEP: deleting @ 01/11/24 12:15:59.036
  STEP: deleting a collection @ 01/11/24 12:15:59.071
  Jan 11 12:15:59.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-7442" for this suite. @ 01/11/24 12:15:59.137
• [0.316 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 01/11/24 12:15:59.166
  Jan 11 12:15:59.166: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename events @ 01/11/24 12:15:59.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:15:59.204
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:15:59.213
  STEP: Create set of events @ 01/11/24 12:15:59.219
  Jan 11 12:15:59.229: INFO: created test-event-1
  Jan 11 12:15:59.240: INFO: created test-event-2
  Jan 11 12:15:59.250: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 01/11/24 12:15:59.25
  STEP: delete collection of events @ 01/11/24 12:15:59.261
  Jan 11 12:15:59.261: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 01/11/24 12:15:59.327
  Jan 11 12:15:59.327: INFO: requesting list of events to confirm quantity
  Jan 11 12:15:59.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-2041" for this suite. @ 01/11/24 12:15:59.349
• [0.204 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 01/11/24 12:15:59.374
  Jan 11 12:15:59.374: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename deployment @ 01/11/24 12:15:59.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:15:59.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:15:59.422
  Jan 11 12:15:59.429: INFO: Creating simple deployment test-new-deployment
  Jan 11 12:15:59.500: INFO: deployment "test-new-deployment" doesn't have the required revision set
  STEP: getting scale subresource @ 01/11/24 12:16:01.59
  STEP: updating a scale subresource @ 01/11/24 12:16:01.602
  STEP: verifying the deployment Spec.Replicas was modified @ 01/11/24 12:16:01.615
  STEP: Patch a scale subresource @ 01/11/24 12:16:01.625
  Jan 11 12:16:01.718: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-8098  0bc51f6c-1c8b-4dec-a494-2eae68ca2f67 187173759 3 2024-01-11 12:15:59 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2024-01-11 12:15:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-11 12:16:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d8f868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-11 12:16:01 +0000 UTC,LastTransitionTime:2024-01-11 12:16:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2024-01-11 12:16:01 +0000 UTC,LastTransitionTime:2024-01-11 12:15:59 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jan 11 12:16:01.742: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-8098  797538b0-3a18-4fb8-b455-073767a31017 187173764 2 2024-01-11 12:15:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 0bc51f6c-1c8b-4dec-a494-2eae68ca2f67 0xc003e47597 0xc003e47598}] [] [{kube-controller-manager Update apps/v1 2024-01-11 12:16:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bc51f6c-1c8b-4dec-a494-2eae68ca2f67\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-11 12:16:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e47648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jan 11 12:16:01.761: INFO: Pod "test-new-deployment-67bd4bf6dc-5g2pl" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-5g2pl test-new-deployment-67bd4bf6dc- deployment-8098  644d7a11-fe62-4b9a-945e-49d417b45da9 187173755 0 2024-01-11 12:15:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 797538b0-3a18-4fb8-b455-073767a31017 0xc003e47a37 0xc003e47a38}] [] [{kube-controller-manager Update v1 2024-01-11 12:15:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797538b0-3a18-4fb8-b455-073767a31017\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 12:16:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.243\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k5frv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k5frv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:15:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:16:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:16:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:15:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:10.233.68.243,StartTime:2024-01-11 12:15:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-11 12:16:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1ba5867bb7e3241316c8dd90d46c12860315b254f1d099e45a1eacc2ff4b1924,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.243,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:16:01.761: INFO: Pod "test-new-deployment-67bd4bf6dc-p9snd" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-p9snd test-new-deployment-67bd4bf6dc- deployment-8098  b6a48239-e6a7-4d31-8db5-4903f1569206 187173768 0 2024-01-11 12:16:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 797538b0-3a18-4fb8-b455-073767a31017 0xc003e47c37 0xc003e47c38}] [] [{kube-controller-manager Update v1 2024-01-11 12:16:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797538b0-3a18-4fb8-b455-073767a31017\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 12:16:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-phngf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-phngf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:16:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:16:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:16:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:16:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.202,PodIP:,StartTime:2024-01-11 12:16:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:16:01.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8098" for this suite. @ 01/11/24 12:16:01.796
• [2.453 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 01/11/24 12:16:01.832
  Jan 11 12:16:01.832: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename configmap @ 01/11/24 12:16:01.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:16:01.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:16:01.894
  Jan 11 12:16:02.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1805" for this suite. @ 01/11/24 12:16:02.066
• [0.257 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 01/11/24 12:16:02.093
  Jan 11 12:16:02.093: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename replication-controller @ 01/11/24 12:16:02.094
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:16:02.128
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:16:02.136
  STEP: Creating ReplicationController "e2e-rc-zbjj6" @ 01/11/24 12:16:02.155
  Jan 11 12:16:02.173: INFO: Get Replication Controller "e2e-rc-zbjj6" to confirm replicas
  Jan 11 12:16:03.185: INFO: Get Replication Controller "e2e-rc-zbjj6" to confirm replicas
  Jan 11 12:16:03.196: INFO: Found 1 replicas for "e2e-rc-zbjj6" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-zbjj6" @ 01/11/24 12:16:03.196
  STEP: Updating a scale subresource @ 01/11/24 12:16:03.205
  STEP: Verifying replicas where modified for replication controller "e2e-rc-zbjj6" @ 01/11/24 12:16:03.217
  Jan 11 12:16:03.217: INFO: Get Replication Controller "e2e-rc-zbjj6" to confirm replicas
  Jan 11 12:16:04.229: INFO: Get Replication Controller "e2e-rc-zbjj6" to confirm replicas
  Jan 11 12:16:04.242: INFO: Found 2 replicas for "e2e-rc-zbjj6" replication controller
  Jan 11 12:16:04.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7537" for this suite. @ 01/11/24 12:16:04.26
• [2.199 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 01/11/24 12:16:04.306
  Jan 11 12:16:04.306: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename downward-api @ 01/11/24 12:16:04.308
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:16:04.395
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:16:04.404
  STEP: Creating a pod to test downward API volume plugin @ 01/11/24 12:16:04.417
  STEP: Saw pod success @ 01/11/24 12:16:08.523
  Jan 11 12:16:08.534: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-9f39e38b-3abe-4a41-82e5-113a20d5314f container client-container: <nil>
  STEP: delete the pod @ 01/11/24 12:16:08.553
  Jan 11 12:16:08.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7846" for this suite. @ 01/11/24 12:16:08.624
• [4.346 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 01/11/24 12:16:08.653
  Jan 11 12:16:08.653: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename pod-network-test @ 01/11/24 12:16:08.656
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:16:08.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:16:08.698
  STEP: Performing setup for networking test in namespace pod-network-test-6634 @ 01/11/24 12:16:08.705
  STEP: creating a selector @ 01/11/24 12:16:08.705
  STEP: Creating the service pods in kubernetes @ 01/11/24 12:16:08.705
  Jan 11 12:16:08.705: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 01/11/24 12:16:20.934
  Jan 11 12:16:23.032: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jan 11 12:16:23.032: INFO: Going to poll 10.233.67.104 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jan 11 12:16:23.038: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.67.104:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6634 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 12:16:23.038: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 12:16:23.040: INFO: ExecWithOptions: Clientset creation
  Jan 11 12:16:23.040: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6634/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.67.104%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jan 11 12:16:23.180: INFO: Found all 1 expected endpoints: [netserver-0]
  Jan 11 12:16:23.180: INFO: Going to poll 10.233.68.246 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jan 11 12:16:23.189: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.68.246:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6634 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 12:16:23.189: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 12:16:23.190: INFO: ExecWithOptions: Clientset creation
  Jan 11 12:16:23.190: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6634/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.68.246%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jan 11 12:16:23.345: INFO: Found all 1 expected endpoints: [netserver-1]
  Jan 11 12:16:23.345: INFO: Going to poll 10.233.69.30 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jan 11 12:16:23.354: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.69.30:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6634 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 12:16:23.354: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 12:16:23.355: INFO: ExecWithOptions: Clientset creation
  Jan 11 12:16:23.355: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6634/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.69.30%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jan 11 12:16:23.500: INFO: Found all 1 expected endpoints: [netserver-2]
  Jan 11 12:16:23.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-6634" for this suite. @ 01/11/24 12:16:23.515
• [14.878 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 01/11/24 12:16:23.532
  Jan 11 12:16:23.532: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/11/24 12:16:23.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:16:23.576
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:16:23.59
  STEP: set up a multi version CRD @ 01/11/24 12:16:23.6
  Jan 11 12:16:23.600: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: rename a version @ 01/11/24 12:16:37.08
  STEP: check the new version name is served @ 01/11/24 12:16:37.128
  STEP: check the old version name is removed @ 01/11/24 12:16:41.263
  STEP: check the other version is not changed @ 01/11/24 12:16:42.789
  Jan 11 12:16:49.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2183" for this suite. @ 01/11/24 12:16:49.206
• [25.692 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 01/11/24 12:16:49.228
  Jan 11 12:16:49.228: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename configmap @ 01/11/24 12:16:49.23
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:16:49.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:16:49.289
  STEP: Creating configMap configmap-3359/configmap-test-2d29fce1-1d67-4e35-b29a-de3d2e249149 @ 01/11/24 12:16:49.295
  STEP: Creating a pod to test consume configMaps @ 01/11/24 12:16:49.305
  STEP: Saw pod success @ 01/11/24 12:16:53.369
  Jan 11 12:16:53.376: INFO: Trying to get logs from node env1-test-worker-1 pod pod-configmaps-ec38285c-29e9-44f0-9953-8002efa372a9 container env-test: <nil>
  STEP: delete the pod @ 01/11/24 12:16:53.399
  Jan 11 12:16:53.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3359" for this suite. @ 01/11/24 12:16:53.466
• [4.264 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 01/11/24 12:16:53.497
  Jan 11 12:16:53.497: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename sched-preemption @ 01/11/24 12:16:53.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:16:53.536
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:16:53.542
  Jan 11 12:16:53.610: INFO: Waiting up to 1m0s for all nodes to be ready
  Jan 11 12:17:53.723: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 01/11/24 12:17:53.737
  Jan 11 12:17:53.737: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename sched-preemption-path @ 01/11/24 12:17:53.739
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:17:53.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:17:53.785
  Jan 11 12:17:53.840: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Jan 11 12:17:53.856: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Jan 11 12:17:53.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 11 12:17:53.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-325" for this suite. @ 01/11/24 12:17:54.202
  STEP: Destroying namespace "sched-preemption-4157" for this suite. @ 01/11/24 12:17:54.235
• [60.755 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 01/11/24 12:17:54.254
  Jan 11 12:17:54.254: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename replicaset @ 01/11/24 12:17:54.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:17:54.296
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:17:54.305
  STEP: Create a Replicaset @ 01/11/24 12:17:54.33
  STEP: Verify that the required pods have come up. @ 01/11/24 12:17:54.346
  Jan 11 12:17:54.358: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jan 11 12:17:59.375: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 01/11/24 12:17:59.375
  STEP: Getting /status @ 01/11/24 12:17:59.376
  Jan 11 12:17:59.389: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 01/11/24 12:17:59.389
  Jan 11 12:17:59.425: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 01/11/24 12:17:59.426
  Jan 11 12:17:59.432: INFO: Observed &ReplicaSet event: ADDED
  Jan 11 12:17:59.432: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 11 12:17:59.432: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 11 12:17:59.432: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 11 12:17:59.432: INFO: Found replicaset test-rs in namespace replicaset-3844 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jan 11 12:17:59.432: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 01/11/24 12:17:59.432
  Jan 11 12:17:59.433: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jan 11 12:17:59.457: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 01/11/24 12:17:59.457
  Jan 11 12:17:59.463: INFO: Observed &ReplicaSet event: ADDED
  Jan 11 12:17:59.463: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 11 12:17:59.463: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 11 12:17:59.464: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 11 12:17:59.464: INFO: Observed replicaset test-rs in namespace replicaset-3844 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 11 12:17:59.464: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 11 12:17:59.464: INFO: Found replicaset test-rs in namespace replicaset-3844 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Jan 11 12:17:59.464: INFO: Replicaset test-rs has a patched status
  Jan 11 12:17:59.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3844" for this suite. @ 01/11/24 12:17:59.49
• [5.262 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 01/11/24 12:17:59.52
  Jan 11 12:17:59.520: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-probe @ 01/11/24 12:17:59.521
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:17:59.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:17:59.614
  STEP: Creating pod test-webserver-7f5ca88b-5b11-4cb2-8969-8b31d5078b1f in namespace container-probe-2361 @ 01/11/24 12:17:59.632
  Jan 11 12:18:01.717: INFO: Started pod test-webserver-7f5ca88b-5b11-4cb2-8969-8b31d5078b1f in namespace container-probe-2361
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/11/24 12:18:01.717
  Jan 11 12:18:01.724: INFO: Initial restart count of pod test-webserver-7f5ca88b-5b11-4cb2-8969-8b31d5078b1f is 0
  Jan 11 12:22:03.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/11/24 12:22:03.156
  STEP: Destroying namespace "container-probe-2361" for this suite. @ 01/11/24 12:22:03.191
• [243.688 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 01/11/24 12:22:03.208
  Jan 11 12:22:03.208: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename replicaset @ 01/11/24 12:22:03.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:22:03.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:22:03.247
  Jan 11 12:22:03.298: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jan 11 12:22:08.316: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 01/11/24 12:22:08.317
  STEP: Scaling up "test-rs" replicaset  @ 01/11/24 12:22:08.317
  Jan 11 12:22:08.384: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 01/11/24 12:22:08.385
  W0111 12:22:08.396650      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jan 11 12:22:08.401: INFO: observed ReplicaSet test-rs in namespace replicaset-8727 with ReadyReplicas 1, AvailableReplicas 1
  Jan 11 12:22:08.445: INFO: observed ReplicaSet test-rs in namespace replicaset-8727 with ReadyReplicas 1, AvailableReplicas 1
  Jan 11 12:22:08.509: INFO: observed ReplicaSet test-rs in namespace replicaset-8727 with ReadyReplicas 1, AvailableReplicas 1
  Jan 11 12:22:08.534: INFO: observed ReplicaSet test-rs in namespace replicaset-8727 with ReadyReplicas 1, AvailableReplicas 1
  Jan 11 12:22:10.138: INFO: observed ReplicaSet test-rs in namespace replicaset-8727 with ReadyReplicas 2, AvailableReplicas 2
  Jan 11 12:22:10.307: INFO: observed Replicaset test-rs in namespace replicaset-8727 with ReadyReplicas 3 found true
  Jan 11 12:22:10.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8727" for this suite. @ 01/11/24 12:22:10.331
• [7.149 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 01/11/24 12:22:10.367
  Jan 11 12:22:10.367: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename gc @ 01/11/24 12:22:10.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:22:10.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:22:10.442
  STEP: create the rc @ 01/11/24 12:22:10.461
  W0111 12:22:10.478443      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 01/11/24 12:22:16.494
  STEP: wait for the rc to be deleted @ 01/11/24 12:22:16.517
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 01/11/24 12:22:21.531
  STEP: Gathering metrics @ 01/11/24 12:22:51.565
  Jan 11 12:22:51.837: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jan 11 12:22:51.837: INFO: Deleting pod "simpletest.rc-242jt" in namespace "gc-2013"
  Jan 11 12:22:51.903: INFO: Deleting pod "simpletest.rc-2tp5g" in namespace "gc-2013"
  Jan 11 12:22:51.966: INFO: Deleting pod "simpletest.rc-2z56b" in namespace "gc-2013"
  Jan 11 12:22:52.015: INFO: Deleting pod "simpletest.rc-4bbgv" in namespace "gc-2013"
  Jan 11 12:22:52.059: INFO: Deleting pod "simpletest.rc-4bd4p" in namespace "gc-2013"
  Jan 11 12:22:52.107: INFO: Deleting pod "simpletest.rc-4qf52" in namespace "gc-2013"
  Jan 11 12:22:52.163: INFO: Deleting pod "simpletest.rc-4w5nh" in namespace "gc-2013"
  Jan 11 12:22:52.213: INFO: Deleting pod "simpletest.rc-59vlt" in namespace "gc-2013"
  Jan 11 12:22:52.252: INFO: Deleting pod "simpletest.rc-5r9rf" in namespace "gc-2013"
  Jan 11 12:22:52.288: INFO: Deleting pod "simpletest.rc-6gcz5" in namespace "gc-2013"
  Jan 11 12:22:52.316: INFO: Deleting pod "simpletest.rc-6qjl7" in namespace "gc-2013"
  Jan 11 12:22:52.378: INFO: Deleting pod "simpletest.rc-72mp8" in namespace "gc-2013"
  Jan 11 12:22:52.422: INFO: Deleting pod "simpletest.rc-7gfr8" in namespace "gc-2013"
  Jan 11 12:22:52.476: INFO: Deleting pod "simpletest.rc-7m2wn" in namespace "gc-2013"
  Jan 11 12:22:52.534: INFO: Deleting pod "simpletest.rc-7wt2x" in namespace "gc-2013"
  Jan 11 12:22:52.574: INFO: Deleting pod "simpletest.rc-7z65q" in namespace "gc-2013"
  Jan 11 12:22:52.639: INFO: Deleting pod "simpletest.rc-7zxsz" in namespace "gc-2013"
  Jan 11 12:22:52.685: INFO: Deleting pod "simpletest.rc-896r5" in namespace "gc-2013"
  Jan 11 12:22:52.724: INFO: Deleting pod "simpletest.rc-89p97" in namespace "gc-2013"
  Jan 11 12:22:52.782: INFO: Deleting pod "simpletest.rc-8xjf8" in namespace "gc-2013"
  Jan 11 12:22:52.824: INFO: Deleting pod "simpletest.rc-9qghb" in namespace "gc-2013"
  Jan 11 12:22:52.866: INFO: Deleting pod "simpletest.rc-9t7nd" in namespace "gc-2013"
  Jan 11 12:22:52.910: INFO: Deleting pod "simpletest.rc-9tbp2" in namespace "gc-2013"
  Jan 11 12:22:52.978: INFO: Deleting pod "simpletest.rc-9tf7g" in namespace "gc-2013"
  Jan 11 12:22:53.007: INFO: Deleting pod "simpletest.rc-b625d" in namespace "gc-2013"
  Jan 11 12:22:53.042: INFO: Deleting pod "simpletest.rc-btgd2" in namespace "gc-2013"
  Jan 11 12:22:53.087: INFO: Deleting pod "simpletest.rc-c2772" in namespace "gc-2013"
  Jan 11 12:22:53.128: INFO: Deleting pod "simpletest.rc-ckpgl" in namespace "gc-2013"
  Jan 11 12:22:53.162: INFO: Deleting pod "simpletest.rc-cv4fn" in namespace "gc-2013"
  Jan 11 12:22:53.193: INFO: Deleting pod "simpletest.rc-cx6gr" in namespace "gc-2013"
  Jan 11 12:22:53.244: INFO: Deleting pod "simpletest.rc-cxzbl" in namespace "gc-2013"
  Jan 11 12:22:53.283: INFO: Deleting pod "simpletest.rc-d9fn7" in namespace "gc-2013"
  Jan 11 12:22:53.305: INFO: Deleting pod "simpletest.rc-djpvc" in namespace "gc-2013"
  Jan 11 12:22:53.339: INFO: Deleting pod "simpletest.rc-dkcsw" in namespace "gc-2013"
  Jan 11 12:22:53.370: INFO: Deleting pod "simpletest.rc-dqmr8" in namespace "gc-2013"
  Jan 11 12:22:53.402: INFO: Deleting pod "simpletest.rc-fq75q" in namespace "gc-2013"
  Jan 11 12:22:53.427: INFO: Deleting pod "simpletest.rc-g8tk2" in namespace "gc-2013"
  Jan 11 12:22:53.455: INFO: Deleting pod "simpletest.rc-gbjkp" in namespace "gc-2013"
  Jan 11 12:22:53.489: INFO: Deleting pod "simpletest.rc-gsfrl" in namespace "gc-2013"
  Jan 11 12:22:53.518: INFO: Deleting pod "simpletest.rc-h96zg" in namespace "gc-2013"
  Jan 11 12:22:53.565: INFO: Deleting pod "simpletest.rc-hc7v9" in namespace "gc-2013"
  Jan 11 12:22:53.598: INFO: Deleting pod "simpletest.rc-hhz2f" in namespace "gc-2013"
  Jan 11 12:22:53.628: INFO: Deleting pod "simpletest.rc-hqmnq" in namespace "gc-2013"
  Jan 11 12:22:53.666: INFO: Deleting pod "simpletest.rc-hqtrc" in namespace "gc-2013"
  Jan 11 12:22:53.694: INFO: Deleting pod "simpletest.rc-hzxdv" in namespace "gc-2013"
  Jan 11 12:22:53.719: INFO: Deleting pod "simpletest.rc-j9qjz" in namespace "gc-2013"
  Jan 11 12:22:53.750: INFO: Deleting pod "simpletest.rc-jb2cc" in namespace "gc-2013"
  Jan 11 12:22:53.780: INFO: Deleting pod "simpletest.rc-jctx7" in namespace "gc-2013"
  Jan 11 12:22:53.817: INFO: Deleting pod "simpletest.rc-jllph" in namespace "gc-2013"
  Jan 11 12:22:53.834: INFO: Deleting pod "simpletest.rc-jv52t" in namespace "gc-2013"
  Jan 11 12:22:53.862: INFO: Deleting pod "simpletest.rc-k2l8x" in namespace "gc-2013"
  Jan 11 12:22:53.913: INFO: Deleting pod "simpletest.rc-kpxds" in namespace "gc-2013"
  Jan 11 12:22:53.942: INFO: Deleting pod "simpletest.rc-kstfr" in namespace "gc-2013"
  Jan 11 12:22:53.976: INFO: Deleting pod "simpletest.rc-kv84r" in namespace "gc-2013"
  Jan 11 12:22:53.999: INFO: Deleting pod "simpletest.rc-ldvd8" in namespace "gc-2013"
  Jan 11 12:22:54.028: INFO: Deleting pod "simpletest.rc-lkhpk" in namespace "gc-2013"
  Jan 11 12:22:54.064: INFO: Deleting pod "simpletest.rc-lpbz9" in namespace "gc-2013"
  Jan 11 12:22:54.101: INFO: Deleting pod "simpletest.rc-lz8cb" in namespace "gc-2013"
  Jan 11 12:22:54.150: INFO: Deleting pod "simpletest.rc-ml4k6" in namespace "gc-2013"
  Jan 11 12:22:54.170: INFO: Deleting pod "simpletest.rc-mzpkg" in namespace "gc-2013"
  Jan 11 12:22:54.199: INFO: Deleting pod "simpletest.rc-n67rg" in namespace "gc-2013"
  Jan 11 12:22:54.246: INFO: Deleting pod "simpletest.rc-nzmvc" in namespace "gc-2013"
  Jan 11 12:22:54.284: INFO: Deleting pod "simpletest.rc-p8lmf" in namespace "gc-2013"
  Jan 11 12:22:54.330: INFO: Deleting pod "simpletest.rc-pg9qk" in namespace "gc-2013"
  Jan 11 12:22:54.353: INFO: Deleting pod "simpletest.rc-plchj" in namespace "gc-2013"
  Jan 11 12:22:54.394: INFO: Deleting pod "simpletest.rc-pr2cf" in namespace "gc-2013"
  Jan 11 12:22:54.425: INFO: Deleting pod "simpletest.rc-ptk58" in namespace "gc-2013"
  Jan 11 12:22:54.453: INFO: Deleting pod "simpletest.rc-q4ds7" in namespace "gc-2013"
  Jan 11 12:22:54.497: INFO: Deleting pod "simpletest.rc-q57n5" in namespace "gc-2013"
  Jan 11 12:22:54.530: INFO: Deleting pod "simpletest.rc-qkml6" in namespace "gc-2013"
  Jan 11 12:22:54.556: INFO: Deleting pod "simpletest.rc-ql95t" in namespace "gc-2013"
  Jan 11 12:22:54.586: INFO: Deleting pod "simpletest.rc-rb55j" in namespace "gc-2013"
  Jan 11 12:22:54.613: INFO: Deleting pod "simpletest.rc-rdd89" in namespace "gc-2013"
  Jan 11 12:22:54.645: INFO: Deleting pod "simpletest.rc-rjg7l" in namespace "gc-2013"
  Jan 11 12:22:54.680: INFO: Deleting pod "simpletest.rc-rw5jt" in namespace "gc-2013"
  Jan 11 12:22:54.711: INFO: Deleting pod "simpletest.rc-sdd2h" in namespace "gc-2013"
  Jan 11 12:22:54.742: INFO: Deleting pod "simpletest.rc-sfxvc" in namespace "gc-2013"
  Jan 11 12:22:54.769: INFO: Deleting pod "simpletest.rc-smlw2" in namespace "gc-2013"
  Jan 11 12:22:54.815: INFO: Deleting pod "simpletest.rc-srmpv" in namespace "gc-2013"
  Jan 11 12:22:54.846: INFO: Deleting pod "simpletest.rc-t4m8z" in namespace "gc-2013"
  Jan 11 12:22:54.879: INFO: Deleting pod "simpletest.rc-tb2nd" in namespace "gc-2013"
  Jan 11 12:22:54.899: INFO: Deleting pod "simpletest.rc-tgcjf" in namespace "gc-2013"
  Jan 11 12:22:54.940: INFO: Deleting pod "simpletest.rc-tj4mf" in namespace "gc-2013"
  Jan 11 12:22:54.973: INFO: Deleting pod "simpletest.rc-tkx5c" in namespace "gc-2013"
  Jan 11 12:22:55.005: INFO: Deleting pod "simpletest.rc-tn2pd" in namespace "gc-2013"
  Jan 11 12:22:55.038: INFO: Deleting pod "simpletest.rc-tr2h2" in namespace "gc-2013"
  Jan 11 12:22:55.070: INFO: Deleting pod "simpletest.rc-vmqbm" in namespace "gc-2013"
  Jan 11 12:22:55.098: INFO: Deleting pod "simpletest.rc-vrc9x" in namespace "gc-2013"
  Jan 11 12:22:55.136: INFO: Deleting pod "simpletest.rc-vtd54" in namespace "gc-2013"
  Jan 11 12:22:55.170: INFO: Deleting pod "simpletest.rc-vvw2l" in namespace "gc-2013"
  Jan 11 12:22:55.205: INFO: Deleting pod "simpletest.rc-wjqjg" in namespace "gc-2013"
  Jan 11 12:22:55.230: INFO: Deleting pod "simpletest.rc-wlq75" in namespace "gc-2013"
  Jan 11 12:22:55.256: INFO: Deleting pod "simpletest.rc-wpd7b" in namespace "gc-2013"
  Jan 11 12:22:55.287: INFO: Deleting pod "simpletest.rc-wpdzw" in namespace "gc-2013"
  Jan 11 12:22:55.341: INFO: Deleting pod "simpletest.rc-wrq29" in namespace "gc-2013"
  Jan 11 12:22:55.380: INFO: Deleting pod "simpletest.rc-xmt5k" in namespace "gc-2013"
  Jan 11 12:22:55.409: INFO: Deleting pod "simpletest.rc-xnj7d" in namespace "gc-2013"
  Jan 11 12:22:55.443: INFO: Deleting pod "simpletest.rc-xzdsw" in namespace "gc-2013"
  Jan 11 12:22:55.471: INFO: Deleting pod "simpletest.rc-xzgfr" in namespace "gc-2013"
  Jan 11 12:22:55.503: INFO: Deleting pod "simpletest.rc-zxdhw" in namespace "gc-2013"
  Jan 11 12:22:55.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2013" for this suite. @ 01/11/24 12:22:55.545
• [45.192 seconds]
------------------------------
S
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 01/11/24 12:22:55.559
  Jan 11 12:22:55.559: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename endpointslice @ 01/11/24 12:22:55.56
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:22:55.596
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:22:55.605
  STEP: referencing a single matching pod @ 01/11/24 12:23:00.762
  STEP: referencing matching pods with named port @ 01/11/24 12:23:05.789
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 01/11/24 12:23:10.821
  STEP: recreating EndpointSlices after they've been deleted @ 01/11/24 12:23:15.859
  Jan 11 12:23:15.912: INFO: EndpointSlice for Service endpointslice-6861/example-named-port not found
  Jan 11 12:23:25.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6861" for this suite. @ 01/11/24 12:23:25.946
• [30.405 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 01/11/24 12:23:25.969
  Jan 11 12:23:25.969: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename init-container @ 01/11/24 12:23:25.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:23:26.006
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:23:26.013
  STEP: creating the pod @ 01/11/24 12:23:26.02
  Jan 11 12:23:26.021: INFO: PodSpec: initContainers in spec.initContainers
  Jan 11 12:23:30.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-8773" for this suite. @ 01/11/24 12:23:30.095
• [4.145 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 01/11/24 12:23:30.118
  Jan 11 12:23:30.118: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename dns @ 01/11/24 12:23:30.121
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:23:30.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:23:30.182
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 01/11/24 12:23:30.191
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 01/11/24 12:23:30.191
  STEP: creating a pod to probe DNS @ 01/11/24 12:23:30.192
  STEP: submitting the pod to kubernetes @ 01/11/24 12:23:30.192
  STEP: retrieving the pod @ 01/11/24 12:23:32.238
  STEP: looking for the results for each expected name from probers @ 01/11/24 12:23:32.246
  Jan 11 12:23:32.279: INFO: DNS probes using dns-1277/dns-test-ce25a18e-7deb-48c8-a085-efbb71158564 succeeded

  Jan 11 12:23:32.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/11/24 12:23:32.295
  STEP: Destroying namespace "dns-1277" for this suite. @ 01/11/24 12:23:32.337
• [2.241 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 01/11/24 12:23:32.359
  Jan 11 12:23:32.359: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename emptydir @ 01/11/24 12:23:32.36
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:23:32.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:23:32.41
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 01/11/24 12:23:32.416
  STEP: Saw pod success @ 01/11/24 12:23:36.492
  Jan 11 12:23:36.505: INFO: Trying to get logs from node env1-test-worker-2 pod pod-c9311715-2327-41f2-99c0-7a0e8777869e container test-container: <nil>
  STEP: delete the pod @ 01/11/24 12:23:36.561
  Jan 11 12:23:36.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-124" for this suite. @ 01/11/24 12:23:36.654
• [4.312 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 01/11/24 12:23:36.673
  Jan 11 12:23:36.673: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename containers @ 01/11/24 12:23:36.675
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:23:36.72
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:23:36.725
  STEP: Creating a pod to test override arguments @ 01/11/24 12:23:36.73
  STEP: Saw pod success @ 01/11/24 12:23:40.791
  Jan 11 12:23:40.802: INFO: Trying to get logs from node env1-test-worker-1 pod client-containers-1dfe5295-f366-4c60-8acc-8669f2d7abf0 container agnhost-container: <nil>
  STEP: delete the pod @ 01/11/24 12:23:40.86
  Jan 11 12:23:40.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-851" for this suite. @ 01/11/24 12:23:40.939
• [4.283 seconds]
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 01/11/24 12:23:40.957
  Jan 11 12:23:40.957: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename pods @ 01/11/24 12:23:40.958
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:23:40.992
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:23:41
  STEP: creating the pod @ 01/11/24 12:23:41.006
  STEP: setting up watch @ 01/11/24 12:23:41.007
  STEP: submitting the pod to kubernetes @ 01/11/24 12:23:41.117
  STEP: verifying the pod is in kubernetes @ 01/11/24 12:23:41.147
  STEP: verifying pod creation was observed @ 01/11/24 12:23:41.168
  STEP: deleting the pod gracefully @ 01/11/24 12:23:43.198
  STEP: verifying pod deletion was observed @ 01/11/24 12:23:43.213
  Jan 11 12:23:44.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5509" for this suite. @ 01/11/24 12:23:44.207
• [3.267 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 01/11/24 12:23:44.226
  Jan 11 12:23:44.226: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename secrets @ 01/11/24 12:23:44.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:23:44.286
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:23:44.293
  STEP: creating secret secrets-6751/secret-test-74e2b937-7069-4c5f-a3ee-232665b46068 @ 01/11/24 12:23:44.299
  STEP: Creating a pod to test consume secrets @ 01/11/24 12:23:44.309
  STEP: Saw pod success @ 01/11/24 12:23:48.382
  Jan 11 12:23:48.391: INFO: Trying to get logs from node env1-test-worker-1 pod pod-configmaps-c5c23860-e5d2-4bec-9949-3add0a2953b3 container env-test: <nil>
  STEP: delete the pod @ 01/11/24 12:23:48.414
  Jan 11 12:23:48.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6751" for this suite. @ 01/11/24 12:23:48.459
• [4.247 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 01/11/24 12:23:48.482
  Jan 11 12:23:48.482: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubectl @ 01/11/24 12:23:48.486
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:23:48.516
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:23:48.523
  STEP: Starting the proxy @ 01/11/24 12:23:48.529
  Jan 11 12:23:48.529: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-5812 proxy --unix-socket=/tmp/kubectl-proxy-unix2126015860/test'
  STEP: retrieving proxy /api/ output @ 01/11/24 12:23:48.655
  Jan 11 12:23:48.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5812" for this suite. @ 01/11/24 12:23:48.676
• [0.207 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 01/11/24 12:23:48.695
  Jan 11 12:23:48.695: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename runtimeclass @ 01/11/24 12:23:48.699
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:23:48.727
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:23:48.733
  STEP: Deleting RuntimeClass runtimeclass-4391-delete-me @ 01/11/24 12:23:48.747
  STEP: Waiting for the RuntimeClass to disappear @ 01/11/24 12:23:48.768
  Jan 11 12:23:48.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-4391" for this suite. @ 01/11/24 12:23:48.818
• [0.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 01/11/24 12:23:48.849
  Jan 11 12:23:48.849: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename taint-multiple-pods @ 01/11/24 12:23:48.851
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:23:48.893
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:23:48.9
  Jan 11 12:23:48.908: INFO: Waiting up to 1m0s for all nodes to be ready
  Jan 11 12:24:49.006: INFO: Waiting for terminating namespaces to be deleted...
  Jan 11 12:24:49.014: INFO: Starting informer...
  STEP: Starting pods... @ 01/11/24 12:24:49.014
  Jan 11 12:24:49.259: INFO: Pod1 is running on env1-test-worker-1. Tainting Node
  Jan 11 12:24:51.516: INFO: Pod2 is running on env1-test-worker-1. Tainting Node
  STEP: Trying to apply a taint on the Node @ 01/11/24 12:24:51.516
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 01/11/24 12:24:51.547
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 01/11/24 12:24:51.575
  Jan 11 12:24:57.573: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  Jan 11 12:25:17.641: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Jan 11 12:25:17.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 01/11/24 12:25:17.7
  STEP: Destroying namespace "taint-multiple-pods-3400" for this suite. @ 01/11/24 12:25:17.73
• [88.938 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 01/11/24 12:25:17.8
  Jan 11 12:25:17.800: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename gc @ 01/11/24 12:25:17.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:25:17.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:25:17.901
  STEP: create the rc @ 01/11/24 12:25:17.923
  W0111 12:25:17.940466      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 01/11/24 12:25:23.062
  STEP: wait for all pods to be garbage collected @ 01/11/24 12:25:23.151
  STEP: Gathering metrics @ 01/11/24 12:25:28.179
  Jan 11 12:25:28.491: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jan 11 12:25:28.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2872" for this suite. @ 01/11/24 12:25:28.518
• [10.751 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 01/11/24 12:25:28.552
  Jan 11 12:25:28.552: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename webhook @ 01/11/24 12:25:28.554
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:25:28.609
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:25:28.624
  STEP: Setting up server cert @ 01/11/24 12:25:28.7
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/11/24 12:25:29.73
  STEP: Deploying the webhook pod @ 01/11/24 12:25:29.762
  STEP: Wait for the deployment to be ready @ 01/11/24 12:25:29.793
  Jan 11 12:25:29.814: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/11/24 12:25:31.84
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 12:25:31.865
  Jan 11 12:25:32.865: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 01/11/24 12:25:32.875
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 01/11/24 12:25:32.878
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 01/11/24 12:25:32.878
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 01/11/24 12:25:32.878
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 01/11/24 12:25:32.881
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 01/11/24 12:25:32.882
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 01/11/24 12:25:32.885
  Jan 11 12:25:32.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5709" for this suite. @ 01/11/24 12:25:33.027
  STEP: Destroying namespace "webhook-markers-8909" for this suite. @ 01/11/24 12:25:33.043
• [4.526 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 01/11/24 12:25:33.078
  Jan 11 12:25:33.078: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename taint-single-pod @ 01/11/24 12:25:33.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:25:33.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:25:33.176
  Jan 11 12:25:33.183: INFO: Waiting up to 1m0s for all nodes to be ready
  Jan 11 12:26:33.270: INFO: Waiting for terminating namespaces to be deleted...
  Jan 11 12:26:33.281: INFO: Starting informer...
  STEP: Starting pod... @ 01/11/24 12:26:33.281
  Jan 11 12:26:33.534: INFO: Pod is running on env1-test-worker-1. Tainting Node
  STEP: Trying to apply a taint on the Node @ 01/11/24 12:26:33.534
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 01/11/24 12:26:33.579
  STEP: Waiting short time to make sure Pod is queued for deletion @ 01/11/24 12:26:33.61
  Jan 11 12:26:33.610: INFO: Pod wasn't evicted. Proceeding
  Jan 11 12:26:33.610: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 01/11/24 12:26:33.704
  STEP: Waiting some time to make sure that toleration time passed. @ 01/11/24 12:26:33.738
  Jan 11 12:27:48.738: INFO: Pod wasn't evicted. Test successful
  Jan 11 12:27:48.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-2713" for this suite. @ 01/11/24 12:27:48.75
• [135.687 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 01/11/24 12:27:48.767
  Jan 11 12:27:48.767: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename services @ 01/11/24 12:27:48.769
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:27:48.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:27:48.816
  STEP: creating service in namespace services-6375 @ 01/11/24 12:27:48.822
  STEP: creating service affinity-clusterip-transition in namespace services-6375 @ 01/11/24 12:27:48.823
  STEP: creating replication controller affinity-clusterip-transition in namespace services-6375 @ 01/11/24 12:27:48.848
  I0111 12:27:48.881541      23 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-6375, replica count: 3
  I0111 12:27:51.934456      23 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 11 12:27:51.950: INFO: Creating new exec pod
  Jan 11 12:27:55.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-6375 exec execpod-affinityzllwv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Jan 11 12:27:55.367: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Jan 11 12:27:55.367: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 11 12:27:55.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-6375 exec execpod-affinityzllwv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.48.184 80'
  Jan 11 12:27:55.681: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.48.184 80\nConnection to 10.233.48.184 80 port [tcp/http] succeeded!\n"
  Jan 11 12:27:55.681: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 11 12:27:55.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-6375 exec execpod-affinityzllwv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.48.184:80/ ; done'
  Jan 11 12:27:56.164: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n"
  Jan 11 12:27:56.164: INFO: stdout: "\naffinity-clusterip-transition-bhp9d\naffinity-clusterip-transition-kv5gd\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-bhp9d\naffinity-clusterip-transition-kv5gd\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-bhp9d\naffinity-clusterip-transition-kv5gd\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-bhp9d\naffinity-clusterip-transition-kv5gd\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-bhp9d\naffinity-clusterip-transition-kv5gd\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-bhp9d"
  Jan 11 12:27:56.164: INFO: Received response from host: affinity-clusterip-transition-bhp9d
  Jan 11 12:27:56.164: INFO: Received response from host: affinity-clusterip-transition-kv5gd
  Jan 11 12:27:56.164: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.164: INFO: Received response from host: affinity-clusterip-transition-bhp9d
  Jan 11 12:27:56.164: INFO: Received response from host: affinity-clusterip-transition-kv5gd
  Jan 11 12:27:56.164: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.164: INFO: Received response from host: affinity-clusterip-transition-bhp9d
  Jan 11 12:27:56.164: INFO: Received response from host: affinity-clusterip-transition-kv5gd
  Jan 11 12:27:56.164: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.164: INFO: Received response from host: affinity-clusterip-transition-bhp9d
  Jan 11 12:27:56.164: INFO: Received response from host: affinity-clusterip-transition-kv5gd
  Jan 11 12:27:56.164: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.164: INFO: Received response from host: affinity-clusterip-transition-bhp9d
  Jan 11 12:27:56.164: INFO: Received response from host: affinity-clusterip-transition-kv5gd
  Jan 11 12:27:56.164: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.164: INFO: Received response from host: affinity-clusterip-transition-bhp9d
  Jan 11 12:27:56.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-6375 exec execpod-affinityzllwv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.48.184:80/ ; done'
  Jan 11 12:27:56.713: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.184:80/\n"
  Jan 11 12:27:56.714: INFO: stdout: "\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-82tkl\naffinity-clusterip-transition-82tkl"
  Jan 11 12:27:56.714: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.714: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.714: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.714: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.714: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.714: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.714: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.714: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.714: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.714: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.714: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.714: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.714: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.714: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.714: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.714: INFO: Received response from host: affinity-clusterip-transition-82tkl
  Jan 11 12:27:56.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 11 12:27:56.727: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6375, will wait for the garbage collector to delete the pods @ 01/11/24 12:27:56.768
  Jan 11 12:27:56.855: INFO: Deleting ReplicationController affinity-clusterip-transition took: 22.712529ms
  Jan 11 12:27:57.056: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 200.977378ms
  STEP: Destroying namespace "services-6375" for this suite. @ 01/11/24 12:27:59.562
• [10.832 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 01/11/24 12:27:59.599
  Jan 11 12:27:59.599: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubectl @ 01/11/24 12:27:59.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:27:59.671
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:27:59.682
  STEP: starting the proxy server @ 01/11/24 12:27:59.696
  Jan 11 12:27:59.697: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-9773 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 01/11/24 12:27:59.86
  Jan 11 12:27:59.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9773" for this suite. @ 01/11/24 12:27:59.905
• [0.328 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 01/11/24 12:27:59.931
  Jan 11 12:27:59.931: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename configmap @ 01/11/24 12:27:59.934
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:27:59.99
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:27:59.997
  STEP: Creating configMap with name cm-test-opt-del-ecb94330-f03a-4852-9784-674fd9c5873e @ 01/11/24 12:28:00.018
  STEP: Creating configMap with name cm-test-opt-upd-57f5e5a1-2c29-44a7-9cd3-70fb0972b6ff @ 01/11/24 12:28:00.036
  STEP: Creating the pod @ 01/11/24 12:28:00.056
  STEP: Deleting configmap cm-test-opt-del-ecb94330-f03a-4852-9784-674fd9c5873e @ 01/11/24 12:28:04.223
  STEP: Updating configmap cm-test-opt-upd-57f5e5a1-2c29-44a7-9cd3-70fb0972b6ff @ 01/11/24 12:28:04.244
  STEP: Creating configMap with name cm-test-opt-create-bba4b007-a09f-4ba9-b81f-12a87610397f @ 01/11/24 12:28:04.258
  STEP: waiting to observe update in volume @ 01/11/24 12:28:04.273
  Jan 11 12:29:09.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6557" for this suite. @ 01/11/24 12:29:09.127
• [69.221 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 01/11/24 12:29:09.153
  Jan 11 12:29:09.153: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename downward-api @ 01/11/24 12:29:09.156
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:29:09.188
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:29:09.196
  STEP: Creating a pod to test downward api env vars @ 01/11/24 12:29:09.202
  STEP: Saw pod success @ 01/11/24 12:29:13.263
  Jan 11 12:29:13.273: INFO: Trying to get logs from node env1-test-worker-2 pod downward-api-3fec57ac-8731-4e89-bc43-570891afd51a container dapi-container: <nil>
  STEP: delete the pod @ 01/11/24 12:29:13.334
  Jan 11 12:29:13.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4981" for this suite. @ 01/11/24 12:29:13.405
• [4.279 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 01/11/24 12:29:13.438
  Jan 11 12:29:13.438: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename configmap @ 01/11/24 12:29:13.44
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:29:13.495
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:29:13.506
  STEP: Creating configMap with name configmap-test-upd-d8d88863-4539-4cc7-8f15-5f37d54bfd4a @ 01/11/24 12:29:13.561
  STEP: Creating the pod @ 01/11/24 12:29:13.577
  STEP: Updating configmap configmap-test-upd-d8d88863-4539-4cc7-8f15-5f37d54bfd4a @ 01/11/24 12:29:15.667
  STEP: waiting to observe update in volume @ 01/11/24 12:29:15.676
  Jan 11 12:30:32.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2454" for this suite. @ 01/11/24 12:30:32.549
• [79.136 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 01/11/24 12:30:32.577
  Jan 11 12:30:32.577: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename emptydir @ 01/11/24 12:30:32.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:30:32.642
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:30:32.647
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 01/11/24 12:30:32.652
  STEP: Saw pod success @ 01/11/24 12:30:36.7
  Jan 11 12:30:36.709: INFO: Trying to get logs from node env1-test-worker-1 pod pod-f6497f58-d517-4934-bf6f-a33257517854 container test-container: <nil>
  STEP: delete the pod @ 01/11/24 12:30:36.731
  Jan 11 12:30:36.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9321" for this suite. @ 01/11/24 12:30:36.796
• [4.238 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 01/11/24 12:30:36.821
  Jan 11 12:30:36.821: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename configmap @ 01/11/24 12:30:36.823
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:30:36.87
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:30:36.878
  STEP: creating a ConfigMap @ 01/11/24 12:30:36.886
  STEP: fetching the ConfigMap @ 01/11/24 12:30:36.901
  STEP: patching the ConfigMap @ 01/11/24 12:30:36.912
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 01/11/24 12:30:36.928
  STEP: deleting the ConfigMap by collection with a label selector @ 01/11/24 12:30:36.941
  STEP: listing all ConfigMaps in test namespace @ 01/11/24 12:30:36.962
  Jan 11 12:30:36.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3256" for this suite. @ 01/11/24 12:30:36.984
• [0.194 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 01/11/24 12:30:37.033
  Jan 11 12:30:37.033: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-probe @ 01/11/24 12:30:37.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:30:37.074
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:30:37.081
  STEP: Creating pod busybox-d2866340-c919-4e65-9ac0-b67f0ea663d8 in namespace container-probe-1784 @ 01/11/24 12:30:37.092
  Jan 11 12:30:39.143: INFO: Started pod busybox-d2866340-c919-4e65-9ac0-b67f0ea663d8 in namespace container-probe-1784
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/11/24 12:30:39.143
  Jan 11 12:30:39.154: INFO: Initial restart count of pod busybox-d2866340-c919-4e65-9ac0-b67f0ea663d8 is 0
  Jan 11 12:34:40.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/11/24 12:34:40.589
  STEP: Destroying namespace "container-probe-1784" for this suite. @ 01/11/24 12:34:40.615
• [243.604 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 01/11/24 12:34:40.644
  Jan 11 12:34:40.645: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename subpath @ 01/11/24 12:34:40.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:34:40.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:34:40.682
  STEP: Setting up data @ 01/11/24 12:34:40.691
  STEP: Creating pod pod-subpath-test-downwardapi-x8q6 @ 01/11/24 12:34:40.714
  STEP: Creating a pod to test atomic-volume-subpath @ 01/11/24 12:34:40.715
  STEP: Saw pod success @ 01/11/24 12:35:04.887
  Jan 11 12:35:04.894: INFO: Trying to get logs from node env1-test-worker-1 pod pod-subpath-test-downwardapi-x8q6 container test-container-subpath-downwardapi-x8q6: <nil>
  STEP: delete the pod @ 01/11/24 12:35:04.942
  STEP: Deleting pod pod-subpath-test-downwardapi-x8q6 @ 01/11/24 12:35:04.983
  Jan 11 12:35:04.984: INFO: Deleting pod "pod-subpath-test-downwardapi-x8q6" in namespace "subpath-4144"
  Jan 11 12:35:04.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4144" for this suite. @ 01/11/24 12:35:05.002
• [24.372 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 01/11/24 12:35:05.028
  Jan 11 12:35:05.029: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/11/24 12:35:05.031
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:35:05.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:35:05.085
  Jan 11 12:35:05.091: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 01/11/24 12:35:15.053
  Jan 11 12:35:15.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-3864 --namespace=crd-publish-openapi-3864 create -f -'
  Jan 11 12:35:18.184: INFO: stderr: ""
  Jan 11 12:35:18.184: INFO: stdout: "e2e-test-crd-publish-openapi-3875-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jan 11 12:35:18.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-3864 --namespace=crd-publish-openapi-3864 delete e2e-test-crd-publish-openapi-3875-crds test-foo'
  Jan 11 12:35:18.364: INFO: stderr: ""
  Jan 11 12:35:18.364: INFO: stdout: "e2e-test-crd-publish-openapi-3875-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Jan 11 12:35:18.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-3864 --namespace=crd-publish-openapi-3864 apply -f -'
  Jan 11 12:35:21.108: INFO: stderr: ""
  Jan 11 12:35:21.108: INFO: stdout: "e2e-test-crd-publish-openapi-3875-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jan 11 12:35:21.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-3864 --namespace=crd-publish-openapi-3864 delete e2e-test-crd-publish-openapi-3875-crds test-foo'
  Jan 11 12:35:21.329: INFO: stderr: ""
  Jan 11 12:35:21.329: INFO: stdout: "e2e-test-crd-publish-openapi-3875-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 01/11/24 12:35:21.33
  Jan 11 12:35:21.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-3864 --namespace=crd-publish-openapi-3864 create -f -'
  Jan 11 12:35:23.941: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 01/11/24 12:35:23.941
  Jan 11 12:35:23.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-3864 --namespace=crd-publish-openapi-3864 create -f -'
  Jan 11 12:35:24.654: INFO: rc: 1
  Jan 11 12:35:24.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-3864 --namespace=crd-publish-openapi-3864 apply -f -'
  Jan 11 12:35:25.412: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 01/11/24 12:35:25.412
  Jan 11 12:35:25.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-3864 --namespace=crd-publish-openapi-3864 create -f -'
  Jan 11 12:35:26.164: INFO: rc: 1
  Jan 11 12:35:26.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-3864 --namespace=crd-publish-openapi-3864 apply -f -'
  Jan 11 12:35:26.978: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 01/11/24 12:35:26.978
  Jan 11 12:35:26.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-3864 explain e2e-test-crd-publish-openapi-3875-crds'
  Jan 11 12:35:27.665: INFO: stderr: ""
  Jan 11 12:35:27.665: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3875-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 01/11/24 12:35:27.667
  Jan 11 12:35:27.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-3864 explain e2e-test-crd-publish-openapi-3875-crds.metadata'
  Jan 11 12:35:28.396: INFO: stderr: ""
  Jan 11 12:35:28.396: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3875-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Jan 11 12:35:28.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-3864 explain e2e-test-crd-publish-openapi-3875-crds.spec'
  Jan 11 12:35:29.160: INFO: stderr: ""
  Jan 11 12:35:29.160: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3875-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Jan 11 12:35:29.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-3864 explain e2e-test-crd-publish-openapi-3875-crds.spec.bars'
  Jan 11 12:35:29.879: INFO: stderr: ""
  Jan 11 12:35:29.879: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3875-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 01/11/24 12:35:29.88
  Jan 11 12:35:29.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-3864 explain e2e-test-crd-publish-openapi-3875-crds.spec.bars2'
  Jan 11 12:35:30.591: INFO: rc: 1
  Jan 11 12:35:34.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3864" for this suite. @ 01/11/24 12:35:34.777
• [29.764 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 01/11/24 12:35:34.795
  Jan 11 12:35:34.795: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename deployment @ 01/11/24 12:35:34.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:35:34.838
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:35:34.846
  Jan 11 12:35:34.854: INFO: Creating deployment "webserver-deployment"
  Jan 11 12:35:34.866: INFO: Waiting for observed generation 1
  Jan 11 12:35:36.895: INFO: Waiting for all required pods to come up
  Jan 11 12:35:36.907: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 01/11/24 12:35:36.907
  Jan 11 12:35:38.934: INFO: Waiting for deployment "webserver-deployment" to complete
  Jan 11 12:35:38.949: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Jan 11 12:35:38.979: INFO: Updating deployment webserver-deployment
  Jan 11 12:35:38.979: INFO: Waiting for observed generation 2
  Jan 11 12:35:41.003: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Jan 11 12:35:41.014: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Jan 11 12:35:41.024: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jan 11 12:35:41.053: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Jan 11 12:35:41.053: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Jan 11 12:35:41.060: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jan 11 12:35:41.073: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Jan 11 12:35:41.073: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Jan 11 12:35:41.099: INFO: Updating deployment webserver-deployment
  Jan 11 12:35:41.099: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Jan 11 12:35:41.128: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Jan 11 12:35:41.165: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Jan 11 12:35:41.211: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-8210  4ee5bb50-a84b-434d-b850-f4b9214487e3 187181852 3 2024-01-11 12:35:34 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-11 12:35:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-11 12:35:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bcc028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2024-01-11 12:35:39 +0000 UTC,LastTransitionTime:2024-01-11 12:35:34 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-01-11 12:35:41 +0000 UTC,LastTransitionTime:2024-01-11 12:35:41 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Jan 11 12:35:41.236: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-8210  09b70250-6090-4811-af71-f1791743b4ed 187181848 3 2024-01-11 12:35:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 4ee5bb50-a84b-434d-b850-f4b9214487e3 0xc004bcc547 0xc004bcc548}] [] [{kube-controller-manager Update apps/v1 2024-01-11 12:35:39 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2024-01-11 12:35:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ee5bb50-a84b-434d-b850-f4b9214487e3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bcc5e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 11 12:35:41.236: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Jan 11 12:35:41.236: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-8210  6d758789-0941-47f8-88b6-69c22d430105 187181846 3 2024-01-11 12:35:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 4ee5bb50-a84b-434d-b850-f4b9214487e3 0xc004bcc457 0xc004bcc458}] [] [{kube-controller-manager Update apps/v1 2024-01-11 12:35:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2024-01-11 12:35:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4ee5bb50-a84b-434d-b850-f4b9214487e3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bcc4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Jan 11 12:35:41.256: INFO: Pod "webserver-deployment-67bd4bf6dc-5ccxf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5ccxf webserver-deployment-67bd4bf6dc- deployment-8210  0442fc02-905f-4cf4-a6ec-484649068ddd 187181861 0 2024-01-11 12:35:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6d758789-0941-47f8-88b6-69c22d430105 0xc004bccad7 0xc004bccad8}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d758789-0941-47f8-88b6-69c22d430105\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8dmr4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8dmr4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.257: INFO: Pod "webserver-deployment-67bd4bf6dc-6r92n" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6r92n webserver-deployment-67bd4bf6dc- deployment-8210  26b21e26-f36a-4674-88a5-cd9109075774 187181763 0 2024-01-11 12:35:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6d758789-0941-47f8-88b6-69c22d430105 0xc004bccc37 0xc004bccc38}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d758789-0941-47f8-88b6-69c22d430105\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 12:35:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.142\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nbtq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nbtq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:10.233.67.142,StartTime:2024-01-11 12:35:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-11 12:35:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://88bb3c003b98fc1e86a2398a4bd293335500cd83c55e5ec98a1ffdcb6345736f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.142,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.258: INFO: Pod "webserver-deployment-67bd4bf6dc-88h6r" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-88h6r webserver-deployment-67bd4bf6dc- deployment-8210  e8bb61a8-f678-46f0-bd61-ef9717d81525 187181750 0 2024-01-11 12:35:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6d758789-0941-47f8-88b6-69c22d430105 0xc004bcce17 0xc004bcce18}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d758789-0941-47f8-88b6-69c22d430105\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 12:35:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.69.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rdlp6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rdlp6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.202,PodIP:10.233.69.74,StartTime:2024-01-11 12:35:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-11 12:35:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d6f635d6980832042c7cb1e56eff630c3ef5c6302bf94fc1ed5f5a50b6edcd9c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.69.74,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.259: INFO: Pod "webserver-deployment-67bd4bf6dc-8zqdw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8zqdw webserver-deployment-67bd4bf6dc- deployment-8210  ab45b9c2-1223-464e-80e1-fc5d30a29226 187181857 0 2024-01-11 12:35:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6d758789-0941-47f8-88b6-69c22d430105 0xc004bcd007 0xc004bcd008}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d758789-0941-47f8-88b6-69c22d430105\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rlp6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rlp6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.260: INFO: Pod "webserver-deployment-67bd4bf6dc-f2d47" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-f2d47 webserver-deployment-67bd4bf6dc- deployment-8210  8bf35583-580a-4f0e-8ffb-b0e22065de81 187181855 0 2024-01-11 12:35:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6d758789-0941-47f8-88b6-69c22d430105 0xc004bcd150 0xc004bcd151}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d758789-0941-47f8-88b6-69c22d430105\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7jlw7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7jlw7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.261: INFO: Pod "webserver-deployment-67bd4bf6dc-hfw5n" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hfw5n webserver-deployment-67bd4bf6dc- deployment-8210  cf9b3836-6af1-4028-a0eb-846452378814 187181753 0 2024-01-11 12:35:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6d758789-0941-47f8-88b6-69c22d430105 0xc004bcd2a7 0xc004bcd2a8}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d758789-0941-47f8-88b6-69c22d430105\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 12:35:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.69.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xz4hj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xz4hj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.202,PodIP:10.233.69.75,StartTime:2024-01-11 12:35:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-11 12:35:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0c1b1d8cbba9ca14cdc67a4cf9c8e4775285e3e5094e3c2ba34e342c68126c77,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.69.75,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.262: INFO: Pod "webserver-deployment-67bd4bf6dc-hwjpz" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hwjpz webserver-deployment-67bd4bf6dc- deployment-8210  65987333-deb2-4161-8b25-a9ee498db74a 187181738 0 2024-01-11 12:35:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6d758789-0941-47f8-88b6-69c22d430105 0xc004bcd487 0xc004bcd488}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d758789-0941-47f8-88b6-69c22d430105\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 12:35:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lvx89,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lvx89,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:10.233.67.140,StartTime:2024-01-11 12:35:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-11 12:35:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://03e567daba44676bf09e0b7deebb11897898153a9bbe39584727a698ed8b9102,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.140,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.264: INFO: Pod "webserver-deployment-67bd4bf6dc-jc8l2" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jc8l2 webserver-deployment-67bd4bf6dc- deployment-8210  06abcc16-5d64-4d06-9ba7-84e9a61762e0 187181729 0 2024-01-11 12:35:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6d758789-0941-47f8-88b6-69c22d430105 0xc004bcd667 0xc004bcd668}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d758789-0941-47f8-88b6-69c22d430105\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 12:35:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tfpxl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tfpxl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:10.233.68.51,StartTime:2024-01-11 12:35:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-11 12:35:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3f4c725f989d9bfaa00f072a47c4ac9fd4db5508c58292491791cbe93b3d8fa2,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.51,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.265: INFO: Pod "webserver-deployment-67bd4bf6dc-nxmfz" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-nxmfz webserver-deployment-67bd4bf6dc- deployment-8210  ea8955c1-c4f3-480a-a59b-b31b41e2deee 187181760 0 2024-01-11 12:35:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6d758789-0941-47f8-88b6-69c22d430105 0xc004bcd857 0xc004bcd858}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d758789-0941-47f8-88b6-69c22d430105\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 12:35:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5gbc7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5gbc7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:10.233.68.52,StartTime:2024-01-11 12:35:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-11 12:35:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a805e5ad3759b740f533cb154a71138da4e08f817dcb69307cd2c36bf7f9473d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.52,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.266: INFO: Pod "webserver-deployment-67bd4bf6dc-rkvqn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rkvqn webserver-deployment-67bd4bf6dc- deployment-8210  574b4c21-bfbf-4a06-a842-707abee28fab 187181851 0 2024-01-11 12:35:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6d758789-0941-47f8-88b6-69c22d430105 0xc004bcda37 0xc004bcda38}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d758789-0941-47f8-88b6-69c22d430105\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qgtr6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qgtr6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.268: INFO: Pod "webserver-deployment-67bd4bf6dc-rqbqn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rqbqn webserver-deployment-67bd4bf6dc- deployment-8210  1e995ac7-c20e-4e71-bc13-031f5ea998e6 187181858 0 2024-01-11 12:35:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6d758789-0941-47f8-88b6-69c22d430105 0xc004bcdb97 0xc004bcdb98}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d758789-0941-47f8-88b6-69c22d430105\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wfbv5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wfbv5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.272: INFO: Pod "webserver-deployment-67bd4bf6dc-t4hct" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-t4hct webserver-deployment-67bd4bf6dc- deployment-8210  db0ddc27-6bc6-474e-b187-1199e353e61a 187181860 0 2024-01-11 12:35:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6d758789-0941-47f8-88b6-69c22d430105 0xc004bcdce0 0xc004bcdce1}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d758789-0941-47f8-88b6-69c22d430105\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qfwb4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qfwb4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.273: INFO: Pod "webserver-deployment-67bd4bf6dc-vdrlr" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vdrlr webserver-deployment-67bd4bf6dc- deployment-8210  8a86c343-eb6c-4dd5-8e44-6158f2a2f5c3 187181735 0 2024-01-11 12:35:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6d758789-0941-47f8-88b6-69c22d430105 0xc004bcde20 0xc004bcde21}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d758789-0941-47f8-88b6-69c22d430105\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 12:35:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qrj7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qrj7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:10.233.67.141,StartTime:2024-01-11 12:35:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-11 12:35:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://dd6551a3c30373d0ba42ad93dcc347595a1c09310aa78319b3c55db844056d6e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.141,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.274: INFO: Pod "webserver-deployment-67bd4bf6dc-vkzl7" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vkzl7 webserver-deployment-67bd4bf6dc- deployment-8210  f215bafe-b3c7-4842-8098-770d92143a8d 187181746 0 2024-01-11 12:35:35 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6d758789-0941-47f8-88b6-69c22d430105 0xc004bcdff7 0xc004bcdff8}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d758789-0941-47f8-88b6-69c22d430105\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 12:35:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.69.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kzzmd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kzzmd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.202,PodIP:10.233.69.76,StartTime:2024-01-11 12:35:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-11 12:35:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://194f9b8fe5614a6d3fa3c716feef37fb8ad4f6048b70241752c9a2fd137c48c1,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.69.76,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.275: INFO: Pod "webserver-deployment-67bd4bf6dc-z4hjv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-z4hjv webserver-deployment-67bd4bf6dc- deployment-8210  c0b6bd39-3325-420f-bdc6-7b4dc8f96668 187181859 0 2024-01-11 12:35:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6d758789-0941-47f8-88b6-69c22d430105 0xc00468c1d7 0xc00468c1d8}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d758789-0941-47f8-88b6-69c22d430105\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2rxv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2rxv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.276: INFO: Pod "webserver-deployment-7b75d79cf5-5rthj" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-5rthj webserver-deployment-7b75d79cf5- deployment-8210  7ea04121-4d1e-4494-be68-df878abc8595 187181804 0 2024-01-11 12:35:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 09b70250-6090-4811-af71-f1791743b4ed 0xc00468c320 0xc00468c321}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09b70250-6090-4811-af71-f1791743b4ed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 12:35:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j8xvk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j8xvk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:,StartTime:2024-01-11 12:35:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.277: INFO: Pod "webserver-deployment-7b75d79cf5-dgp4f" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-dgp4f webserver-deployment-7b75d79cf5- deployment-8210  ce071dad-4606-47d3-8216-e19987cbbaf1 187181827 0 2024-01-11 12:35:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 09b70250-6090-4811-af71-f1791743b4ed 0xc00468c507 0xc00468c508}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09b70250-6090-4811-af71-f1791743b4ed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 12:35:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jxggb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jxggb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.202,PodIP:,StartTime:2024-01-11 12:35:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.278: INFO: Pod "webserver-deployment-7b75d79cf5-gn9hn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-gn9hn webserver-deployment-7b75d79cf5- deployment-8210  339bc996-9440-4b22-bcd9-c2095984d5ed 187181792 0 2024-01-11 12:35:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 09b70250-6090-4811-af71-f1791743b4ed 0xc00468c6e7 0xc00468c6e8}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09b70250-6090-4811-af71-f1791743b4ed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 12:35:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j59qp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j59qp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.202,PodIP:,StartTime:2024-01-11 12:35:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.279: INFO: Pod "webserver-deployment-7b75d79cf5-hkm8b" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-hkm8b webserver-deployment-7b75d79cf5- deployment-8210  7635119f-081b-4bbb-9d04-f7ac061188a9 187181829 0 2024-01-11 12:35:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 09b70250-6090-4811-af71-f1791743b4ed 0xc00468c8c7 0xc00468c8c8}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09b70250-6090-4811-af71-f1791743b4ed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 12:35:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vwncr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vwncr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:,StartTime:2024-01-11 12:35:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.280: INFO: Pod "webserver-deployment-7b75d79cf5-ptc7c" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-ptc7c webserver-deployment-7b75d79cf5- deployment-8210  3a6abfe2-3f5d-4365-a0f2-bf24d971bbdc 187181853 0 2024-01-11 12:35:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 09b70250-6090-4811-af71-f1791743b4ed 0xc00468caa7 0xc00468caa8}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09b70250-6090-4811-af71-f1791743b4ed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6dpgg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6dpgg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.281: INFO: Pod "webserver-deployment-7b75d79cf5-wfxmb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-wfxmb webserver-deployment-7b75d79cf5- deployment-8210  24778f77-791c-4f89-9b9f-bd5b74eec144 187181803 0 2024-01-11 12:35:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 09b70250-6090-4811-af71-f1791743b4ed 0xc00468cc00 0xc00468cc01}] [] [{kube-controller-manager Update v1 2024-01-11 12:35:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09b70250-6090-4811-af71-f1791743b4ed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 12:35:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ht8cv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ht8cv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:35:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:,StartTime:2024-01-11 12:35:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:35:41.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8210" for this suite. @ 01/11/24 12:35:41.34
• [6.591 seconds]
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 01/11/24 12:35:41.386
  Jan 11 12:35:41.386: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename cronjob @ 01/11/24 12:35:41.388
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:35:41.702
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:35:41.709
  STEP: Creating a suspended cronjob @ 01/11/24 12:35:41.714
  STEP: Ensuring no jobs are scheduled @ 01/11/24 12:35:41.733
  STEP: Ensuring no job exists by listing jobs explicitly @ 01/11/24 12:40:41.753
  STEP: Removing cronjob @ 01/11/24 12:40:41.762
  Jan 11 12:40:41.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2671" for this suite. @ 01/11/24 12:40:41.813
• [300.448 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 01/11/24 12:40:41.841
  Jan 11 12:40:41.841: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename services @ 01/11/24 12:40:41.843
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:40:41.889
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:40:41.897
  STEP: creating an Endpoint @ 01/11/24 12:40:41.916
  STEP: waiting for available Endpoint @ 01/11/24 12:40:41.931
  STEP: listing all Endpoints @ 01/11/24 12:40:41.936
  STEP: updating the Endpoint @ 01/11/24 12:40:41.948
  STEP: fetching the Endpoint @ 01/11/24 12:40:41.964
  STEP: patching the Endpoint @ 01/11/24 12:40:41.973
  STEP: fetching the Endpoint @ 01/11/24 12:40:41.996
  STEP: deleting the Endpoint by Collection @ 01/11/24 12:40:42.004
  STEP: waiting for Endpoint deletion @ 01/11/24 12:40:42.024
  STEP: fetching the Endpoint @ 01/11/24 12:40:42.028
  Jan 11 12:40:42.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4173" for this suite. @ 01/11/24 12:40:42.054
• [0.238 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 01/11/24 12:40:42.093
  Jan 11 12:40:42.094: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename configmap @ 01/11/24 12:40:42.096
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:40:42.135
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:40:42.144
  STEP: Creating configMap that has name configmap-test-emptyKey-ba85f2e9-e25d-4d98-a71d-da40864b4e15 @ 01/11/24 12:40:42.149
  Jan 11 12:40:42.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9785" for this suite. @ 01/11/24 12:40:42.181
• [0.110 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 01/11/24 12:40:42.207
  Jan 11 12:40:42.207: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename job @ 01/11/24 12:40:42.209
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:40:42.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:40:42.253
  STEP: Creating a job @ 01/11/24 12:40:42.259
  STEP: Ensuring active pods == parallelism @ 01/11/24 12:40:42.272
  STEP: delete a job @ 01/11/24 12:40:44.284
  STEP: deleting Job.batch foo in namespace job-4121, will wait for the garbage collector to delete the pods @ 01/11/24 12:40:44.285
  Jan 11 12:40:44.363: INFO: Deleting Job.batch foo took: 16.094526ms
  Jan 11 12:40:44.564: INFO: Terminating Job.batch foo pods took: 200.423835ms
  STEP: Ensuring job was deleted @ 01/11/24 12:41:16.065
  Jan 11 12:41:16.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-4121" for this suite. @ 01/11/24 12:41:16.09
• [33.912 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 01/11/24 12:41:16.121
  Jan 11 12:41:16.121: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename replicaset @ 01/11/24 12:41:16.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:41:16.165
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:41:16.171
  Jan 11 12:41:16.177: INFO: Creating ReplicaSet my-hostname-basic-de7e3dd9-5bc4-4bf9-9d4e-3c52608a8380
  Jan 11 12:41:16.197: INFO: Pod name my-hostname-basic-de7e3dd9-5bc4-4bf9-9d4e-3c52608a8380: Found 0 pods out of 1
  Jan 11 12:41:21.207: INFO: Pod name my-hostname-basic-de7e3dd9-5bc4-4bf9-9d4e-3c52608a8380: Found 1 pods out of 1
  Jan 11 12:41:21.207: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-de7e3dd9-5bc4-4bf9-9d4e-3c52608a8380" is running
  Jan 11 12:41:21.215: INFO: Pod "my-hostname-basic-de7e3dd9-5bc4-4bf9-9d4e-3c52608a8380-8ggfv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-11 12:41:16 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-11 12:41:17 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-11 12:41:17 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-11 12:41:16 +0000 UTC Reason: Message:}])
  Jan 11 12:41:21.215: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 01/11/24 12:41:21.215
  Jan 11 12:41:21.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2153" for this suite. @ 01/11/24 12:41:21.277
• [5.179 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:385
  STEP: Creating a kubernetes client @ 01/11/24 12:41:21.303
  Jan 11 12:41:21.303: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename daemonsets @ 01/11/24 12:41:21.306
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:41:21.367
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:41:21.375
  Jan 11 12:41:21.456: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 01/11/24 12:41:21.486
  Jan 11 12:41:21.505: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:21.505: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:21.506: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:21.516: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 12:41:21.516: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  Jan 11 12:41:22.536: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:22.536: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:22.536: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:22.547: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 12:41:22.547: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  Jan 11 12:41:23.530: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:23.531: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:23.531: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:23.540: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jan 11 12:41:23.540: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 01/11/24 12:41:23.574
  STEP: Check that daemon pods images are updated. @ 01/11/24 12:41:23.606
  Jan 11 12:41:23.614: INFO: Wrong image for pod: daemon-set-4rt7w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jan 11 12:41:23.614: INFO: Wrong image for pod: daemon-set-96b29. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jan 11 12:41:23.614: INFO: Wrong image for pod: daemon-set-zl6nw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jan 11 12:41:23.639: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:23.639: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:23.639: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:24.651: INFO: Wrong image for pod: daemon-set-4rt7w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jan 11 12:41:24.652: INFO: Pod daemon-set-cnltd is not available
  Jan 11 12:41:24.652: INFO: Wrong image for pod: daemon-set-zl6nw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jan 11 12:41:24.667: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:24.668: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:24.668: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:25.648: INFO: Wrong image for pod: daemon-set-4rt7w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jan 11 12:41:25.648: INFO: Pod daemon-set-cnltd is not available
  Jan 11 12:41:25.648: INFO: Wrong image for pod: daemon-set-zl6nw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jan 11 12:41:25.660: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:25.660: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:25.660: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:26.654: INFO: Wrong image for pod: daemon-set-4rt7w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jan 11 12:41:26.654: INFO: Pod daemon-set-hrgts is not available
  Jan 11 12:41:26.681: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:26.682: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:26.682: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:27.659: INFO: Wrong image for pod: daemon-set-4rt7w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jan 11 12:41:27.660: INFO: Pod daemon-set-hrgts is not available
  Jan 11 12:41:27.683: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:27.684: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:27.684: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:28.653: INFO: Pod daemon-set-2h9b6 is not available
  Jan 11 12:41:28.669: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:28.669: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:28.669: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 01/11/24 12:41:28.669
  Jan 11 12:41:28.699: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:28.699: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:28.700: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:28.710: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 11 12:41:28.710: INFO: Node env1-test-worker-2 is running 0 daemon pod, expected 1
  Jan 11 12:41:29.729: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:29.729: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:29.729: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:41:29.743: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jan 11 12:41:29.743: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 01/11/24 12:41:29.801
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9131, will wait for the garbage collector to delete the pods @ 01/11/24 12:41:29.801
  Jan 11 12:41:29.882: INFO: Deleting DaemonSet.extensions daemon-set took: 19.952877ms
  Jan 11 12:41:29.983: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.983522ms
  Jan 11 12:41:31.193: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 12:41:31.193: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jan 11 12:41:31.201: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"187183794"},"items":null}

  Jan 11 12:41:31.210: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"187183794"},"items":null}

  Jan 11 12:41:31.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9131" for this suite. @ 01/11/24 12:41:31.287
• [10.009 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 01/11/24 12:41:31.318
  Jan 11 12:41:31.318: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename deployment @ 01/11/24 12:41:31.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:41:31.374
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:41:31.382
  STEP: creating a Deployment @ 01/11/24 12:41:31.403
  Jan 11 12:41:31.403: INFO: Creating simple deployment test-deployment-ndw4d
  Jan 11 12:41:31.449: INFO: deployment "test-deployment-ndw4d" doesn't have the required revision set
  STEP: Getting /status @ 01/11/24 12:41:33.501
  Jan 11 12:41:33.512: INFO: Deployment test-deployment-ndw4d has Conditions: [{Available True 2024-01-11 12:41:32 +0000 UTC 2024-01-11 12:41:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2024-01-11 12:41:32 +0000 UTC 2024-01-11 12:41:31 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-ndw4d-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 01/11/24 12:41:33.512
  Jan 11 12:41:33.588: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 12, 41, 32, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 12, 41, 32, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 12, 41, 32, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 12, 41, 31, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-ndw4d-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 01/11/24 12:41:33.589
  Jan 11 12:41:33.593: INFO: Observed &Deployment event: ADDED
  Jan 11 12:41:33.593: INFO: Observed Deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-11 12:41:31 +0000 UTC 2024-01-11 12:41:31 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-ndw4d-5994cf9475"}
  Jan 11 12:41:33.593: INFO: Observed &Deployment event: MODIFIED
  Jan 11 12:41:33.593: INFO: Observed Deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-11 12:41:31 +0000 UTC 2024-01-11 12:41:31 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-ndw4d-5994cf9475"}
  Jan 11 12:41:33.593: INFO: Observed Deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-11 12:41:31 +0000 UTC 2024-01-11 12:41:31 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jan 11 12:41:33.594: INFO: Observed &Deployment event: MODIFIED
  Jan 11 12:41:33.594: INFO: Observed Deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-11 12:41:31 +0000 UTC 2024-01-11 12:41:31 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jan 11 12:41:33.594: INFO: Observed Deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-11 12:41:31 +0000 UTC 2024-01-11 12:41:31 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-ndw4d-5994cf9475" is progressing.}
  Jan 11 12:41:33.594: INFO: Observed &Deployment event: MODIFIED
  Jan 11 12:41:33.595: INFO: Observed Deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-11 12:41:32 +0000 UTC 2024-01-11 12:41:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jan 11 12:41:33.595: INFO: Observed Deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-11 12:41:32 +0000 UTC 2024-01-11 12:41:31 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-ndw4d-5994cf9475" has successfully progressed.}
  Jan 11 12:41:33.595: INFO: Observed &Deployment event: MODIFIED
  Jan 11 12:41:33.595: INFO: Observed Deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-11 12:41:32 +0000 UTC 2024-01-11 12:41:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jan 11 12:41:33.595: INFO: Observed Deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-11 12:41:32 +0000 UTC 2024-01-11 12:41:31 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-ndw4d-5994cf9475" has successfully progressed.}
  Jan 11 12:41:33.595: INFO: Found Deployment test-deployment-ndw4d in namespace deployment-368 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 11 12:41:33.595: INFO: Deployment test-deployment-ndw4d has an updated status
  STEP: patching the Statefulset Status @ 01/11/24 12:41:33.595
  Jan 11 12:41:33.595: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jan 11 12:41:33.620: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 01/11/24 12:41:33.62
  Jan 11 12:41:33.624: INFO: Observed &Deployment event: ADDED
  Jan 11 12:41:33.624: INFO: Observed deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-11 12:41:31 +0000 UTC 2024-01-11 12:41:31 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-ndw4d-5994cf9475"}
  Jan 11 12:41:33.625: INFO: Observed &Deployment event: MODIFIED
  Jan 11 12:41:33.626: INFO: Observed deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-11 12:41:31 +0000 UTC 2024-01-11 12:41:31 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-ndw4d-5994cf9475"}
  Jan 11 12:41:33.626: INFO: Observed deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-11 12:41:31 +0000 UTC 2024-01-11 12:41:31 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jan 11 12:41:33.627: INFO: Observed &Deployment event: MODIFIED
  Jan 11 12:41:33.627: INFO: Observed deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-11 12:41:31 +0000 UTC 2024-01-11 12:41:31 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jan 11 12:41:33.627: INFO: Observed deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-11 12:41:31 +0000 UTC 2024-01-11 12:41:31 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-ndw4d-5994cf9475" is progressing.}
  Jan 11 12:41:33.627: INFO: Observed &Deployment event: MODIFIED
  Jan 11 12:41:33.628: INFO: Observed deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-11 12:41:32 +0000 UTC 2024-01-11 12:41:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jan 11 12:41:33.628: INFO: Observed deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-11 12:41:32 +0000 UTC 2024-01-11 12:41:31 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-ndw4d-5994cf9475" has successfully progressed.}
  Jan 11 12:41:33.629: INFO: Observed &Deployment event: MODIFIED
  Jan 11 12:41:33.629: INFO: Observed deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-11 12:41:32 +0000 UTC 2024-01-11 12:41:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jan 11 12:41:33.629: INFO: Observed deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-11 12:41:32 +0000 UTC 2024-01-11 12:41:31 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-ndw4d-5994cf9475" has successfully progressed.}
  Jan 11 12:41:33.629: INFO: Observed deployment test-deployment-ndw4d in namespace deployment-368 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 11 12:41:33.630: INFO: Observed &Deployment event: MODIFIED
  Jan 11 12:41:33.630: INFO: Found deployment test-deployment-ndw4d in namespace deployment-368 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Jan 11 12:41:33.630: INFO: Deployment test-deployment-ndw4d has a patched status
  Jan 11 12:41:33.649: INFO: Deployment "test-deployment-ndw4d":
  &Deployment{ObjectMeta:{test-deployment-ndw4d  deployment-368  85be6f0f-e67e-4b83-a4f6-b45dcae39cd3 187183827 1 2024-01-11 12:41:31 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2024-01-11 12:41:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2024-01-11 12:41:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2024-01-11 12:41:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00707b478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-ndw4d-5994cf9475",LastUpdateTime:2024-01-11 12:41:33 +0000 UTC,LastTransitionTime:2024-01-11 12:41:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jan 11 12:41:33.665: INFO: New ReplicaSet "test-deployment-ndw4d-5994cf9475" of Deployment "test-deployment-ndw4d":
  &ReplicaSet{ObjectMeta:{test-deployment-ndw4d-5994cf9475  deployment-368  5161789e-b664-4fb2-b72a-71216108634e 187183819 1 2024-01-11 12:41:31 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-ndw4d 85be6f0f-e67e-4b83-a4f6-b45dcae39cd3 0xc002e2d137 0xc002e2d138}] [] [{kube-controller-manager Update apps/v1 2024-01-11 12:41:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85be6f0f-e67e-4b83-a4f6-b45dcae39cd3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-11 12:41:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e2d2c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jan 11 12:41:33.677: INFO: Pod "test-deployment-ndw4d-5994cf9475-ldzxt" is available:
  &Pod{ObjectMeta:{test-deployment-ndw4d-5994cf9475-ldzxt test-deployment-ndw4d-5994cf9475- deployment-368  5b4ea7f5-97e5-45fc-888e-4bf20962b262 187183818 0 2024-01-11 12:41:31 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-ndw4d-5994cf9475 5161789e-b664-4fb2-b72a-71216108634e 0xc002e2d677 0xc002e2d678}] [] [{kube-controller-manager Update v1 2024-01-11 12:41:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5161789e-b664-4fb2-b72a-71216108634e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 12:41:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-69xjb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-69xjb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:41:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:41:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:41:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 12:41:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:10.233.68.62,StartTime:2024-01-11 12:41:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-11 12:41:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://905f8e0e60c2566407c7b8d1f34981932bdbfa7014239e0e5fbf2379f10f86b0,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.62,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 12:41:33.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-368" for this suite. @ 01/11/24 12:41:33.695
• [2.405 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 01/11/24 12:41:33.726
  Jan 11 12:41:33.726: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename dns @ 01/11/24 12:41:33.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:41:33.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:41:33.789
  STEP: Creating a test headless service @ 01/11/24 12:41:33.797
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9458.svc.cluster.local;sleep 1; done
   @ 01/11/24 12:41:33.814
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9458.svc.cluster.local;sleep 1; done
   @ 01/11/24 12:41:33.814
  STEP: creating a pod to probe DNS @ 01/11/24 12:41:33.814
  STEP: submitting the pod to kubernetes @ 01/11/24 12:41:33.815
  STEP: retrieving the pod @ 01/11/24 12:41:37.886
  STEP: looking for the results for each expected name from probers @ 01/11/24 12:41:37.895
  Jan 11 12:41:37.909: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-caf59d30-545b-42df-a557-db3e55a0ea03: the server could not find the requested resource (get pods dns-test-caf59d30-545b-42df-a557-db3e55a0ea03)
  Jan 11 12:41:37.923: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-caf59d30-545b-42df-a557-db3e55a0ea03: the server could not find the requested resource (get pods dns-test-caf59d30-545b-42df-a557-db3e55a0ea03)
  Jan 11 12:41:37.932: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-caf59d30-545b-42df-a557-db3e55a0ea03: the server could not find the requested resource (get pods dns-test-caf59d30-545b-42df-a557-db3e55a0ea03)
  Jan 11 12:41:37.944: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-caf59d30-545b-42df-a557-db3e55a0ea03: the server could not find the requested resource (get pods dns-test-caf59d30-545b-42df-a557-db3e55a0ea03)
  Jan 11 12:41:37.959: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-caf59d30-545b-42df-a557-db3e55a0ea03: the server could not find the requested resource (get pods dns-test-caf59d30-545b-42df-a557-db3e55a0ea03)
  Jan 11 12:41:37.967: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-caf59d30-545b-42df-a557-db3e55a0ea03: the server could not find the requested resource (get pods dns-test-caf59d30-545b-42df-a557-db3e55a0ea03)
  Jan 11 12:41:37.975: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-caf59d30-545b-42df-a557-db3e55a0ea03: the server could not find the requested resource (get pods dns-test-caf59d30-545b-42df-a557-db3e55a0ea03)
  Jan 11 12:41:37.985: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-caf59d30-545b-42df-a557-db3e55a0ea03: the server could not find the requested resource (get pods dns-test-caf59d30-545b-42df-a557-db3e55a0ea03)
  Jan 11 12:41:37.986: INFO: Lookups using dns-9458/dns-test-caf59d30-545b-42df-a557-db3e55a0ea03 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9458.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local jessie_udp@dns-test-service-2.dns-9458.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9458.svc.cluster.local]

  Jan 11 12:41:43.078: INFO: DNS probes using dns-9458/dns-test-caf59d30-545b-42df-a557-db3e55a0ea03 succeeded

  Jan 11 12:41:43.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/11/24 12:41:43.094
  STEP: deleting the test headless service @ 01/11/24 12:41:43.159
  STEP: Destroying namespace "dns-9458" for this suite. @ 01/11/24 12:41:43.226
• [9.545 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 01/11/24 12:41:43.273
  Jan 11 12:41:43.273: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubectl @ 01/11/24 12:41:43.275
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:41:43.329
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:41:43.335
  STEP: creating all guestbook components @ 01/11/24 12:41:43.343
  Jan 11 12:41:43.343: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Jan 11 12:41:43.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1884 create -f -'
  Jan 11 12:41:46.069: INFO: stderr: ""
  Jan 11 12:41:46.069: INFO: stdout: "service/agnhost-replica created\n"
  Jan 11 12:41:46.069: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Jan 11 12:41:46.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1884 create -f -'
  Jan 11 12:41:48.960: INFO: stderr: ""
  Jan 11 12:41:48.960: INFO: stdout: "service/agnhost-primary created\n"
  Jan 11 12:41:48.960: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Jan 11 12:41:48.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1884 create -f -'
  Jan 11 12:41:51.867: INFO: stderr: ""
  Jan 11 12:41:51.867: INFO: stdout: "service/frontend created\n"
  Jan 11 12:41:51.867: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Jan 11 12:41:51.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1884 create -f -'
  Jan 11 12:41:52.673: INFO: stderr: ""
  Jan 11 12:41:52.673: INFO: stdout: "deployment.apps/frontend created\n"
  Jan 11 12:41:52.673: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jan 11 12:41:52.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1884 create -f -'
  Jan 11 12:41:53.586: INFO: stderr: ""
  Jan 11 12:41:53.586: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Jan 11 12:41:53.586: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jan 11 12:41:53.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1884 create -f -'
  Jan 11 12:41:54.525: INFO: stderr: ""
  Jan 11 12:41:54.525: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 01/11/24 12:41:54.525
  Jan 11 12:41:54.525: INFO: Waiting for all frontend pods to be Running.
  Jan 11 12:41:54.576: INFO: Waiting for frontend to serve content.
  Jan 11 12:41:55.628: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
  Jan 11 12:42:00.661: INFO: Trying to add a new entry to the guestbook.
  Jan 11 12:42:00.696: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 01/11/24 12:42:00.722
  Jan 11 12:42:00.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1884 delete --grace-period=0 --force -f -'
  Jan 11 12:42:00.972: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 11 12:42:00.972: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 01/11/24 12:42:00.972
  Jan 11 12:42:00.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1884 delete --grace-period=0 --force -f -'
  Jan 11 12:42:01.248: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 11 12:42:01.248: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 01/11/24 12:42:01.248
  Jan 11 12:42:01.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1884 delete --grace-period=0 --force -f -'
  Jan 11 12:42:01.558: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 11 12:42:01.558: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 01/11/24 12:42:01.558
  Jan 11 12:42:01.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1884 delete --grace-period=0 --force -f -'
  Jan 11 12:42:01.769: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 11 12:42:01.770: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 01/11/24 12:42:01.77
  Jan 11 12:42:01.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1884 delete --grace-period=0 --force -f -'
  Jan 11 12:42:01.980: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 11 12:42:01.980: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 01/11/24 12:42:01.98
  Jan 11 12:42:01.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-1884 delete --grace-period=0 --force -f -'
  Jan 11 12:42:02.166: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 11 12:42:02.166: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Jan 11 12:42:02.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1884" for this suite. @ 01/11/24 12:42:02.179
• [18.920 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 01/11/24 12:42:02.197
  Jan 11 12:42:02.197: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename sched-pred @ 01/11/24 12:42:02.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:42:02.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:42:02.247
  Jan 11 12:42:02.282: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jan 11 12:42:02.305: INFO: Waiting for terminating namespaces to be deleted...
  Jan 11 12:42:02.314: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-0 before test
  Jan 11 12:42:02.333: INFO: kube-flannel-r8g5h from kube-system started at 2024-01-09 16:24:44 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.333: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 11 12:42:02.333: INFO: kube-proxy-c8shb from kube-system started at 2024-01-11 00:56:00 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.333: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 11 12:42:02.333: INFO: metrics-server-6b7574f5b-jmbtm from kube-system started at 2024-01-09 16:32:13 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.333: INFO: 	Container metrics-server ready: true, restart count 0
  Jan 11 12:42:02.333: INFO: nginx-proxy-env1-test-worker-0 from kube-system started at 2024-01-09 16:31:48 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.333: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 11 12:42:02.333: INFO: nodelocaldns-vkvkp from kube-system started at 2024-01-09 15:52:20 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.333: INFO: 	Container node-cache ready: true, restart count 0
  Jan 11 12:42:02.333: INFO: vsphere-csi-node-5gf9n from kube-system started at 2024-01-11 01:05:49 +0000 UTC (3 container statuses recorded)
  Jan 11 12:42:02.333: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 11 12:42:02.333: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 11 12:42:02.333: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 11 12:42:02.333: INFO: frontend-5b6f6d589f-cc974 from kubectl-1884 started at 2024-01-11 12:41:52 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.333: INFO: 	Container guestbook-frontend ready: false, restart count 0
  Jan 11 12:42:02.333: INFO: sonobuoy-systemd-logs-daemon-set-7b26ca18c17c40c9-cqtts from sonobuoy started at 2024-01-11 11:18:31 +0000 UTC (2 container statuses recorded)
  Jan 11 12:42:02.333: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 11 12:42:02.333: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 11 12:42:02.333: INFO: traefik-ingress-g4tjs from traefik-ingress started at 2024-01-10 14:39:42 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.333: INFO: 	Container traefik-ingress ready: true, restart count 0
  Jan 11 12:42:02.333: INFO: velero-794b84894f-nwdwf from velero started at 2024-01-10 13:26:26 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.333: INFO: 	Container velero ready: true, restart count 0
  Jan 11 12:42:02.333: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-1 before test
  Jan 11 12:42:02.354: INFO: kube-flannel-jxf5s from kube-system started at 2024-01-09 16:24:09 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.354: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 11 12:42:02.354: INFO: kube-proxy-sdfjc from kube-system started at 2024-01-11 00:56:00 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.354: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 11 12:42:02.354: INFO: nginx-proxy-env1-test-worker-1 from kube-system started at 2024-01-09 16:38:31 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.354: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 11 12:42:02.354: INFO: nodelocaldns-7qx4w from kube-system started at 2024-01-09 15:52:12 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.354: INFO: 	Container node-cache ready: true, restart count 0
  Jan 11 12:42:02.354: INFO: vsphere-csi-node-pwx5m from kube-system started at 2024-01-11 01:05:49 +0000 UTC (3 container statuses recorded)
  Jan 11 12:42:02.354: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 11 12:42:02.354: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 11 12:42:02.354: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 11 12:42:02.354: INFO: agnhost-primary-779fbc64d9-qz2w8 from kubectl-1884 started at 2024-01-11 12:41:53 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.354: INFO: 	Container primary ready: true, restart count 0
  Jan 11 12:42:02.354: INFO: agnhost-replica-dc6f7f69c-nx4wz from kubectl-1884 started at 2024-01-11 12:41:54 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.354: INFO: 	Container replica ready: true, restart count 0
  Jan 11 12:42:02.354: INFO: frontend-5b6f6d589f-47ppl from kubectl-1884 started at 2024-01-11 12:41:52 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.354: INFO: 	Container guestbook-frontend ready: false, restart count 0
  Jan 11 12:42:02.354: INFO: sonobuoy from sonobuoy started at 2024-01-11 11:18:29 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.354: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jan 11 12:42:02.354: INFO: sonobuoy-systemd-logs-daemon-set-7b26ca18c17c40c9-zn5jg from sonobuoy started at 2024-01-11 11:18:31 +0000 UTC (2 container statuses recorded)
  Jan 11 12:42:02.354: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 11 12:42:02.354: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 11 12:42:02.354: INFO: traefik-ingress-j2r8r from traefik-ingress started at 2024-01-11 12:26:34 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.354: INFO: 	Container traefik-ingress ready: true, restart count 0
  Jan 11 12:42:02.354: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-2 before test
  Jan 11 12:42:02.376: INFO: kube-flannel-hlhld from kube-system started at 2024-01-11 00:55:50 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.376: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 11 12:42:02.376: INFO: kube-proxy-s78x8 from kube-system started at 2024-01-11 00:56:00 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.376: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 11 12:42:02.376: INFO: nginx-proxy-env1-test-worker-2 from kube-system started at 2024-01-11 01:03:57 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.376: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 11 12:42:02.376: INFO: nodelocaldns-49gbl from kube-system started at 2024-01-11 00:55:50 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.376: INFO: 	Container node-cache ready: true, restart count 0
  Jan 11 12:42:02.376: INFO: vsphere-csi-node-75mjp from kube-system started at 2024-01-11 01:05:49 +0000 UTC (3 container statuses recorded)
  Jan 11 12:42:02.376: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 11 12:42:02.376: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 11 12:42:02.376: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 11 12:42:02.376: INFO: agnhost-replica-dc6f7f69c-xw424 from kubectl-1884 started at 2024-01-11 12:41:54 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.376: INFO: 	Container replica ready: true, restart count 0
  Jan 11 12:42:02.376: INFO: frontend-5b6f6d589f-9k8g9 from kubectl-1884 started at 2024-01-11 12:41:52 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.376: INFO: 	Container guestbook-frontend ready: false, restart count 0
  Jan 11 12:42:02.376: INFO: sonobuoy-e2e-job-4f4795256cf94e3a from sonobuoy started at 2024-01-11 11:18:30 +0000 UTC (2 container statuses recorded)
  Jan 11 12:42:02.376: INFO: 	Container e2e ready: true, restart count 0
  Jan 11 12:42:02.376: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 11 12:42:02.376: INFO: sonobuoy-systemd-logs-daemon-set-7b26ca18c17c40c9-7pxz5 from sonobuoy started at 2024-01-11 11:18:31 +0000 UTC (2 container statuses recorded)
  Jan 11 12:42:02.376: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 11 12:42:02.376: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 11 12:42:02.376: INFO: traefik-ingress-mplg7 from traefik-ingress started at 2024-01-11 09:01:54 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:02.376: INFO: 	Container traefik-ingress ready: true, restart count 0
  STEP: verifying the node has the label node env1-test-worker-0 @ 01/11/24 12:42:02.427
  STEP: verifying the node has the label node env1-test-worker-1 @ 01/11/24 12:42:02.475
  STEP: verifying the node has the label node env1-test-worker-2 @ 01/11/24 12:42:02.52
  Jan 11 12:42:02.562: INFO: Pod kube-flannel-hlhld requesting resource cpu=150m on Node env1-test-worker-2
  Jan 11 12:42:02.562: INFO: Pod kube-flannel-jxf5s requesting resource cpu=150m on Node env1-test-worker-1
  Jan 11 12:42:02.563: INFO: Pod kube-flannel-r8g5h requesting resource cpu=150m on Node env1-test-worker-0
  Jan 11 12:42:02.563: INFO: Pod kube-proxy-c8shb requesting resource cpu=0m on Node env1-test-worker-0
  Jan 11 12:42:02.563: INFO: Pod kube-proxy-s78x8 requesting resource cpu=0m on Node env1-test-worker-2
  Jan 11 12:42:02.563: INFO: Pod kube-proxy-sdfjc requesting resource cpu=0m on Node env1-test-worker-1
  Jan 11 12:42:02.563: INFO: Pod metrics-server-6b7574f5b-jmbtm requesting resource cpu=100m on Node env1-test-worker-0
  Jan 11 12:42:02.563: INFO: Pod nginx-proxy-env1-test-worker-0 requesting resource cpu=25m on Node env1-test-worker-0
  Jan 11 12:42:02.563: INFO: Pod nginx-proxy-env1-test-worker-1 requesting resource cpu=25m on Node env1-test-worker-1
  Jan 11 12:42:02.563: INFO: Pod nginx-proxy-env1-test-worker-2 requesting resource cpu=25m on Node env1-test-worker-2
  Jan 11 12:42:02.563: INFO: Pod nodelocaldns-49gbl requesting resource cpu=100m on Node env1-test-worker-2
  Jan 11 12:42:02.563: INFO: Pod nodelocaldns-7qx4w requesting resource cpu=100m on Node env1-test-worker-1
  Jan 11 12:42:02.563: INFO: Pod nodelocaldns-vkvkp requesting resource cpu=100m on Node env1-test-worker-0
  Jan 11 12:42:02.563: INFO: Pod vsphere-csi-node-5gf9n requesting resource cpu=0m on Node env1-test-worker-0
  Jan 11 12:42:02.563: INFO: Pod vsphere-csi-node-75mjp requesting resource cpu=0m on Node env1-test-worker-2
  Jan 11 12:42:02.563: INFO: Pod vsphere-csi-node-pwx5m requesting resource cpu=0m on Node env1-test-worker-1
  Jan 11 12:42:02.563: INFO: Pod agnhost-replica-dc6f7f69c-nx4wz requesting resource cpu=100m on Node env1-test-worker-1
  Jan 11 12:42:02.563: INFO: Pod agnhost-replica-dc6f7f69c-xw424 requesting resource cpu=100m on Node env1-test-worker-2
  Jan 11 12:42:02.563: INFO: Pod sonobuoy requesting resource cpu=0m on Node env1-test-worker-1
  Jan 11 12:42:02.563: INFO: Pod sonobuoy-e2e-job-4f4795256cf94e3a requesting resource cpu=0m on Node env1-test-worker-2
  Jan 11 12:42:02.563: INFO: Pod sonobuoy-systemd-logs-daemon-set-7b26ca18c17c40c9-7pxz5 requesting resource cpu=0m on Node env1-test-worker-2
  Jan 11 12:42:02.563: INFO: Pod sonobuoy-systemd-logs-daemon-set-7b26ca18c17c40c9-cqtts requesting resource cpu=0m on Node env1-test-worker-0
  Jan 11 12:42:02.563: INFO: Pod sonobuoy-systemd-logs-daemon-set-7b26ca18c17c40c9-zn5jg requesting resource cpu=0m on Node env1-test-worker-1
  Jan 11 12:42:02.563: INFO: Pod traefik-ingress-g4tjs requesting resource cpu=300m on Node env1-test-worker-0
  Jan 11 12:42:02.563: INFO: Pod traefik-ingress-j2r8r requesting resource cpu=300m on Node env1-test-worker-1
  Jan 11 12:42:02.563: INFO: Pod traefik-ingress-mplg7 requesting resource cpu=300m on Node env1-test-worker-2
  Jan 11 12:42:02.563: INFO: Pod velero-794b84894f-nwdwf requesting resource cpu=500m on Node env1-test-worker-0
  STEP: Starting Pods to consume most of the cluster CPU. @ 01/11/24 12:42:02.563
  Jan 11 12:42:02.563: INFO: Creating a pod which consumes cpu=1977m on Node env1-test-worker-0
  Jan 11 12:42:02.588: INFO: Creating a pod which consumes cpu=2327m on Node env1-test-worker-1
  Jan 11 12:42:02.611: INFO: Creating a pod which consumes cpu=2327m on Node env1-test-worker-2
  STEP: Creating another pod that requires unavailable amount of CPU. @ 01/11/24 12:42:04.692
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6e4e434e-5b0a-4fa4-a298-721b84e54ae5.17a94b7a4f557af4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1973/filler-pod-6e4e434e-5b0a-4fa4-a298-721b84e54ae5 to env1-test-worker-0] @ 01/11/24 12:42:04.703
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6e4e434e-5b0a-4fa4-a298-721b84e54ae5.17a94b7a73efe7d2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 01/11/24 12:42:04.705
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6e4e434e-5b0a-4fa4-a298-721b84e54ae5.17a94b7a752420f8], Reason = [Created], Message = [Created container filler-pod-6e4e434e-5b0a-4fa4-a298-721b84e54ae5] @ 01/11/24 12:42:04.705
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6e4e434e-5b0a-4fa4-a298-721b84e54ae5.17a94b7a82130856], Reason = [Started], Message = [Started container filler-pod-6e4e434e-5b0a-4fa4-a298-721b84e54ae5] @ 01/11/24 12:42:04.706
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-715f3416-c495-455a-8436-71959b927349.17a94b7a5267eac9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1973/filler-pod-715f3416-c495-455a-8436-71959b927349 to env1-test-worker-2] @ 01/11/24 12:42:04.707
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-715f3416-c495-455a-8436-71959b927349.17a94b7a74eafc5a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 01/11/24 12:42:04.708
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-715f3416-c495-455a-8436-71959b927349.17a94b7a76017aa2], Reason = [Created], Message = [Created container filler-pod-715f3416-c495-455a-8436-71959b927349] @ 01/11/24 12:42:04.708
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-715f3416-c495-455a-8436-71959b927349.17a94b7a80bee3a0], Reason = [Started], Message = [Started container filler-pod-715f3416-c495-455a-8436-71959b927349] @ 01/11/24 12:42:04.709
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-fee27dec-4d33-48c1-a913-28cdc85402ff.17a94b7a515f61d2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1973/filler-pod-fee27dec-4d33-48c1-a913-28cdc85402ff to env1-test-worker-1] @ 01/11/24 12:42:04.71
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-fee27dec-4d33-48c1-a913-28cdc85402ff.17a94b7a74bebaa4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 01/11/24 12:42:04.711
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-fee27dec-4d33-48c1-a913-28cdc85402ff.17a94b7a7611e73d], Reason = [Created], Message = [Created container filler-pod-fee27dec-4d33-48c1-a913-28cdc85402ff] @ 01/11/24 12:42:04.712
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-fee27dec-4d33-48c1-a913-28cdc85402ff.17a94b7a83691fa5], Reason = [Started], Message = [Started container filler-pod-fee27dec-4d33-48c1-a913-28cdc85402ff] @ 01/11/24 12:42:04.713
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.17a94b7acc57a432], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] @ 01/11/24 12:42:04.735
  STEP: removing the label node off the node env1-test-worker-0 @ 01/11/24 12:42:05.732
  STEP: verifying the node doesn't have the label node @ 01/11/24 12:42:05.78
  STEP: removing the label node off the node env1-test-worker-1 @ 01/11/24 12:42:05.794
  STEP: verifying the node doesn't have the label node @ 01/11/24 12:42:05.845
  STEP: removing the label node off the node env1-test-worker-2 @ 01/11/24 12:42:05.862
  STEP: verifying the node doesn't have the label node @ 01/11/24 12:42:05.916
  Jan 11 12:42:05.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1973" for this suite. @ 01/11/24 12:42:05.952
• [3.777 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 01/11/24 12:42:05.983
  Jan 11 12:42:05.983: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename limitrange @ 01/11/24 12:42:05.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:42:06.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:42:06.033
  STEP: Creating LimitRange "e2e-limitrange-8cdzj" in namespace "limitrange-9594" @ 01/11/24 12:42:06.038
  STEP: Creating another limitRange in another namespace @ 01/11/24 12:42:06.055
  Jan 11 12:42:06.100: INFO: Namespace "e2e-limitrange-8cdzj-2122" created
  Jan 11 12:42:06.100: INFO: Creating LimitRange "e2e-limitrange-8cdzj" in namespace "e2e-limitrange-8cdzj-2122"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-8cdzj" @ 01/11/24 12:42:06.113
  Jan 11 12:42:06.125: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-8cdzj" in "limitrange-9594" namespace @ 01/11/24 12:42:06.126
  Jan 11 12:42:06.148: INFO: LimitRange "e2e-limitrange-8cdzj" has been patched
  STEP: Delete LimitRange "e2e-limitrange-8cdzj" by Collection with labelSelector: "e2e-limitrange-8cdzj=patched" @ 01/11/24 12:42:06.148
  STEP: Confirm that the limitRange "e2e-limitrange-8cdzj" has been deleted @ 01/11/24 12:42:06.182
  Jan 11 12:42:06.182: INFO: Requesting list of LimitRange to confirm quantity
  Jan 11 12:42:06.195: INFO: Found 0 LimitRange with label "e2e-limitrange-8cdzj=patched"
  Jan 11 12:42:06.195: INFO: LimitRange "e2e-limitrange-8cdzj" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-8cdzj" @ 01/11/24 12:42:06.195
  Jan 11 12:42:06.208: INFO: Found 1 limitRange
  Jan 11 12:42:06.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-9594" for this suite. @ 01/11/24 12:42:06.228
  STEP: Destroying namespace "e2e-limitrange-8cdzj-2122" for this suite. @ 01/11/24 12:42:06.247
• [0.290 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 01/11/24 12:42:06.275
  Jan 11 12:42:06.275: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename job @ 01/11/24 12:42:06.277
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:42:06.326
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:42:06.333
  STEP: Creating a job @ 01/11/24 12:42:06.34
  STEP: Ensuring active pods == parallelism @ 01/11/24 12:42:06.353
  STEP: Orphaning one of the Job's Pods @ 01/11/24 12:42:08.377
  Jan 11 12:42:08.932: INFO: Successfully updated pod "adopt-release-h2297"
  STEP: Checking that the Job readopts the Pod @ 01/11/24 12:42:08.933
  STEP: Removing the labels from the Job's Pod @ 01/11/24 12:42:10.953
  Jan 11 12:42:11.498: INFO: Successfully updated pod "adopt-release-h2297"
  STEP: Checking that the Job releases the Pod @ 01/11/24 12:42:11.498
  Jan 11 12:42:13.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2429" for this suite. @ 01/11/24 12:42:13.548
• [7.311 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 01/11/24 12:42:13.589
  Jan 11 12:42:13.589: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename init-container @ 01/11/24 12:42:13.59
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:42:13.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:42:13.669
  STEP: creating the pod @ 01/11/24 12:42:13.675
  Jan 11 12:42:13.676: INFO: PodSpec: initContainers in spec.initContainers
  Jan 11 12:42:18.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6021" for this suite. @ 01/11/24 12:42:18.676
• [5.103 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 01/11/24 12:42:18.697
  Jan 11 12:42:18.697: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename svc-latency @ 01/11/24 12:42:18.699
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:42:18.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:42:18.769
  Jan 11 12:42:18.776: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-3312 @ 01/11/24 12:42:18.778
  I0111 12:42:18.791880      23 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3312, replica count: 1
  I0111 12:42:19.843438      23 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0111 12:42:20.843743      23 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 11 12:42:21.002: INFO: Created: latency-svc-tqgcp
  Jan 11 12:42:21.002: INFO: Got endpoints: latency-svc-tqgcp [57.972923ms]
  Jan 11 12:42:21.051: INFO: Created: latency-svc-dcghn
  Jan 11 12:42:21.079: INFO: Got endpoints: latency-svc-dcghn [75.564975ms]
  Jan 11 12:42:21.100: INFO: Created: latency-svc-srnqb
  Jan 11 12:42:21.122: INFO: Got endpoints: latency-svc-srnqb [118.07866ms]
  Jan 11 12:42:21.187: INFO: Created: latency-svc-5tvt2
  Jan 11 12:42:21.206: INFO: Got endpoints: latency-svc-5tvt2 [201.821695ms]
  Jan 11 12:42:21.236: INFO: Created: latency-svc-6zgtn
  Jan 11 12:42:21.280: INFO: Got endpoints: latency-svc-6zgtn [275.539303ms]
  Jan 11 12:42:21.281: INFO: Created: latency-svc-jslsk
  Jan 11 12:42:21.303: INFO: Got endpoints: latency-svc-jslsk [298.295944ms]
  Jan 11 12:42:21.328: INFO: Created: latency-svc-wchqx
  Jan 11 12:42:21.372: INFO: Got endpoints: latency-svc-wchqx [367.316334ms]
  Jan 11 12:42:21.401: INFO: Created: latency-svc-d9lhh
  Jan 11 12:42:21.403: INFO: Got endpoints: latency-svc-d9lhh [398.73603ms]
  Jan 11 12:42:21.437: INFO: Created: latency-svc-g5rpg
  Jan 11 12:42:21.457: INFO: Got endpoints: latency-svc-g5rpg [452.084741ms]
  Jan 11 12:42:21.476: INFO: Created: latency-svc-ktgqw
  Jan 11 12:42:21.486: INFO: Got endpoints: latency-svc-ktgqw [481.530938ms]
  Jan 11 12:42:21.515: INFO: Created: latency-svc-wc4cn
  Jan 11 12:42:21.529: INFO: Created: latency-svc-c4hph
  Jan 11 12:42:21.582: INFO: Got endpoints: latency-svc-wc4cn [576.782926ms]
  Jan 11 12:42:21.585: INFO: Got endpoints: latency-svc-c4hph [580.106389ms]
  Jan 11 12:42:21.604: INFO: Created: latency-svc-tzd8b
  Jan 11 12:42:21.614: INFO: Got endpoints: latency-svc-tzd8b [608.568318ms]
  Jan 11 12:42:21.654: INFO: Created: latency-svc-7jzdh
  Jan 11 12:42:21.683: INFO: Created: latency-svc-5mxgk
  Jan 11 12:42:21.693: INFO: Got endpoints: latency-svc-7jzdh [687.813892ms]
  Jan 11 12:42:21.696: INFO: Got endpoints: latency-svc-5mxgk [692.309317ms]
  Jan 11 12:42:21.730: INFO: Created: latency-svc-qjkrg
  Jan 11 12:42:21.746: INFO: Got endpoints: latency-svc-qjkrg [740.327322ms]
  Jan 11 12:42:21.761: INFO: Created: latency-svc-5k7tf
  Jan 11 12:42:21.768: INFO: Got endpoints: latency-svc-5k7tf [688.373009ms]
  Jan 11 12:42:21.786: INFO: Created: latency-svc-czwfz
  Jan 11 12:42:21.805: INFO: Created: latency-svc-xg24m
  Jan 11 12:42:21.807: INFO: Got endpoints: latency-svc-czwfz [684.937112ms]
  Jan 11 12:42:21.817: INFO: Got endpoints: latency-svc-xg24m [610.802063ms]
  Jan 11 12:42:21.842: INFO: Created: latency-svc-8jkzd
  Jan 11 12:42:21.844: INFO: Got endpoints: latency-svc-8jkzd [564.227941ms]
  Jan 11 12:42:21.856: INFO: Created: latency-svc-vx5ck
  Jan 11 12:42:21.864: INFO: Got endpoints: latency-svc-vx5ck [561.200684ms]
  Jan 11 12:42:21.871: INFO: Created: latency-svc-gp68g
  Jan 11 12:42:21.896: INFO: Got endpoints: latency-svc-gp68g [523.326484ms]
  Jan 11 12:42:21.920: INFO: Created: latency-svc-mnknq
  Jan 11 12:42:21.922: INFO: Got endpoints: latency-svc-mnknq [515.101967ms]
  Jan 11 12:42:21.942: INFO: Created: latency-svc-h4xpk
  Jan 11 12:42:21.949: INFO: Got endpoints: latency-svc-h4xpk [491.571312ms]
  Jan 11 12:42:21.961: INFO: Created: latency-svc-tmfbd
  Jan 11 12:42:21.973: INFO: Got endpoints: latency-svc-tmfbd [485.459311ms]
  Jan 11 12:42:21.977: INFO: Created: latency-svc-7mm9l
  Jan 11 12:42:21.984: INFO: Got endpoints: latency-svc-7mm9l [401.807151ms]
  Jan 11 12:42:22.000: INFO: Created: latency-svc-89hgx
  Jan 11 12:42:22.020: INFO: Created: latency-svc-8nngl
  Jan 11 12:42:22.019: INFO: Got endpoints: latency-svc-89hgx [433.706743ms]
  Jan 11 12:42:22.040: INFO: Got endpoints: latency-svc-8nngl [425.594009ms]
  Jan 11 12:42:22.045: INFO: Created: latency-svc-zdwps
  Jan 11 12:42:22.073: INFO: Created: latency-svc-pc2tq
  Jan 11 12:42:22.092: INFO: Got endpoints: latency-svc-zdwps [397.91829ms]
  Jan 11 12:42:22.093: INFO: Got endpoints: latency-svc-pc2tq [396.553418ms]
  Jan 11 12:42:22.119: INFO: Created: latency-svc-mhkzn
  Jan 11 12:42:22.132: INFO: Got endpoints: latency-svc-mhkzn [385.331588ms]
  Jan 11 12:42:22.144: INFO: Created: latency-svc-6g74d
  Jan 11 12:42:22.163: INFO: Created: latency-svc-2bx8j
  Jan 11 12:42:22.164: INFO: Got endpoints: latency-svc-6g74d [396.609048ms]
  Jan 11 12:42:22.179: INFO: Created: latency-svc-nxd5c
  Jan 11 12:42:22.193: INFO: Got endpoints: latency-svc-2bx8j [386.392776ms]
  Jan 11 12:42:22.212: INFO: Created: latency-svc-kvg6w
  Jan 11 12:42:22.220: INFO: Got endpoints: latency-svc-nxd5c [403.173591ms]
  Jan 11 12:42:22.229: INFO: Got endpoints: latency-svc-kvg6w [383.509237ms]
  Jan 11 12:42:22.290: INFO: Created: latency-svc-4lb9s
  Jan 11 12:42:22.301: INFO: Got endpoints: latency-svc-4lb9s [405.172683ms]
  Jan 11 12:42:22.312: INFO: Created: latency-svc-bh2h7
  Jan 11 12:42:22.320: INFO: Got endpoints: latency-svc-bh2h7 [397.419862ms]
  Jan 11 12:42:22.331: INFO: Created: latency-svc-dxlz9
  Jan 11 12:42:22.345: INFO: Created: latency-svc-nxvzx
  Jan 11 12:42:22.354: INFO: Created: latency-svc-v5v5w
  Jan 11 12:42:22.361: INFO: Got endpoints: latency-svc-dxlz9 [412.128572ms]
  Jan 11 12:42:22.394: INFO: Created: latency-svc-blcrs
  Jan 11 12:42:22.394: INFO: Got endpoints: latency-svc-nxvzx [419.177747ms]
  Jan 11 12:42:22.400: INFO: Got endpoints: latency-svc-v5v5w [416.041035ms]
  Jan 11 12:42:22.403: INFO: Created: latency-svc-b2555
  Jan 11 12:42:22.416: INFO: Got endpoints: latency-svc-blcrs [394.937276ms]
  Jan 11 12:42:22.429: INFO: Got endpoints: latency-svc-b2555 [388.996093ms]
  Jan 11 12:42:22.438: INFO: Created: latency-svc-v6zcw
  Jan 11 12:42:22.444: INFO: Got endpoints: latency-svc-v6zcw [352.594425ms]
  Jan 11 12:42:22.454: INFO: Created: latency-svc-zpkqk
  Jan 11 12:42:22.466: INFO: Got endpoints: latency-svc-zpkqk [373.057291ms]
  Jan 11 12:42:22.489: INFO: Created: latency-svc-qvlq6
  Jan 11 12:42:22.489: INFO: Got endpoints: latency-svc-qvlq6 [357.256531ms]
  Jan 11 12:42:22.502: INFO: Created: latency-svc-jvr9t
  Jan 11 12:42:22.511: INFO: Got endpoints: latency-svc-jvr9t [346.401484ms]
  Jan 11 12:42:22.556: INFO: Created: latency-svc-g4hn4
  Jan 11 12:42:22.564: INFO: Got endpoints: latency-svc-g4hn4 [370.68172ms]
  Jan 11 12:42:22.567: INFO: Created: latency-svc-qm4w9
  Jan 11 12:42:22.590: INFO: Got endpoints: latency-svc-qm4w9 [369.003264ms]
  Jan 11 12:42:22.598: INFO: Created: latency-svc-gw7dv
  Jan 11 12:42:22.600: INFO: Created: latency-svc-xsnq5
  Jan 11 12:42:22.624: INFO: Created: latency-svc-nhfrb
  Jan 11 12:42:22.633: INFO: Got endpoints: latency-svc-gw7dv [404.22035ms]
  Jan 11 12:42:22.634: INFO: Got endpoints: latency-svc-xsnq5 [768.492539ms]
  Jan 11 12:42:22.656: INFO: Got endpoints: latency-svc-nhfrb [355.255045ms]
  Jan 11 12:42:22.671: INFO: Created: latency-svc-g8f4l
  Jan 11 12:42:22.682: INFO: Got endpoints: latency-svc-g8f4l [362.217627ms]
  Jan 11 12:42:22.705: INFO: Created: latency-svc-2wp72
  Jan 11 12:42:22.707: INFO: Got endpoints: latency-svc-2wp72 [346.08554ms]
  Jan 11 12:42:22.712: INFO: Created: latency-svc-wchjm
  Jan 11 12:42:22.723: INFO: Got endpoints: latency-svc-wchjm [328.577073ms]
  Jan 11 12:42:22.739: INFO: Created: latency-svc-wvwcs
  Jan 11 12:42:22.756: INFO: Got endpoints: latency-svc-wvwcs [355.194576ms]
  Jan 11 12:42:22.772: INFO: Created: latency-svc-drtc6
  Jan 11 12:42:22.778: INFO: Got endpoints: latency-svc-drtc6 [360.934298ms]
  Jan 11 12:42:22.788: INFO: Created: latency-svc-p287v
  Jan 11 12:42:22.798: INFO: Got endpoints: latency-svc-p287v [368.608761ms]
  Jan 11 12:42:22.802: INFO: Created: latency-svc-nfxdk
  Jan 11 12:42:22.814: INFO: Got endpoints: latency-svc-nfxdk [368.420722ms]
  Jan 11 12:42:22.822: INFO: Created: latency-svc-kwp77
  Jan 11 12:42:22.833: INFO: Created: latency-svc-4rxc6
  Jan 11 12:42:22.835: INFO: Got endpoints: latency-svc-kwp77 [369.37436ms]
  Jan 11 12:42:22.854: INFO: Created: latency-svc-bsddb
  Jan 11 12:42:22.858: INFO: Got endpoints: latency-svc-4rxc6 [368.992904ms]
  Jan 11 12:42:22.865: INFO: Created: latency-svc-58rl6
  Jan 11 12:42:22.870: INFO: Created: latency-svc-jvrvq
  Jan 11 12:42:22.888: INFO: Created: latency-svc-vst8l
  Jan 11 12:42:22.899: INFO: Got endpoints: latency-svc-58rl6 [334.957754ms]
  Jan 11 12:42:22.914: INFO: Created: latency-svc-stss2
  Jan 11 12:42:22.917: INFO: Got endpoints: latency-svc-bsddb [404.622583ms]
  Jan 11 12:42:22.921: INFO: Got endpoints: latency-svc-vst8l [287.504963ms]
  Jan 11 12:42:22.934: INFO: Got endpoints: latency-svc-jvrvq [344.309677ms]
  Jan 11 12:42:22.937: INFO: Created: latency-svc-bg4xw
  Jan 11 12:42:22.942: INFO: Got endpoints: latency-svc-stss2 [308.14745ms]
  Jan 11 12:42:22.948: INFO: Got endpoints: latency-svc-bg4xw [291.347546ms]
  Jan 11 12:42:22.960: INFO: Created: latency-svc-6h6ld
  Jan 11 12:42:22.982: INFO: Got endpoints: latency-svc-6h6ld [300.020778ms]
  Jan 11 12:42:22.991: INFO: Created: latency-svc-srs69
  Jan 11 12:42:23.004: INFO: Got endpoints: latency-svc-srs69 [295.495452ms]
  Jan 11 12:42:23.013: INFO: Created: latency-svc-wkmkb
  Jan 11 12:42:23.029: INFO: Got endpoints: latency-svc-wkmkb [305.298549ms]
  Jan 11 12:42:23.034: INFO: Created: latency-svc-qn4j4
  Jan 11 12:42:23.055: INFO: Got endpoints: latency-svc-qn4j4 [299.562604ms]
  Jan 11 12:42:23.058: INFO: Created: latency-svc-9wb2d
  Jan 11 12:42:23.075: INFO: Created: latency-svc-9qpvj
  Jan 11 12:42:23.090: INFO: Created: latency-svc-p228s
  Jan 11 12:42:23.105: INFO: Got endpoints: latency-svc-9wb2d [325.938187ms]
  Jan 11 12:42:23.118: INFO: Created: latency-svc-7ghcc
  Jan 11 12:42:23.138: INFO: Created: latency-svc-l7tfg
  Jan 11 12:42:23.154: INFO: Got endpoints: latency-svc-9qpvj [354.839909ms]
  Jan 11 12:42:23.171: INFO: Created: latency-svc-kfrdq
  Jan 11 12:42:23.177: INFO: Created: latency-svc-r9lb5
  Jan 11 12:42:23.190: INFO: Created: latency-svc-cxhlg
  Jan 11 12:42:23.202: INFO: Got endpoints: latency-svc-p228s [387.217315ms]
  Jan 11 12:42:23.213: INFO: Created: latency-svc-hrp5s
  Jan 11 12:42:23.228: INFO: Created: latency-svc-6pvc2
  Jan 11 12:42:23.245: INFO: Created: latency-svc-28msk
  Jan 11 12:42:23.254: INFO: Got endpoints: latency-svc-7ghcc [416.719247ms]
  Jan 11 12:42:23.262: INFO: Created: latency-svc-jfv24
  Jan 11 12:42:23.276: INFO: Created: latency-svc-rvlkp
  Jan 11 12:42:23.290: INFO: Created: latency-svc-lqzlv
  Jan 11 12:42:23.306: INFO: Got endpoints: latency-svc-l7tfg [447.333433ms]
  Jan 11 12:42:23.306: INFO: Created: latency-svc-znkwc
  Jan 11 12:42:23.322: INFO: Created: latency-svc-2zl6l
  Jan 11 12:42:23.333: INFO: Created: latency-svc-6ppg8
  Jan 11 12:42:23.352: INFO: Created: latency-svc-btctt
  Jan 11 12:42:23.366: INFO: Got endpoints: latency-svc-kfrdq [466.670674ms]
  Jan 11 12:42:23.370: INFO: Created: latency-svc-7bdj5
  Jan 11 12:42:23.384: INFO: Created: latency-svc-nrtgg
  Jan 11 12:42:23.404: INFO: Got endpoints: latency-svc-r9lb5 [487.505398ms]
  Jan 11 12:42:23.416: INFO: Created: latency-svc-ffg47
  Jan 11 12:42:23.434: INFO: Created: latency-svc-kjxsj
  Jan 11 12:42:23.454: INFO: Got endpoints: latency-svc-cxhlg [532.123237ms]
  Jan 11 12:42:23.488: INFO: Created: latency-svc-5zhfd
  Jan 11 12:42:23.499: INFO: Got endpoints: latency-svc-hrp5s [564.279679ms]
  Jan 11 12:42:23.535: INFO: Created: latency-svc-x7kw8
  Jan 11 12:42:23.553: INFO: Got endpoints: latency-svc-6pvc2 [610.859462ms]
  Jan 11 12:42:23.578: INFO: Created: latency-svc-bsr4c
  Jan 11 12:42:23.597: INFO: Got endpoints: latency-svc-28msk [648.9245ms]
  Jan 11 12:42:23.620: INFO: Created: latency-svc-b98hg
  Jan 11 12:42:23.649: INFO: Got endpoints: latency-svc-jfv24 [665.161661ms]
  Jan 11 12:42:23.680: INFO: Created: latency-svc-snct9
  Jan 11 12:42:23.700: INFO: Got endpoints: latency-svc-rvlkp [693.860572ms]
  Jan 11 12:42:23.738: INFO: Created: latency-svc-gkmkx
  Jan 11 12:42:23.745: INFO: Got endpoints: latency-svc-lqzlv [716.423972ms]
  Jan 11 12:42:23.774: INFO: Created: latency-svc-sqk5p
  Jan 11 12:42:23.800: INFO: Got endpoints: latency-svc-znkwc [744.423068ms]
  Jan 11 12:42:23.825: INFO: Created: latency-svc-nljn5
  Jan 11 12:42:23.853: INFO: Got endpoints: latency-svc-2zl6l [747.959462ms]
  Jan 11 12:42:23.884: INFO: Created: latency-svc-d25g9
  Jan 11 12:42:23.909: INFO: Got endpoints: latency-svc-6ppg8 [754.791071ms]
  Jan 11 12:42:23.960: INFO: Created: latency-svc-qqwg2
  Jan 11 12:42:23.965: INFO: Got endpoints: latency-svc-btctt [762.720221ms]
  Jan 11 12:42:23.993: INFO: Created: latency-svc-nlpmh
  Jan 11 12:42:24.003: INFO: Got endpoints: latency-svc-7bdj5 [748.823116ms]
  Jan 11 12:42:24.026: INFO: Created: latency-svc-jvxp2
  Jan 11 12:42:24.052: INFO: Got endpoints: latency-svc-nrtgg [744.273273ms]
  Jan 11 12:42:24.071: INFO: Created: latency-svc-hbfj4
  Jan 11 12:42:24.102: INFO: Got endpoints: latency-svc-ffg47 [735.837781ms]
  Jan 11 12:42:24.128: INFO: Created: latency-svc-9mxmv
  Jan 11 12:42:24.156: INFO: Got endpoints: latency-svc-kjxsj [750.695463ms]
  Jan 11 12:42:24.189: INFO: Created: latency-svc-cdt7k
  Jan 11 12:42:24.201: INFO: Got endpoints: latency-svc-5zhfd [745.739777ms]
  Jan 11 12:42:24.236: INFO: Created: latency-svc-dq6r4
  Jan 11 12:42:24.246: INFO: Got endpoints: latency-svc-x7kw8 [746.977347ms]
  Jan 11 12:42:24.279: INFO: Created: latency-svc-kdldk
  Jan 11 12:42:24.303: INFO: Got endpoints: latency-svc-bsr4c [750.240306ms]
  Jan 11 12:42:24.348: INFO: Created: latency-svc-lv5dh
  Jan 11 12:42:24.357: INFO: Got endpoints: latency-svc-b98hg [760.06048ms]
  Jan 11 12:42:24.404: INFO: Created: latency-svc-zc8q4
  Jan 11 12:42:24.409: INFO: Got endpoints: latency-svc-snct9 [760.125463ms]
  Jan 11 12:42:24.454: INFO: Got endpoints: latency-svc-gkmkx [754.221678ms]
  Jan 11 12:42:24.468: INFO: Created: latency-svc-2s4lx
  Jan 11 12:42:24.505: INFO: Created: latency-svc-96k2d
  Jan 11 12:42:24.506: INFO: Got endpoints: latency-svc-sqk5p [760.661098ms]
  Jan 11 12:42:24.552: INFO: Got endpoints: latency-svc-nljn5 [751.729397ms]
  Jan 11 12:42:24.557: INFO: Created: latency-svc-fmvqk
  Jan 11 12:42:24.588: INFO: Created: latency-svc-z72ff
  Jan 11 12:42:24.599: INFO: Got endpoints: latency-svc-d25g9 [745.710031ms]
  Jan 11 12:42:24.621: INFO: Created: latency-svc-jb7fv
  Jan 11 12:42:24.646: INFO: Got endpoints: latency-svc-qqwg2 [736.949703ms]
  Jan 11 12:42:24.682: INFO: Created: latency-svc-4695z
  Jan 11 12:42:24.707: INFO: Got endpoints: latency-svc-nlpmh [742.402592ms]
  Jan 11 12:42:24.740: INFO: Created: latency-svc-h9x9q
  Jan 11 12:42:24.765: INFO: Got endpoints: latency-svc-jvxp2 [762.164261ms]
  Jan 11 12:42:24.805: INFO: Got endpoints: latency-svc-hbfj4 [753.388624ms]
  Jan 11 12:42:24.814: INFO: Created: latency-svc-8p4dg
  Jan 11 12:42:24.831: INFO: Created: latency-svc-hg9qb
  Jan 11 12:42:24.851: INFO: Got endpoints: latency-svc-9mxmv [749.269202ms]
  Jan 11 12:42:24.877: INFO: Created: latency-svc-rstk6
  Jan 11 12:42:24.902: INFO: Got endpoints: latency-svc-cdt7k [746.642188ms]
  Jan 11 12:42:24.931: INFO: Created: latency-svc-t8cxw
  Jan 11 12:42:24.949: INFO: Got endpoints: latency-svc-dq6r4 [748.480316ms]
  Jan 11 12:42:24.975: INFO: Created: latency-svc-nbsc8
  Jan 11 12:42:24.997: INFO: Got endpoints: latency-svc-kdldk [749.191438ms]
  Jan 11 12:42:25.023: INFO: Created: latency-svc-fzqr9
  Jan 11 12:42:25.046: INFO: Got endpoints: latency-svc-lv5dh [741.775613ms]
  Jan 11 12:42:25.064: INFO: Created: latency-svc-6r7vg
  Jan 11 12:42:25.097: INFO: Got endpoints: latency-svc-zc8q4 [739.680214ms]
  Jan 11 12:42:25.115: INFO: Created: latency-svc-qq244
  Jan 11 12:42:25.150: INFO: Got endpoints: latency-svc-2s4lx [741.025848ms]
  Jan 11 12:42:25.179: INFO: Created: latency-svc-pj74g
  Jan 11 12:42:25.206: INFO: Got endpoints: latency-svc-96k2d [750.629202ms]
  Jan 11 12:42:25.229: INFO: Created: latency-svc-dk7bf
  Jan 11 12:42:25.251: INFO: Got endpoints: latency-svc-fmvqk [745.14165ms]
  Jan 11 12:42:25.281: INFO: Created: latency-svc-6989t
  Jan 11 12:42:25.303: INFO: Got endpoints: latency-svc-z72ff [750.944373ms]
  Jan 11 12:42:25.329: INFO: Created: latency-svc-6vqhd
  Jan 11 12:42:25.352: INFO: Got endpoints: latency-svc-jb7fv [752.728012ms]
  Jan 11 12:42:25.381: INFO: Created: latency-svc-jhjln
  Jan 11 12:42:25.417: INFO: Got endpoints: latency-svc-4695z [771.096807ms]
  Jan 11 12:42:25.442: INFO: Created: latency-svc-q8pq2
  Jan 11 12:42:25.452: INFO: Got endpoints: latency-svc-h9x9q [744.802581ms]
  Jan 11 12:42:25.493: INFO: Created: latency-svc-rdmqn
  Jan 11 12:42:25.521: INFO: Got endpoints: latency-svc-8p4dg [755.507963ms]
  Jan 11 12:42:25.560: INFO: Created: latency-svc-blr9f
  Jan 11 12:42:25.561: INFO: Got endpoints: latency-svc-hg9qb [755.747326ms]
  Jan 11 12:42:25.602: INFO: Got endpoints: latency-svc-rstk6 [751.016096ms]
  Jan 11 12:42:25.614: INFO: Created: latency-svc-m5bj9
  Jan 11 12:42:25.674: INFO: Created: latency-svc-972xm
  Jan 11 12:42:25.678: INFO: Got endpoints: latency-svc-t8cxw [776.017772ms]
  Jan 11 12:42:25.712: INFO: Got endpoints: latency-svc-nbsc8 [762.789467ms]
  Jan 11 12:42:25.730: INFO: Created: latency-svc-hs4bb
  Jan 11 12:42:25.786: INFO: Created: latency-svc-4x7ms
  Jan 11 12:42:25.788: INFO: Got endpoints: latency-svc-fzqr9 [790.899297ms]
  Jan 11 12:42:25.806: INFO: Got endpoints: latency-svc-6r7vg [759.014762ms]
  Jan 11 12:42:25.853: INFO: Created: latency-svc-skz6s
  Jan 11 12:42:25.857: INFO: Created: latency-svc-qfdqf
  Jan 11 12:42:25.866: INFO: Got endpoints: latency-svc-qq244 [768.1639ms]
  Jan 11 12:42:25.893: INFO: Created: latency-svc-2btbs
  Jan 11 12:42:25.909: INFO: Got endpoints: latency-svc-pj74g [758.759943ms]
  Jan 11 12:42:25.940: INFO: Created: latency-svc-vr7b7
  Jan 11 12:42:25.953: INFO: Got endpoints: latency-svc-dk7bf [746.941378ms]
  Jan 11 12:42:25.973: INFO: Created: latency-svc-smhhx
  Jan 11 12:42:26.003: INFO: Got endpoints: latency-svc-6989t [751.417043ms]
  Jan 11 12:42:26.029: INFO: Created: latency-svc-hn28f
  Jan 11 12:42:26.061: INFO: Got endpoints: latency-svc-6vqhd [757.37723ms]
  Jan 11 12:42:26.088: INFO: Created: latency-svc-fnz7x
  Jan 11 12:42:26.102: INFO: Got endpoints: latency-svc-jhjln [749.201561ms]
  Jan 11 12:42:26.127: INFO: Created: latency-svc-7gkk5
  Jan 11 12:42:26.152: INFO: Got endpoints: latency-svc-q8pq2 [734.863257ms]
  Jan 11 12:42:26.178: INFO: Created: latency-svc-4ljdx
  Jan 11 12:42:26.202: INFO: Got endpoints: latency-svc-rdmqn [749.430189ms]
  Jan 11 12:42:26.238: INFO: Created: latency-svc-dp29g
  Jan 11 12:42:26.253: INFO: Got endpoints: latency-svc-blr9f [732.133748ms]
  Jan 11 12:42:26.294: INFO: Created: latency-svc-bfzlv
  Jan 11 12:42:26.304: INFO: Got endpoints: latency-svc-m5bj9 [742.702097ms]
  Jan 11 12:42:26.332: INFO: Created: latency-svc-8f575
  Jan 11 12:42:26.353: INFO: Got endpoints: latency-svc-972xm [750.190145ms]
  Jan 11 12:42:26.383: INFO: Created: latency-svc-5xl6c
  Jan 11 12:42:26.402: INFO: Got endpoints: latency-svc-hs4bb [723.330603ms]
  Jan 11 12:42:26.426: INFO: Created: latency-svc-m47f8
  Jan 11 12:42:26.450: INFO: Got endpoints: latency-svc-4x7ms [737.817826ms]
  Jan 11 12:42:26.488: INFO: Created: latency-svc-2v9r6
  Jan 11 12:42:26.505: INFO: Got endpoints: latency-svc-qfdqf [716.975955ms]
  Jan 11 12:42:26.534: INFO: Created: latency-svc-27cfp
  Jan 11 12:42:26.559: INFO: Got endpoints: latency-svc-skz6s [753.616933ms]
  Jan 11 12:42:26.584: INFO: Created: latency-svc-989cd
  Jan 11 12:42:26.607: INFO: Got endpoints: latency-svc-2btbs [740.246851ms]
  Jan 11 12:42:26.634: INFO: Created: latency-svc-w4sjj
  Jan 11 12:42:26.654: INFO: Got endpoints: latency-svc-vr7b7 [745.079586ms]
  Jan 11 12:42:26.698: INFO: Created: latency-svc-qsktg
  Jan 11 12:42:26.713: INFO: Got endpoints: latency-svc-smhhx [760.174259ms]
  Jan 11 12:42:26.752: INFO: Created: latency-svc-ftgbr
  Jan 11 12:42:26.753: INFO: Got endpoints: latency-svc-hn28f [749.678127ms]
  Jan 11 12:42:26.789: INFO: Created: latency-svc-xqkc5
  Jan 11 12:42:26.808: INFO: Got endpoints: latency-svc-fnz7x [747.425184ms]
  Jan 11 12:42:26.840: INFO: Created: latency-svc-bcj8v
  Jan 11 12:42:26.857: INFO: Got endpoints: latency-svc-7gkk5 [755.166529ms]
  Jan 11 12:42:26.892: INFO: Created: latency-svc-7rncv
  Jan 11 12:42:26.907: INFO: Got endpoints: latency-svc-4ljdx [754.882068ms]
  Jan 11 12:42:26.941: INFO: Created: latency-svc-vbk8b
  Jan 11 12:42:26.950: INFO: Got endpoints: latency-svc-dp29g [747.904479ms]
  Jan 11 12:42:26.977: INFO: Created: latency-svc-tfgn8
  Jan 11 12:42:26.999: INFO: Got endpoints: latency-svc-bfzlv [744.256859ms]
  Jan 11 12:42:27.044: INFO: Created: latency-svc-jsfzg
  Jan 11 12:42:27.070: INFO: Got endpoints: latency-svc-8f575 [766.112079ms]
  Jan 11 12:42:27.112: INFO: Got endpoints: latency-svc-5xl6c [758.248567ms]
  Jan 11 12:42:27.160: INFO: Created: latency-svc-q2x5n
  Jan 11 12:42:27.173: INFO: Got endpoints: latency-svc-m47f8 [769.230182ms]
  Jan 11 12:42:27.183: INFO: Created: latency-svc-vfpgb
  Jan 11 12:42:27.202: INFO: Got endpoints: latency-svc-2v9r6 [751.394981ms]
  Jan 11 12:42:27.206: INFO: Created: latency-svc-j9nzx
  Jan 11 12:42:27.236: INFO: Created: latency-svc-g8qzc
  Jan 11 12:42:27.249: INFO: Got endpoints: latency-svc-27cfp [743.576861ms]
  Jan 11 12:42:27.281: INFO: Created: latency-svc-d2x26
  Jan 11 12:42:27.298: INFO: Got endpoints: latency-svc-989cd [738.18036ms]
  Jan 11 12:42:27.341: INFO: Created: latency-svc-7ws4w
  Jan 11 12:42:27.361: INFO: Got endpoints: latency-svc-w4sjj [753.148292ms]
  Jan 11 12:42:27.388: INFO: Created: latency-svc-6vzcm
  Jan 11 12:42:27.398: INFO: Got endpoints: latency-svc-qsktg [744.034481ms]
  Jan 11 12:42:27.426: INFO: Created: latency-svc-qr6kc
  Jan 11 12:42:27.455: INFO: Got endpoints: latency-svc-ftgbr [741.843954ms]
  Jan 11 12:42:27.489: INFO: Created: latency-svc-m9s89
  Jan 11 12:42:27.504: INFO: Got endpoints: latency-svc-xqkc5 [751.590232ms]
  Jan 11 12:42:27.523: INFO: Created: latency-svc-ckrgq
  Jan 11 12:42:27.547: INFO: Got endpoints: latency-svc-bcj8v [738.644242ms]
  Jan 11 12:42:27.594: INFO: Created: latency-svc-mkx8r
  Jan 11 12:42:27.608: INFO: Got endpoints: latency-svc-7rncv [750.992883ms]
  Jan 11 12:42:27.628: INFO: Created: latency-svc-656tl
  Jan 11 12:42:27.651: INFO: Got endpoints: latency-svc-vbk8b [743.488478ms]
  Jan 11 12:42:27.672: INFO: Created: latency-svc-ckb7j
  Jan 11 12:42:27.699: INFO: Got endpoints: latency-svc-tfgn8 [749.207673ms]
  Jan 11 12:42:27.729: INFO: Created: latency-svc-kgtwr
  Jan 11 12:42:27.748: INFO: Got endpoints: latency-svc-jsfzg [749.333572ms]
  Jan 11 12:42:27.768: INFO: Created: latency-svc-mmrr8
  Jan 11 12:42:27.806: INFO: Got endpoints: latency-svc-q2x5n [735.317715ms]
  Jan 11 12:42:27.827: INFO: Created: latency-svc-t6xx8
  Jan 11 12:42:27.849: INFO: Got endpoints: latency-svc-vfpgb [736.744681ms]
  Jan 11 12:42:27.882: INFO: Created: latency-svc-rlwxl
  Jan 11 12:42:27.905: INFO: Got endpoints: latency-svc-j9nzx [731.67574ms]
  Jan 11 12:42:27.933: INFO: Created: latency-svc-5ntdc
  Jan 11 12:42:27.948: INFO: Got endpoints: latency-svc-g8qzc [746.37747ms]
  Jan 11 12:42:27.982: INFO: Created: latency-svc-zl97s
  Jan 11 12:42:28.000: INFO: Got endpoints: latency-svc-d2x26 [751.376987ms]
  Jan 11 12:42:28.035: INFO: Created: latency-svc-xbb8p
  Jan 11 12:42:28.057: INFO: Got endpoints: latency-svc-7ws4w [759.259923ms]
  Jan 11 12:42:28.085: INFO: Created: latency-svc-p8dl7
  Jan 11 12:42:28.099: INFO: Got endpoints: latency-svc-6vzcm [737.440991ms]
  Jan 11 12:42:28.128: INFO: Created: latency-svc-n7mcc
  Jan 11 12:42:28.151: INFO: Got endpoints: latency-svc-qr6kc [752.286858ms]
  Jan 11 12:42:28.182: INFO: Created: latency-svc-gwql7
  Jan 11 12:42:28.204: INFO: Got endpoints: latency-svc-m9s89 [748.697734ms]
  Jan 11 12:42:28.233: INFO: Created: latency-svc-62bvn
  Jan 11 12:42:28.256: INFO: Got endpoints: latency-svc-ckrgq [751.557527ms]
  Jan 11 12:42:28.293: INFO: Created: latency-svc-qq87j
  Jan 11 12:42:28.305: INFO: Got endpoints: latency-svc-mkx8r [757.954518ms]
  Jan 11 12:42:28.349: INFO: Created: latency-svc-9pvjd
  Jan 11 12:42:28.357: INFO: Got endpoints: latency-svc-656tl [748.891991ms]
  Jan 11 12:42:28.390: INFO: Created: latency-svc-fqxjs
  Jan 11 12:42:28.400: INFO: Got endpoints: latency-svc-ckb7j [748.92345ms]
  Jan 11 12:42:28.466: INFO: Got endpoints: latency-svc-kgtwr [767.165053ms]
  Jan 11 12:42:28.467: INFO: Created: latency-svc-wtqt6
  Jan 11 12:42:28.513: INFO: Got endpoints: latency-svc-mmrr8 [764.284039ms]
  Jan 11 12:42:28.521: INFO: Created: latency-svc-pg5qx
  Jan 11 12:42:28.561: INFO: Created: latency-svc-9mfzc
  Jan 11 12:42:28.603: INFO: Got endpoints: latency-svc-t6xx8 [796.672358ms]
  Jan 11 12:42:28.608: INFO: Got endpoints: latency-svc-rlwxl [759.137791ms]
  Jan 11 12:42:28.630: INFO: Created: latency-svc-gxzqh
  Jan 11 12:42:28.652: INFO: Created: latency-svc-n2vfk
  Jan 11 12:42:28.663: INFO: Got endpoints: latency-svc-5ntdc [757.409334ms]
  Jan 11 12:42:28.712: INFO: Got endpoints: latency-svc-zl97s [763.494281ms]
  Jan 11 12:42:28.715: INFO: Created: latency-svc-4h99l
  Jan 11 12:42:28.746: INFO: Created: latency-svc-gxr6r
  Jan 11 12:42:28.758: INFO: Got endpoints: latency-svc-xbb8p [757.370555ms]
  Jan 11 12:42:28.788: INFO: Created: latency-svc-h6trt
  Jan 11 12:42:28.801: INFO: Got endpoints: latency-svc-p8dl7 [742.521499ms]
  Jan 11 12:42:28.834: INFO: Created: latency-svc-chqxf
  Jan 11 12:42:28.854: INFO: Got endpoints: latency-svc-n7mcc [754.716424ms]
  Jan 11 12:42:28.877: INFO: Created: latency-svc-2hq4v
  Jan 11 12:42:28.899: INFO: Got endpoints: latency-svc-gwql7 [746.733463ms]
  Jan 11 12:42:28.952: INFO: Got endpoints: latency-svc-62bvn [748.632275ms]
  Jan 11 12:42:29.007: INFO: Got endpoints: latency-svc-qq87j [749.809964ms]
  Jan 11 12:42:29.049: INFO: Got endpoints: latency-svc-9pvjd [743.989463ms]
  Jan 11 12:42:29.099: INFO: Got endpoints: latency-svc-fqxjs [741.421808ms]
  Jan 11 12:42:29.156: INFO: Got endpoints: latency-svc-wtqt6 [755.891184ms]
  Jan 11 12:42:29.225: INFO: Got endpoints: latency-svc-pg5qx [758.061987ms]
  Jan 11 12:42:29.257: INFO: Got endpoints: latency-svc-9mfzc [744.806376ms]
  Jan 11 12:42:29.323: INFO: Got endpoints: latency-svc-gxzqh [720.092076ms]
  Jan 11 12:42:29.353: INFO: Got endpoints: latency-svc-n2vfk [743.922037ms]
  Jan 11 12:42:29.398: INFO: Got endpoints: latency-svc-4h99l [735.047315ms]
  Jan 11 12:42:29.457: INFO: Got endpoints: latency-svc-gxr6r [745.312189ms]
  Jan 11 12:42:29.499: INFO: Got endpoints: latency-svc-h6trt [741.489882ms]
  Jan 11 12:42:29.616: INFO: Got endpoints: latency-svc-2hq4v [762.307152ms]
  Jan 11 12:42:29.624: INFO: Got endpoints: latency-svc-chqxf [822.458622ms]
  Jan 11 12:42:29.624: INFO: Latencies: [75.564975ms 118.07866ms 201.821695ms 275.539303ms 287.504963ms 291.347546ms 295.495452ms 298.295944ms 299.562604ms 300.020778ms 305.298549ms 308.14745ms 325.938187ms 328.577073ms 334.957754ms 344.309677ms 346.08554ms 346.401484ms 352.594425ms 354.839909ms 355.194576ms 355.255045ms 357.256531ms 360.934298ms 362.217627ms 367.316334ms 368.420722ms 368.608761ms 368.992904ms 369.003264ms 369.37436ms 370.68172ms 373.057291ms 383.509237ms 385.331588ms 386.392776ms 387.217315ms 388.996093ms 394.937276ms 396.553418ms 396.609048ms 397.419862ms 397.91829ms 398.73603ms 401.807151ms 403.173591ms 404.22035ms 404.622583ms 405.172683ms 412.128572ms 416.041035ms 416.719247ms 419.177747ms 425.594009ms 433.706743ms 447.333433ms 452.084741ms 466.670674ms 481.530938ms 485.459311ms 487.505398ms 491.571312ms 515.101967ms 523.326484ms 532.123237ms 561.200684ms 564.227941ms 564.279679ms 576.782926ms 580.106389ms 608.568318ms 610.802063ms 610.859462ms 648.9245ms 665.161661ms 684.937112ms 687.813892ms 688.373009ms 692.309317ms 693.860572ms 716.423972ms 716.975955ms 720.092076ms 723.330603ms 731.67574ms 732.133748ms 734.863257ms 735.047315ms 735.317715ms 735.837781ms 736.744681ms 736.949703ms 737.440991ms 737.817826ms 738.18036ms 738.644242ms 739.680214ms 740.246851ms 740.327322ms 741.025848ms 741.421808ms 741.489882ms 741.775613ms 741.843954ms 742.402592ms 742.521499ms 742.702097ms 743.488478ms 743.576861ms 743.922037ms 743.989463ms 744.034481ms 744.256859ms 744.273273ms 744.423068ms 744.802581ms 744.806376ms 745.079586ms 745.14165ms 745.312189ms 745.710031ms 745.739777ms 746.37747ms 746.642188ms 746.733463ms 746.941378ms 746.977347ms 747.425184ms 747.904479ms 747.959462ms 748.480316ms 748.632275ms 748.697734ms 748.823116ms 748.891991ms 748.92345ms 749.191438ms 749.201561ms 749.207673ms 749.269202ms 749.333572ms 749.430189ms 749.678127ms 749.809964ms 750.190145ms 750.240306ms 750.629202ms 750.695463ms 750.944373ms 750.992883ms 751.016096ms 751.376987ms 751.394981ms 751.417043ms 751.557527ms 751.590232ms 751.729397ms 752.286858ms 752.728012ms 753.148292ms 753.388624ms 753.616933ms 754.221678ms 754.716424ms 754.791071ms 754.882068ms 755.166529ms 755.507963ms 755.747326ms 755.891184ms 757.370555ms 757.37723ms 757.409334ms 757.954518ms 758.061987ms 758.248567ms 758.759943ms 759.014762ms 759.137791ms 759.259923ms 760.06048ms 760.125463ms 760.174259ms 760.661098ms 762.164261ms 762.307152ms 762.720221ms 762.789467ms 763.494281ms 764.284039ms 766.112079ms 767.165053ms 768.1639ms 768.492539ms 769.230182ms 771.096807ms 776.017772ms 790.899297ms 796.672358ms 822.458622ms]
  Jan 11 12:42:29.624: INFO: 50 %ile: 741.421808ms
  Jan 11 12:42:29.624: INFO: 90 %ile: 760.06048ms
  Jan 11 12:42:29.624: INFO: 99 %ile: 796.672358ms
  Jan 11 12:42:29.624: INFO: Total sample count: 200
  Jan 11 12:42:29.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-3312" for this suite. @ 01/11/24 12:42:29.719
• [11.071 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 01/11/24 12:42:29.77
  Jan 11 12:42:29.770: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename sched-pred @ 01/11/24 12:42:29.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:42:29.838
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:42:29.848
  Jan 11 12:42:29.855: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jan 11 12:42:29.891: INFO: Waiting for terminating namespaces to be deleted...
  Jan 11 12:42:29.903: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-0 before test
  Jan 11 12:42:29.931: INFO: kube-flannel-r8g5h from kube-system started at 2024-01-09 16:24:44 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:29.931: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 11 12:42:29.931: INFO: kube-proxy-c8shb from kube-system started at 2024-01-11 00:56:00 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:29.931: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 11 12:42:29.931: INFO: metrics-server-6b7574f5b-jmbtm from kube-system started at 2024-01-09 16:32:13 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:29.931: INFO: 	Container metrics-server ready: true, restart count 0
  Jan 11 12:42:29.931: INFO: nginx-proxy-env1-test-worker-0 from kube-system started at 2024-01-09 16:31:48 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:29.931: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 11 12:42:29.931: INFO: nodelocaldns-vkvkp from kube-system started at 2024-01-09 15:52:20 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:29.931: INFO: 	Container node-cache ready: true, restart count 0
  Jan 11 12:42:29.931: INFO: vsphere-csi-node-5gf9n from kube-system started at 2024-01-11 01:05:49 +0000 UTC (3 container statuses recorded)
  Jan 11 12:42:29.931: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 11 12:42:29.931: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 11 12:42:29.931: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 11 12:42:29.931: INFO: sonobuoy-systemd-logs-daemon-set-7b26ca18c17c40c9-cqtts from sonobuoy started at 2024-01-11 11:18:31 +0000 UTC (2 container statuses recorded)
  Jan 11 12:42:29.931: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 11 12:42:29.931: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 11 12:42:29.931: INFO: traefik-ingress-g4tjs from traefik-ingress started at 2024-01-10 14:39:42 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:29.931: INFO: 	Container traefik-ingress ready: true, restart count 0
  Jan 11 12:42:29.931: INFO: velero-794b84894f-nwdwf from velero started at 2024-01-10 13:26:26 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:29.931: INFO: 	Container velero ready: true, restart count 0
  Jan 11 12:42:29.931: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-1 before test
  Jan 11 12:42:29.959: INFO: adopt-release-h2297 from job-2429 started at 2024-01-11 12:42:06 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:29.959: INFO: 	Container c ready: true, restart count 0
  Jan 11 12:42:29.959: INFO: adopt-release-n5g48 from job-2429 started at 2024-01-11 12:42:06 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:29.959: INFO: 	Container c ready: true, restart count 0
  Jan 11 12:42:29.959: INFO: kube-flannel-jxf5s from kube-system started at 2024-01-09 16:24:09 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:29.959: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 11 12:42:29.959: INFO: kube-proxy-sdfjc from kube-system started at 2024-01-11 00:56:00 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:29.959: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 11 12:42:29.959: INFO: nginx-proxy-env1-test-worker-1 from kube-system started at 2024-01-09 16:38:31 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:29.959: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 11 12:42:29.959: INFO: nodelocaldns-7qx4w from kube-system started at 2024-01-09 15:52:12 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:29.959: INFO: 	Container node-cache ready: true, restart count 0
  Jan 11 12:42:29.959: INFO: vsphere-csi-node-pwx5m from kube-system started at 2024-01-11 01:05:49 +0000 UTC (3 container statuses recorded)
  Jan 11 12:42:29.959: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 11 12:42:29.959: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 11 12:42:29.959: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 11 12:42:29.959: INFO: sonobuoy from sonobuoy started at 2024-01-11 11:18:29 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:29.959: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jan 11 12:42:29.959: INFO: sonobuoy-systemd-logs-daemon-set-7b26ca18c17c40c9-zn5jg from sonobuoy started at 2024-01-11 11:18:31 +0000 UTC (2 container statuses recorded)
  Jan 11 12:42:29.959: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 11 12:42:29.959: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 11 12:42:29.959: INFO: svc-latency-rc-npn85 from svc-latency-3312 started at 2024-01-11 12:42:18 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:29.959: INFO: 	Container svc-latency-rc ready: true, restart count 0
  Jan 11 12:42:29.959: INFO: traefik-ingress-j2r8r from traefik-ingress started at 2024-01-11 12:26:34 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:29.959: INFO: 	Container traefik-ingress ready: true, restart count 0
  Jan 11 12:42:29.959: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-2 before test
  Jan 11 12:42:30.001: INFO: adopt-release-rqr76 from job-2429 started at 2024-01-11 12:42:12 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:30.001: INFO: 	Container c ready: true, restart count 0
  Jan 11 12:42:30.001: INFO: kube-flannel-hlhld from kube-system started at 2024-01-11 00:55:50 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:30.001: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 11 12:42:30.001: INFO: kube-proxy-s78x8 from kube-system started at 2024-01-11 00:56:00 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:30.001: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 11 12:42:30.001: INFO: nginx-proxy-env1-test-worker-2 from kube-system started at 2024-01-11 01:03:57 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:30.001: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 11 12:42:30.001: INFO: nodelocaldns-49gbl from kube-system started at 2024-01-11 00:55:50 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:30.001: INFO: 	Container node-cache ready: true, restart count 0
  Jan 11 12:42:30.001: INFO: vsphere-csi-node-75mjp from kube-system started at 2024-01-11 01:05:49 +0000 UTC (3 container statuses recorded)
  Jan 11 12:42:30.001: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 11 12:42:30.001: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 11 12:42:30.001: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 11 12:42:30.001: INFO: sonobuoy-e2e-job-4f4795256cf94e3a from sonobuoy started at 2024-01-11 11:18:30 +0000 UTC (2 container statuses recorded)
  Jan 11 12:42:30.001: INFO: 	Container e2e ready: true, restart count 0
  Jan 11 12:42:30.001: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 11 12:42:30.001: INFO: sonobuoy-systemd-logs-daemon-set-7b26ca18c17c40c9-7pxz5 from sonobuoy started at 2024-01-11 11:18:31 +0000 UTC (2 container statuses recorded)
  Jan 11 12:42:30.001: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 11 12:42:30.001: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 11 12:42:30.001: INFO: traefik-ingress-mplg7 from traefik-ingress started at 2024-01-11 09:01:54 +0000 UTC (1 container statuses recorded)
  Jan 11 12:42:30.001: INFO: 	Container traefik-ingress ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 01/11/24 12:42:30.001
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.17a94b80b48802ce], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling..] @ 01/11/24 12:42:30.103
  Jan 11 12:42:31.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8634" for this suite. @ 01/11/24 12:42:31.136
• [1.388 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 01/11/24 12:42:31.161
  Jan 11 12:42:31.161: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename custom-resource-definition @ 01/11/24 12:42:31.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:42:31.214
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:42:31.226
  Jan 11 12:42:31.236: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 12:42:39.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2082" for this suite. @ 01/11/24 12:42:39.699
• [8.609 seconds]
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 01/11/24 12:42:39.77
  Jan 11 12:42:39.770: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-runtime @ 01/11/24 12:42:39.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:42:39.883
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:42:39.892
  STEP: create the container @ 01/11/24 12:42:39.907
  W0111 12:42:39.953212      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 01/11/24 12:42:39.953
  STEP: get the container status @ 01/11/24 12:42:43.026
  STEP: the container should be terminated @ 01/11/24 12:42:43.037
  STEP: the termination message should be set @ 01/11/24 12:42:43.037
  Jan 11 12:42:43.037: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 01/11/24 12:42:43.037
  Jan 11 12:42:43.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3310" for this suite. @ 01/11/24 12:42:43.109
• [3.371 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 01/11/24 12:42:43.143
  Jan 11 12:42:43.143: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename hostport @ 01/11/24 12:42:43.145
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:42:43.213
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:42:43.233
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 01/11/24 12:42:43.283
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.61.1.200 on the node which pod1 resides and expect scheduled @ 01/11/24 12:42:45.373
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.61.1.200 but use UDP protocol on the node which pod2 resides @ 01/11/24 12:42:47.416
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 01/11/24 12:42:51.537
  Jan 11 12:42:51.537: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.61.1.200 http://127.0.0.1:54323/hostname] Namespace:hostport-7328 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 12:42:51.537: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 12:42:51.540: INFO: ExecWithOptions: Clientset creation
  Jan 11 12:42:51.541: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-7328/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.61.1.200+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.61.1.200, port: 54323 @ 01/11/24 12:42:51.7
  Jan 11 12:42:51.701: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.61.1.200:54323/hostname] Namespace:hostport-7328 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 12:42:51.702: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 12:42:51.704: INFO: ExecWithOptions: Clientset creation
  Jan 11 12:42:51.704: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-7328/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.61.1.200%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.61.1.200, port: 54323 UDP @ 01/11/24 12:42:51.877
  Jan 11 12:42:51.877: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.61.1.200 54323] Namespace:hostport-7328 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 12:42:51.877: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 12:42:51.879: INFO: ExecWithOptions: Clientset creation
  Jan 11 12:42:51.879: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-7328/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.61.1.200+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  Jan 11 12:42:57.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-7328" for this suite. @ 01/11/24 12:42:57.07
• [13.948 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 01/11/24 12:42:57.101
  Jan 11 12:42:57.101: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename events @ 01/11/24 12:42:57.103
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:42:57.17
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:42:57.178
  STEP: creating a test event @ 01/11/24 12:42:57.189
  STEP: listing all events in all namespaces @ 01/11/24 12:42:57.21
  STEP: patching the test event @ 01/11/24 12:42:57.227
  STEP: fetching the test event @ 01/11/24 12:42:57.256
  STEP: updating the test event @ 01/11/24 12:42:57.268
  STEP: getting the test event @ 01/11/24 12:42:57.297
  STEP: deleting the test event @ 01/11/24 12:42:57.305
  STEP: listing all events in all namespaces @ 01/11/24 12:42:57.338
  Jan 11 12:42:57.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5521" for this suite. @ 01/11/24 12:42:57.387
• [0.306 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 01/11/24 12:42:57.407
  Jan 11 12:42:57.407: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename webhook @ 01/11/24 12:42:57.409
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:42:57.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:42:57.449
  STEP: Setting up server cert @ 01/11/24 12:42:57.53
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/11/24 12:42:58.637
  STEP: Deploying the webhook pod @ 01/11/24 12:42:58.661
  STEP: Wait for the deployment to be ready @ 01/11/24 12:42:58.686
  Jan 11 12:42:58.710: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/11/24 12:43:00.739
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 12:43:00.77
  Jan 11 12:43:01.771: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 01/11/24 12:43:01.779
  STEP: create a pod that should be denied by the webhook @ 01/11/24 12:43:01.82
  STEP: create a pod that causes the webhook to hang @ 01/11/24 12:43:01.85
  STEP: create a configmap that should be denied by the webhook @ 01/11/24 12:43:11.871
  STEP: create a configmap that should be admitted by the webhook @ 01/11/24 12:43:11.902
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 01/11/24 12:43:11.934
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 01/11/24 12:43:11.953
  STEP: create a namespace that bypass the webhook @ 01/11/24 12:43:11.969
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 01/11/24 12:43:12.008
  Jan 11 12:43:12.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2164" for this suite. @ 01/11/24 12:43:12.194
  STEP: Destroying namespace "webhook-markers-553" for this suite. @ 01/11/24 12:43:12.234
  STEP: Destroying namespace "exempted-namespace-206" for this suite. @ 01/11/24 12:43:12.257
• [14.870 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 01/11/24 12:43:12.286
  Jan 11 12:43:12.287: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename downward-api @ 01/11/24 12:43:12.291
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:43:12.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:43:12.365
  STEP: Creating a pod to test downward API volume plugin @ 01/11/24 12:43:12.377
  STEP: Saw pod success @ 01/11/24 12:43:16.475
  Jan 11 12:43:16.492: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-3f4c1d41-7cb6-488e-ada6-f246512f0fb4 container client-container: <nil>
  STEP: delete the pod @ 01/11/24 12:43:16.552
  Jan 11 12:43:16.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9238" for this suite. @ 01/11/24 12:43:16.628
• [4.365 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 01/11/24 12:43:16.661
  Jan 11 12:43:16.661: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename replicaset @ 01/11/24 12:43:16.664
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:43:16.72
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:43:16.733
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 01/11/24 12:43:16.74
  STEP: When a replicaset with a matching selector is created @ 01/11/24 12:43:18.788
  STEP: Then the orphan pod is adopted @ 01/11/24 12:43:18.817
  STEP: When the matched label of one of its pods change @ 01/11/24 12:43:19.845
  Jan 11 12:43:19.860: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 01/11/24 12:43:19.891
  Jan 11 12:43:20.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6862" for this suite. @ 01/11/24 12:43:20.921
• [4.278 seconds]
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 01/11/24 12:43:20.939
  Jan 11 12:43:20.940: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubelet-test @ 01/11/24 12:43:20.941
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:43:20.974
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:43:20.981
  Jan 11 12:43:23.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8570" for this suite. @ 01/11/24 12:43:23.086
• [2.166 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 01/11/24 12:43:23.107
  Jan 11 12:43:23.107: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename webhook @ 01/11/24 12:43:23.109
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:43:23.158
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:43:23.167
  STEP: Setting up server cert @ 01/11/24 12:43:23.228
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/11/24 12:43:25.371
  STEP: Deploying the webhook pod @ 01/11/24 12:43:25.386
  STEP: Wait for the deployment to be ready @ 01/11/24 12:43:25.413
  Jan 11 12:43:25.429: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 01/11/24 12:43:27.455
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 12:43:27.498
  Jan 11 12:43:28.498: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jan 11 12:43:28.509: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1667-crds.webhook.example.com via the AdmissionRegistration API @ 01/11/24 12:43:34.04
  STEP: Creating a custom resource that should be mutated by the webhook @ 01/11/24 12:43:34.079
  Jan 11 12:43:36.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3710" for this suite. @ 01/11/24 12:43:36.999
  STEP: Destroying namespace "webhook-markers-6934" for this suite. @ 01/11/24 12:43:37.013
• [13.922 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 01/11/24 12:43:37.029
  Jan 11 12:43:37.029: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 12:43:37.031
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:43:37.123
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:43:37.128
  STEP: Creating projection with secret that has name projected-secret-test-map-e39aee59-ed7e-42aa-966a-54b07b87803d @ 01/11/24 12:43:37.132
  STEP: Creating a pod to test consume secrets @ 01/11/24 12:43:37.145
  STEP: Saw pod success @ 01/11/24 12:43:41.199
  Jan 11 12:43:41.211: INFO: Trying to get logs from node env1-test-worker-2 pod pod-projected-secrets-ba9cc4e1-c08d-4699-9791-0468970ad1d8 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 01/11/24 12:43:41.269
  Jan 11 12:43:41.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5711" for this suite. @ 01/11/24 12:43:41.362
• [4.362 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 01/11/24 12:43:41.393
  Jan 11 12:43:41.393: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 12:43:41.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:43:41.439
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:43:41.447
  STEP: Creating configMap with name projected-configmap-test-volume-96099d46-fa5d-4e5a-842c-402c92cb3e0f @ 01/11/24 12:43:41.455
  STEP: Creating a pod to test consume configMaps @ 01/11/24 12:43:41.48
  STEP: Saw pod success @ 01/11/24 12:43:45.548
  Jan 11 12:43:45.593: INFO: Trying to get logs from node env1-test-worker-2 pod pod-projected-configmaps-13b73733-3053-43b2-a1b1-a3c5ad02c339 container agnhost-container: <nil>
  STEP: delete the pod @ 01/11/24 12:43:45.618
  Jan 11 12:43:45.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-516" for this suite. @ 01/11/24 12:43:45.683
• [4.308 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 01/11/24 12:43:45.703
  Jan 11 12:43:45.703: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename secrets @ 01/11/24 12:43:45.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:43:45.793
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:43:45.8
  STEP: Creating secret with name secret-test-df978e17-334d-4fcc-b962-b946dc8723e1 @ 01/11/24 12:43:45.806
  STEP: Creating a pod to test consume secrets @ 01/11/24 12:43:45.819
  STEP: Saw pod success @ 01/11/24 12:43:49.901
  Jan 11 12:43:49.923: INFO: Trying to get logs from node env1-test-worker-2 pod pod-secrets-2c26b8f5-d9ac-4c6a-9473-e95abcb665ef container secret-volume-test: <nil>
  STEP: delete the pod @ 01/11/24 12:43:49.941
  Jan 11 12:43:49.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6627" for this suite. @ 01/11/24 12:43:50.009
• [4.323 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 01/11/24 12:43:50.029
  Jan 11 12:43:50.029: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename pods @ 01/11/24 12:43:50.031
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:43:50.07
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:43:50.077
  Jan 11 12:43:50.084: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: creating the pod @ 01/11/24 12:43:50.086
  STEP: submitting the pod to kubernetes @ 01/11/24 12:43:50.087
  Jan 11 12:43:52.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8895" for this suite. @ 01/11/24 12:43:52.414
• [2.413 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 01/11/24 12:43:52.448
  Jan 11 12:43:52.448: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 12:43:52.45
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:43:52.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:43:52.508
  STEP: Creating a pod to test downward API volume plugin @ 01/11/24 12:43:52.525
  STEP: Saw pod success @ 01/11/24 12:43:56.672
  Jan 11 12:43:56.689: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-b01317d5-2b8a-42cf-b174-a160e3261611 container client-container: <nil>
  STEP: delete the pod @ 01/11/24 12:43:56.712
  Jan 11 12:43:56.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4298" for this suite. @ 01/11/24 12:43:56.769
• [4.340 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 01/11/24 12:43:56.79
  Jan 11 12:43:56.790: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename endpointslicemirroring @ 01/11/24 12:43:56.792
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:43:56.842
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:43:56.852
  STEP: mirroring a new custom Endpoint @ 01/11/24 12:43:56.89
  Jan 11 12:43:56.921: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 01/11/24 12:43:58.93
  Jan 11 12:43:58.953: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  STEP: mirroring deletion of a custom Endpoint @ 01/11/24 12:44:00.964
  Jan 11 12:44:01.005: INFO: Waiting for 0 EndpointSlices to exist, got 1
  Jan 11 12:44:03.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-9316" for this suite. @ 01/11/24 12:44:03.035
• [6.263 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 01/11/24 12:44:03.061
  Jan 11 12:44:03.061: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename watch @ 01/11/24 12:44:03.063
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:44:03.112
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:44:03.118
  STEP: creating a watch on configmaps @ 01/11/24 12:44:03.123
  STEP: creating a new configmap @ 01/11/24 12:44:03.129
  STEP: modifying the configmap once @ 01/11/24 12:44:03.14
  STEP: closing the watch once it receives two notifications @ 01/11/24 12:44:03.16
  Jan 11 12:44:03.207: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8374  6250a99b-6320-436c-a993-8195dfe0a228 187187488 0 2024-01-11 12:44:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-11 12:44:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 11 12:44:03.207: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8374  6250a99b-6320-436c-a993-8195dfe0a228 187187489 0 2024-01-11 12:44:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-11 12:44:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 01/11/24 12:44:03.208
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 01/11/24 12:44:03.229
  STEP: deleting the configmap @ 01/11/24 12:44:03.233
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 01/11/24 12:44:03.252
  Jan 11 12:44:03.253: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8374  6250a99b-6320-436c-a993-8195dfe0a228 187187490 0 2024-01-11 12:44:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-11 12:44:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 11 12:44:03.253: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8374  6250a99b-6320-436c-a993-8195dfe0a228 187187491 0 2024-01-11 12:44:03 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-11 12:44:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 11 12:44:03.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8374" for this suite. @ 01/11/24 12:44:03.272
• [0.229 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 01/11/24 12:44:03.291
  Jan 11 12:44:03.291: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename disruption @ 01/11/24 12:44:03.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:44:03.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:44:03.366
  STEP: Creating a pdb that targets all three pods in a test replica set @ 01/11/24 12:44:03.371
  STEP: Waiting for the pdb to be processed @ 01/11/24 12:44:03.384
  STEP: First trying to evict a pod which shouldn't be evictable @ 01/11/24 12:44:05.422
  STEP: Waiting for all pods to be running @ 01/11/24 12:44:05.422
  Jan 11 12:44:05.435: INFO: pods: 0 < 3
  STEP: locating a running pod @ 01/11/24 12:44:07.443
  STEP: Updating the pdb to allow a pod to be evicted @ 01/11/24 12:44:07.46
  STEP: Waiting for the pdb to be processed @ 01/11/24 12:44:07.48
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 01/11/24 12:44:09.508
  STEP: Waiting for all pods to be running @ 01/11/24 12:44:09.508
  STEP: Waiting for the pdb to observed all healthy pods @ 01/11/24 12:44:09.525
  STEP: Patching the pdb to disallow a pod to be evicted @ 01/11/24 12:44:09.696
  STEP: Waiting for the pdb to be processed @ 01/11/24 12:44:09.73
  STEP: Waiting for all pods to be running @ 01/11/24 12:44:11.804
  STEP: locating a running pod @ 01/11/24 12:44:11.819
  STEP: Deleting the pdb to allow a pod to be evicted @ 01/11/24 12:44:11.848
  STEP: Waiting for the pdb to be deleted @ 01/11/24 12:44:11.868
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 01/11/24 12:44:11.875
  STEP: Waiting for all pods to be running @ 01/11/24 12:44:11.875
  Jan 11 12:44:11.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-242" for this suite. @ 01/11/24 12:44:11.967
• [8.714 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 01/11/24 12:44:12.007
  Jan 11 12:44:12.007: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename pods @ 01/11/24 12:44:12.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:44:12.06
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:44:12.069
  STEP: creating pod @ 01/11/24 12:44:12.076
  Jan 11 12:44:14.151: INFO: Pod pod-hostip-3f470d0e-360c-46b4-aa0d-6776202f3aa0 has hostIP: 10.61.1.201
  Jan 11 12:44:14.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3531" for this suite. @ 01/11/24 12:44:14.168
• [2.183 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 01/11/24 12:44:14.19
  Jan 11 12:44:14.190: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubectl @ 01/11/24 12:44:14.192
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:44:14.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:44:14.242
  STEP: create deployment with httpd image @ 01/11/24 12:44:14.25
  Jan 11 12:44:14.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-117 create -f -'
  Jan 11 12:44:15.466: INFO: stderr: ""
  Jan 11 12:44:15.467: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 01/11/24 12:44:15.467
  Jan 11 12:44:15.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-117 diff -f -'
  Jan 11 12:44:16.612: INFO: rc: 1
  Jan 11 12:44:16.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-117 delete -f -'
  Jan 11 12:44:16.772: INFO: stderr: ""
  Jan 11 12:44:16.772: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Jan 11 12:44:16.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-117" for this suite. @ 01/11/24 12:44:16.788
• [2.625 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 01/11/24 12:44:16.816
  Jan 11 12:44:16.816: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubectl @ 01/11/24 12:44:16.818
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:44:16.883
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:44:16.892
  Jan 11 12:44:16.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-8086 version'
  Jan 11 12:44:17.043: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Jan 11 12:44:17.043: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.5\", GitCommit:\"93e0d7146fb9c3e9f68aa41b2b4265b2fcdb0a4c\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:48:26Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.5\", GitCommit:\"93e0d7146fb9c3e9f68aa41b2b4265b2fcdb0a4c\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:42:11Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Jan 11 12:44:17.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8086" for this suite. @ 01/11/24 12:44:17.065
• [0.275 seconds]
------------------------------
SSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 01/11/24 12:44:17.093
  Jan 11 12:44:17.093: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename dns @ 01/11/24 12:44:17.096
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:44:17.144
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:44:17.15
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7050.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7050.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 01/11/24 12:44:17.156
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7050.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7050.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 01/11/24 12:44:17.156
  STEP: creating a pod to probe /etc/hosts @ 01/11/24 12:44:17.156
  STEP: submitting the pod to kubernetes @ 01/11/24 12:44:17.156
  STEP: retrieving the pod @ 01/11/24 12:44:19.208
  STEP: looking for the results for each expected name from probers @ 01/11/24 12:44:19.222
  Jan 11 12:44:19.271: INFO: DNS probes using dns-7050/dns-test-10860f7e-45bb-4e89-8a77-e8a4acda3440 succeeded

  Jan 11 12:44:19.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/11/24 12:44:19.291
  STEP: Destroying namespace "dns-7050" for this suite. @ 01/11/24 12:44:19.329
• [2.259 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 01/11/24 12:44:19.353
  Jan 11 12:44:19.353: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubectl @ 01/11/24 12:44:19.355
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:44:19.408
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:44:19.418
  Jan 11 12:44:19.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-2260 create -f -'
  Jan 11 12:44:20.388: INFO: stderr: ""
  Jan 11 12:44:20.388: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Jan 11 12:44:20.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-2260 create -f -'
  Jan 11 12:44:21.814: INFO: stderr: ""
  Jan 11 12:44:21.814: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 01/11/24 12:44:21.814
  Jan 11 12:44:22.824: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 11 12:44:22.824: INFO: Found 1 / 1
  Jan 11 12:44:22.824: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jan 11 12:44:22.834: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 11 12:44:22.834: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jan 11 12:44:22.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-2260 describe pod agnhost-primary-qccrt'
  Jan 11 12:44:23.047: INFO: stderr: ""
  Jan 11 12:44:23.047: INFO: stdout: "Name:             agnhost-primary-qccrt\nNamespace:        kubectl-2260\nPriority:         0\nService Account:  default\nNode:             env1-test-worker-2/10.61.1.202\nStart Time:       Thu, 11 Jan 2024 12:44:20 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.69.97\nIPs:\n  IP:           10.233.69.97\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://efbf7c6566b57906627dd529ec4199a302bdaf3aa72180f8d5800a47e1095f1a\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 11 Jan 2024 12:44:21 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2986v (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-2986v:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-2260/agnhost-primary-qccrt to env1-test-worker-2\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
  Jan 11 12:44:23.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-2260 describe rc agnhost-primary'
  Jan 11 12:44:23.313: INFO: stderr: ""
  Jan 11 12:44:23.313: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2260\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-qccrt\n"
  Jan 11 12:44:23.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-2260 describe service agnhost-primary'
  Jan 11 12:44:23.527: INFO: stderr: ""
  Jan 11 12:44:23.527: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2260\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.44.155\nIPs:               10.233.44.155\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.69.97:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Jan 11 12:44:23.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-2260 describe node env1-test-master-0'
  Jan 11 12:44:23.829: INFO: stderr: ""
  Jan 11 12:44:23.830: INFO: stdout: "Name:               env1-test-master-0\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=vsphere-vm.cpu-2.mem-7gb.os-ubuntu\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=env1-test-master-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=vsphere-vm.cpu-2.mem-7gb.os-ubuntu\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 10.61.1.197\n                    csi.volume.kubernetes.io/nodeid: {\"csi.vsphere.vmware.com\":\"42086e21-1067-e398-4609-6a5784f3d5d6\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":4096,\"VtepMAC\":\"26:d1:5d:16:e7:71\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.61.1.197\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 30 Mar 2022 13:31:56 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  env1-test-master-0\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 11 Jan 2024 12:44:18 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 09 Jan 2024 16:25:58 +0000   Tue, 09 Jan 2024 16:25:58 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Thu, 11 Jan 2024 12:44:15 +0000   Tue, 09 Jan 2024 15:50:24 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 11 Jan 2024 12:44:15 +0000   Tue, 09 Jan 2024 15:50:24 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 11 Jan 2024 12:44:15 +0000   Tue, 09 Jan 2024 15:50:24 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 11 Jan 2024 12:44:15 +0000   Tue, 09 Jan 2024 15:53:19 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  Hostname:    env1-test-master-0\n  InternalIP:  10.61.1.197\n  ExternalIP:  10.61.1.197\nCapacity:\n  cpu:                2\n  ephemeral-storage:  60795672Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8054876Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  56029291223\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7952476Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 4284f3e04f9c47a4b59e54ce697ed9a6\n  System UUID:                42086e21-1067-e398-4609-6a5784f3d5d6\n  Boot ID:                    24165872-ba8d-4771-a127-74ef58934caf\n  Kernel Version:             5.4.0-67-generic\n  OS Image:                   Ubuntu 20.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.5\n  Kubelet Version:            v1.27.5\n  Kube-Proxy Version:         v1.27.5\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nProviderID:                   vsphere://42086e21-1067-e398-4609-6a5784f3d5d6\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 dns-autoscaler-7f7b458498-ncsvl                            20m (1%)      0 (0%)      10Mi (0%)        0 (0%)         44h\n  kube-system                 kube-apiserver-env1-test-master-0                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         44h\n  kube-system                 kube-controller-manager-env1-test-master-0                 200m (10%)    0 (0%)      0 (0%)           0 (0%)         44h\n  kube-system                 kube-flannel-pxzh2                                         150m (7%)     300m (15%)  64M (0%)         500M (6%)      44h\n  kube-system                 kube-proxy-t42bq                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         11h\n  kube-system                 kube-scheduler-env1-test-master-0                          100m (5%)     0 (0%)      0 (0%)           0 (0%)         44h\n  kube-system                 nodelocaldns-llks8                                         100m (5%)     0 (0%)      70Mi (0%)        200Mi (2%)     44h\n  kube-system                 vsphere-cloud-controller-manager-gbzjw                     200m (10%)    0 (0%)      0 (0%)           0 (0%)         22h\n  kube-system                 vsphere-csi-controller-c6bb68754-hwc5z                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         11h\n  kube-system                 vsphere-csi-node-s8n4m                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         11h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-7b26ca18c17c40c9-nffpx    0 (0%)        0 (0%)      0 (0%)           0 (0%)         85m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                1020m (51%)    300m (15%)\n  memory             144420Ki (1%)  709715200 (8%)\n  ephemeral-storage  0 (0%)         0 (0%)\n  hugepages-1Gi      0 (0%)         0 (0%)\n  hugepages-2Mi      0 (0%)         0 (0%)\nEvents:              <none>\n"
  Jan 11 12:44:23.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-2260 describe namespace kubectl-2260'
  Jan 11 12:44:24.059: INFO: stderr: ""
  Jan 11 12:44:24.059: INFO: stdout: "Name:         kubectl-2260\nLabels:       e2e-framework=kubectl\n              e2e-run=44be952d-0456-4382-9b38-ff2850af2e84\n              kubernetes.io/metadata.name=kubectl-2260\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Jan 11 12:44:24.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2260" for this suite. @ 01/11/24 12:44:24.08
• [4.748 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 01/11/24 12:44:24.107
  Jan 11 12:44:24.107: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename podtemplate @ 01/11/24 12:44:24.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:44:24.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:44:24.171
  STEP: Create set of pod templates @ 01/11/24 12:44:24.18
  Jan 11 12:44:24.196: INFO: created test-podtemplate-1
  Jan 11 12:44:24.211: INFO: created test-podtemplate-2
  Jan 11 12:44:24.230: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 01/11/24 12:44:24.23
  STEP: delete collection of pod templates @ 01/11/24 12:44:24.238
  Jan 11 12:44:24.238: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 01/11/24 12:44:24.308
  Jan 11 12:44:24.309: INFO: requesting list of pod templates to confirm quantity
  Jan 11 12:44:24.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-5121" for this suite. @ 01/11/24 12:44:24.347
• [0.268 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 01/11/24 12:44:24.378
  Jan 11 12:44:24.378: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename pod-network-test @ 01/11/24 12:44:24.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:44:24.459
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:44:24.469
  STEP: Performing setup for networking test in namespace pod-network-test-8024 @ 01/11/24 12:44:24.486
  STEP: creating a selector @ 01/11/24 12:44:24.486
  STEP: Creating the service pods in kubernetes @ 01/11/24 12:44:24.486
  Jan 11 12:44:24.486: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 01/11/24 12:44:46.802
  Jan 11 12:44:48.919: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jan 11 12:44:48.920: INFO: Going to poll 10.233.67.152 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jan 11 12:44:48.927: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.67.152 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8024 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 12:44:48.927: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 12:44:48.928: INFO: ExecWithOptions: Clientset creation
  Jan 11 12:44:48.928: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8024/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.67.152+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jan 11 12:44:50.074: INFO: Found all 1 expected endpoints: [netserver-0]
  Jan 11 12:44:50.074: INFO: Going to poll 10.233.68.79 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jan 11 12:44:50.086: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.68.79 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8024 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 12:44:50.087: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 12:44:50.088: INFO: ExecWithOptions: Clientset creation
  Jan 11 12:44:50.089: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8024/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.68.79+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jan 11 12:44:51.241: INFO: Found all 1 expected endpoints: [netserver-1]
  Jan 11 12:44:51.242: INFO: Going to poll 10.233.69.98 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jan 11 12:44:51.261: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.69.98 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8024 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 11 12:44:51.262: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  Jan 11 12:44:51.265: INFO: ExecWithOptions: Clientset creation
  Jan 11 12:44:51.265: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8024/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.69.98+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jan 11 12:44:52.442: INFO: Found all 1 expected endpoints: [netserver-2]
  Jan 11 12:44:52.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-8024" for this suite. @ 01/11/24 12:44:52.457
• [28.110 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 01/11/24 12:44:52.493
  Jan 11 12:44:52.493: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename pods @ 01/11/24 12:44:52.495
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:44:52.55
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:44:52.559
  STEP: Create set of pods @ 01/11/24 12:44:52.594
  Jan 11 12:44:52.618: INFO: created test-pod-1
  Jan 11 12:44:52.642: INFO: created test-pod-2
  Jan 11 12:44:52.664: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 01/11/24 12:44:52.664
  STEP: waiting for all pods to be deleted @ 01/11/24 12:44:56.88
  Jan 11 12:44:56.893: INFO: Pod quantity 3 is different from expected quantity 0
  Jan 11 12:44:57.906: INFO: Pod quantity 2 is different from expected quantity 0
  Jan 11 12:44:58.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6595" for this suite. @ 01/11/24 12:44:58.957
• [6.487 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 01/11/24 12:44:59.003
  Jan 11 12:44:59.003: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename job @ 01/11/24 12:44:59.006
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:44:59.063
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:44:59.07
  STEP: Creating a suspended job @ 01/11/24 12:44:59.086
  STEP: Patching the Job @ 01/11/24 12:44:59.108
  STEP: Watching for Job to be patched @ 01/11/24 12:44:59.135
  Jan 11 12:44:59.143: INFO: Event ADDED observed for Job e2e-q6x66 in namespace job-6318 with labels: map[e2e-job-label:e2e-q6x66] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jan 11 12:44:59.143: INFO: Event MODIFIED found for Job e2e-q6x66 in namespace job-6318 with labels: map[e2e-job-label:e2e-q6x66 e2e-q6x66:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 01/11/24 12:44:59.144
  STEP: Watching for Job to be updated @ 01/11/24 12:44:59.213
  Jan 11 12:44:59.219: INFO: Event MODIFIED found for Job e2e-q6x66 in namespace job-6318 with labels: map[e2e-job-label:e2e-q6x66 e2e-q6x66:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 11 12:44:59.219: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 01/11/24 12:44:59.219
  Jan 11 12:44:59.240: INFO: Job: e2e-q6x66 as labels: map[e2e-job-label:e2e-q6x66 e2e-q6x66:patched]
  STEP: Waiting for job to complete @ 01/11/24 12:44:59.24
  STEP: Delete a job collection with a labelselector @ 01/11/24 12:45:09.25
  STEP: Watching for Job to be deleted @ 01/11/24 12:45:09.282
  Jan 11 12:45:09.288: INFO: Event MODIFIED observed for Job e2e-q6x66 in namespace job-6318 with labels: map[e2e-job-label:e2e-q6x66 e2e-q6x66:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 11 12:45:09.289: INFO: Event MODIFIED observed for Job e2e-q6x66 in namespace job-6318 with labels: map[e2e-job-label:e2e-q6x66 e2e-q6x66:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 11 12:45:09.289: INFO: Event MODIFIED observed for Job e2e-q6x66 in namespace job-6318 with labels: map[e2e-job-label:e2e-q6x66 e2e-q6x66:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 11 12:45:09.290: INFO: Event MODIFIED observed for Job e2e-q6x66 in namespace job-6318 with labels: map[e2e-job-label:e2e-q6x66 e2e-q6x66:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 11 12:45:09.290: INFO: Event MODIFIED observed for Job e2e-q6x66 in namespace job-6318 with labels: map[e2e-job-label:e2e-q6x66 e2e-q6x66:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 11 12:45:09.290: INFO: Event MODIFIED observed for Job e2e-q6x66 in namespace job-6318 with labels: map[e2e-job-label:e2e-q6x66 e2e-q6x66:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 11 12:45:09.290: INFO: Event MODIFIED observed for Job e2e-q6x66 in namespace job-6318 with labels: map[e2e-job-label:e2e-q6x66 e2e-q6x66:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 11 12:45:09.291: INFO: Event MODIFIED observed for Job e2e-q6x66 in namespace job-6318 with labels: map[e2e-job-label:e2e-q6x66 e2e-q6x66:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 11 12:45:09.291: INFO: Event MODIFIED observed for Job e2e-q6x66 in namespace job-6318 with labels: map[e2e-job-label:e2e-q6x66 e2e-q6x66:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 11 12:45:09.292: INFO: Event MODIFIED observed for Job e2e-q6x66 in namespace job-6318 with labels: map[e2e-job-label:e2e-q6x66 e2e-q6x66:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 11 12:45:09.292: INFO: Event MODIFIED observed for Job e2e-q6x66 in namespace job-6318 with labels: map[e2e-job-label:e2e-q6x66 e2e-q6x66:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 11 12:45:09.292: INFO: Event DELETED found for Job e2e-q6x66 in namespace job-6318 with labels: map[e2e-job-label:e2e-q6x66 e2e-q6x66:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 01/11/24 12:45:09.292
  Jan 11 12:45:09.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6318" for this suite. @ 01/11/24 12:45:09.411
• [10.448 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:350
  STEP: Creating a kubernetes client @ 01/11/24 12:45:09.454
  Jan 11 12:45:09.454: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename field-validation @ 01/11/24 12:45:09.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:45:09.582
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:45:09.593
  Jan 11 12:45:09.602: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  W0111 12:45:09.604866      23 field_validation.go:423] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc000d79460 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  W0111 12:45:17.404049      23 warnings.go:70] unknown field "alpha"
  W0111 12:45:17.404108      23 warnings.go:70] unknown field "beta"
  W0111 12:45:17.404146      23 warnings.go:70] unknown field "delta"
  W0111 12:45:17.404158      23 warnings.go:70] unknown field "epsilon"
  W0111 12:45:17.404168      23 warnings.go:70] unknown field "gamma"
  Jan 11 12:45:18.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1800" for this suite. @ 01/11/24 12:45:18.07
• [8.632 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 01/11/24 12:45:18.086
  Jan 11 12:45:18.086: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename emptydir @ 01/11/24 12:45:18.089
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:45:18.128
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:45:18.134
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 01/11/24 12:45:18.139
  STEP: Saw pod success @ 01/11/24 12:45:22.19
  Jan 11 12:45:22.197: INFO: Trying to get logs from node env1-test-worker-2 pod pod-6d911837-8484-48f9-bf5c-298e9f6a0448 container test-container: <nil>
  STEP: delete the pod @ 01/11/24 12:45:22.231
  Jan 11 12:45:22.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2670" for this suite. @ 01/11/24 12:45:22.295
• [4.233 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 01/11/24 12:45:22.326
  Jan 11 12:45:22.326: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename namespaces @ 01/11/24 12:45:22.329
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:45:22.377
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:45:22.383
  STEP: Creating a test namespace @ 01/11/24 12:45:22.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:45:22.43
  STEP: Creating a service in the namespace @ 01/11/24 12:45:22.437
  STEP: Deleting the namespace @ 01/11/24 12:45:22.482
  STEP: Waiting for the namespace to be removed. @ 01/11/24 12:45:22.515
  STEP: Recreating the namespace @ 01/11/24 12:45:29.524
  STEP: Verifying there is no service in the namespace @ 01/11/24 12:45:29.577
  Jan 11 12:45:29.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-2227" for this suite. @ 01/11/24 12:45:29.611
  STEP: Destroying namespace "nsdeletetest-8517" for this suite. @ 01/11/24 12:45:29.634
  Jan 11 12:45:29.662: INFO: Namespace nsdeletetest-8517 was already deleted
  STEP: Destroying namespace "nsdeletetest-9229" for this suite. @ 01/11/24 12:45:29.663
• [7.356 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 01/11/24 12:45:29.685
  Jan 11 12:45:29.685: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename emptydir @ 01/11/24 12:45:29.687
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:45:29.773
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:45:29.782
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 01/11/24 12:45:29.788
  STEP: Saw pod success @ 01/11/24 12:45:33.882
  Jan 11 12:45:33.893: INFO: Trying to get logs from node env1-test-worker-2 pod pod-58aeeab8-0f6b-4d3f-aecc-3cf0fe563207 container test-container: <nil>
  STEP: delete the pod @ 01/11/24 12:45:33.914
  Jan 11 12:45:33.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4205" for this suite. @ 01/11/24 12:45:33.967
• [4.294 seconds]
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 01/11/24 12:45:33.979
  Jan 11 12:45:33.979: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubelet-test @ 01/11/24 12:45:33.982
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:45:34.042
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:45:34.051
  STEP: Waiting for pod completion @ 01/11/24 12:45:34.135
  Jan 11 12:45:38.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6885" for this suite. @ 01/11/24 12:45:38.221
• [4.258 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 01/11/24 12:45:38.239
  Jan 11 12:45:38.239: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 12:45:38.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:45:38.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:45:38.292
  STEP: Creating configMap with name projected-configmap-test-volume-map-9179209e-b0a8-499f-918b-7de1e09560f7 @ 01/11/24 12:45:38.299
  STEP: Creating a pod to test consume configMaps @ 01/11/24 12:45:38.312
  STEP: Saw pod success @ 01/11/24 12:45:42.377
  Jan 11 12:45:42.388: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-configmaps-e9600101-d20d-4106-a359-5059cc2f3b3d container agnhost-container: <nil>
  STEP: delete the pod @ 01/11/24 12:45:42.423
  Jan 11 12:45:42.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8959" for this suite. @ 01/11/24 12:45:42.472
• [4.249 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 01/11/24 12:45:42.502
  Jan 11 12:45:42.502: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubectl @ 01/11/24 12:45:42.504
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:45:42.538
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:45:42.545
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 01/11/24 12:45:42.559
  Jan 11 12:45:42.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-7151 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jan 11 12:45:42.746: INFO: stderr: ""
  Jan 11 12:45:42.746: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 01/11/24 12:45:42.746
  STEP: verifying the pod e2e-test-httpd-pod was created @ 01/11/24 12:45:47.797
  Jan 11 12:45:47.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-7151 get pod e2e-test-httpd-pod -o json'
  Jan 11 12:45:47.980: INFO: stderr: ""
  Jan 11 12:45:47.980: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2024-01-11T12:45:42Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7151\",\n        \"resourceVersion\": \"187188634\",\n        \"uid\": \"ed9f1c79-0e84-4d33-9fbc-64c5fb74bbe2\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-k7d2p\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"env1-test-worker-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-k7d2p\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-11T12:45:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-11T12:45:44Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-11T12:45:44Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-11T12:45:42Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://f8ee2441bf09592338b52bf8b59f1af57c912c76f01fa27cc6841adc5bdd6196\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2024-01-11T12:45:43Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.61.1.201\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.68.86\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.68.86\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2024-01-11T12:45:42Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 01/11/24 12:45:47.98
  Jan 11 12:45:47.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-7151 replace -f -'
  Jan 11 12:45:49.218: INFO: stderr: ""
  Jan 11 12:45:49.218: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 01/11/24 12:45:49.218
  Jan 11 12:45:49.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-7151 delete pods e2e-test-httpd-pod'
  Jan 11 12:45:51.217: INFO: stderr: ""
  Jan 11 12:45:51.217: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jan 11 12:45:51.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7151" for this suite. @ 01/11/24 12:45:51.231
• [8.749 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 01/11/24 12:45:51.252
  Jan 11 12:45:51.252: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename configmap @ 01/11/24 12:45:51.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:45:51.304
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:45:51.311
  STEP: Creating configMap with name configmap-test-volume-map-23df831c-683c-49ac-b690-a114e3026690 @ 01/11/24 12:45:51.318
  STEP: Creating a pod to test consume configMaps @ 01/11/24 12:45:51.332
  STEP: Saw pod success @ 01/11/24 12:45:55.408
  Jan 11 12:45:55.415: INFO: Trying to get logs from node env1-test-worker-2 pod pod-configmaps-7cc8352d-d63c-44d7-bfed-803d3b78673a container agnhost-container: <nil>
  STEP: delete the pod @ 01/11/24 12:45:55.431
  Jan 11 12:45:55.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8370" for this suite. @ 01/11/24 12:45:55.481
• [4.250 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 01/11/24 12:45:55.508
  Jan 11 12:45:55.508: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename gc @ 01/11/24 12:45:55.511
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:45:55.545
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:45:55.55
  Jan 11 12:45:55.653: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d5dd1f4b-a209-474e-b800-536299cd6b2b", Controller:(*bool)(0xc00693145a), BlockOwnerDeletion:(*bool)(0xc00693145b)}}
  Jan 11 12:45:55.692: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1d7ddd0f-9c8f-4e2e-b9b7-9bd3188f102c", Controller:(*bool)(0xc0085714e2), BlockOwnerDeletion:(*bool)(0xc0085714e3)}}
  Jan 11 12:45:55.714: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"229d59c7-126e-4833-8ed0-2f998ed3a50e", Controller:(*bool)(0xc008571782), BlockOwnerDeletion:(*bool)(0xc008571783)}}
  Jan 11 12:46:00.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2349" for this suite. @ 01/11/24 12:46:00.778
• [5.294 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 01/11/24 12:46:00.808
  Jan 11 12:46:00.809: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename cronjob @ 01/11/24 12:46:00.812
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:46:00.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:46:00.903
  STEP: Creating a ReplaceConcurrent cronjob @ 01/11/24 12:46:00.91
  STEP: Ensuring a job is scheduled @ 01/11/24 12:46:00.929
  STEP: Ensuring exactly one is scheduled @ 01/11/24 12:47:00.937
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 01/11/24 12:47:00.945
  STEP: Ensuring the job is replaced with a new one @ 01/11/24 12:47:00.952
  STEP: Removing cronjob @ 01/11/24 12:48:00.963
  Jan 11 12:48:00.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-1534" for this suite. @ 01/11/24 12:48:01.001
• [120.216 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 01/11/24 12:48:01.028
  Jan 11 12:48:01.028: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename emptydir-wrapper @ 01/11/24 12:48:01.03
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:48:01.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:48:01.089
  Jan 11 12:48:03.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 01/11/24 12:48:03.192
  STEP: Cleaning up the configmap @ 01/11/24 12:48:03.215
  STEP: Cleaning up the pod @ 01/11/24 12:48:03.231
  STEP: Destroying namespace "emptydir-wrapper-7983" for this suite. @ 01/11/24 12:48:03.269
• [2.255 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:177
  STEP: Creating a kubernetes client @ 01/11/24 12:48:03.293
  Jan 11 12:48:03.293: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename daemonsets @ 01/11/24 12:48:03.295
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:48:03.361
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:48:03.369
  STEP: Creating simple DaemonSet "daemon-set" @ 01/11/24 12:48:03.443
  STEP: Check that daemon pods launch on every node of the cluster. @ 01/11/24 12:48:03.457
  Jan 11 12:48:03.473: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:03.473: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:03.473: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:03.482: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 12:48:03.482: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  Jan 11 12:48:04.520: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:04.520: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:04.520: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:04.535: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 11 12:48:04.535: INFO: Node env1-test-worker-1 is running 0 daemon pod, expected 1
  Jan 11 12:48:05.496: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:05.496: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:05.496: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:05.509: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jan 11 12:48:05.509: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 01/11/24 12:48:05.517
  Jan 11 12:48:05.580: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:05.580: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:05.581: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:05.594: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 11 12:48:05.594: INFO: Node env1-test-worker-1 is running 0 daemon pod, expected 1
  Jan 11 12:48:06.629: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:06.630: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:06.631: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:06.646: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 11 12:48:06.647: INFO: Node env1-test-worker-1 is running 0 daemon pod, expected 1
  Jan 11 12:48:07.611: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:07.612: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:07.612: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:07.626: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 11 12:48:07.626: INFO: Node env1-test-worker-1 is running 0 daemon pod, expected 1
  Jan 11 12:48:08.612: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:08.612: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:08.612: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:48:08.624: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jan 11 12:48:08.624: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 01/11/24 12:48:08.635
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3337, will wait for the garbage collector to delete the pods @ 01/11/24 12:48:08.635
  Jan 11 12:48:08.712: INFO: Deleting DaemonSet.extensions daemon-set took: 17.752745ms
  Jan 11 12:48:08.812: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.221544ms
  Jan 11 12:48:10.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 11 12:48:10.327: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jan 11 12:48:10.337: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"187189508"},"items":null}

  Jan 11 12:48:10.349: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"187189508"},"items":null}

  Jan 11 12:48:10.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3337" for this suite. @ 01/11/24 12:48:10.425
• [7.155 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 01/11/24 12:48:10.45
  Jan 11 12:48:10.450: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename containers @ 01/11/24 12:48:10.452
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:48:10.494
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:48:10.501
  Jan 11 12:48:12.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5140" for this suite. @ 01/11/24 12:48:12.679
• [2.272 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 01/11/24 12:48:12.726
  Jan 11 12:48:12.726: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename subjectreview @ 01/11/24 12:48:12.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:48:12.786
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:48:12.8
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-8841" @ 01/11/24 12:48:12.81
  Jan 11 12:48:12.822: INFO: saUsername: "system:serviceaccount:subjectreview-8841:e2e"
  Jan 11 12:48:12.823: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-8841"}
  Jan 11 12:48:12.823: INFO: saUID: "86dcde36-294a-4870-a4ac-621f33bd924b"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-8841:e2e" @ 01/11/24 12:48:12.823
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-8841:e2e" @ 01/11/24 12:48:12.823
  Jan 11 12:48:12.829: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-8841:e2e" api 'list' configmaps in "subjectreview-8841" namespace @ 01/11/24 12:48:12.829
  Jan 11 12:48:12.834: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-8841:e2e" @ 01/11/24 12:48:12.834
  Jan 11 12:48:12.840: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Jan 11 12:48:12.840: INFO: LocalSubjectAccessReview has been verified
  Jan 11 12:48:12.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-8841" for this suite. @ 01/11/24 12:48:12.855
• [0.152 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 01/11/24 12:48:12.878
  Jan 11 12:48:12.878: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename statefulset @ 01/11/24 12:48:12.88
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:48:12.938
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:48:12.952
  STEP: Creating service test in namespace statefulset-6662 @ 01/11/24 12:48:12.96
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 01/11/24 12:48:12.972
  STEP: Creating stateful set ss in namespace statefulset-6662 @ 01/11/24 12:48:12.984
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6662 @ 01/11/24 12:48:12.996
  Jan 11 12:48:13.011: INFO: Found 0 stateful pods, waiting for 1
  Jan 11 12:48:23.023: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 01/11/24 12:48:23.023
  Jan 11 12:48:23.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-6662 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 11 12:48:23.351: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 11 12:48:23.351: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 11 12:48:23.351: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 11 12:48:23.359: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Jan 11 12:48:33.374: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jan 11 12:48:33.374: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 11 12:48:33.428: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999929s
  Jan 11 12:48:34.438: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.989265496s
  Jan 11 12:48:35.458: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.978873374s
  Jan 11 12:48:36.468: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.959346466s
  Jan 11 12:48:37.488: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.948179684s
  Jan 11 12:48:38.505: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.928748135s
  Jan 11 12:48:39.514: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.912340716s
  Jan 11 12:48:40.526: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.902158921s
  Jan 11 12:48:41.539: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.890482843s
  Jan 11 12:48:42.551: INFO: Verifying statefulset ss doesn't scale past 1 for another 877.291213ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6662 @ 01/11/24 12:48:43.551
  Jan 11 12:48:43.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-6662 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 11 12:48:43.868: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 11 12:48:43.868: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 11 12:48:43.868: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 11 12:48:43.881: INFO: Found 1 stateful pods, waiting for 3
  Jan 11 12:48:53.895: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 11 12:48:53.895: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jan 11 12:48:53.895: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 01/11/24 12:48:53.895
  STEP: Scale down will halt with unhealthy stateful pod @ 01/11/24 12:48:53.895
  Jan 11 12:48:53.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-6662 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 11 12:48:54.219: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 11 12:48:54.219: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 11 12:48:54.219: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 11 12:48:54.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-6662 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 11 12:48:54.584: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 11 12:48:54.584: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 11 12:48:54.584: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 11 12:48:54.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-6662 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 11 12:48:54.937: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 11 12:48:54.937: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 11 12:48:54.937: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 11 12:48:54.937: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 11 12:48:54.946: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  Jan 11 12:49:04.970: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jan 11 12:49:04.970: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jan 11 12:49:04.970: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jan 11 12:49:05.015: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999417s
  Jan 11 12:49:06.027: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.984719867s
  Jan 11 12:49:07.040: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.972156423s
  Jan 11 12:49:08.050: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.958861626s
  Jan 11 12:49:09.063: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.949422299s
  Jan 11 12:49:10.074: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.937188698s
  Jan 11 12:49:11.086: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.925249113s
  Jan 11 12:49:12.098: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.912631638s
  Jan 11 12:49:13.108: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.902396746s
  Jan 11 12:49:14.119: INFO: Verifying statefulset ss doesn't scale past 3 for another 891.658525ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6662 @ 01/11/24 12:49:15.119
  Jan 11 12:49:15.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-6662 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 11 12:49:15.432: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 11 12:49:15.432: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 11 12:49:15.432: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 11 12:49:15.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-6662 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 11 12:49:15.766: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 11 12:49:15.766: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 11 12:49:15.766: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 11 12:49:15.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-6662 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 11 12:49:16.126: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 11 12:49:16.126: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 11 12:49:16.126: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 11 12:49:16.126: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 01/11/24 12:49:26.171
  Jan 11 12:49:26.171: INFO: Deleting all statefulset in ns statefulset-6662
  Jan 11 12:49:26.184: INFO: Scaling statefulset ss to 0
  Jan 11 12:49:26.217: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 11 12:49:26.227: INFO: Deleting statefulset ss
  Jan 11 12:49:26.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6662" for this suite. @ 01/11/24 12:49:26.283
• [73.425 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 01/11/24 12:49:26.312
  Jan 11 12:49:26.312: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename replication-controller @ 01/11/24 12:49:26.314
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:49:26.356
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:49:26.362
  STEP: Given a ReplicationController is created @ 01/11/24 12:49:26.369
  STEP: When the matched label of one of its pods change @ 01/11/24 12:49:26.384
  Jan 11 12:49:26.394: INFO: Pod name pod-release: Found 0 pods out of 1
  Jan 11 12:49:31.406: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 01/11/24 12:49:31.441
  Jan 11 12:49:32.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8530" for this suite. @ 01/11/24 12:49:32.488
• [6.197 seconds]
------------------------------
SS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 01/11/24 12:49:32.513
  Jan 11 12:49:32.513: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename sysctl @ 01/11/24 12:49:32.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:49:32.553
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:49:32.564
  STEP: Creating a pod with one valid and two invalid sysctls @ 01/11/24 12:49:32.573
  Jan 11 12:49:32.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-3009" for this suite. @ 01/11/24 12:49:32.609
• [0.123 seconds]
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 01/11/24 12:49:32.637
  Jan 11 12:49:32.637: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename downward-api @ 01/11/24 12:49:32.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:49:32.695
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:49:32.704
  STEP: Creating a pod to test downward API volume plugin @ 01/11/24 12:49:32.712
  STEP: Saw pod success @ 01/11/24 12:49:36.788
  Jan 11 12:49:36.800: INFO: Trying to get logs from node env1-test-worker-2 pod downwardapi-volume-e1e924e4-2a87-4f88-88e4-ee484c3754d9 container client-container: <nil>
  STEP: delete the pod @ 01/11/24 12:49:36.853
  Jan 11 12:49:36.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5095" for this suite. @ 01/11/24 12:49:36.924
• [4.313 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 01/11/24 12:49:36.95
  Jan 11 12:49:36.950: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename replication-controller @ 01/11/24 12:49:36.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:49:37.001
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:49:37.009
  STEP: creating a ReplicationController @ 01/11/24 12:49:37.036
  STEP: waiting for RC to be added @ 01/11/24 12:49:37.049
  STEP: waiting for available Replicas @ 01/11/24 12:49:37.049
  STEP: patching ReplicationController @ 01/11/24 12:49:38.572
  STEP: waiting for RC to be modified @ 01/11/24 12:49:38.588
  STEP: patching ReplicationController status @ 01/11/24 12:49:38.588
  STEP: waiting for RC to be modified @ 01/11/24 12:49:38.615
  STEP: waiting for available Replicas @ 01/11/24 12:49:38.617
  STEP: fetching ReplicationController status @ 01/11/24 12:49:38.642
  STEP: patching ReplicationController scale @ 01/11/24 12:49:38.664
  STEP: waiting for RC to be modified @ 01/11/24 12:49:38.685
  STEP: waiting for ReplicationController's scale to be the max amount @ 01/11/24 12:49:38.686
  STEP: fetching ReplicationController; ensuring that it's patched @ 01/11/24 12:49:40.189
  STEP: updating ReplicationController status @ 01/11/24 12:49:40.203
  STEP: waiting for RC to be modified @ 01/11/24 12:49:40.227
  STEP: listing all ReplicationControllers @ 01/11/24 12:49:40.227
  STEP: checking that ReplicationController has expected values @ 01/11/24 12:49:40.241
  STEP: deleting ReplicationControllers by collection @ 01/11/24 12:49:40.241
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 01/11/24 12:49:40.263
  Jan 11 12:49:40.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0111 12:49:40.364319      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-5899" for this suite. @ 01/11/24 12:49:40.382
• [3.454 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 01/11/24 12:49:40.407
  Jan 11 12:49:40.407: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename disruption @ 01/11/24 12:49:40.41
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:49:40.45
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:49:40.457
  STEP: Creating a kubernetes client @ 01/11/24 12:49:40.462
  Jan 11 12:49:40.462: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename disruption-2 @ 01/11/24 12:49:40.464
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:49:40.509
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:49:40.516
  STEP: Waiting for the pdb to be processed @ 01/11/24 12:49:40.529
  E0111 12:49:41.365517      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:49:42.366095      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 01/11/24 12:49:42.565
  STEP: Waiting for the pdb to be processed @ 01/11/24 12:49:42.632
  E0111 12:49:43.366279      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:49:44.366603      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 01/11/24 12:49:44.655
  STEP: listing a collection of PDBs in namespace disruption-3297 @ 01/11/24 12:49:44.666
  STEP: deleting a collection of PDBs @ 01/11/24 12:49:44.673
  STEP: Waiting for the PDB collection to be deleted @ 01/11/24 12:49:44.707
  Jan 11 12:49:44.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 11 12:49:44.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-5531" for this suite. @ 01/11/24 12:49:44.746
  STEP: Destroying namespace "disruption-3297" for this suite. @ 01/11/24 12:49:44.766
• [4.379 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 01/11/24 12:49:44.787
  Jan 11 12:49:44.787: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename webhook @ 01/11/24 12:49:44.789
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:49:44.828
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:49:44.836
  STEP: Setting up server cert @ 01/11/24 12:49:44.897
  E0111 12:49:45.367155      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:49:46.368062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/11/24 12:49:46.394
  STEP: Deploying the webhook pod @ 01/11/24 12:49:46.414
  STEP: Wait for the deployment to be ready @ 01/11/24 12:49:46.445
  Jan 11 12:49:46.482: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0111 12:49:47.368332      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:49:48.368661      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/11/24 12:49:48.515
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 12:49:48.548
  E0111 12:49:49.368845      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:49:49.549: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jan 11 12:49:49.560: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  E0111 12:49:50.369044      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:49:51.370395      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:49:52.370983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:49:53.371416      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:49:54.371570      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9076-crds.webhook.example.com via the AdmissionRegistration API @ 01/11/24 12:49:55.11
  STEP: Creating a custom resource that should be mutated by the webhook @ 01/11/24 12:49:55.154
  E0111 12:49:55.372079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:49:56.373125      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:49:57.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0111 12:49:57.373933      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-6820" for this suite. @ 01/11/24 12:49:58.066
  STEP: Destroying namespace "webhook-markers-2300" for this suite. @ 01/11/24 12:49:58.081
• [13.306 seconds]
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 01/11/24 12:49:58.093
  Jan 11 12:49:58.093: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename services @ 01/11/24 12:49:58.095
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:49:58.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:49:58.139
  STEP: creating a collection of services @ 01/11/24 12:49:58.145
  Jan 11 12:49:58.146: INFO: Creating e2e-svc-a-kxrb7
  Jan 11 12:49:58.183: INFO: Creating e2e-svc-b-zbn9t
  Jan 11 12:49:58.212: INFO: Creating e2e-svc-c-q8nlz
  STEP: deleting service collection @ 01/11/24 12:49:58.241
  Jan 11 12:49:58.323: INFO: Collection of services has been deleted
  Jan 11 12:49:58.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3618" for this suite. @ 01/11/24 12:49:58.335
• [0.277 seconds]
------------------------------
SS  E0111 12:49:58.373911      23 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 01/11/24 12:49:58.377
  Jan 11 12:49:58.377: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename controllerrevisions @ 01/11/24 12:49:58.379
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:49:58.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:49:58.427
  STEP: Creating DaemonSet "e2e-mlsq7-daemon-set" @ 01/11/24 12:49:58.495
  STEP: Check that daemon pods launch on every node of the cluster. @ 01/11/24 12:49:58.509
  Jan 11 12:49:58.529: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:49:58.530: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:49:58.530: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:49:58.540: INFO: Number of nodes with available pods controlled by daemonset e2e-mlsq7-daemon-set: 0
  Jan 11 12:49:58.540: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  E0111 12:49:59.374209      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:49:59.554: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:49:59.554: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:49:59.555: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:49:59.567: INFO: Number of nodes with available pods controlled by daemonset e2e-mlsq7-daemon-set: 0
  Jan 11 12:49:59.567: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  E0111 12:50:00.374499      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:50:00.566: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:50:00.566: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:50:00.567: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 11 12:50:00.585: INFO: Number of nodes with available pods controlled by daemonset e2e-mlsq7-daemon-set: 3
  Jan 11 12:50:00.585: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-mlsq7-daemon-set
  STEP: Confirm DaemonSet "e2e-mlsq7-daemon-set" successfully created with "daemonset-name=e2e-mlsq7-daemon-set" label @ 01/11/24 12:50:00.605
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-mlsq7-daemon-set" @ 01/11/24 12:50:00.63
  Jan 11 12:50:00.651: INFO: Located ControllerRevision: "e2e-mlsq7-daemon-set-84d8f66875"
  STEP: Patching ControllerRevision "e2e-mlsq7-daemon-set-84d8f66875" @ 01/11/24 12:50:00.658
  Jan 11 12:50:00.675: INFO: e2e-mlsq7-daemon-set-84d8f66875 has been patched
  STEP: Create a new ControllerRevision @ 01/11/24 12:50:00.676
  Jan 11 12:50:00.688: INFO: Created ControllerRevision: e2e-mlsq7-daemon-set-69fdd85c47
  STEP: Confirm that there are two ControllerRevisions @ 01/11/24 12:50:00.689
  Jan 11 12:50:00.689: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jan 11 12:50:00.696: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-mlsq7-daemon-set-84d8f66875" @ 01/11/24 12:50:00.696
  STEP: Confirm that there is only one ControllerRevision @ 01/11/24 12:50:00.711
  Jan 11 12:50:00.711: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jan 11 12:50:00.717: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-mlsq7-daemon-set-69fdd85c47" @ 01/11/24 12:50:00.725
  Jan 11 12:50:00.749: INFO: e2e-mlsq7-daemon-set-69fdd85c47 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 01/11/24 12:50:00.749
  W0111 12:50:00.765782      23 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 01/11/24 12:50:00.766
  Jan 11 12:50:00.766: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0111 12:50:01.375893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:50:01.775: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jan 11 12:50:01.786: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-mlsq7-daemon-set-69fdd85c47=updated" @ 01/11/24 12:50:01.787
  STEP: Confirm that there is only one ControllerRevision @ 01/11/24 12:50:01.81
  Jan 11 12:50:01.811: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jan 11 12:50:01.819: INFO: Found 1 ControllerRevisions
  Jan 11 12:50:01.826: INFO: ControllerRevision "e2e-mlsq7-daemon-set-996f6fbb" has revision 3
  STEP: Deleting DaemonSet "e2e-mlsq7-daemon-set" @ 01/11/24 12:50:01.834
  STEP: deleting DaemonSet.extensions e2e-mlsq7-daemon-set in namespace controllerrevisions-2222, will wait for the garbage collector to delete the pods @ 01/11/24 12:50:01.834
  Jan 11 12:50:01.919: INFO: Deleting DaemonSet.extensions e2e-mlsq7-daemon-set took: 21.451248ms
  Jan 11 12:50:02.020: INFO: Terminating DaemonSet.extensions e2e-mlsq7-daemon-set pods took: 100.493691ms
  E0111 12:50:02.376341      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:03.376687      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:50:04.133: INFO: Number of nodes with available pods controlled by daemonset e2e-mlsq7-daemon-set: 0
  Jan 11 12:50:04.133: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-mlsq7-daemon-set
  Jan 11 12:50:04.151: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"187190618"},"items":null}

  Jan 11 12:50:04.161: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"187190618"},"items":null}

  Jan 11 12:50:04.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-2222" for this suite. @ 01/11/24 12:50:04.233
• [5.886 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 01/11/24 12:50:04.271
  Jan 11 12:50:04.271: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename emptydir-wrapper @ 01/11/24 12:50:04.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:50:04.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:50:04.347
  STEP: Creating 50 configmaps @ 01/11/24 12:50:04.355
  E0111 12:50:04.377609      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 01/11/24 12:50:05.258
  Jan 11 12:50:05.292: INFO: Pod name wrapped-volume-race-b541e659-08fa-4b92-80b4-011b8175358b: Found 0 pods out of 5
  E0111 12:50:05.379068      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:06.379844      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:07.380179      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:08.380592      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:09.380760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:50:10.313: INFO: Pod name wrapped-volume-race-b541e659-08fa-4b92-80b4-011b8175358b: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 01/11/24 12:50:10.313
  E0111 12:50:10.380895      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 01/11/24 12:50:10.399
  Jan 11 12:50:10.440: INFO: Pod name wrapped-volume-race-7dc4357f-6e58-4c09-9489-f35b6a80b86e: Found 0 pods out of 5
  E0111 12:50:11.384781      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:12.385835      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:13.386287      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:14.386812      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:15.387261      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:50:15.464: INFO: Pod name wrapped-volume-race-7dc4357f-6e58-4c09-9489-f35b6a80b86e: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 01/11/24 12:50:15.464
  STEP: Creating RC which spawns configmap-volume pods @ 01/11/24 12:50:15.585
  Jan 11 12:50:15.656: INFO: Pod name wrapped-volume-race-4e90e35f-4c77-4442-aba6-e0dbd4659229: Found 0 pods out of 5
  E0111 12:50:16.388107      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:17.388712      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:18.389558      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:19.389967      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:20.390369      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:50:20.683: INFO: Pod name wrapped-volume-race-4e90e35f-4c77-4442-aba6-e0dbd4659229: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 01/11/24 12:50:20.684
  Jan 11 12:50:20.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-4e90e35f-4c77-4442-aba6-e0dbd4659229 in namespace emptydir-wrapper-4012, will wait for the garbage collector to delete the pods @ 01/11/24 12:50:20.762
  Jan 11 12:50:20.847: INFO: Deleting ReplicationController wrapped-volume-race-4e90e35f-4c77-4442-aba6-e0dbd4659229 took: 26.181275ms
  Jan 11 12:50:21.092: INFO: Terminating ReplicationController wrapped-volume-race-4e90e35f-4c77-4442-aba6-e0dbd4659229 pods took: 244.909079ms
  E0111 12:50:21.390678      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:22.391656      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:23.392130      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-7dc4357f-6e58-4c09-9489-f35b6a80b86e in namespace emptydir-wrapper-4012, will wait for the garbage collector to delete the pods @ 01/11/24 12:50:23.394
  Jan 11 12:50:23.467: INFO: Deleting ReplicationController wrapped-volume-race-7dc4357f-6e58-4c09-9489-f35b6a80b86e took: 13.062166ms
  Jan 11 12:50:23.668: INFO: Terminating ReplicationController wrapped-volume-race-7dc4357f-6e58-4c09-9489-f35b6a80b86e pods took: 200.826919ms
  E0111 12:50:24.392367      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:25.393166      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-b541e659-08fa-4b92-80b4-011b8175358b in namespace emptydir-wrapper-4012, will wait for the garbage collector to delete the pods @ 01/11/24 12:50:25.569
  Jan 11 12:50:25.655: INFO: Deleting ReplicationController wrapped-volume-race-b541e659-08fa-4b92-80b4-011b8175358b took: 22.798719ms
  Jan 11 12:50:25.855: INFO: Terminating ReplicationController wrapped-volume-race-b541e659-08fa-4b92-80b4-011b8175358b pods took: 200.710359ms
  E0111 12:50:26.393816      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:27.394498      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 01/11/24 12:50:27.657
  E0111 12:50:28.395080      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "emptydir-wrapper-4012" for this suite. @ 01/11/24 12:50:28.599
• [24.344 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 01/11/24 12:50:28.628
  Jan 11 12:50:28.628: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename services @ 01/11/24 12:50:28.631
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:50:28.689
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:50:28.695
  STEP: creating service in namespace services-14 @ 01/11/24 12:50:28.7
  STEP: creating service affinity-nodeport in namespace services-14 @ 01/11/24 12:50:28.7
  STEP: creating replication controller affinity-nodeport in namespace services-14 @ 01/11/24 12:50:28.754
  I0111 12:50:28.780631      23 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-14, replica count: 3
  E0111 12:50:29.395564      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:30.397030      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:31.396785      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0111 12:50:31.832918      23 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 11 12:50:31.866: INFO: Creating new exec pod
  E0111 12:50:32.397412      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:33.398559      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:34.398955      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:50:34.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-14 exec execpod-affinity84gr4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Jan 11 12:50:35.262: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Jan 11 12:50:35.262: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 11 12:50:35.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-14 exec execpod-affinity84gr4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.53.42 80'
  E0111 12:50:35.400058      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:50:35.577: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.53.42 80\nConnection to 10.233.53.42 80 port [tcp/http] succeeded!\n"
  Jan 11 12:50:35.577: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 11 12:50:35.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-14 exec execpod-affinity84gr4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.200 32569'
  Jan 11 12:50:35.914: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.61.1.200 32569\nConnection to 10.61.1.200 32569 port [tcp/*] succeeded!\n"
  Jan 11 12:50:35.914: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 11 12:50:35.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-14 exec execpod-affinity84gr4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.202 32569'
  Jan 11 12:50:36.247: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.61.1.202 32569\nConnection to 10.61.1.202 32569 port [tcp/*] succeeded!\n"
  Jan 11 12:50:36.248: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 11 12:50:36.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-14 exec execpod-affinity84gr4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.61.1.200:32569/ ; done'
  E0111 12:50:36.400226      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:50:36.769: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:32569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:32569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:32569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:32569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:32569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:32569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:32569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:32569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:32569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:32569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:32569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:32569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:32569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:32569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:32569/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:32569/\n"
  Jan 11 12:50:36.769: INFO: stdout: "\naffinity-nodeport-57zmg\naffinity-nodeport-57zmg\naffinity-nodeport-57zmg\naffinity-nodeport-57zmg\naffinity-nodeport-57zmg\naffinity-nodeport-57zmg\naffinity-nodeport-57zmg\naffinity-nodeport-57zmg\naffinity-nodeport-57zmg\naffinity-nodeport-57zmg\naffinity-nodeport-57zmg\naffinity-nodeport-57zmg\naffinity-nodeport-57zmg\naffinity-nodeport-57zmg\naffinity-nodeport-57zmg\naffinity-nodeport-57zmg"
  Jan 11 12:50:36.769: INFO: Received response from host: affinity-nodeport-57zmg
  Jan 11 12:50:36.769: INFO: Received response from host: affinity-nodeport-57zmg
  Jan 11 12:50:36.769: INFO: Received response from host: affinity-nodeport-57zmg
  Jan 11 12:50:36.769: INFO: Received response from host: affinity-nodeport-57zmg
  Jan 11 12:50:36.769: INFO: Received response from host: affinity-nodeport-57zmg
  Jan 11 12:50:36.769: INFO: Received response from host: affinity-nodeport-57zmg
  Jan 11 12:50:36.769: INFO: Received response from host: affinity-nodeport-57zmg
  Jan 11 12:50:36.769: INFO: Received response from host: affinity-nodeport-57zmg
  Jan 11 12:50:36.769: INFO: Received response from host: affinity-nodeport-57zmg
  Jan 11 12:50:36.769: INFO: Received response from host: affinity-nodeport-57zmg
  Jan 11 12:50:36.769: INFO: Received response from host: affinity-nodeport-57zmg
  Jan 11 12:50:36.769: INFO: Received response from host: affinity-nodeport-57zmg
  Jan 11 12:50:36.769: INFO: Received response from host: affinity-nodeport-57zmg
  Jan 11 12:50:36.769: INFO: Received response from host: affinity-nodeport-57zmg
  Jan 11 12:50:36.769: INFO: Received response from host: affinity-nodeport-57zmg
  Jan 11 12:50:36.769: INFO: Received response from host: affinity-nodeport-57zmg
  Jan 11 12:50:36.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 11 12:50:36.787: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-14, will wait for the garbage collector to delete the pods @ 01/11/24 12:50:36.826
  Jan 11 12:50:36.907: INFO: Deleting ReplicationController affinity-nodeport took: 20.872101ms
  Jan 11 12:50:37.008: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.872989ms
  E0111 12:50:37.400675      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:38.401687      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:39.402142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-14" for this suite. @ 01/11/24 12:50:39.444
• [10.853 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 01/11/24 12:50:39.484
  Jan 11 12:50:39.485: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 12:50:39.488
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:50:39.541
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:50:39.55
  STEP: Creating a pod to test downward API volume plugin @ 01/11/24 12:50:39.558
  E0111 12:50:40.402397      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:41.402713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:42.402901      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:43.403734      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/11/24 12:50:43.628
  Jan 11 12:50:43.635: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-6a985e2c-6e1d-4871-af5c-73d9b7b75bd2 container client-container: <nil>
  STEP: delete the pod @ 01/11/24 12:50:43.681
  Jan 11 12:50:43.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3951" for this suite. @ 01/11/24 12:50:43.745
• [4.290 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 01/11/24 12:50:43.777
  Jan 11 12:50:43.777: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename services @ 01/11/24 12:50:43.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:50:43.821
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:50:43.827
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-2615 @ 01/11/24 12:50:43.834
  STEP: changing the ExternalName service to type=NodePort @ 01/11/24 12:50:43.844
  STEP: creating replication controller externalname-service in namespace services-2615 @ 01/11/24 12:50:43.892
  I0111 12:50:43.915876      23 runners.go:194] Created replication controller with name: externalname-service, namespace: services-2615, replica count: 2
  E0111 12:50:44.404765      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:45.406066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:46.406745      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0111 12:50:46.967445      23 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 11 12:50:46.967: INFO: Creating new exec pod
  E0111 12:50:47.406558      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:48.407275      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:49.408008      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:50:50.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2615 exec execpodhj6b6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jan 11 12:50:50.395: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jan 11 12:50:50.395: INFO: stdout: ""
  E0111 12:50:50.408779      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:50:51.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2615 exec execpodhj6b6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  E0111 12:50:51.409584      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:50:51.691: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jan 11 12:50:51.691: INFO: stdout: "externalname-service-vlkz9"
  Jan 11 12:50:51.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2615 exec execpodhj6b6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.18.120 80'
  Jan 11 12:50:51.986: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.18.120 80\nConnection to 10.233.18.120 80 port [tcp/http] succeeded!\n"
  Jan 11 12:50:51.986: INFO: stdout: "externalname-service-5mg79"
  Jan 11 12:50:51.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2615 exec execpodhj6b6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.202 30892'
  Jan 11 12:50:52.293: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.61.1.202 30892\nConnection to 10.61.1.202 30892 port [tcp/*] succeeded!\n"
  Jan 11 12:50:52.294: INFO: stdout: "externalname-service-5mg79"
  Jan 11 12:50:52.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2615 exec execpodhj6b6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.201 30892'
  E0111 12:50:52.409915      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:50:52.612: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.61.1.201 30892\nConnection to 10.61.1.201 30892 port [tcp/*] succeeded!\n"
  Jan 11 12:50:52.612: INFO: stdout: "externalname-service-5mg79"
  Jan 11 12:50:52.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 11 12:50:52.626: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-2615" for this suite. @ 01/11/24 12:50:52.698
• [8.953 seconds]
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 01/11/24 12:50:52.732
  Jan 11 12:50:52.732: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename statefulset @ 01/11/24 12:50:52.734
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:50:52.792
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:50:52.799
  STEP: Creating service test in namespace statefulset-3515 @ 01/11/24 12:50:52.806
  STEP: Creating statefulset ss in namespace statefulset-3515 @ 01/11/24 12:50:52.817
  Jan 11 12:50:52.846: INFO: Found 0 stateful pods, waiting for 1
  E0111 12:50:53.410097      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:54.410632      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:55.411350      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:56.412453      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:57.412879      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:58.413211      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:50:59.413834      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:00.414407      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:01.415583      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:02.416487      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:51:02.859: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 01/11/24 12:51:02.873
  STEP: updating a scale subresource @ 01/11/24 12:51:02.88
  STEP: verifying the statefulset Spec.Replicas was modified @ 01/11/24 12:51:02.913
  STEP: Patch a scale subresource @ 01/11/24 12:51:02.938
  STEP: verifying the statefulset Spec.Replicas was modified @ 01/11/24 12:51:02.963
  Jan 11 12:51:02.974: INFO: Deleting all statefulset in ns statefulset-3515
  Jan 11 12:51:02.982: INFO: Scaling statefulset ss to 0
  E0111 12:51:03.417590      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:04.418476      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:05.418915      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:06.419429      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:07.419640      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:08.420367      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:09.420862      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:10.421292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:11.422468      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:12.422855      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:51:13.077: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 11 12:51:13.084: INFO: Deleting statefulset ss
  Jan 11 12:51:13.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3515" for this suite. @ 01/11/24 12:51:13.12
• [20.401 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 01/11/24 12:51:13.139
  Jan 11 12:51:13.140: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename secrets @ 01/11/24 12:51:13.144
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:51:13.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:51:13.184
  STEP: Creating secret with name secret-test-b10e3ffa-c64d-4b5d-9a59-143aca4a2820 @ 01/11/24 12:51:13.19
  STEP: Creating a pod to test consume secrets @ 01/11/24 12:51:13.204
  E0111 12:51:13.423927      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:14.424203      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:15.424852      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:16.426244      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/11/24 12:51:17.264
  Jan 11 12:51:17.272: INFO: Trying to get logs from node env1-test-worker-1 pod pod-secrets-c69917e8-c32e-4642-b54a-c1495d91f00e container secret-volume-test: <nil>
  STEP: delete the pod @ 01/11/24 12:51:17.294
  Jan 11 12:51:17.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3349" for this suite. @ 01/11/24 12:51:17.366
• [4.251 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 01/11/24 12:51:17.393
  Jan 11 12:51:17.393: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename runtimeclass @ 01/11/24 12:51:17.395
  E0111 12:51:17.426281      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:51:17.441
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:51:17.448
  Jan 11 12:51:17.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-675" for this suite. @ 01/11/24 12:51:17.489
• [0.120 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 01/11/24 12:51:17.513
  Jan 11 12:51:17.513: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename services @ 01/11/24 12:51:17.515
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:51:17.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:51:17.572
  STEP: creating a Service @ 01/11/24 12:51:17.587
  STEP: watching for the Service to be added @ 01/11/24 12:51:17.614
  Jan 11 12:51:17.619: INFO: Found Service test-service-kmbbl in namespace services-9461 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Jan 11 12:51:17.619: INFO: Service test-service-kmbbl created
  STEP: Getting /status @ 01/11/24 12:51:17.62
  Jan 11 12:51:17.629: INFO: Service test-service-kmbbl has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 01/11/24 12:51:17.629
  STEP: watching for the Service to be patched @ 01/11/24 12:51:17.672
  Jan 11 12:51:17.679: INFO: observed Service test-service-kmbbl in namespace services-9461 with annotations: map[] & LoadBalancer: {[]}
  Jan 11 12:51:17.679: INFO: Found Service test-service-kmbbl in namespace services-9461 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Jan 11 12:51:17.680: INFO: Service test-service-kmbbl has service status patched
  STEP: updating the ServiceStatus @ 01/11/24 12:51:17.681
  Jan 11 12:51:17.716: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 01/11/24 12:51:17.717
  Jan 11 12:51:17.722: INFO: Observed Service test-service-kmbbl in namespace services-9461 with annotations: map[] & Conditions: {[]}
  Jan 11 12:51:17.722: INFO: Observed event: &Service{ObjectMeta:{test-service-kmbbl  services-9461  2f9b5b0c-df00-4ac6-8573-287a23bd6ea3 187191726 0 2024-01-11 12:51:17 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2024-01-11 12:51:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2024-01-11 12:51:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.8.202,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.8.202],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Jan 11 12:51:17.723: INFO: Found Service test-service-kmbbl in namespace services-9461 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jan 11 12:51:17.724: INFO: Service test-service-kmbbl has service status updated
  STEP: patching the service @ 01/11/24 12:51:17.724
  STEP: watching for the Service to be patched @ 01/11/24 12:51:17.745
  Jan 11 12:51:17.750: INFO: observed Service test-service-kmbbl in namespace services-9461 with labels: map[test-service-static:true]
  Jan 11 12:51:17.750: INFO: observed Service test-service-kmbbl in namespace services-9461 with labels: map[test-service-static:true]
  Jan 11 12:51:17.750: INFO: observed Service test-service-kmbbl in namespace services-9461 with labels: map[test-service-static:true]
  Jan 11 12:51:17.750: INFO: Found Service test-service-kmbbl in namespace services-9461 with labels: map[test-service:patched test-service-static:true]
  Jan 11 12:51:17.750: INFO: Service test-service-kmbbl patched
  STEP: deleting the service @ 01/11/24 12:51:17.751
  STEP: watching for the Service to be deleted @ 01/11/24 12:51:17.814
  Jan 11 12:51:17.818: INFO: Observed event: ADDED
  Jan 11 12:51:17.818: INFO: Observed event: MODIFIED
  Jan 11 12:51:17.818: INFO: Observed event: MODIFIED
  Jan 11 12:51:17.819: INFO: Observed event: MODIFIED
  Jan 11 12:51:17.819: INFO: Found Service test-service-kmbbl in namespace services-9461 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Jan 11 12:51:17.820: INFO: Service test-service-kmbbl deleted
  Jan 11 12:51:17.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9461" for this suite. @ 01/11/24 12:51:17.864
• [0.372 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 01/11/24 12:51:17.887
  Jan 11 12:51:17.887: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename proxy @ 01/11/24 12:51:17.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:51:17.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:51:17.941
  Jan 11 12:51:17.950: INFO: Creating pod...
  E0111 12:51:18.426446      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:19.427181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:51:19.998: INFO: Creating service...
  Jan 11 12:51:20.028: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7494/pods/agnhost/proxy/some/path/with/DELETE
  Jan 11 12:51:20.044: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jan 11 12:51:20.044: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7494/pods/agnhost/proxy/some/path/with/GET
  Jan 11 12:51:20.055: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jan 11 12:51:20.055: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7494/pods/agnhost/proxy/some/path/with/HEAD
  Jan 11 12:51:20.063: INFO: http.Client request:HEAD | StatusCode:200
  Jan 11 12:51:20.063: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7494/pods/agnhost/proxy/some/path/with/OPTIONS
  Jan 11 12:51:20.073: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jan 11 12:51:20.074: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7494/pods/agnhost/proxy/some/path/with/PATCH
  Jan 11 12:51:20.090: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jan 11 12:51:20.090: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7494/pods/agnhost/proxy/some/path/with/POST
  Jan 11 12:51:20.108: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jan 11 12:51:20.109: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7494/pods/agnhost/proxy/some/path/with/PUT
  Jan 11 12:51:20.120: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jan 11 12:51:20.120: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7494/services/test-service/proxy/some/path/with/DELETE
  Jan 11 12:51:20.139: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jan 11 12:51:20.140: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7494/services/test-service/proxy/some/path/with/GET
  Jan 11 12:51:20.162: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jan 11 12:51:20.162: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7494/services/test-service/proxy/some/path/with/HEAD
  Jan 11 12:51:20.181: INFO: http.Client request:HEAD | StatusCode:200
  Jan 11 12:51:20.182: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7494/services/test-service/proxy/some/path/with/OPTIONS
  Jan 11 12:51:20.201: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jan 11 12:51:20.201: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7494/services/test-service/proxy/some/path/with/PATCH
  Jan 11 12:51:20.214: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jan 11 12:51:20.215: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7494/services/test-service/proxy/some/path/with/POST
  Jan 11 12:51:20.230: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jan 11 12:51:20.231: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7494/services/test-service/proxy/some/path/with/PUT
  Jan 11 12:51:20.253: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jan 11 12:51:20.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-7494" for this suite. @ 01/11/24 12:51:20.272
• [2.416 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:474
  STEP: Creating a kubernetes client @ 01/11/24 12:51:20.307
  Jan 11 12:51:20.307: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename field-validation @ 01/11/24 12:51:20.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:51:20.359
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:51:20.365
  Jan 11 12:51:20.373: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  E0111 12:51:20.427807      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:21.429117      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:22.429564      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:23.430454      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:24.431126      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:25.432113      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:26.432233      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:27.433044      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0111 12:51:28.038254      23 warnings.go:70] unknown field "alpha"
  W0111 12:51:28.038377      23 warnings.go:70] unknown field "beta"
  W0111 12:51:28.038396      23 warnings.go:70] unknown field "delta"
  W0111 12:51:28.038411      23 warnings.go:70] unknown field "epsilon"
  W0111 12:51:28.038444      23 warnings.go:70] unknown field "gamma"
  E0111 12:51:28.433204      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:51:28.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9942" for this suite. @ 01/11/24 12:51:29.238
• [9.013 seconds]
------------------------------
SSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 01/11/24 12:51:29.321
  Jan 11 12:51:29.321: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename secrets @ 01/11/24 12:51:29.323
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:51:29.354
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:51:29.36
  STEP: Creating secret with name secret-test-7108112b-7335-4585-b729-158c2ea67275 @ 01/11/24 12:51:29.368
  STEP: Creating a pod to test consume secrets @ 01/11/24 12:51:29.379
  E0111 12:51:29.434027      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:30.434304      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:31.434834      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:32.435327      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/11/24 12:51:33.43
  E0111 12:51:33.435914      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:51:33.438: INFO: Trying to get logs from node env1-test-worker-1 pod pod-secrets-99a4d83a-b63f-4b6e-9b3d-c1557cd28599 container secret-env-test: <nil>
  STEP: delete the pod @ 01/11/24 12:51:33.455
  Jan 11 12:51:33.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2558" for this suite. @ 01/11/24 12:51:33.507
• [4.203 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 01/11/24 12:51:33.525
  Jan 11 12:51:33.525: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename kubectl @ 01/11/24 12:51:33.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:51:33.567
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:51:33.572
  STEP: creating Agnhost RC @ 01/11/24 12:51:33.578
  Jan 11 12:51:33.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-3160 create -f -'
  E0111 12:51:34.436612      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:51:34.716: INFO: stderr: ""
  Jan 11 12:51:34.716: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 01/11/24 12:51:34.716
  E0111 12:51:35.436880      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:51:35.728: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 11 12:51:35.728: INFO: Found 0 / 1
  E0111 12:51:36.436990      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:51:36.726: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 11 12:51:36.726: INFO: Found 1 / 1
  Jan 11 12:51:36.726: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 01/11/24 12:51:36.726
  Jan 11 12:51:36.741: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 11 12:51:36.741: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jan 11 12:51:36.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=kubectl-3160 patch pod agnhost-primary-gj8rx -p {"metadata":{"annotations":{"x":"y"}}}'
  Jan 11 12:51:36.950: INFO: stderr: ""
  Jan 11 12:51:36.950: INFO: stdout: "pod/agnhost-primary-gj8rx patched\n"
  STEP: checking annotations @ 01/11/24 12:51:36.95
  Jan 11 12:51:36.967: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 11 12:51:36.967: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jan 11 12:51:36.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3160" for this suite. @ 01/11/24 12:51:36.983
• [3.478 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 01/11/24 12:51:37.006
  Jan 11 12:51:37.006: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename sched-preemption @ 01/11/24 12:51:37.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:51:37.062
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:51:37.075
  Jan 11 12:51:37.122: INFO: Waiting up to 1m0s for all nodes to be ready
  E0111 12:51:37.437706      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:38.437960      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:39.438120      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:40.438382      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:41.438773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:42.439416      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:43.440266      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:44.440570      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:45.441630      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:46.442823      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:47.443377      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:48.443909      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:49.444787      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:50.445131      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:51.445515      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:52.445989      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:53.446916      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:54.447535      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:55.448360      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:56.448700      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:57.448848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:58.449287      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:51:59.449671      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:00.450311      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:01.451310      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:02.452010      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:03.453096      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:04.453609      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:05.454434      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:06.455748      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:07.456221      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:08.456749      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:09.457499      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:10.457784      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:11.457950      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:12.458427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:13.459121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:14.459607      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:15.459923      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:16.460955      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:17.461232      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:18.462054      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:19.462585      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:20.463367      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:21.464247      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:22.465129      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:23.466211      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:24.466738      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:25.467710      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:26.468849      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:27.468992      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:28.469710      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:29.469847      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:30.470504      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:31.471315      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:32.471730      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:33.471856      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:34.472024      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:35.473042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:36.473186      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:52:37.255: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 01/11/24 12:52:37.266
  Jan 11 12:52:37.266: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename sched-preemption-path @ 01/11/24 12:52:37.268
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:52:37.31
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:52:37.318
  STEP: Finding an available node @ 01/11/24 12:52:37.328
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 01/11/24 12:52:37.328
  E0111 12:52:37.473680      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:38.474777      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 01/11/24 12:52:39.398
  Jan 11 12:52:39.440: INFO: found a healthy node: env1-test-worker-1
  E0111 12:52:39.475004      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:40.476179      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:41.477031      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:42.477266      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:43.478040      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:44.478155      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:45.479294      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:52:45.648: INFO: pods created so far: [1 1 1]
  Jan 11 12:52:45.648: INFO: length of pods created so far: 3
  E0111 12:52:46.479492      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:47.479941      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:52:47.672: INFO: pods created so far: [2 2 1]
  E0111 12:52:48.481108      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:49.481616      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:50.481839      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:51.481950      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:52.482106      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:53.482767      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:54.483440      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:52:54.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 11 12:52:54.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-6065" for this suite. @ 01/11/24 12:52:54.941
  STEP: Destroying namespace "sched-preemption-6024" for this suite. @ 01/11/24 12:52:54.974
• [77.994 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 01/11/24 12:52:55.012
  Jan 11 12:52:55.013: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename sched-pred @ 01/11/24 12:52:55.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:52:55.077
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:52:55.086
  Jan 11 12:52:55.093: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jan 11 12:52:55.125: INFO: Waiting for terminating namespaces to be deleted...
  Jan 11 12:52:55.134: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-0 before test
  Jan 11 12:52:55.178: INFO: kube-flannel-r8g5h from kube-system started at 2024-01-09 16:24:44 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.178: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 11 12:52:55.178: INFO: kube-proxy-c8shb from kube-system started at 2024-01-11 00:56:00 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.178: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 11 12:52:55.178: INFO: metrics-server-6b7574f5b-jmbtm from kube-system started at 2024-01-09 16:32:13 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.178: INFO: 	Container metrics-server ready: true, restart count 0
  Jan 11 12:52:55.179: INFO: nginx-proxy-env1-test-worker-0 from kube-system started at 2024-01-09 16:31:48 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.179: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 11 12:52:55.179: INFO: nodelocaldns-vkvkp from kube-system started at 2024-01-09 15:52:20 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.179: INFO: 	Container node-cache ready: true, restart count 0
  Jan 11 12:52:55.179: INFO: vsphere-csi-node-5gf9n from kube-system started at 2024-01-11 01:05:49 +0000 UTC (3 container statuses recorded)
  Jan 11 12:52:55.179: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 11 12:52:55.179: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 11 12:52:55.179: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 11 12:52:55.179: INFO: sonobuoy-systemd-logs-daemon-set-7b26ca18c17c40c9-cqtts from sonobuoy started at 2024-01-11 11:18:31 +0000 UTC (2 container statuses recorded)
  Jan 11 12:52:55.179: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 11 12:52:55.179: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 11 12:52:55.179: INFO: traefik-ingress-g4tjs from traefik-ingress started at 2024-01-10 14:39:42 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.179: INFO: 	Container traefik-ingress ready: true, restart count 0
  Jan 11 12:52:55.179: INFO: velero-794b84894f-nwdwf from velero started at 2024-01-10 13:26:26 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.179: INFO: 	Container velero ready: true, restart count 0
  Jan 11 12:52:55.179: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-1 before test
  Jan 11 12:52:55.208: INFO: kube-flannel-jxf5s from kube-system started at 2024-01-09 16:24:09 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.208: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 11 12:52:55.208: INFO: kube-proxy-sdfjc from kube-system started at 2024-01-11 00:56:00 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.208: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 11 12:52:55.208: INFO: nginx-proxy-env1-test-worker-1 from kube-system started at 2024-01-09 16:38:31 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.208: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 11 12:52:55.208: INFO: nodelocaldns-7qx4w from kube-system started at 2024-01-09 15:52:12 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.208: INFO: 	Container node-cache ready: true, restart count 0
  Jan 11 12:52:55.208: INFO: vsphere-csi-node-pwx5m from kube-system started at 2024-01-11 01:05:49 +0000 UTC (3 container statuses recorded)
  Jan 11 12:52:55.208: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 11 12:52:55.208: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 11 12:52:55.208: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 11 12:52:55.208: INFO: pod4 from sched-preemption-path-6065 started at 2024-01-11 12:52:47 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.208: INFO: 	Container pod4 ready: true, restart count 0
  Jan 11 12:52:55.208: INFO: rs-pod3-nhswc from sched-preemption-path-6065 started at 2024-01-11 12:52:43 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.208: INFO: 	Container pod3 ready: true, restart count 0
  Jan 11 12:52:55.208: INFO: sonobuoy from sonobuoy started at 2024-01-11 11:18:29 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.208: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jan 11 12:52:55.208: INFO: sonobuoy-systemd-logs-daemon-set-7b26ca18c17c40c9-zn5jg from sonobuoy started at 2024-01-11 11:18:31 +0000 UTC (2 container statuses recorded)
  Jan 11 12:52:55.208: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 11 12:52:55.208: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 11 12:52:55.208: INFO: traefik-ingress-j2r8r from traefik-ingress started at 2024-01-11 12:26:34 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.208: INFO: 	Container traefik-ingress ready: true, restart count 0
  Jan 11 12:52:55.208: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-2 before test
  Jan 11 12:52:55.251: INFO: kube-flannel-hlhld from kube-system started at 2024-01-11 00:55:50 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.251: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 11 12:52:55.251: INFO: kube-proxy-s78x8 from kube-system started at 2024-01-11 00:56:00 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.251: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 11 12:52:55.251: INFO: nginx-proxy-env1-test-worker-2 from kube-system started at 2024-01-11 01:03:57 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.251: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 11 12:52:55.251: INFO: nodelocaldns-49gbl from kube-system started at 2024-01-11 00:55:50 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.251: INFO: 	Container node-cache ready: true, restart count 0
  Jan 11 12:52:55.251: INFO: vsphere-csi-node-75mjp from kube-system started at 2024-01-11 01:05:49 +0000 UTC (3 container statuses recorded)
  Jan 11 12:52:55.251: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 11 12:52:55.251: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 11 12:52:55.251: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 11 12:52:55.251: INFO: sonobuoy-e2e-job-4f4795256cf94e3a from sonobuoy started at 2024-01-11 11:18:30 +0000 UTC (2 container statuses recorded)
  Jan 11 12:52:55.251: INFO: 	Container e2e ready: true, restart count 0
  Jan 11 12:52:55.251: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 11 12:52:55.251: INFO: sonobuoy-systemd-logs-daemon-set-7b26ca18c17c40c9-7pxz5 from sonobuoy started at 2024-01-11 11:18:31 +0000 UTC (2 container statuses recorded)
  Jan 11 12:52:55.251: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 11 12:52:55.251: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 11 12:52:55.251: INFO: traefik-ingress-mplg7 from traefik-ingress started at 2024-01-11 09:01:54 +0000 UTC (1 container statuses recorded)
  Jan 11 12:52:55.251: INFO: 	Container traefik-ingress ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 01/11/24 12:52:55.251
  E0111 12:52:55.484257      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:56.484950      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 01/11/24 12:52:57.304
  STEP: Trying to apply a random label on the found node. @ 01/11/24 12:52:57.343
  STEP: verifying the node has the label kubernetes.io/e2e-c924749c-b42c-4138-bc33-e3a79b863b6d 95 @ 01/11/24 12:52:57.387
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 01/11/24 12:52:57.395
  E0111 12:52:57.485448      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:52:58.485294      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.61.1.202 on the node which pod4 resides and expect not scheduled @ 01/11/24 12:52:59.447
  E0111 12:52:59.486367      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:00.486849      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:01.487573      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:02.488653      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:03.488867      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:04.489456      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:05.489719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:06.490627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:07.491745      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:08.492347      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:09.492362      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:10.492732      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:11.493823      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:12.494082      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:13.494397      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:14.495075      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:15.495665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:16.496325      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:17.496890      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:18.497143      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:19.497851      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:20.498741      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:21.499927      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:22.500320      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:23.501027      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:24.500979      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:25.501475      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:26.502061      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:27.502590      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:28.503200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:29.503738      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:30.504124      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:31.505177      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:32.505724      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:33.506109      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:34.506427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:35.507005      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:36.508112      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:37.508502      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:38.508884      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:39.509660      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:40.509693      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:41.509873      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:42.510079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:43.510758      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:44.510977      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:45.511849      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:46.512422      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:47.513032      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:48.513359      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:49.513483      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:50.514076      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:51.514335      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:52.514642      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:53.514973      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:54.515285      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:55.516008      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:56.517108      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:57.517898      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:58.518112      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:53:59.518320      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:00.519445      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:01.519662      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:02.519948      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:03.520599      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:04.521532      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:05.521753      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:06.521855      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:07.522109      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:08.522193      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:09.522577      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:10.523665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:11.524411      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:12.524836      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:13.525364      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:14.525627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:15.526106      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:16.526457      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:17.527251      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:18.528444      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:19.528986      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:20.529447      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:21.530649      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:22.530778      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:23.531247      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:24.531441      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:25.531844      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:26.533528      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:27.533923      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:28.534689      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:29.535992      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:30.536839      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:31.536982      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:32.537326      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:33.538245      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:34.538775      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:35.538813      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:36.539279      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:37.540378      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:38.541657      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:39.541724      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:40.542371      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:41.542587      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:42.543486      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:43.543686      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:44.544431      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:45.545014      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:46.546473      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:47.546562      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:48.547951      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:49.547610      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:50.548550      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:51.549061      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:52.549720      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:53.549916      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:54.550563      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:55.551371      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:56.551288      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:57.551900      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:58.552054      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:54:59.552468      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:00.553432      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:01.554438      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:02.554484      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:03.554791      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:04.555589      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:05.556514      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:06.556905      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:07.558113      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:08.558136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:09.558534      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:10.558850      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:11.559098      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:12.559922      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:13.560143      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:14.561144      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:15.561882      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:16.562373      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:17.562601      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:18.562729      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:19.563230      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:20.564165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:21.565160      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:22.566006      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:23.566280      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:24.566269      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:25.567073      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:26.567347      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:27.567965      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:28.568909      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:29.569510      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:30.569995      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:31.570197      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:32.570981      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:33.571311      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:34.572028      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:35.572429      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:36.573377      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:37.573960      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:38.574137      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:39.575291      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:40.576345      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:41.576697      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:42.577031      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:43.577166      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:44.577431      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:45.577802      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:46.578213      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:47.579105      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:48.580108      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:49.580436      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:50.580708      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:51.581658      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:52.581960      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:53.582224      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:54.583372      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:55.583960      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:56.584846      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:57.585088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:58.585773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:55:59.586494      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:00.587466      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:01.587569      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:02.588332      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:03.589115      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:04.589781      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:05.590073      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:06.591132      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:07.591871      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:08.592821      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:09.593013      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:10.594103      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:11.594555      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:12.594809      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:13.595554      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:14.596060      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:15.596612      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:16.597319      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:17.597529      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:18.598133      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:19.598366      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:20.598930      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:21.600062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:22.600748      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:23.600899      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:24.601134      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:25.601397      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:26.601710      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:27.601906      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:28.602583      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:29.602672      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:30.603092      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:31.603481      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:32.604408      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:33.604671      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:34.604964      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:35.605193      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:36.606020      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:37.606382      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:38.606916      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:39.607966      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:40.608737      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:41.609578      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:42.610141      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:43.610651      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:44.611072      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:45.611667      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:46.612572      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:47.613292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:48.613486      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:49.613759      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:50.613918      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:51.614164      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:52.615105      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:53.615119      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:54.615397      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:55.616819      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:56.617131      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:57.617321      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:58.618030      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:56:59.618761      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:00.619216      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:01.619397      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:02.620985      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:03.621163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:04.622214      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:05.622497      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:06.622588      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:07.622842      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:08.623051      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:09.623187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:10.623662      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:11.624734      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:12.625316      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:13.626052      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:14.626578      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:15.626716      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:16.626995      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:17.627522      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:18.627966      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:19.628760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:20.628980      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:21.629329      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:22.629578      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:23.629701      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:24.630204      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:25.630381      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:26.631254      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:27.631396      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:28.631610      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:29.632489      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:30.632782      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:31.633811      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:32.634335      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:33.635401      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:34.635799      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:35.637066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:36.637628      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:37.637768      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:38.637909      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:39.638659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:40.639382      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:41.639971      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:42.640287      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:43.640684      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:44.642027      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:45.641624      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:46.641641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:47.642870      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:48.643142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:49.643900      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:50.644350      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:51.645311      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:52.645862      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:53.645920      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:54.646593      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:55.647477      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:56.647966      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:57.648284      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:57:58.648903      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-c924749c-b42c-4138-bc33-e3a79b863b6d off the node env1-test-worker-2 @ 01/11/24 12:57:59.467
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-c924749c-b42c-4138-bc33-e3a79b863b6d @ 01/11/24 12:57:59.517
  Jan 11 12:57:59.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-4765" for this suite. @ 01/11/24 12:57:59.545
• [304.553 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 01/11/24 12:57:59.579
  Jan 11 12:57:59.579: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename downward-api @ 01/11/24 12:57:59.582
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:57:59.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:57:59.647
  E0111 12:57:59.648941      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a pod to test downward api env vars @ 01/11/24 12:57:59.657
  E0111 12:58:00.649333      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:01.650055      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:02.650405      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:03.651008      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/11/24 12:58:03.724
  Jan 11 12:58:03.734: INFO: Trying to get logs from node env1-test-worker-1 pod downward-api-a4d1b8c6-c193-4963-a07b-923d6959d400 container dapi-container: <nil>
  STEP: delete the pod @ 01/11/24 12:58:03.782
  Jan 11 12:58:03.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6726" for this suite. @ 01/11/24 12:58:03.823
• [4.259 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 01/11/24 12:58:03.839
  Jan 11 12:58:03.839: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename job @ 01/11/24 12:58:03.841
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:58:03.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:58:03.886
  STEP: Creating a job @ 01/11/24 12:58:03.894
  STEP: Ensure pods equal to parallelism count is attached to the job @ 01/11/24 12:58:03.908
  E0111 12:58:04.651130      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:05.651853      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 01/11/24 12:58:05.921
  STEP: updating /status @ 01/11/24 12:58:05.95
  STEP: get /status @ 01/11/24 12:58:06.004
  Jan 11 12:58:06.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7692" for this suite. @ 01/11/24 12:58:06.031
• [2.225 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 01/11/24 12:58:06.065
  Jan 11 12:58:06.065: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename services @ 01/11/24 12:58:06.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:58:06.112
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:58:06.117
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5352 @ 01/11/24 12:58:06.126
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 01/11/24 12:58:06.184
  STEP: creating service externalsvc in namespace services-5352 @ 01/11/24 12:58:06.184
  STEP: creating replication controller externalsvc in namespace services-5352 @ 01/11/24 12:58:06.227
  I0111 12:58:06.245640      23 runners.go:194] Created replication controller with name: externalsvc, namespace: services-5352, replica count: 2
  E0111 12:58:06.653262      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:07.653620      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:08.653806      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0111 12:58:09.296688      23 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 01/11/24 12:58:09.331
  Jan 11 12:58:09.410: INFO: Creating new exec pod
  E0111 12:58:09.656899      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:10.657107      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:58:11.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-5352 exec execpodnlr2k -- /bin/sh -x -c nslookup clusterip-service.services-5352.svc.cluster.local'
  E0111 12:58:11.657890      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:58:11.866: INFO: stderr: "+ nslookup clusterip-service.services-5352.svc.cluster.local\n"
  Jan 11 12:58:11.866: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-5352.svc.cluster.local\tcanonical name = externalsvc.services-5352.svc.cluster.local.\nName:\texternalsvc.services-5352.svc.cluster.local\nAddress: 10.233.59.199\n\n"
  Jan 11 12:58:11.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-5352, will wait for the garbage collector to delete the pods @ 01/11/24 12:58:11.884
  Jan 11 12:58:11.972: INFO: Deleting ReplicationController externalsvc took: 22.683279ms
  Jan 11 12:58:12.073: INFO: Terminating ReplicationController externalsvc pods took: 100.471379ms
  E0111 12:58:12.658871      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:13.659693      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:58:14.116: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-5352" for this suite. @ 01/11/24 12:58:14.152
• [8.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 01/11/24 12:58:14.172
  Jan 11 12:58:14.172: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/11/24 12:58:14.174
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:58:14.219
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:58:14.226
  Jan 11 12:58:14.232: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  E0111 12:58:14.659930      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:15.660881      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:16.661647      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:17.661690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:18.662389      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:19.663311      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:20.664407      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:21.665501      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:22.665636      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:23.666619      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 01/11/24 12:58:24.2
  Jan 11 12:58:24.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-2209 --namespace=crd-publish-openapi-2209 create -f -'
  E0111 12:58:24.667598      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:25.668101      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:26.668321      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:58:27.389: INFO: stderr: ""
  Jan 11 12:58:27.389: INFO: stdout: "e2e-test-crd-publish-openapi-3665-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jan 11 12:58:27.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-2209 --namespace=crd-publish-openapi-2209 delete e2e-test-crd-publish-openapi-3665-crds test-cr'
  Jan 11 12:58:27.569: INFO: stderr: ""
  Jan 11 12:58:27.569: INFO: stdout: "e2e-test-crd-publish-openapi-3665-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Jan 11 12:58:27.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-2209 --namespace=crd-publish-openapi-2209 apply -f -'
  E0111 12:58:27.669389      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:28.669623      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:29.670175      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:58:30.160: INFO: stderr: ""
  Jan 11 12:58:30.160: INFO: stdout: "e2e-test-crd-publish-openapi-3665-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jan 11 12:58:30.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-2209 --namespace=crd-publish-openapi-2209 delete e2e-test-crd-publish-openapi-3665-crds test-cr'
  Jan 11 12:58:30.377: INFO: stderr: ""
  Jan 11 12:58:30.377: INFO: stdout: "e2e-test-crd-publish-openapi-3665-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 01/11/24 12:58:30.377
  Jan 11 12:58:30.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=crd-publish-openapi-2209 explain e2e-test-crd-publish-openapi-3665-crds'
  E0111 12:58:30.670667      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:31.670970      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:32.671066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:58:32.822: INFO: stderr: ""
  Jan 11 12:58:32.822: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-3665-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0111 12:58:33.672001      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:34.672296      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:35.673259      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:36.674038      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 12:58:37.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2209" for this suite. @ 01/11/24 12:58:37.481
• [23.324 seconds]
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 01/11/24 12:58:37.496
  Jan 11 12:58:37.496: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename var-expansion @ 01/11/24 12:58:37.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 12:58:37.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 12:58:37.548
  STEP: creating the pod with failed condition @ 01/11/24 12:58:37.555
  E0111 12:58:37.674718      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:38.675177      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:39.675240      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:40.675788      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:41.676246      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:42.676798      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:43.677891      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:44.680698      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:45.681428      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:46.682211      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:47.683132      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:48.683045      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:49.684008      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:50.684046      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:51.685163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:52.685252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:53.686476      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:54.686753      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:55.687724      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:56.687752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:57.688256      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:58.688776      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:58:59.689325      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:00.690132      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:01.690552      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:02.690915      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:03.691552      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:04.692128      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:05.692811      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:06.693531      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:07.694256      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:08.694485      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:09.694912      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:10.695473      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:11.695592      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:12.695929      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:13.696297      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:14.697225      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:15.697894      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:16.698272      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:17.698524      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:18.699035      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:19.699558      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:20.700087      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:21.700056      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:22.701198      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:23.701889      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:24.702222      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:25.702537      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:26.703287      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:27.703726      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:28.704038      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:29.704736      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:30.705041      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:31.705326      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:32.705602      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:33.706001      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:34.706435      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:35.707059      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:36.707723      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:37.708320      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:38.708768      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:39.709160      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:40.709849      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:41.710002      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:42.710037      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:43.710303      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:44.710502      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:45.711022      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:46.711888      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:47.712186      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:48.712658      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:49.713224      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:50.714166      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:51.715142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:52.715346      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:53.715839      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:54.716176      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:55.716377      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:56.716694      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:57.716989      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:58.717102      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 12:59:59.717475      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:00.717859      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:01.718841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:02.719848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:03.720353      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:04.721457      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:05.721873      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:06.721952      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:07.722377      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:08.722803      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:09.723427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:10.724443      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:11.724491      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:12.724877      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:13.725129      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:14.725702      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:15.726157      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:16.726779      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:17.727909      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:18.728123      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:19.728406      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:20.729048      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:21.729259      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:22.730151      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:23.730742      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:24.731597      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:25.731971      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:26.732148      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:27.732499      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:28.732718      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:29.732892      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:30.733272      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:31.733565      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:32.733881      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:33.733969      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:34.734903      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:35.735095      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:36.736397      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 01/11/24 13:00:37.581
  E0111 13:00:37.736419      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:00:38.104: INFO: Successfully updated pod "var-expansion-83518d04-1e89-41b2-86ba-ea6b9e3fa635"
  STEP: waiting for pod running @ 01/11/24 13:00:38.104
  E0111 13:00:38.737194      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:39.737523      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 01/11/24 13:00:40.122
  Jan 11 13:00:40.122: INFO: Deleting pod "var-expansion-83518d04-1e89-41b2-86ba-ea6b9e3fa635" in namespace "var-expansion-2033"
  Jan 11 13:00:40.138: INFO: Wait up to 5m0s for pod "var-expansion-83518d04-1e89-41b2-86ba-ea6b9e3fa635" to be fully deleted
  E0111 13:00:40.737843      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:41.738186      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:42.738456      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:43.738458      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:44.738677      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:45.738823      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:46.738912      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:47.739739      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:48.740022      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:49.740387      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:50.741258      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:51.741580      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:52.742555      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:53.743150      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:54.743518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:55.743902      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:56.744262      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:57.744772      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:58.745261      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:00:59.745685      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:00.745898      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:01.746291      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:02.746458      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:03.746843      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:04.747755      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:05.748652      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:06.748516      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:07.748848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:08.749194      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:09.749578      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:10.750059      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:11.750244      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:01:12.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2033" for this suite. @ 01/11/24 13:01:12.362
• [154.876 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 01/11/24 13:01:12.374
  Jan 11 13:01:12.374: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 13:01:12.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:01:12.407
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:01:12.413
  STEP: Creating configMap with name configmap-projected-all-test-volume-0e4bb36c-4e2f-4173-b0dd-8420cb4d1337 @ 01/11/24 13:01:12.42
  STEP: Creating secret with name secret-projected-all-test-volume-baee4aad-2d67-4ab1-8b96-22d5c442b6cf @ 01/11/24 13:01:12.429
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 01/11/24 13:01:12.443
  E0111 13:01:12.750990      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:13.751340      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:14.751386      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:15.752243      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/11/24 13:01:16.497
  Jan 11 13:01:16.504: INFO: Trying to get logs from node env1-test-worker-1 pod projected-volume-08955408-a5eb-45d7-890f-4c35c9bbc481 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 01/11/24 13:01:16.545
  Jan 11 13:01:16.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4623" for this suite. @ 01/11/24 13:01:16.611
• [4.250 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 01/11/24 13:01:16.625
  Jan 11 13:01:16.625: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 13:01:16.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:01:16.667
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:01:16.674
  STEP: Creating secret with name s-test-opt-del-9752f557-e17a-4e9b-8229-377617c96a75 @ 01/11/24 13:01:16.7
  STEP: Creating secret with name s-test-opt-upd-7926e4b3-ab91-4c2c-a2c1-533e75f504b4 @ 01/11/24 13:01:16.714
  STEP: Creating the pod @ 01/11/24 13:01:16.724
  E0111 13:01:16.752830      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:17.753279      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:18.754231      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-9752f557-e17a-4e9b-8229-377617c96a75 @ 01/11/24 13:01:18.83
  STEP: Updating secret s-test-opt-upd-7926e4b3-ab91-4c2c-a2c1-533e75f504b4 @ 01/11/24 13:01:18.846
  STEP: Creating secret with name s-test-opt-create-05bb0758-dc1f-45d6-a294-16f59f0222ec @ 01/11/24 13:01:18.859
  STEP: waiting to observe update in volume @ 01/11/24 13:01:18.873
  E0111 13:01:19.754662      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:20.755167      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:21.757023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:22.757399      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:23.758026      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:24.758160      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:25.758747      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:26.759321      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:27.760097      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:28.761046      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:29.761323      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:30.761621      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:31.762638      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:32.763195      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:33.763783      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:34.764586      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:35.765380      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:36.765664      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:37.766196      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:38.766635      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:39.767067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:40.767579      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:41.768191      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:42.768192      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:43.769119      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:44.769587      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:45.769916      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:46.770953      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:47.771159      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:48.771655      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:49.771913      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:50.772188      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:51.772839      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:52.773160      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:53.773210      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:54.773873      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:55.774886      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:56.775400      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:57.775545      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:58.775938      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:01:59.776643      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:00.777223      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:01.777610      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:02.778387      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:03.778799      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:04.779117      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:05.779985      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:06.780823      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:07.781490      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:08.782203      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:09.782519      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:10.783159      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:11.783411      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:12.783797      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:13.784114      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:14.784696      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:15.784804      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:16.785665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:17.786271      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:18.786756      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:19.787441      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:20.787608      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:21.788810      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:22.789597      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:23.790142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:24.790461      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:25.791402      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:26.791834      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:27.792668      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:28.792866      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:29.793872      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:30.794130      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:31.794659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:32.795125      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:33.795237      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:34.795573      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:35.796441      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:36.797285      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:37.797265      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:38.797588      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:39.798359      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:40.799396      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:41.799778      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:42.799899      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:43.800401      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:44.800634      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:45.801095      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:46.801391      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:47.801850      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:48.802075      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:49.802613      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:02:50.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5983" for this suite. @ 01/11/24 13:02:50.024
• [93.418 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 01/11/24 13:02:50.045
  Jan 11 13:02:50.045: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename downward-api @ 01/11/24 13:02:50.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:02:50.106
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:02:50.114
  STEP: Creating a pod to test downward api env vars @ 01/11/24 13:02:50.12
  E0111 13:02:50.803242      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:51.803787      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:52.804131      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:53.804801      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/11/24 13:02:54.181
  Jan 11 13:02:54.189: INFO: Trying to get logs from node env1-test-worker-2 pod downward-api-7d5f3147-eb72-4dfd-8c19-180a9b94691e container dapi-container: <nil>
  STEP: delete the pod @ 01/11/24 13:02:54.241
  Jan 11 13:02:54.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4905" for this suite. @ 01/11/24 13:02:54.299
• [4.279 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 01/11/24 13:02:54.324
  Jan 11 13:02:54.324: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename resourcequota @ 01/11/24 13:02:54.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:02:54.402
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:02:54.41
  STEP: Counting existing ResourceQuota @ 01/11/24 13:02:54.416
  E0111 13:02:54.805046      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:55.805070      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:56.806065      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:57.807014      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:02:58.808028      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 01/11/24 13:02:59.43
  STEP: Ensuring resource quota status is calculated @ 01/11/24 13:02:59.446
  E0111 13:02:59.808882      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:00.809197      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 01/11/24 13:03:01.458
  STEP: Ensuring resource quota status captures replicaset creation @ 01/11/24 13:03:01.513
  E0111 13:03:01.809458      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:02.809690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 01/11/24 13:03:03.527
  STEP: Ensuring resource quota status released usage @ 01/11/24 13:03:03.541
  E0111 13:03:03.810718      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:04.811184      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:03:05.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4375" for this suite. @ 01/11/24 13:03:05.562
• [11.261 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 01/11/24 13:03:05.586
  Jan 11 13:03:05.586: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename configmap @ 01/11/24 13:03:05.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:03:05.628
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:03:05.634
  STEP: Creating configMap with name configmap-test-volume-5028c975-27f7-4b77-b69b-a7c9906e426b @ 01/11/24 13:03:05.641
  STEP: Creating a pod to test consume configMaps @ 01/11/24 13:03:05.658
  E0111 13:03:05.811495      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:06.812449      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:07.812758      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:08.813231      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/11/24 13:03:09.734
  Jan 11 13:03:09.743: INFO: Trying to get logs from node env1-test-worker-1 pod pod-configmaps-61e3ba50-7f90-45a3-9fbc-0e82130d98d8 container configmap-volume-test: <nil>
  STEP: delete the pod @ 01/11/24 13:03:09.783
  E0111 13:03:09.814274      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:03:09.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-935" for this suite. @ 01/11/24 13:03:09.838
• [4.272 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 01/11/24 13:03:09.859
  Jan 11 13:03:09.859: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename container-probe @ 01/11/24 13:03:09.86
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:03:09.894
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:03:09.901
  STEP: Creating pod test-grpc-20b32e12-cdbe-420c-b1db-eb2b2039e448 in namespace container-probe-6579 @ 01/11/24 13:03:09.909
  E0111 13:03:10.815248      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:11.815871      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:03:12.039: INFO: Started pod test-grpc-20b32e12-cdbe-420c-b1db-eb2b2039e448 in namespace container-probe-6579
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/11/24 13:03:12.039
  Jan 11 13:03:12.053: INFO: Initial restart count of pod test-grpc-20b32e12-cdbe-420c-b1db-eb2b2039e448 is 0
  E0111 13:03:12.815827      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:13.816290      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:14.817102      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:15.817716      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:16.818475      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:17.818772      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:18.819644      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:19.820007      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:20.819999      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:21.820201      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:22.820433      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:23.820498      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:24.821370      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:25.821589      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:26.821690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:27.822194      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:28.823264      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:29.823766      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:30.823863      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:31.824746      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:32.825834      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:33.826305      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:34.827223      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:35.827836      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:36.828978      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:37.829526      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:38.830310      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:39.830711      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:40.831630      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:41.832114      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:42.833051      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:43.833633      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:44.833690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:45.834352      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:46.835401      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:47.835520      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:48.836717      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:49.837352      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:50.837776      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:51.838105      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:52.838913      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:53.839857      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:54.840187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:55.840687      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:56.840932      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:57.841407      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:58.841547      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:03:59.841822      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:00.842842      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:01.843160      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:02.843682      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:03.843951      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:04.844487      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:05.845079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:06.845281      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:07.845999      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:08.846261      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:09.846907      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:10.847559      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:11.847753      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:12.848222      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:13.848872      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:14.848805      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:15.849570      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:04:16.454: INFO: Restart count of pod container-probe-6579/test-grpc-20b32e12-cdbe-420c-b1db-eb2b2039e448 is now 1 (1m4.400919895s elapsed)
  Jan 11 13:04:16.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/11/24 13:04:16.481
  STEP: Destroying namespace "container-probe-6579" for this suite. @ 01/11/24 13:04:16.518
• [66.683 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 01/11/24 13:04:16.546
  Jan 11 13:04:16.546: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename emptydir @ 01/11/24 13:04:16.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:04:16.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:04:16.605
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 01/11/24 13:04:16.613
  E0111 13:04:16.850022      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:17.850877      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:18.851518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:19.851749      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/11/24 13:04:20.686
  Jan 11 13:04:20.693: INFO: Trying to get logs from node env1-test-worker-1 pod pod-37055fa2-269b-4df0-b4d1-8f58e242fe12 container test-container: <nil>
  STEP: delete the pod @ 01/11/24 13:04:20.711
  Jan 11 13:04:20.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1974" for this suite. @ 01/11/24 13:04:20.766
• [4.240 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 01/11/24 13:04:20.787
  Jan 11 13:04:20.787: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename emptydir @ 01/11/24 13:04:20.789
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:04:20.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:04:20.831
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 01/11/24 13:04:20.837
  E0111 13:04:20.851965      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:21.852790      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:22.853543      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:23.853702      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:24.853846      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/11/24 13:04:24.888
  Jan 11 13:04:24.894: INFO: Trying to get logs from node env1-test-worker-1 pod pod-4c516193-c7f3-4ea8-94a6-ec7a492127e1 container test-container: <nil>
  STEP: delete the pod @ 01/11/24 13:04:24.91
  Jan 11 13:04:24.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6276" for this suite. @ 01/11/24 13:04:24.988
• [4.222 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 01/11/24 13:04:25.012
  Jan 11 13:04:25.012: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename statefulset @ 01/11/24 13:04:25.014
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:04:25.054
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:04:25.061
  STEP: Creating service test in namespace statefulset-4394 @ 01/11/24 13:04:25.067
  STEP: Creating a new StatefulSet @ 01/11/24 13:04:25.082
  Jan 11 13:04:25.113: INFO: Found 0 stateful pods, waiting for 3
  E0111 13:04:25.854547      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:26.855440      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:27.855967      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:28.856603      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:29.856786      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:30.857719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:31.858007      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:32.858206      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:33.858380      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:34.858648      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:04:35.124: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 11 13:04:35.124: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jan 11 13:04:35.124: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Jan 11 13:04:35.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-4394 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 11 13:04:35.487: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 11 13:04:35.487: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 11 13:04:35.487: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0111 13:04:35.858871      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:36.859401      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:37.860321      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:38.860896      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:39.861488      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:40.862104      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:41.862583      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:42.863164      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:43.863908      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:44.864349      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 01/11/24 13:04:45.544
  Jan 11 13:04:45.601: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 01/11/24 13:04:45.601
  E0111 13:04:45.864881      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:46.865079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:47.865705      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:48.865928      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:49.866604      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:50.867213      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:51.867843      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:52.868188      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:53.868260      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:54.868495      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 01/11/24 13:04:55.671
  Jan 11 13:04:55.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-4394 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0111 13:04:55.868904      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:04:55.997: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 11 13:04:55.997: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 11 13:04:55.997: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0111 13:04:56.869398      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:57.869557      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:58.869700      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:04:59.870317      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:00.870491      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:01.870611      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:02.871043      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:03.871216      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:04.871635      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:05.872333      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 01/11/24 13:05:06.045
  Jan 11 13:05:06.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-4394 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 11 13:05:06.384: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 11 13:05:06.384: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 11 13:05:06.384: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0111 13:05:06.873557      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:07.873851      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:08.874449      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:09.874645      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:10.874944      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:11.875266      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:12.875427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:13.875661      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:14.876040      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:15.876192      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:05:16.464: INFO: Updating stateful set ss2
  E0111 13:05:16.877272      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:17.877941      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:18.878466      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:19.878867      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:20.879318      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:21.879588      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:22.879866      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:23.880147      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:24.880239      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:25.881179      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 01/11/24 13:05:26.535
  Jan 11 13:05:26.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=statefulset-4394 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0111 13:05:26.881651      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:05:26.905: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 11 13:05:26.905: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 11 13:05:26.905: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0111 13:05:27.882577      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:28.882798      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:29.883533      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:30.884596      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:31.885104      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:32.885559      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:33.886053      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:34.886556      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:35.887435      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:36.887980      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:05:36.970: INFO: Deleting all statefulset in ns statefulset-4394
  Jan 11 13:05:36.981: INFO: Scaling statefulset ss2 to 0
  E0111 13:05:37.889156      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:38.889964      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:39.890158      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:40.890690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:41.890984      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:42.891500      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:43.891661      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:44.891849      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:45.892192      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:46.892474      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:05:47.030: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 11 13:05:47.038: INFO: Deleting statefulset ss2
  Jan 11 13:05:47.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4394" for this suite. @ 01/11/24 13:05:47.096
• [82.113 seconds]
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:289
  STEP: Creating a kubernetes client @ 01/11/24 13:05:47.127
  Jan 11 13:05:47.127: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename field-validation @ 01/11/24 13:05:47.129
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:05:47.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:05:47.186
  Jan 11 13:05:47.193: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  E0111 13:05:47.892892      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:48.892907      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:49.893480      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:50.894303      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:51.894815      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:52.894894      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:53.895145      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:54.895803      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:05:55.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6850" for this suite. @ 01/11/24 13:05:55.723
• [8.704 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 01/11/24 13:05:55.843
  Jan 11 13:05:55.844: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 13:05:55.848
  E0111 13:05:55.895939      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:05:56.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:05:56.021
  STEP: Creating a pod to test downward API volume plugin @ 01/11/24 13:05:56.037
  E0111 13:05:56.896337      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:57.896342      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:58.896862      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:05:59.896984      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/11/24 13:06:00.09
  Jan 11 13:06:00.105: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-752f924f-db8b-4ce0-acc2-9bbf7224a124 container client-container: <nil>
  STEP: delete the pod @ 01/11/24 13:06:00.151
  Jan 11 13:06:00.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8347" for this suite. @ 01/11/24 13:06:00.217
• [4.392 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 01/11/24 13:06:00.237
  Jan 11 13:06:00.237: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename sched-preemption @ 01/11/24 13:06:00.238
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:06:00.3
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:06:00.307
  Jan 11 13:06:00.371: INFO: Waiting up to 1m0s for all nodes to be ready
  E0111 13:06:00.897328      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:01.897869      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:02.897996      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:03.898883      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:04.899687      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:05.900632      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:06.901062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:07.902037      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:08.903087      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:09.903585      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:10.904144      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:11.904380      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:12.904641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:13.905721      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:14.906553      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:15.907546      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:16.907526      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:17.908213      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:18.909515      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:19.909917      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:20.910333      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:21.911066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:22.911247      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:23.911486      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:24.912287      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:25.913360      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:26.913776      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:27.913893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:28.914017      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:29.914206      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:30.915381      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:31.915459      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:32.915721      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:33.916252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:34.916662      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:35.917303      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:36.917651      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:37.917945      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:38.918829      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:39.919382      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:40.919953      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:41.920657      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:42.921544      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:43.922088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:44.922148      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:45.923308      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:46.923773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:47.924192      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:48.924832      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:49.924878      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:50.926058      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:51.926562      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:52.926889      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:53.927455      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:54.927701      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:55.928142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:56.928310      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:57.928637      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:58.928787      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:06:59.929259      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:07:00.494: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 01/11/24 13:07:00.506
  Jan 11 13:07:00.594: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jan 11 13:07:00.607: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jan 11 13:07:00.704: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jan 11 13:07:00.724: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jan 11 13:07:00.802: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jan 11 13:07:00.815: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 01/11/24 13:07:00.815
  E0111 13:07:00.930548      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:01.930756      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 01/11/24 13:07:02.896
  E0111 13:07:02.931503      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:03.932089      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:04.932690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:05.933591      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:06.933691      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:07:07.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-9684" for this suite. @ 01/11/24 13:07:07.202
• [66.990 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 01/11/24 13:07:07.23
  Jan 11 13:07:07.230: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename downward-api @ 01/11/24 13:07:07.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:07:07.304
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:07:07.314
  STEP: Creating a pod to test downward API volume plugin @ 01/11/24 13:07:07.323
  E0111 13:07:07.934099      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:08.934665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:09.935012      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:10.935634      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/11/24 13:07:11.4
  Jan 11 13:07:11.409: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-1a5ddbdf-1651-4556-bdce-a952476e576e container client-container: <nil>
  STEP: delete the pod @ 01/11/24 13:07:11.431
  Jan 11 13:07:11.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4049" for this suite. @ 01/11/24 13:07:11.481
• [4.272 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 01/11/24 13:07:11.516
  Jan 11 13:07:11.516: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename services @ 01/11/24 13:07:11.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:07:11.557
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:07:11.565
  STEP: creating service endpoint-test2 in namespace services-2632 @ 01/11/24 13:07:11.576
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2632 to expose endpoints map[] @ 01/11/24 13:07:11.598
  Jan 11 13:07:11.628: INFO: successfully validated that service endpoint-test2 in namespace services-2632 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-2632 @ 01/11/24 13:07:11.629
  E0111 13:07:11.936471      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:12.937327      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2632 to expose endpoints map[pod1:[80]] @ 01/11/24 13:07:13.691
  Jan 11 13:07:13.726: INFO: successfully validated that service endpoint-test2 in namespace services-2632 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 01/11/24 13:07:13.726
  Jan 11 13:07:13.726: INFO: Creating new exec pod
  E0111 13:07:13.937705      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:14.938281      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:15.939427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:07:16.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2632 exec execpod2d6ns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0111 13:07:16.939966      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:07:17.143: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jan 11 13:07:17.144: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 11 13:07:17.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2632 exec execpod2d6ns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.11.3 80'
  Jan 11 13:07:17.480: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.11.3 80\nConnection to 10.233.11.3 80 port [tcp/http] succeeded!\n"
  Jan 11 13:07:17.480: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-2632 @ 01/11/24 13:07:17.48
  E0111 13:07:17.940201      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:18.940905      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2632 to expose endpoints map[pod1:[80] pod2:[80]] @ 01/11/24 13:07:19.534
  Jan 11 13:07:19.575: INFO: successfully validated that service endpoint-test2 in namespace services-2632 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 01/11/24 13:07:19.575
  E0111 13:07:19.941183      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:07:20.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2632 exec execpod2d6ns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0111 13:07:20.941974      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:21.942515      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:07:22.927: INFO: rc: 1
  Jan 11 13:07:22.928: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2632 exec execpod2d6ns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 endpoint-test2 80
  nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  E0111 13:07:22.942625      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:07:23.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2632 exec execpod2d6ns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0111 13:07:23.943168      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:07:24.265: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jan 11 13:07:24.265: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 11 13:07:24.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2632 exec execpod2d6ns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.11.3 80'
  E0111 13:07:24.944184      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:25.945224      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:07:26.621: INFO: rc: 1
  Jan 11 13:07:26.621: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2632 exec execpod2d6ns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.11.3 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 10.233.11.3 80
  nc: connect to 10.233.11.3 port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  E0111 13:07:26.945630      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:07:27.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2632 exec execpod2d6ns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.11.3 80'
  E0111 13:07:27.945657      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:07:27.950: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.11.3 80\nConnection to 10.233.11.3 80 port [tcp/http] succeeded!\n"
  Jan 11 13:07:27.950: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-2632 @ 01/11/24 13:07:27.95
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2632 to expose endpoints map[pod2:[80]] @ 01/11/24 13:07:27.998
  Jan 11 13:07:28.059: INFO: successfully validated that service endpoint-test2 in namespace services-2632 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 01/11/24 13:07:28.059
  E0111 13:07:28.946424      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:07:29.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2632 exec execpod2d6ns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jan 11 13:07:29.371: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jan 11 13:07:29.371: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 11 13:07:29.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2632 exec execpod2d6ns -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.11.3 80'
  Jan 11 13:07:29.741: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.11.3 80\nConnection to 10.233.11.3 80 port [tcp/http] succeeded!\n"
  Jan 11 13:07:29.741: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-2632 @ 01/11/24 13:07:29.741
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2632 to expose endpoints map[] @ 01/11/24 13:07:29.793
  Jan 11 13:07:29.830: INFO: successfully validated that service endpoint-test2 in namespace services-2632 exposes endpoints map[]
  Jan 11 13:07:29.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0111 13:07:29.947154      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-2632" for this suite. @ 01/11/24 13:07:29.969
• [18.490 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 01/11/24 13:07:30.008
  Jan 11 13:07:30.008: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename endpointslice @ 01/11/24 13:07:30.01
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:07:30.059
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:07:30.067
  Jan 11 13:07:30.112: INFO: Endpoints addresses: [10.61.1.197 10.61.1.198 10.61.1.199] , ports: [6443]
  Jan 11 13:07:30.112: INFO: EndpointSlices addresses: [10.61.1.197 10.61.1.198 10.61.1.199] , ports: [6443]
  Jan 11 13:07:30.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7096" for this suite. @ 01/11/24 13:07:30.126
• [0.136 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 01/11/24 13:07:30.146
  Jan 11 13:07:30.146: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename svcaccounts @ 01/11/24 13:07:30.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:07:30.2
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:07:30.207
  STEP: Creating a pod to test service account token:  @ 01/11/24 13:07:30.216
  E0111 13:07:30.947539      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:31.948750      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:32.949893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:33.950405      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/11/24 13:07:34.297
  Jan 11 13:07:34.306: INFO: Trying to get logs from node env1-test-worker-1 pod test-pod-066a80d7-18e0-4183-9c7a-bf5c65ec5a66 container agnhost-container: <nil>
  STEP: delete the pod @ 01/11/24 13:07:34.32
  Jan 11 13:07:34.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6885" for this suite. @ 01/11/24 13:07:34.382
• [4.251 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 01/11/24 13:07:34.407
  Jan 11 13:07:34.407: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename pods @ 01/11/24 13:07:34.409
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:07:34.446
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:07:34.453
  STEP: creating the pod @ 01/11/24 13:07:34.459
  STEP: submitting the pod to kubernetes @ 01/11/24 13:07:34.459
  E0111 13:07:34.950555      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:35.950998      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 01/11/24 13:07:36.508
  STEP: updating the pod @ 01/11/24 13:07:36.524
  E0111 13:07:36.951991      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:07:37.057: INFO: Successfully updated pod "pod-update-4686368e-f77f-4a0c-ae7a-8d25e9e9b32b"
  STEP: verifying the updated pod is in kubernetes @ 01/11/24 13:07:37.068
  Jan 11 13:07:37.080: INFO: Pod update OK
  Jan 11 13:07:37.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-295" for this suite. @ 01/11/24 13:07:37.092
• [2.704 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 01/11/24 13:07:37.117
  Jan 11 13:07:37.117: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename endpointslice @ 01/11/24 13:07:37.119
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:07:37.163
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:07:37.171
  STEP: getting /apis @ 01/11/24 13:07:37.178
  STEP: getting /apis/discovery.k8s.io @ 01/11/24 13:07:37.193
  STEP: getting /apis/discovery.k8s.iov1 @ 01/11/24 13:07:37.195
  STEP: creating @ 01/11/24 13:07:37.199
  STEP: getting @ 01/11/24 13:07:37.242
  STEP: listing @ 01/11/24 13:07:37.25
  STEP: watching @ 01/11/24 13:07:37.264
  Jan 11 13:07:37.264: INFO: starting watch
  STEP: cluster-wide listing @ 01/11/24 13:07:37.268
  STEP: cluster-wide watching @ 01/11/24 13:07:37.285
  Jan 11 13:07:37.285: INFO: starting watch
  STEP: patching @ 01/11/24 13:07:37.289
  STEP: updating @ 01/11/24 13:07:37.306
  Jan 11 13:07:37.351: INFO: waiting for watch events with expected annotations
  Jan 11 13:07:37.351: INFO: saw patched and updated annotations
  STEP: deleting @ 01/11/24 13:07:37.351
  STEP: deleting a collection @ 01/11/24 13:07:37.411
  Jan 11 13:07:37.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5766" for this suite. @ 01/11/24 13:07:37.482
• [0.387 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 01/11/24 13:07:37.511
  Jan 11 13:07:37.511: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename security-context-test @ 01/11/24 13:07:37.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:07:37.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:07:37.599
  E0111 13:07:37.952358      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:38.953056      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:39.953802      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:40.954455      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:07:41.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8318" for this suite. @ 01/11/24 13:07:41.69
• [4.197 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 01/11/24 13:07:41.71
  Jan 11 13:07:41.710: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename webhook @ 01/11/24 13:07:41.712
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:07:41.764
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:07:41.773
  STEP: Setting up server cert @ 01/11/24 13:07:41.835
  E0111 13:07:41.954773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/11/24 13:07:42.24
  STEP: Deploying the webhook pod @ 01/11/24 13:07:42.259
  STEP: Wait for the deployment to be ready @ 01/11/24 13:07:42.291
  Jan 11 13:07:42.313: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0111 13:07:42.955895      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:43.955940      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:07:44.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 11, 13, 7, 42, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 13, 7, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 13, 7, 42, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 13, 7, 42, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0111 13:07:44.955834      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:45.956337      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/11/24 13:07:46.366
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 13:07:46.397
  E0111 13:07:46.957997      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:07:47.398: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 01/11/24 13:07:47.407
  STEP: create a pod @ 01/11/24 13:07:47.449
  E0111 13:07:47.957758      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:48.958669      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 01/11/24 13:07:49.505
  Jan 11 13:07:49.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=webhook-6832 attach --namespace=webhook-6832 to-be-attached-pod -i -c=container1'
  Jan 11 13:07:49.748: INFO: rc: 1
  Jan 11 13:07:49.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0111 13:07:49.959804      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-6832" for this suite. @ 01/11/24 13:07:50.066
  STEP: Destroying namespace "webhook-markers-4086" for this suite. @ 01/11/24 13:07:50.094
• [8.401 seconds]
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 01/11/24 13:07:50.111
  Jan 11 13:07:50.112: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename secrets @ 01/11/24 13:07:50.114
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:07:50.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:07:50.175
  STEP: Creating secret with name secret-test-map-1cd4b3db-b3e5-44cb-8381-9e5b37b00f80 @ 01/11/24 13:07:50.182
  STEP: Creating a pod to test consume secrets @ 01/11/24 13:07:50.193
  E0111 13:07:50.960175      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:51.960170      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:52.960519      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:53.961366      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/11/24 13:07:54.243
  Jan 11 13:07:54.258: INFO: Trying to get logs from node env1-test-worker-2 pod pod-secrets-b3596672-d110-4c4a-98aa-140f9b1b0324 container secret-volume-test: <nil>
  STEP: delete the pod @ 01/11/24 13:07:54.319
  Jan 11 13:07:54.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8476" for this suite. @ 01/11/24 13:07:54.451
• [4.369 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 01/11/24 13:07:54.495
  Jan 11 13:07:54.495: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename limitrange @ 01/11/24 13:07:54.496
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:07:54.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:07:54.55
  STEP: Creating a LimitRange @ 01/11/24 13:07:54.558
  STEP: Setting up watch @ 01/11/24 13:07:54.558
  STEP: Submitting a LimitRange @ 01/11/24 13:07:54.67
  STEP: Verifying LimitRange creation was observed @ 01/11/24 13:07:54.697
  STEP: Fetching the LimitRange to ensure it has proper values @ 01/11/24 13:07:54.697
  Jan 11 13:07:54.704: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jan 11 13:07:54.705: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 01/11/24 13:07:54.705
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 01/11/24 13:07:54.72
  Jan 11 13:07:54.743: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jan 11 13:07:54.743: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 01/11/24 13:07:54.743
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 01/11/24 13:07:54.777
  Jan 11 13:07:54.789: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Jan 11 13:07:54.790: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 01/11/24 13:07:54.79
  STEP: Failing to create a Pod with more than max resources @ 01/11/24 13:07:54.799
  STEP: Updating a LimitRange @ 01/11/24 13:07:54.812
  STEP: Verifying LimitRange updating is effective @ 01/11/24 13:07:54.83
  E0111 13:07:54.962409      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:55.963014      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 01/11/24 13:07:56.841
  STEP: Failing to create a Pod with more than max resources @ 01/11/24 13:07:56.855
  STEP: Deleting a LimitRange @ 01/11/24 13:07:56.861
  STEP: Verifying the LimitRange was deleted @ 01/11/24 13:07:56.89
  E0111 13:07:56.963185      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:57.963859      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:58.963987      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:07:59.964629      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:00.965215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:08:01.913: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 01/11/24 13:08:01.913
  Jan 11 13:08:01.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0111 13:08:01.965778      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "limitrange-8012" for this suite. @ 01/11/24 13:08:02.012
• [7.535 seconds]
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 01/11/24 13:08:02.031
  Jan 11 13:08:02.031: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 13:08:02.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:08:02.081
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:08:02.087
  STEP: Creating projection with secret that has name projected-secret-test-ec39bf08-245c-409c-893b-1850f013f0a3 @ 01/11/24 13:08:02.092
  STEP: Creating a pod to test consume secrets @ 01/11/24 13:08:02.106
  E0111 13:08:02.966573      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:03.967442      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:04.968080      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:05.968471      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/11/24 13:08:06.16
  Jan 11 13:08:06.168: INFO: Trying to get logs from node env1-test-worker-2 pod pod-projected-secrets-7e68070e-b3da-4025-b54b-c85979a24529 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 01/11/24 13:08:06.187
  Jan 11 13:08:06.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8101" for this suite. @ 01/11/24 13:08:06.255
• [4.248 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 01/11/24 13:08:06.281
  Jan 11 13:08:06.281: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename projected @ 01/11/24 13:08:06.282
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:08:06.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:08:06.338
  STEP: Creating a pod to test downward API volume plugin @ 01/11/24 13:08:06.343
  E0111 13:08:06.969150      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:07.969450      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:08.969535      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:09.970255      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/11/24 13:08:10.411
  Jan 11 13:08:10.428: INFO: Trying to get logs from node env1-test-worker-2 pod downwardapi-volume-6705ce0b-443c-4dcc-9582-bfba13ea868a container client-container: <nil>
  STEP: delete the pod @ 01/11/24 13:08:10.453
  Jan 11 13:08:10.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-135" for this suite. @ 01/11/24 13:08:10.512
• [4.251 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 01/11/24 13:08:10.538
  Jan 11 13:08:10.538: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename ingressclass @ 01/11/24 13:08:10.542
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:08:10.604
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:08:10.61
  STEP: getting /apis @ 01/11/24 13:08:10.615
  STEP: getting /apis/networking.k8s.io @ 01/11/24 13:08:10.628
  STEP: getting /apis/networking.k8s.iov1 @ 01/11/24 13:08:10.634
  STEP: creating @ 01/11/24 13:08:10.636
  STEP: getting @ 01/11/24 13:08:10.669
  STEP: listing @ 01/11/24 13:08:10.678
  STEP: watching @ 01/11/24 13:08:10.687
  Jan 11 13:08:10.687: INFO: starting watch
  STEP: patching @ 01/11/24 13:08:10.689
  STEP: updating @ 01/11/24 13:08:10.704
  Jan 11 13:08:10.720: INFO: waiting for watch events with expected annotations
  Jan 11 13:08:10.720: INFO: saw patched and updated annotations
  STEP: deleting @ 01/11/24 13:08:10.721
  STEP: deleting a collection @ 01/11/24 13:08:10.756
  Jan 11 13:08:10.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-1169" for this suite. @ 01/11/24 13:08:10.821
• [0.301 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 01/11/24 13:08:10.841
  Jan 11 13:08:10.841: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename watch @ 01/11/24 13:08:10.843
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:08:10.873
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:08:10.882
  STEP: creating a watch on configmaps with a certain label @ 01/11/24 13:08:10.891
  STEP: creating a new configmap @ 01/11/24 13:08:10.895
  STEP: modifying the configmap once @ 01/11/24 13:08:10.937
  STEP: changing the label value of the configmap @ 01/11/24 13:08:10.964
  E0111 13:08:10.970614      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Expecting to observe a delete notification for the watched object @ 01/11/24 13:08:10.985
  Jan 11 13:08:10.985: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7642  0dfc3185-3721-4382-a175-f6e48ff845b7 187197707 0 2024-01-11 13:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-11 13:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 11 13:08:10.986: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7642  0dfc3185-3721-4382-a175-f6e48ff845b7 187197708 0 2024-01-11 13:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-11 13:08:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 11 13:08:10.986: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7642  0dfc3185-3721-4382-a175-f6e48ff845b7 187197709 0 2024-01-11 13:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-11 13:08:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 01/11/24 13:08:10.986
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 01/11/24 13:08:11.015
  E0111 13:08:11.971762      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:12.972286      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:13.972793      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:14.973398      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:15.974498      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:16.974808      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:17.975457      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:18.975973      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:19.976325      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:20.976637      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 01/11/24 13:08:21.016
  STEP: modifying the configmap a third time @ 01/11/24 13:08:21.054
  STEP: deleting the configmap @ 01/11/24 13:08:21.08
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 01/11/24 13:08:21.097
  Jan 11 13:08:21.097: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7642  0dfc3185-3721-4382-a175-f6e48ff845b7 187197775 0 2024-01-11 13:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-11 13:08:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 11 13:08:21.098: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7642  0dfc3185-3721-4382-a175-f6e48ff845b7 187197776 0 2024-01-11 13:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-11 13:08:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 11 13:08:21.098: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7642  0dfc3185-3721-4382-a175-f6e48ff845b7 187197777 0 2024-01-11 13:08:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-11 13:08:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 11 13:08:21.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7642" for this suite. @ 01/11/24 13:08:21.116
• [10.303 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 01/11/24 13:08:21.145
  Jan 11 13:08:21.145: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename proxy @ 01/11/24 13:08:21.147
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:08:21.195
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:08:21.204
  Jan 11 13:08:21.213: INFO: Creating pod...
  E0111 13:08:21.977084      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:22.977295      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:08:23.271: INFO: Creating service...
  Jan 11 13:08:23.298: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2954/pods/agnhost/proxy?method=DELETE
  Jan 11 13:08:23.317: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jan 11 13:08:23.317: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2954/pods/agnhost/proxy?method=OPTIONS
  Jan 11 13:08:23.342: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jan 11 13:08:23.342: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2954/pods/agnhost/proxy?method=PATCH
  Jan 11 13:08:23.352: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jan 11 13:08:23.352: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2954/pods/agnhost/proxy?method=POST
  Jan 11 13:08:23.372: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jan 11 13:08:23.372: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2954/pods/agnhost/proxy?method=PUT
  Jan 11 13:08:23.382: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jan 11 13:08:23.382: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2954/services/e2e-proxy-test-service/proxy?method=DELETE
  Jan 11 13:08:23.400: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jan 11 13:08:23.400: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2954/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Jan 11 13:08:23.418: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jan 11 13:08:23.418: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2954/services/e2e-proxy-test-service/proxy?method=PATCH
  Jan 11 13:08:23.437: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jan 11 13:08:23.437: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2954/services/e2e-proxy-test-service/proxy?method=POST
  Jan 11 13:08:23.450: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jan 11 13:08:23.451: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2954/services/e2e-proxy-test-service/proxy?method=PUT
  Jan 11 13:08:23.463: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jan 11 13:08:23.464: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2954/pods/agnhost/proxy?method=GET
  Jan 11 13:08:23.477: INFO: http.Client request:GET StatusCode:301
  Jan 11 13:08:23.479: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2954/services/e2e-proxy-test-service/proxy?method=GET
  Jan 11 13:08:23.499: INFO: http.Client request:GET StatusCode:301
  Jan 11 13:08:23.500: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2954/pods/agnhost/proxy?method=HEAD
  Jan 11 13:08:23.513: INFO: http.Client request:HEAD StatusCode:301
  Jan 11 13:08:23.513: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2954/services/e2e-proxy-test-service/proxy?method=HEAD
  Jan 11 13:08:23.530: INFO: http.Client request:HEAD StatusCode:301
  Jan 11 13:08:23.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-2954" for this suite. @ 01/11/24 13:08:23.547
• [2.420 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 01/11/24 13:08:23.569
  Jan 11 13:08:23.569: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename pods @ 01/11/24 13:08:23.571
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:08:23.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:08:23.623
  Jan 11 13:08:23.630: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: creating the pod @ 01/11/24 13:08:23.633
  STEP: submitting the pod to kubernetes @ 01/11/24 13:08:23.634
  E0111 13:08:23.977722      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:24.978571      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:08:25.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6854" for this suite. @ 01/11/24 13:08:25.741
• [2.189 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 01/11/24 13:08:25.762
  Jan 11 13:08:25.762: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename security-context @ 01/11/24 13:08:25.765
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:08:25.797
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:08:25.803
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 01/11/24 13:08:25.809
  E0111 13:08:25.979015      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:26.980157      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:27.980169      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:28.980657      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/11/24 13:08:29.868
  Jan 11 13:08:29.881: INFO: Trying to get logs from node env1-test-worker-2 pod security-context-f041e7e8-890e-4dbe-b4c1-a3c6db693025 container test-container: <nil>
  STEP: delete the pod @ 01/11/24 13:08:29.917
  E0111 13:08:29.981848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:08:30.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-9827" for this suite. @ 01/11/24 13:08:30.054
• [4.327 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 01/11/24 13:08:30.096
  Jan 11 13:08:30.096: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename var-expansion @ 01/11/24 13:08:30.098
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:08:30.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:08:30.168
  E0111 13:08:30.982625      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:31.983161      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:08:32.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 11 13:08:32.246: INFO: Deleting pod "var-expansion-b4309262-2ee8-4a64-ac9d-93ffc18c1798" in namespace "var-expansion-7999"
  Jan 11 13:08:32.265: INFO: Wait up to 5m0s for pod "var-expansion-b4309262-2ee8-4a64-ac9d-93ffc18c1798" to be fully deleted
  E0111 13:08:32.983407      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:33.983649      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-7999" for this suite. @ 01/11/24 13:08:34.29
• [4.211 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 01/11/24 13:08:34.31
  Jan 11 13:08:34.310: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename certificates @ 01/11/24 13:08:34.313
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:08:34.364
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:08:34.373
  E0111 13:08:34.983874      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting /apis @ 01/11/24 13:08:35.839
  STEP: getting /apis/certificates.k8s.io @ 01/11/24 13:08:35.852
  STEP: getting /apis/certificates.k8s.io/v1 @ 01/11/24 13:08:35.854
  STEP: creating @ 01/11/24 13:08:35.857
  STEP: getting @ 01/11/24 13:08:35.916
  STEP: listing @ 01/11/24 13:08:35.922
  STEP: watching @ 01/11/24 13:08:35.937
  Jan 11 13:08:35.937: INFO: starting watch
  STEP: patching @ 01/11/24 13:08:35.94
  STEP: updating @ 01/11/24 13:08:35.958
  Jan 11 13:08:35.971: INFO: waiting for watch events with expected annotations
  Jan 11 13:08:35.971: INFO: saw patched and updated annotations
  STEP: getting /approval @ 01/11/24 13:08:35.972
  STEP: patching /approval @ 01/11/24 13:08:35.981
  E0111 13:08:35.984603      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating /approval @ 01/11/24 13:08:36.008
  STEP: getting /status @ 01/11/24 13:08:36.02
  STEP: patching /status @ 01/11/24 13:08:36.034
  STEP: updating /status @ 01/11/24 13:08:36.054
  STEP: deleting @ 01/11/24 13:08:36.087
  STEP: deleting a collection @ 01/11/24 13:08:36.142
  Jan 11 13:08:36.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-6797" for this suite. @ 01/11/24 13:08:36.228
• [1.939 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 01/11/24 13:08:36.25
  Jan 11 13:08:36.250: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename services @ 01/11/24 13:08:36.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:08:36.311
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:08:36.317
  STEP: creating service in namespace services-2219 @ 01/11/24 13:08:36.323
  STEP: creating service affinity-clusterip in namespace services-2219 @ 01/11/24 13:08:36.323
  STEP: creating replication controller affinity-clusterip in namespace services-2219 @ 01/11/24 13:08:36.377
  I0111 13:08:36.394377      23 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-2219, replica count: 3
  E0111 13:08:36.985892      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:37.985910      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:38.986096      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0111 13:08:39.446787      23 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 11 13:08:39.473: INFO: Creating new exec pod
  E0111 13:08:39.987121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:40.987726      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:41.988477      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:08:42.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2219 exec execpod-affinitykvklc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Jan 11 13:08:42.862: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Jan 11 13:08:42.862: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 11 13:08:42.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2219 exec execpod-affinitykvklc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.32.75 80'
  E0111 13:08:42.988993      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:08:43.170: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.32.75 80\nConnection to 10.233.32.75 80 port [tcp/http] succeeded!\n"
  Jan 11 13:08:43.170: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 11 13:08:43.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4043580974 --namespace=services-2219 exec execpod-affinitykvklc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.32.75:80/ ; done'
  Jan 11 13:08:43.693: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.32.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.32.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.32.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.32.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.32.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.32.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.32.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.32.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.32.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.32.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.32.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.32.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.32.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.32.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.32.75:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.32.75:80/\n"
  Jan 11 13:08:43.693: INFO: stdout: "\naffinity-clusterip-bm88r\naffinity-clusterip-bm88r\naffinity-clusterip-bm88r\naffinity-clusterip-bm88r\naffinity-clusterip-bm88r\naffinity-clusterip-bm88r\naffinity-clusterip-bm88r\naffinity-clusterip-bm88r\naffinity-clusterip-bm88r\naffinity-clusterip-bm88r\naffinity-clusterip-bm88r\naffinity-clusterip-bm88r\naffinity-clusterip-bm88r\naffinity-clusterip-bm88r\naffinity-clusterip-bm88r\naffinity-clusterip-bm88r"
  Jan 11 13:08:43.693: INFO: Received response from host: affinity-clusterip-bm88r
  Jan 11 13:08:43.693: INFO: Received response from host: affinity-clusterip-bm88r
  Jan 11 13:08:43.693: INFO: Received response from host: affinity-clusterip-bm88r
  Jan 11 13:08:43.693: INFO: Received response from host: affinity-clusterip-bm88r
  Jan 11 13:08:43.693: INFO: Received response from host: affinity-clusterip-bm88r
  Jan 11 13:08:43.693: INFO: Received response from host: affinity-clusterip-bm88r
  Jan 11 13:08:43.693: INFO: Received response from host: affinity-clusterip-bm88r
  Jan 11 13:08:43.693: INFO: Received response from host: affinity-clusterip-bm88r
  Jan 11 13:08:43.694: INFO: Received response from host: affinity-clusterip-bm88r
  Jan 11 13:08:43.694: INFO: Received response from host: affinity-clusterip-bm88r
  Jan 11 13:08:43.694: INFO: Received response from host: affinity-clusterip-bm88r
  Jan 11 13:08:43.694: INFO: Received response from host: affinity-clusterip-bm88r
  Jan 11 13:08:43.694: INFO: Received response from host: affinity-clusterip-bm88r
  Jan 11 13:08:43.694: INFO: Received response from host: affinity-clusterip-bm88r
  Jan 11 13:08:43.694: INFO: Received response from host: affinity-clusterip-bm88r
  Jan 11 13:08:43.694: INFO: Received response from host: affinity-clusterip-bm88r
  Jan 11 13:08:43.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 11 13:08:43.710: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-2219, will wait for the garbage collector to delete the pods @ 01/11/24 13:08:43.748
  Jan 11 13:08:43.827: INFO: Deleting ReplicationController affinity-clusterip took: 19.660957ms
  Jan 11 13:08:43.928: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.984557ms
  E0111 13:08:43.989371      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:44.991859      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-2219" for this suite. @ 01/11/24 13:08:45.871
• [9.639 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 01/11/24 13:08:45.892
  Jan 11 13:08:45.892: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename webhook @ 01/11/24 13:08:45.894
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:08:45.948
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:08:45.958
  E0111 13:08:45.992320      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 01/11/24 13:08:46.02
  E0111 13:08:46.993033      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/11/24 13:08:47.896
  STEP: Deploying the webhook pod @ 01/11/24 13:08:47.92
  STEP: Wait for the deployment to be ready @ 01/11/24 13:08:47.953
  Jan 11 13:08:47.976: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0111 13:08:47.993871      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:48.994467      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:49.994958      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/11/24 13:08:50.003
  STEP: Verifying the service has paired with the endpoint @ 01/11/24 13:08:50.03
  E0111 13:08:50.995384      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:08:51.032: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 01/11/24 13:08:51.217
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 01/11/24 13:08:51.332
  STEP: Deleting the collection of validation webhooks @ 01/11/24 13:08:51.392
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 01/11/24 13:08:51.679
  Jan 11 13:08:51.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7597" for this suite. @ 01/11/24 13:08:51.901
  STEP: Destroying namespace "webhook-markers-7457" for this suite. @ 01/11/24 13:08:51.956
  E0111 13:08:51.996287      23 retrywatcher.go:130] "Watch failed" err="context canceled"
• [6.109 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 01/11/24 13:08:52.008
  Jan 11 13:08:52.009: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename endpointslice @ 01/11/24 13:08:52.013
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:08:52.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:08:52.081
  Jan 11 13:08:52.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4424" for this suite. @ 01/11/24 13:08:52.294
• [0.317 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 01/11/24 13:08:52.326
  Jan 11 13:08:52.326: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename deployment @ 01/11/24 13:08:52.329
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:08:52.373
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:08:52.387
  Jan 11 13:08:52.443: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0111 13:08:52.997386      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:53.997936      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:54.998462      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:55.999338      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:56.999785      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:08:57.460: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 01/11/24 13:08:57.461
  Jan 11 13:08:57.461: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0111 13:08:58.000874      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:08:59.001299      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:08:59.474: INFO: Creating deployment "test-rollover-deployment"
  Jan 11 13:08:59.510: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0111 13:09:00.002022      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:09:01.002076      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:09:01.532: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Jan 11 13:09:01.547: INFO: Ensure that both replica sets have 1 created replica
  Jan 11 13:09:01.564: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Jan 11 13:09:01.594: INFO: Updating deployment test-rollover-deployment
  Jan 11 13:09:01.594: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0111 13:09:02.002953      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:09:03.004072      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:09:03.612: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Jan 11 13:09:03.634: INFO: Make sure deployment "test-rollover-deployment" is complete
  Jan 11 13:09:03.651: INFO: all replica sets need to contain the pod-template-hash label
  Jan 11 13:09:03.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 13, 8, 59, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 13, 8, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 13, 9, 2, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 13, 8, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0111 13:09:04.004772      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:09:05.005655      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:09:05.671: INFO: all replica sets need to contain the pod-template-hash label
  Jan 11 13:09:05.671: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 13, 8, 59, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 13, 8, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 13, 9, 2, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 13, 8, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0111 13:09:06.006511      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:09:07.007168      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:09:07.672: INFO: all replica sets need to contain the pod-template-hash label
  Jan 11 13:09:07.672: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 13, 8, 59, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 13, 8, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 13, 9, 2, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 13, 8, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0111 13:09:08.007274      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:09:09.007395      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:09:09.674: INFO: all replica sets need to contain the pod-template-hash label
  Jan 11 13:09:09.674: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 13, 8, 59, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 13, 8, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 13, 9, 2, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 13, 8, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0111 13:09:10.008171      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:09:11.008823      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:09:11.670: INFO: all replica sets need to contain the pod-template-hash label
  Jan 11 13:09:11.671: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 13, 8, 59, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 13, 8, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 11, 13, 9, 2, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 11, 13, 8, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0111 13:09:12.009629      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:09:13.010317      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:09:13.670: INFO: 
  Jan 11 13:09:13.671: INFO: Ensure that both old replica sets have no replicas
  Jan 11 13:09:13.697: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-7280  e8fa082b-2be6-4a7e-a0dd-8fdb3d30f891 187198413 2 2024-01-11 13:08:59 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-11 13:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-11 13:09:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e2d278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-11 13:08:59 +0000 UTC,LastTransitionTime:2024-01-11 13:08:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2024-01-11 13:09:12 +0000 UTC,LastTransitionTime:2024-01-11 13:08:59 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jan 11 13:09:13.706: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-7280  c03818c7-5425-49e7-a2f4-b3dc74618071 187198401 2 2024-01-11 13:09:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment e8fa082b-2be6-4a7e-a0dd-8fdb3d30f891 0xc004e2d747 0xc004e2d748}] [] [{kube-controller-manager Update apps/v1 2024-01-11 13:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e8fa082b-2be6-4a7e-a0dd-8fdb3d30f891\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-11 13:09:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e2d7f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jan 11 13:09:13.706: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Jan 11 13:09:13.706: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7280  db6f537c-63c8-4786-8e1b-0fb89d459cdd 187198411 2 2024-01-11 13:08:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment e8fa082b-2be6-4a7e-a0dd-8fdb3d30f891 0xc004e2d617 0xc004e2d618}] [] [{e2e.test Update apps/v1 2024-01-11 13:08:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-11 13:09:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e8fa082b-2be6-4a7e-a0dd-8fdb3d30f891\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2024-01-11 13:09:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004e2d6d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 11 13:09:13.706: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-7280  32e7747c-2ed0-4aac-a549-301ec07dd28d 187198341 2 2024-01-11 13:08:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment e8fa082b-2be6-4a7e-a0dd-8fdb3d30f891 0xc004e2d867 0xc004e2d868}] [] [{kube-controller-manager Update apps/v1 2024-01-11 13:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e8fa082b-2be6-4a7e-a0dd-8fdb3d30f891\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-11 13:09:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e2d918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 11 13:09:13.716: INFO: Pod "test-rollover-deployment-57777854c9-7xj6l" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-7xj6l test-rollover-deployment-57777854c9- deployment-7280  b02da9ea-aca7-4667-a6be-433e811390d2 187198358 0 2024-01-11 13:09:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 c03818c7-5425-49e7-a2f4-b3dc74618071 0xc004e2dea7 0xc004e2dea8}] [] [{kube-controller-manager Update v1 2024-01-11 13:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c03818c7-5425-49e7-a2f4-b3dc74618071\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 13:09:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.69.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t4k7s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t4k7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 13:09:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 13:09:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 13:09:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 13:09:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.202,PodIP:10.233.69.152,StartTime:2024-01-11 13:09:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-11 13:09:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://dff33866b98269ef989abed382e6982d9124180a84c920602f50a0273043e28b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.69.152,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 13:09:13.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7280" for this suite. @ 01/11/24 13:09:13.733
• [21.424 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 01/11/24 13:09:13.755
  Jan 11 13:09:13.756: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename secrets @ 01/11/24 13:09:13.76
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:09:13.805
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:09:13.811
  STEP: Creating secret with name secret-test-37203bb5-caf9-4c86-b20f-f30ea4cf1f5d @ 01/11/24 13:09:13.818
  STEP: Creating a pod to test consume secrets @ 01/11/24 13:09:13.83
  E0111 13:09:14.011359      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:09:15.011920      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:09:16.011950      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:09:17.013131      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/11/24 13:09:17.888
  Jan 11 13:09:17.896: INFO: Trying to get logs from node env1-test-worker-1 pod pod-secrets-f8643f78-7541-4e58-b7d4-e08ca3189ba5 container secret-volume-test: <nil>
  STEP: delete the pod @ 01/11/24 13:09:17.911
  Jan 11 13:09:17.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4826" for this suite. @ 01/11/24 13:09:17.972
• [4.232 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 01/11/24 13:09:17.988
  Jan 11 13:09:17.988: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename downward-api @ 01/11/24 13:09:17.989
  E0111 13:09:18.012973      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:09:18.025
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:09:18.032
  STEP: Creating a pod to test downward API volume plugin @ 01/11/24 13:09:18.04
  E0111 13:09:19.013785      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:09:20.014508      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:09:21.015181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:09:22.015579      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/11/24 13:09:22.108
  Jan 11 13:09:22.117: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-2073eb7c-3b49-4e59-857d-40f52dd29e25 container client-container: <nil>
  STEP: delete the pod @ 01/11/24 13:09:22.136
  Jan 11 13:09:22.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1565" for this suite. @ 01/11/24 13:09:22.196
• [4.224 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 01/11/24 13:09:22.222
  Jan 11 13:09:22.223: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename namespaces @ 01/11/24 13:09:22.225
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:09:22.259
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:09:22.27
  STEP: Creating namespace "e2e-ns-8vnpr" @ 01/11/24 13:09:22.279
  Jan 11 13:09:22.315: INFO: Namespace "e2e-ns-8vnpr-3332" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-8vnpr-3332" @ 01/11/24 13:09:22.315
  Jan 11 13:09:22.350: INFO: Namespace "e2e-ns-8vnpr-3332" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-8vnpr-3332" @ 01/11/24 13:09:22.351
  Jan 11 13:09:22.373: INFO: Namespace "e2e-ns-8vnpr-3332" has []v1.FinalizerName{"kubernetes"}
  Jan 11 13:09:22.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9885" for this suite. @ 01/11/24 13:09:22.39
  STEP: Destroying namespace "e2e-ns-8vnpr-3332" for this suite. @ 01/11/24 13:09:22.405
• [0.200 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 01/11/24 13:09:22.425
  Jan 11 13:09:22.425: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename deployment @ 01/11/24 13:09:22.428
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:09:22.465
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:09:22.473
  Jan 11 13:09:22.479: INFO: Creating deployment "test-recreate-deployment"
  Jan 11 13:09:22.497: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Jan 11 13:09:22.527: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  E0111 13:09:23.015659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:09:24.016828      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:09:24.548: INFO: Waiting deployment "test-recreate-deployment" to complete
  Jan 11 13:09:24.557: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Jan 11 13:09:24.583: INFO: Updating deployment test-recreate-deployment
  Jan 11 13:09:24.583: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Jan 11 13:09:24.831: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-3983  57ed23cd-21e2-4a48-936f-32e360ee1525 187198593 2 2024-01-11 13:09:22 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-11 13:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-11 13:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0081fe6e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-01-11 13:09:24 +0000 UTC,LastTransitionTime:2024-01-11 13:09:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2024-01-11 13:09:24 +0000 UTC,LastTransitionTime:2024-01-11 13:09:22 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Jan 11 13:09:24.840: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-3983  ebd280ec-0f38-4be8-80de-61b6dae58658 187198590 1 2024-01-11 13:09:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 57ed23cd-21e2-4a48-936f-32e360ee1525 0xc00707a047 0xc00707a048}] [] [{kube-controller-manager Update apps/v1 2024-01-11 13:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57ed23cd-21e2-4a48-936f-32e360ee1525\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-11 13:09:24 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00707a0e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 11 13:09:24.840: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Jan 11 13:09:24.840: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-3983  a2f0376c-fe14-4aba-84f8-29ef72097b70 187198581 2 2024-01-11 13:09:22 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 57ed23cd-21e2-4a48-936f-32e360ee1525 0xc00707a157 0xc00707a158}] [] [{kube-controller-manager Update apps/v1 2024-01-11 13:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57ed23cd-21e2-4a48-936f-32e360ee1525\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-11 13:09:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00707a208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 11 13:09:24.848: INFO: Pod "test-recreate-deployment-54757ffd6c-fl79w" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-fl79w test-recreate-deployment-54757ffd6c- deployment-3983  114dc69b-7741-4fba-a9a5-f79952a968f7 187198592 0 2024-01-11 13:09:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c ebd280ec-0f38-4be8-80de-61b6dae58658 0xc00707a697 0xc00707a698}] [] [{kube-controller-manager Update v1 2024-01-11 13:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ebd280ec-0f38-4be8-80de-61b6dae58658\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-11 13:09:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f4hlt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f4hlt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 13:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 13:09:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 13:09:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-11 13:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:,StartTime:2024-01-11 13:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 11 13:09:24.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3983" for this suite. @ 01/11/24 13:09:24.863
• [2.457 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 01/11/24 13:09:24.898
  Jan 11 13:09:24.898: INFO: >>> kubeConfig: /tmp/kubeconfig-4043580974
  STEP: Building a namespace api object, basename var-expansion @ 01/11/24 13:09:24.901
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/11/24 13:09:24.942
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/11/24 13:09:24.947
  E0111 13:09:25.017119      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:09:26.017327      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:09:26.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 11 13:09:27.007: INFO: Deleting pod "var-expansion-71989bf8-f49a-46d6-ac7b-6d0d4f4f3b43" in namespace "var-expansion-5522"
  E0111 13:09:27.017334      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 11 13:09:27.022: INFO: Wait up to 5m0s for pod "var-expansion-71989bf8-f49a-46d6-ac7b-6d0d4f4f3b43" to be fully deleted
  E0111 13:09:28.018328      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0111 13:09:29.019142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-5522" for this suite. @ 01/11/24 13:09:29.043
• [4.209 seconds]
------------------------------
S
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Jan 11 13:09:29.113: INFO: Running AfterSuite actions on node 1
  Jan 11 13:09:29.113: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.186 seconds]
------------------------------

Ran 378 of 7207 Specs in 6627.711 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h50m28.668455396s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

