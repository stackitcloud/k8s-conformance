  I0415 06:16:26.472267      13 e2e.go:117] Starting e2e run "fe35d8fe-a901-4578-84fa-433ba542a568" on Ginkgo node 1
  Apr 15 06:16:26.584: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1713161785 - will randomize all specs

Will run 378 of 7209 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Apr 15 06:16:27.168: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 06:16:27.197: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Apr 15 06:16:27.321: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Apr 15 06:16:27.332: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
  Apr 15 06:16:27.333: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  Apr 15 06:16:27.334: INFO: e2e test version: v1.27.12
  Apr 15 06:16:27.338: INFO: kube-apiserver version: v1.27.12
  Apr 15 06:16:27.338: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 06:16:27.360: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.193 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 04/15/24 06:16:28.012
  Apr 15 06:16:28.012: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 06:16:28.014
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:16:28.058
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:16:28.067
  Apr 15 06:16:50.264: INFO: Container started at 2024-04-15 06:16:28 +0000 UTC, pod became ready at 2024-04-15 06:16:48 +0000 UTC
  Apr 15 06:16:50.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-3755" for this suite. @ 04/15/24 06:16:50.275
• [22.277 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 04/15/24 06:16:50.29
  Apr 15 06:16:50.290: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename limitrange @ 04/15/24 06:16:50.292
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:16:50.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:16:50.331
  STEP: Creating a LimitRange @ 04/15/24 06:16:50.336
  STEP: Setting up watch @ 04/15/24 06:16:50.337
  STEP: Submitting a LimitRange @ 04/15/24 06:16:50.444
  STEP: Verifying LimitRange creation was observed @ 04/15/24 06:16:50.456
  STEP: Fetching the LimitRange to ensure it has proper values @ 04/15/24 06:16:50.461
  Apr 15 06:16:50.469: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Apr 15 06:16:50.470: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 04/15/24 06:16:50.47
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 04/15/24 06:16:50.485
  Apr 15 06:16:50.491: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Apr 15 06:16:50.491: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 04/15/24 06:16:50.491
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 04/15/24 06:16:50.512
  Apr 15 06:16:50.520: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Apr 15 06:16:50.520: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 04/15/24 06:16:50.52
  STEP: Failing to create a Pod with more than max resources @ 04/15/24 06:16:50.526
  STEP: Updating a LimitRange @ 04/15/24 06:16:50.535
  STEP: Verifying LimitRange updating is effective @ 04/15/24 06:16:50.55
  STEP: Creating a Pod with less than former min resources @ 04/15/24 06:16:52.557
  STEP: Failing to create a Pod with more than max resources @ 04/15/24 06:16:52.57
  STEP: Deleting a LimitRange @ 04/15/24 06:16:52.575
  STEP: Verifying the LimitRange was deleted @ 04/15/24 06:16:52.595
  Apr 15 06:16:57.604: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 04/15/24 06:16:57.604
  Apr 15 06:16:57.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-9267" for this suite. @ 04/15/24 06:16:57.642
• [7.385 seconds]
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 04/15/24 06:16:57.677
  Apr 15 06:16:57.678: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename dns @ 04/15/24 06:16:57.682
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:16:57.717
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:16:57.728
  STEP: Creating a test headless service @ 04/15/24 06:16:57.736
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1928 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1928;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1928 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1928;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1928.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1928.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1928.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1928.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1928.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1928.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1928.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1928.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1928.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1928.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1928.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1928.svc;check="$$(dig +notcp +noall +answer +search 114.15.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.15.114_udp@PTR;check="$$(dig +tcp +noall +answer +search 114.15.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.15.114_tcp@PTR;sleep 1; done
   @ 04/15/24 06:16:57.814
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1928 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1928;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1928 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1928;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1928.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1928.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1928.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1928.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1928.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1928.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1928.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1928.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1928.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1928.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1928.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1928.svc;check="$$(dig +notcp +noall +answer +search 114.15.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.15.114_udp@PTR;check="$$(dig +tcp +noall +answer +search 114.15.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.15.114_tcp@PTR;sleep 1; done
   @ 04/15/24 06:16:57.814
  STEP: creating a pod to probe DNS @ 04/15/24 06:16:57.814
  STEP: submitting the pod to kubernetes @ 04/15/24 06:16:57.814
  STEP: retrieving the pod @ 04/15/24 06:16:59.903
  STEP: looking for the results for each expected name from probers @ 04/15/24 06:16:59.911
  Apr 15 06:16:59.926: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:16:59.934: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:16:59.946: INFO: Unable to read wheezy_udp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:16:59.965: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:16:59.975: INFO: Unable to read wheezy_udp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:16:59.984: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:16:59.993: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:00.000: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:00.050: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:00.057: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:00.065: INFO: Unable to read jessie_udp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:00.076: INFO: Unable to read jessie_tcp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:00.083: INFO: Unable to read jessie_udp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:00.093: INFO: Unable to read jessie_tcp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:00.103: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:00.114: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:00.150: INFO: Lookups using dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1928 wheezy_tcp@dns-test-service.dns-1928 wheezy_udp@dns-test-service.dns-1928.svc wheezy_tcp@dns-test-service.dns-1928.svc wheezy_udp@_http._tcp.dns-test-service.dns-1928.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1928.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1928 jessie_tcp@dns-test-service.dns-1928 jessie_udp@dns-test-service.dns-1928.svc jessie_tcp@dns-test-service.dns-1928.svc jessie_udp@_http._tcp.dns-test-service.dns-1928.svc jessie_tcp@_http._tcp.dns-test-service.dns-1928.svc]

  Apr 15 06:17:05.164: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:05.174: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:05.185: INFO: Unable to read wheezy_udp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:05.201: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:05.211: INFO: Unable to read wheezy_udp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:05.222: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:05.230: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:05.242: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:05.295: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:05.306: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:05.317: INFO: Unable to read jessie_udp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:05.327: INFO: Unable to read jessie_tcp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:05.336: INFO: Unable to read jessie_udp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:05.345: INFO: Unable to read jessie_tcp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:05.367: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:05.386: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:05.422: INFO: Lookups using dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1928 wheezy_tcp@dns-test-service.dns-1928 wheezy_udp@dns-test-service.dns-1928.svc wheezy_tcp@dns-test-service.dns-1928.svc wheezy_udp@_http._tcp.dns-test-service.dns-1928.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1928.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1928 jessie_tcp@dns-test-service.dns-1928 jessie_udp@dns-test-service.dns-1928.svc jessie_tcp@dns-test-service.dns-1928.svc jessie_udp@_http._tcp.dns-test-service.dns-1928.svc jessie_tcp@_http._tcp.dns-test-service.dns-1928.svc]

  Apr 15 06:17:10.165: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:10.176: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:10.185: INFO: Unable to read wheezy_udp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:10.195: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:10.203: INFO: Unable to read wheezy_udp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:10.212: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:10.221: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:10.230: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:10.290: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:10.300: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:10.312: INFO: Unable to read jessie_udp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:10.323: INFO: Unable to read jessie_tcp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:10.333: INFO: Unable to read jessie_udp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:10.345: INFO: Unable to read jessie_tcp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:10.374: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:10.384: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:10.429: INFO: Lookups using dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1928 wheezy_tcp@dns-test-service.dns-1928 wheezy_udp@dns-test-service.dns-1928.svc wheezy_tcp@dns-test-service.dns-1928.svc wheezy_udp@_http._tcp.dns-test-service.dns-1928.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1928.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1928 jessie_tcp@dns-test-service.dns-1928 jessie_udp@dns-test-service.dns-1928.svc jessie_tcp@dns-test-service.dns-1928.svc jessie_udp@_http._tcp.dns-test-service.dns-1928.svc jessie_tcp@_http._tcp.dns-test-service.dns-1928.svc]

  Apr 15 06:17:15.166: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:15.174: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:15.183: INFO: Unable to read wheezy_udp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:15.193: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:15.202: INFO: Unable to read wheezy_udp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:15.213: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:15.222: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:15.228: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:15.263: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:15.270: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:15.278: INFO: Unable to read jessie_udp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:15.284: INFO: Unable to read jessie_tcp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:15.291: INFO: Unable to read jessie_udp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:15.298: INFO: Unable to read jessie_tcp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:15.305: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:15.313: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:15.345: INFO: Lookups using dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1928 wheezy_tcp@dns-test-service.dns-1928 wheezy_udp@dns-test-service.dns-1928.svc wheezy_tcp@dns-test-service.dns-1928.svc wheezy_udp@_http._tcp.dns-test-service.dns-1928.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1928.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1928 jessie_tcp@dns-test-service.dns-1928 jessie_udp@dns-test-service.dns-1928.svc jessie_tcp@dns-test-service.dns-1928.svc jessie_udp@_http._tcp.dns-test-service.dns-1928.svc jessie_tcp@_http._tcp.dns-test-service.dns-1928.svc]

  Apr 15 06:17:20.165: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:20.177: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:20.188: INFO: Unable to read wheezy_udp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:20.196: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:20.207: INFO: Unable to read wheezy_udp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:20.219: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:20.226: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:20.238: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:20.293: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:20.306: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:20.317: INFO: Unable to read jessie_udp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:20.326: INFO: Unable to read jessie_tcp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:20.338: INFO: Unable to read jessie_udp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:20.346: INFO: Unable to read jessie_tcp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:20.355: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:20.365: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:20.395: INFO: Lookups using dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1928 wheezy_tcp@dns-test-service.dns-1928 wheezy_udp@dns-test-service.dns-1928.svc wheezy_tcp@dns-test-service.dns-1928.svc wheezy_udp@_http._tcp.dns-test-service.dns-1928.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1928.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1928 jessie_tcp@dns-test-service.dns-1928 jessie_udp@dns-test-service.dns-1928.svc jessie_tcp@dns-test-service.dns-1928.svc jessie_udp@_http._tcp.dns-test-service.dns-1928.svc jessie_tcp@_http._tcp.dns-test-service.dns-1928.svc]

  Apr 15 06:17:25.163: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:25.170: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:25.181: INFO: Unable to read wheezy_udp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:25.194: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:25.201: INFO: Unable to read wheezy_udp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:25.210: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:25.218: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:25.227: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:25.277: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:25.285: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:25.293: INFO: Unable to read jessie_udp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:25.300: INFO: Unable to read jessie_tcp@dns-test-service.dns-1928 from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:25.309: INFO: Unable to read jessie_udp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:25.318: INFO: Unable to read jessie_tcp@dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:25.326: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:25.334: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:25.370: INFO: Lookups using dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1928 wheezy_tcp@dns-test-service.dns-1928 wheezy_udp@dns-test-service.dns-1928.svc wheezy_tcp@dns-test-service.dns-1928.svc wheezy_udp@_http._tcp.dns-test-service.dns-1928.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1928.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1928 jessie_tcp@dns-test-service.dns-1928 jessie_udp@dns-test-service.dns-1928.svc jessie_tcp@dns-test-service.dns-1928.svc jessie_udp@_http._tcp.dns-test-service.dns-1928.svc jessie_tcp@_http._tcp.dns-test-service.dns-1928.svc]

  Apr 15 06:17:30.207: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:30.215: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:30.253: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:30.262: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:30.295: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:30.303: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1928.svc from pod dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856: the server could not find the requested resource (get pods dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856)
  Apr 15 06:17:30.333: INFO: Lookups using dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-1928.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1928.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@_http._tcp.dns-test-service.dns-1928.svc jessie_tcp@_http._tcp.dns-test-service.dns-1928.svc]

  Apr 15 06:17:35.402: INFO: DNS probes using dns-1928/dns-test-94ad09c5-d85f-4dac-8f9c-5f689c481856 succeeded

  Apr 15 06:17:35.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 06:17:35.42
  STEP: deleting the test service @ 04/15/24 06:17:35.481
  STEP: deleting the test headless service @ 04/15/24 06:17:35.543
  STEP: Destroying namespace "dns-1928" for this suite. @ 04/15/24 06:17:35.596
• [37.940 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 04/15/24 06:17:35.632
  Apr 15 06:17:35.632: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename containers @ 04/15/24 06:17:35.64
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:17:35.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:17:35.699
  STEP: Creating a pod to test override command @ 04/15/24 06:17:35.71
  STEP: Saw pod success @ 04/15/24 06:17:39.766
  Apr 15 06:17:39.774: INFO: Trying to get logs from node zaigh3ewotoh-3 pod client-containers-0786afef-3c8a-48af-bc1d-98e106083579 container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 06:17:39.818
  Apr 15 06:17:39.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-3335" for this suite. @ 04/15/24 06:17:39.864
• [4.251 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 04/15/24 06:17:39.896
  Apr 15 06:17:39.896: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename events @ 04/15/24 06:17:39.901
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:17:39.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:17:39.942
  STEP: Create set of events @ 04/15/24 06:17:39.949
  Apr 15 06:17:39.961: INFO: created test-event-1
  Apr 15 06:17:39.976: INFO: created test-event-2
  Apr 15 06:17:39.990: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 04/15/24 06:17:39.99
  STEP: delete collection of events @ 04/15/24 06:17:40.001
  Apr 15 06:17:40.002: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 04/15/24 06:17:40.06
  Apr 15 06:17:40.060: INFO: requesting list of events to confirm quantity
  Apr 15 06:17:40.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-9110" for this suite. @ 04/15/24 06:17:40.075
• [0.199 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 04/15/24 06:17:40.109
  Apr 15 06:17:40.109: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename events @ 04/15/24 06:17:40.113
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:17:40.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:17:40.154
  STEP: creating a test event @ 04/15/24 06:17:40.16
  STEP: listing events in all namespaces @ 04/15/24 06:17:40.17
  STEP: listing events in test namespace @ 04/15/24 06:17:40.199
  STEP: listing events with field selection filtering on source @ 04/15/24 06:17:40.209
  STEP: listing events with field selection filtering on reportingController @ 04/15/24 06:17:40.217
  STEP: getting the test event @ 04/15/24 06:17:40.225
  STEP: patching the test event @ 04/15/24 06:17:40.232
  STEP: getting the test event @ 04/15/24 06:17:40.262
  STEP: updating the test event @ 04/15/24 06:17:40.269
  STEP: getting the test event @ 04/15/24 06:17:40.284
  STEP: deleting the test event @ 04/15/24 06:17:40.29
  STEP: listing events in all namespaces @ 04/15/24 06:17:40.306
  STEP: listing events in test namespace @ 04/15/24 06:17:40.349
  Apr 15 06:17:40.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-7686" for this suite. @ 04/15/24 06:17:40.371
• [0.276 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 04/15/24 06:17:40.389
  Apr 15 06:17:40.389: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename sched-pred @ 04/15/24 06:17:40.393
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:17:40.43
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:17:40.438
  Apr 15 06:17:40.445: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 15 06:17:40.465: INFO: Waiting for terminating namespaces to be deleted...
  Apr 15 06:17:40.473: INFO: 
  Logging pods the apiserver thinks is on node zaigh3ewotoh-1 before test
  Apr 15 06:17:40.490: INFO: coredns-5d78c9869d-t6h7j from kube-system started at 2024-04-15 06:16:15 +0000 UTC (1 container statuses recorded)
  Apr 15 06:17:40.490: INFO: 	Container coredns ready: true, restart count 0
  Apr 15 06:17:40.490: INFO: kube-addon-manager-zaigh3ewotoh-1 from kube-system started at 2024-04-15 06:11:57 +0000 UTC (1 container statuses recorded)
  Apr 15 06:17:40.491: INFO: 	Container kube-addon-manager ready: true, restart count 4
  Apr 15 06:17:40.491: INFO: kube-apiserver-zaigh3ewotoh-1 from kube-system started at 2024-04-15 06:11:57 +0000 UTC (1 container statuses recorded)
  Apr 15 06:17:40.491: INFO: 	Container kube-apiserver ready: true, restart count 4
  Apr 15 06:17:40.492: INFO: kube-controller-manager-zaigh3ewotoh-1 from kube-system started at 2024-04-15 06:11:57 +0000 UTC (1 container statuses recorded)
  Apr 15 06:17:40.492: INFO: 	Container kube-controller-manager ready: true, restart count 4
  Apr 15 06:17:40.493: INFO: kube-flannel-ds-wwbkw from kube-system started at 2024-04-15 06:16:17 +0000 UTC (1 container statuses recorded)
  Apr 15 06:17:40.494: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 06:17:40.494: INFO: kube-proxy-v9dxk from kube-system started at 2024-04-15 06:16:16 +0000 UTC (1 container statuses recorded)
  Apr 15 06:17:40.495: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 06:17:40.495: INFO: kube-scheduler-zaigh3ewotoh-1 from kube-system started at 2024-04-15 06:11:57 +0000 UTC (1 container statuses recorded)
  Apr 15 06:17:40.496: INFO: 	Container kube-scheduler ready: true, restart count 4
  Apr 15 06:17:40.496: INFO: sonobuoy-systemd-logs-daemon-set-409cd1623c554ca6-k6p5l from sonobuoy started at 2024-04-15 06:16:23 +0000 UTC (2 container statuses recorded)
  Apr 15 06:17:40.497: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 06:17:40.498: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 15 06:17:40.498: INFO: 
  Logging pods the apiserver thinks is on node zaigh3ewotoh-2 before test
  Apr 15 06:17:40.517: INFO: kube-addon-manager-zaigh3ewotoh-2 from kube-system started at 2024-04-15 06:07:13 +0000 UTC (1 container statuses recorded)
  Apr 15 06:17:40.521: INFO: 	Container kube-addon-manager ready: true, restart count 4
  Apr 15 06:17:40.521: INFO: kube-apiserver-zaigh3ewotoh-2 from kube-system started at 2024-04-15 06:07:13 +0000 UTC (1 container statuses recorded)
  Apr 15 06:17:40.522: INFO: 	Container kube-apiserver ready: true, restart count 4
  Apr 15 06:17:40.522: INFO: kube-controller-manager-zaigh3ewotoh-2 from kube-system started at 2024-04-15 06:07:13 +0000 UTC (1 container statuses recorded)
  Apr 15 06:17:40.522: INFO: 	Container kube-controller-manager ready: true, restart count 4
  Apr 15 06:17:40.523: INFO: kube-flannel-ds-gs792 from kube-system started at 2024-04-15 06:16:17 +0000 UTC (1 container statuses recorded)
  Apr 15 06:17:40.523: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 06:17:40.524: INFO: kube-proxy-sqtk9 from kube-system started at 2024-04-15 06:16:16 +0000 UTC (1 container statuses recorded)
  Apr 15 06:17:40.524: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 06:17:40.525: INFO: kube-scheduler-zaigh3ewotoh-2 from kube-system started at 2024-04-15 06:07:13 +0000 UTC (1 container statuses recorded)
  Apr 15 06:17:40.525: INFO: 	Container kube-scheduler ready: true, restart count 4
  Apr 15 06:17:40.525: INFO: sonobuoy-systemd-logs-daemon-set-409cd1623c554ca6-pp7qg from sonobuoy started at 2024-04-15 06:16:23 +0000 UTC (2 container statuses recorded)
  Apr 15 06:17:40.525: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 06:17:40.525: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 15 06:17:40.525: INFO: 
  Logging pods the apiserver thinks is on node zaigh3ewotoh-3 before test
  Apr 15 06:17:40.539: INFO: coredns-5d78c9869d-4w296 from kube-system started at 2024-04-15 06:16:15 +0000 UTC (1 container statuses recorded)
  Apr 15 06:17:40.539: INFO: 	Container coredns ready: true, restart count 0
  Apr 15 06:17:40.540: INFO: kube-flannel-ds-6xxbt from kube-system started at 2024-04-15 06:16:17 +0000 UTC (1 container statuses recorded)
  Apr 15 06:17:40.540: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 06:17:40.541: INFO: kube-proxy-mxvh9 from kube-system started at 2024-04-15 06:16:16 +0000 UTC (1 container statuses recorded)
  Apr 15 06:17:40.541: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 06:17:40.542: INFO: sonobuoy from sonobuoy started at 2024-04-15 06:16:22 +0000 UTC (1 container statuses recorded)
  Apr 15 06:17:40.542: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 15 06:17:40.542: INFO: sonobuoy-e2e-job-7bb47b0523524b58 from sonobuoy started at 2024-04-15 06:16:23 +0000 UTC (2 container statuses recorded)
  Apr 15 06:17:40.543: INFO: 	Container e2e ready: true, restart count 0
  Apr 15 06:17:40.543: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 06:17:40.543: INFO: sonobuoy-systemd-logs-daemon-set-409cd1623c554ca6-9fkvf from sonobuoy started at 2024-04-15 06:16:23 +0000 UTC (2 container statuses recorded)
  Apr 15 06:17:40.543: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 06:17:40.543: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/15/24 06:17:40.544
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/15/24 06:17:42.597
  STEP: Trying to apply a random label on the found node. @ 04/15/24 06:17:42.631
  STEP: verifying the node has the label kubernetes.io/e2e-c1ab93d4-fa8c-4a0d-8162-f9138b7d8579 95 @ 04/15/24 06:17:42.666
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 04/15/24 06:17:42.683
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.131 on the node which pod4 resides and expect not scheduled @ 04/15/24 06:17:44.716
  STEP: removing the label kubernetes.io/e2e-c1ab93d4-fa8c-4a0d-8162-f9138b7d8579 off the node zaigh3ewotoh-3 @ 04/15/24 06:22:44.739
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-c1ab93d4-fa8c-4a0d-8162-f9138b7d8579 @ 04/15/24 06:22:44.775
  Apr 15 06:22:44.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8912" for this suite. @ 04/15/24 06:22:44.812
• [304.441 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 04/15/24 06:22:44.844
  Apr 15 06:22:44.845: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename pods @ 04/15/24 06:22:44.854
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:22:44.907
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:22:44.914
  STEP: creating the pod @ 04/15/24 06:22:44.924
  STEP: submitting the pod to kubernetes @ 04/15/24 06:22:44.925
  W0415 06:22:44.965133      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: verifying the pod is in kubernetes @ 04/15/24 06:22:46.988
  STEP: updating the pod @ 04/15/24 06:22:46.996
  Apr 15 06:22:47.522: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b15a3ad0-81ea-42f4-b1fc-38cbdd496614"
  Apr 15 06:22:51.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2560" for this suite. @ 04/15/24 06:22:51.564
• [6.742 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 04/15/24 06:22:51.585
  Apr 15 06:22:51.585: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/15/24 06:22:51.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:22:51.619
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:22:51.625
  Apr 15 06:22:51.631: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/15/24 06:22:53.622
  Apr 15 06:22:53.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-9779 --namespace=crd-publish-openapi-9779 create -f -'
  Apr 15 06:22:55.753: INFO: stderr: ""
  Apr 15 06:22:55.753: INFO: stdout: "e2e-test-crd-publish-openapi-8437-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Apr 15 06:22:55.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-9779 --namespace=crd-publish-openapi-9779 delete e2e-test-crd-publish-openapi-8437-crds test-cr'
  Apr 15 06:22:55.938: INFO: stderr: ""
  Apr 15 06:22:55.938: INFO: stdout: "e2e-test-crd-publish-openapi-8437-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Apr 15 06:22:55.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-9779 --namespace=crd-publish-openapi-9779 apply -f -'
  Apr 15 06:22:56.562: INFO: stderr: ""
  Apr 15 06:22:56.562: INFO: stdout: "e2e-test-crd-publish-openapi-8437-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Apr 15 06:22:56.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-9779 --namespace=crd-publish-openapi-9779 delete e2e-test-crd-publish-openapi-8437-crds test-cr'
  Apr 15 06:22:56.877: INFO: stderr: ""
  Apr 15 06:22:56.877: INFO: stdout: "e2e-test-crd-publish-openapi-8437-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 04/15/24 06:22:56.877
  Apr 15 06:22:56.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-9779 explain e2e-test-crd-publish-openapi-8437-crds'
  Apr 15 06:22:57.453: INFO: stderr: ""
  Apr 15 06:22:57.453: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-8437-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  Apr 15 06:22:59.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9779" for this suite. @ 04/15/24 06:22:59.361
• [7.797 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 04/15/24 06:22:59.409
  Apr 15 06:22:59.409: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename services @ 04/15/24 06:22:59.413
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:22:59.453
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:22:59.46
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-3370 @ 04/15/24 06:22:59.468
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 04/15/24 06:22:59.505
  STEP: creating service externalsvc in namespace services-3370 @ 04/15/24 06:22:59.513
  STEP: creating replication controller externalsvc in namespace services-3370 @ 04/15/24 06:22:59.539
  I0415 06:22:59.566474      13 runners.go:194] Created replication controller with name: externalsvc, namespace: services-3370, replica count: 2
  I0415 06:23:02.618123      13 runners.go:194] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0415 06:23:05.621257      13 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 04/15/24 06:23:05.631
  Apr 15 06:23:05.674: INFO: Creating new exec pod
  Apr 15 06:23:07.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-3370 exec execpodr7tfq -- /bin/sh -x -c nslookup nodeport-service.services-3370.svc.cluster.local'
  Apr 15 06:23:08.196: INFO: stderr: "+ nslookup nodeport-service.services-3370.svc.cluster.local\n"
  Apr 15 06:23:08.196: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-3370.svc.cluster.local\tcanonical name = externalsvc.services-3370.svc.cluster.local.\nName:\texternalsvc.services-3370.svc.cluster.local\nAddress: 10.233.40.210\n\n"
  Apr 15 06:23:08.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-3370, will wait for the garbage collector to delete the pods @ 04/15/24 06:23:08.209
  Apr 15 06:23:08.280: INFO: Deleting ReplicationController externalsvc took: 14.244719ms
  Apr 15 06:23:08.382: INFO: Terminating ReplicationController externalsvc pods took: 101.96976ms
  Apr 15 06:23:10.238: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-3370" for this suite. @ 04/15/24 06:23:10.273
• [10.884 seconds]
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 04/15/24 06:23:10.296
  Apr 15 06:23:10.296: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 04/15/24 06:23:10.301
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:23:10.334
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:23:10.339
  STEP: Setting up the test @ 04/15/24 06:23:10.352
  STEP: Creating hostNetwork=false pod @ 04/15/24 06:23:10.352
  STEP: Creating hostNetwork=true pod @ 04/15/24 06:23:12.412
  STEP: Running the test @ 04/15/24 06:23:14.455
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 04/15/24 06:23:14.455
  Apr 15 06:23:14.455: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8313 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 06:23:14.455: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 06:23:14.459: INFO: ExecWithOptions: Clientset creation
  Apr 15 06:23:14.460: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8313/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 15 06:23:14.621: INFO: Exec stderr: ""
  Apr 15 06:23:14.621: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8313 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 06:23:14.621: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 06:23:14.622: INFO: ExecWithOptions: Clientset creation
  Apr 15 06:23:14.622: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8313/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 15 06:23:14.744: INFO: Exec stderr: ""
  Apr 15 06:23:14.744: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8313 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 06:23:14.745: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 06:23:14.747: INFO: ExecWithOptions: Clientset creation
  Apr 15 06:23:14.747: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8313/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 15 06:23:14.842: INFO: Exec stderr: ""
  Apr 15 06:23:14.842: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8313 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 06:23:14.842: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 06:23:14.844: INFO: ExecWithOptions: Clientset creation
  Apr 15 06:23:14.845: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8313/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 15 06:23:14.950: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 04/15/24 06:23:14.95
  Apr 15 06:23:14.950: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8313 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 06:23:14.951: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 06:23:14.953: INFO: ExecWithOptions: Clientset creation
  Apr 15 06:23:14.953: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8313/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Apr 15 06:23:15.078: INFO: Exec stderr: ""
  Apr 15 06:23:15.080: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8313 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 06:23:15.080: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 06:23:15.083: INFO: ExecWithOptions: Clientset creation
  Apr 15 06:23:15.083: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8313/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Apr 15 06:23:15.205: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 04/15/24 06:23:15.206
  Apr 15 06:23:15.206: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8313 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 06:23:15.206: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 06:23:15.210: INFO: ExecWithOptions: Clientset creation
  Apr 15 06:23:15.210: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8313/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 15 06:23:15.465: INFO: Exec stderr: ""
  Apr 15 06:23:15.465: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8313 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 06:23:15.465: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 06:23:15.469: INFO: ExecWithOptions: Clientset creation
  Apr 15 06:23:15.470: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8313/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 15 06:23:15.614: INFO: Exec stderr: ""
  Apr 15 06:23:15.614: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8313 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 06:23:15.614: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 06:23:15.616: INFO: ExecWithOptions: Clientset creation
  Apr 15 06:23:15.616: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8313/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 15 06:23:15.811: INFO: Exec stderr: ""
  Apr 15 06:23:15.811: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8313 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 06:23:15.811: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 06:23:15.813: INFO: ExecWithOptions: Clientset creation
  Apr 15 06:23:15.813: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8313/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 15 06:23:15.994: INFO: Exec stderr: ""
  Apr 15 06:23:15.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-8313" for this suite. @ 04/15/24 06:23:16.005
• [5.726 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:305
  STEP: Creating a kubernetes client @ 04/15/24 06:23:16.026
  Apr 15 06:23:16.026: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename daemonsets @ 04/15/24 06:23:16.031
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:23:16.06
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:23:16.066
  STEP: Creating a simple DaemonSet "daemon-set" @ 04/15/24 06:23:16.133
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/15/24 06:23:16.149
  Apr 15 06:23:16.183: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 06:23:16.184: INFO: Node zaigh3ewotoh-1 is running 0 daemon pod, expected 1
  Apr 15 06:23:17.204: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 15 06:23:17.204: INFO: Node zaigh3ewotoh-1 is running 0 daemon pod, expected 1
  Apr 15 06:23:18.206: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 15 06:23:18.206: INFO: Node zaigh3ewotoh-2 is running 0 daemon pod, expected 1
  Apr 15 06:23:19.204: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 15 06:23:19.205: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 04/15/24 06:23:19.213
  Apr 15 06:23:19.261: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 15 06:23:19.262: INFO: Node zaigh3ewotoh-3 is running 0 daemon pod, expected 1
  Apr 15 06:23:20.285: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 15 06:23:20.285: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 04/15/24 06:23:20.285
  STEP: Deleting DaemonSet "daemon-set" @ 04/15/24 06:23:20.297
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-837, will wait for the garbage collector to delete the pods @ 04/15/24 06:23:20.297
  Apr 15 06:23:20.369: INFO: Deleting DaemonSet.extensions daemon-set took: 14.175734ms
  Apr 15 06:23:20.470: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.888122ms
  Apr 15 06:23:22.276: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 06:23:22.276: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 15 06:23:22.284: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"144684"},"items":null}

  Apr 15 06:23:22.291: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"144684"},"items":null}

  Apr 15 06:23:22.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-837" for this suite. @ 04/15/24 06:23:22.338
• [6.328 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 04/15/24 06:23:22.358
  Apr 15 06:23:22.358: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename namespaces @ 04/15/24 06:23:22.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:23:22.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:23:22.393
  STEP: Read namespace status @ 04/15/24 06:23:22.399
  Apr 15 06:23:22.409: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 04/15/24 06:23:22.409
  Apr 15 06:23:22.425: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 04/15/24 06:23:22.426
  Apr 15 06:23:22.447: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Apr 15 06:23:22.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-661" for this suite. @ 04/15/24 06:23:22.463
• [0.116 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 04/15/24 06:23:22.474
  Apr 15 06:23:22.474: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename containers @ 04/15/24 06:23:22.477
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:23:22.513
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:23:22.519
  STEP: Creating a pod to test override arguments @ 04/15/24 06:23:22.523
  STEP: Saw pod success @ 04/15/24 06:23:26.582
  Apr 15 06:23:26.588: INFO: Trying to get logs from node zaigh3ewotoh-3 pod client-containers-275b7b27-8b63-4ae7-b508-64649844c850 container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 06:23:26.623
  Apr 15 06:23:26.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-4206" for this suite. @ 04/15/24 06:23:26.663
• [4.202 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 04/15/24 06:23:26.681
  Apr 15 06:23:26.681: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename cronjob @ 04/15/24 06:23:26.683
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:23:26.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:23:26.715
  STEP: Creating a suspended cronjob @ 04/15/24 06:23:26.72
  STEP: Ensuring no jobs are scheduled @ 04/15/24 06:23:26.735
  STEP: Ensuring no job exists by listing jobs explicitly @ 04/15/24 06:28:26.75
  STEP: Removing cronjob @ 04/15/24 06:28:26.759
  Apr 15 06:28:26.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8528" for this suite. @ 04/15/24 06:28:26.826
• [300.167 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 04/15/24 06:28:26.856
  Apr 15 06:28:26.856: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename cronjob @ 04/15/24 06:28:26.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:28:26.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:28:26.909
  STEP: Creating a ForbidConcurrent cronjob @ 04/15/24 06:28:26.916
  STEP: Ensuring a job is scheduled @ 04/15/24 06:28:26.93
  STEP: Ensuring exactly one is scheduled @ 04/15/24 06:29:00.94
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 04/15/24 06:29:00.953
  STEP: Ensuring no more jobs are scheduled @ 04/15/24 06:29:00.974
  STEP: Removing cronjob @ 04/15/24 06:34:01.027
  Apr 15 06:34:01.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5104" for this suite. @ 04/15/24 06:34:01.081
• [334.243 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 04/15/24 06:34:01.108
  Apr 15 06:34:01.108: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename webhook @ 04/15/24 06:34:01.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:34:01.16
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:34:01.167
  STEP: Setting up server cert @ 04/15/24 06:34:01.234
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 06:34:02.723
  STEP: Deploying the webhook pod @ 04/15/24 06:34:02.765
  STEP: Wait for the deployment to be ready @ 04/15/24 06:34:02.833
  Apr 15 06:34:02.848: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 04/15/24 06:34:04.876
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 06:34:04.904
  Apr 15 06:34:05.905: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 15 06:34:05.917: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 04/15/24 06:34:06.458
  STEP: Creating a custom resource that should be denied by the webhook @ 04/15/24 06:34:06.512
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 04/15/24 06:34:08.649
  STEP: Updating the custom resource with disallowed data should be denied @ 04/15/24 06:34:08.669
  STEP: Deleting the custom resource should be denied @ 04/15/24 06:34:08.696
  STEP: Remove the offending key and value from the custom resource data @ 04/15/24 06:34:08.717
  STEP: Deleting the updated custom resource should be successful @ 04/15/24 06:34:08.748
  Apr 15 06:34:08.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1867" for this suite. @ 04/15/24 06:34:09.445
  STEP: Destroying namespace "webhook-markers-9412" for this suite. @ 04/15/24 06:34:09.46
• [8.367 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 04/15/24 06:34:09.477
  Apr 15 06:34:09.478: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename namespaces @ 04/15/24 06:34:09.481
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:34:09.509
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:34:09.515
  STEP: Creating a test namespace @ 04/15/24 06:34:09.52
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:34:09.561
  STEP: Creating a pod in the namespace @ 04/15/24 06:34:09.567
  STEP: Waiting for the pod to have running status @ 04/15/24 06:34:09.586
  STEP: Deleting the namespace @ 04/15/24 06:34:11.603
  STEP: Waiting for the namespace to be removed. @ 04/15/24 06:34:11.618
  STEP: Recreating the namespace @ 04/15/24 06:34:22.628
  STEP: Verifying there are no pods in the namespace @ 04/15/24 06:34:22.663
  Apr 15 06:34:22.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1970" for this suite. @ 04/15/24 06:34:22.685
  STEP: Destroying namespace "nsdeletetest-4674" for this suite. @ 04/15/24 06:34:22.699
  Apr 15 06:34:22.709: INFO: Namespace nsdeletetest-4674 was already deleted
  STEP: Destroying namespace "nsdeletetest-2283" for this suite. @ 04/15/24 06:34:22.71
• [13.246 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 04/15/24 06:34:22.73
  Apr 15 06:34:22.730: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename namespaces @ 04/15/24 06:34:22.734
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:34:22.771
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:34:22.777
  STEP: creating a Namespace @ 04/15/24 06:34:22.788
  STEP: patching the Namespace @ 04/15/24 06:34:22.822
  STEP: get the Namespace and ensuring it has the label @ 04/15/24 06:34:22.834
  Apr 15 06:34:22.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8796" for this suite. @ 04/15/24 06:34:22.85
  STEP: Destroying namespace "nspatchtest-1b9db2eb-df9a-485e-a422-6735076cdda3-5590" for this suite. @ 04/15/24 06:34:22.862
• [0.149 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 04/15/24 06:34:22.881
  Apr 15 06:34:22.881: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename certificates @ 04/15/24 06:34:22.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:34:22.919
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:34:22.925
  STEP: getting /apis @ 04/15/24 06:34:23.819
  STEP: getting /apis/certificates.k8s.io @ 04/15/24 06:34:23.831
  STEP: getting /apis/certificates.k8s.io/v1 @ 04/15/24 06:34:23.834
  STEP: creating @ 04/15/24 06:34:23.836
  STEP: getting @ 04/15/24 06:34:23.871
  STEP: listing @ 04/15/24 06:34:23.88
  STEP: watching @ 04/15/24 06:34:23.889
  Apr 15 06:34:23.889: INFO: starting watch
  STEP: patching @ 04/15/24 06:34:23.892
  STEP: updating @ 04/15/24 06:34:23.906
  Apr 15 06:34:23.924: INFO: waiting for watch events with expected annotations
  Apr 15 06:34:23.925: INFO: saw patched and updated annotations
  STEP: getting /approval @ 04/15/24 06:34:23.925
  STEP: patching /approval @ 04/15/24 06:34:23.933
  STEP: updating /approval @ 04/15/24 06:34:23.944
  STEP: getting /status @ 04/15/24 06:34:23.958
  STEP: patching /status @ 04/15/24 06:34:23.965
  STEP: updating /status @ 04/15/24 06:34:23.98
  STEP: deleting @ 04/15/24 06:34:23.996
  STEP: deleting a collection @ 04/15/24 06:34:24.026
  Apr 15 06:34:24.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-8854" for this suite. @ 04/15/24 06:34:24.07
• [1.204 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 04/15/24 06:34:24.09
  Apr 15 06:34:24.090: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 06:34:24.094
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:34:24.127
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:34:24.133
  STEP: Creating configMap with name projected-configmap-test-volume-map-75939a42-1ebe-4c08-8898-596be0e805e0 @ 04/15/24 06:34:24.138
  STEP: Creating a pod to test consume configMaps @ 04/15/24 06:34:24.148
  STEP: Saw pod success @ 04/15/24 06:34:28.209
  Apr 15 06:34:28.222: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-projected-configmaps-8479afba-28e1-4d69-8030-9a312f5fcbe6 container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 06:34:28.288
  Apr 15 06:34:28.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4583" for this suite. @ 04/15/24 06:34:28.341
• [4.264 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 04/15/24 06:34:28.359
  Apr 15 06:34:28.360: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename var-expansion @ 04/15/24 06:34:28.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:34:28.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:34:28.409
  Apr 15 06:34:30.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 06:34:30.477: INFO: Deleting pod "var-expansion-626dd60e-dc2a-417f-a000-cd6c56f7ef7a" in namespace "var-expansion-9730"
  Apr 15 06:34:30.495: INFO: Wait up to 5m0s for pod "var-expansion-626dd60e-dc2a-417f-a000-cd6c56f7ef7a" to be fully deleted
  STEP: Destroying namespace "var-expansion-9730" for this suite. @ 04/15/24 06:34:32.56
• [4.216 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 04/15/24 06:34:32.592
  Apr 15 06:34:32.592: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 06:34:32.594
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:34:32.632
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:34:32.638
  STEP: Creating a pod to test downward api env vars @ 04/15/24 06:34:32.644
  STEP: Saw pod success @ 04/15/24 06:34:36.71
  Apr 15 06:34:36.716: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downward-api-fdb74360-de13-4d64-a9cd-626bbe8e15c5 container dapi-container: <nil>
  STEP: delete the pod @ 04/15/24 06:34:36.732
  Apr 15 06:34:36.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6698" for this suite. @ 04/15/24 06:34:36.782
• [4.205 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 04/15/24 06:34:36.802
  Apr 15 06:34:36.802: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 06:34:36.805
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:34:36.839
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:34:36.848
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 04/15/24 06:34:36.856
  STEP: Saw pod success @ 04/15/24 06:34:40.912
  Apr 15 06:34:40.919: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-5c50d27b-cfe2-4768-80d3-cfe32bed340a container test-container: <nil>
  STEP: delete the pod @ 04/15/24 06:34:40.937
  Apr 15 06:34:40.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7132" for this suite. @ 04/15/24 06:34:40.982
• [4.196 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 04/15/24 06:34:41
  Apr 15 06:34:41.000: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 06:34:41.002
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:34:41.057
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:34:41.062
  STEP: Creating pod liveness-4e81c9f3-09b6-482a-8cb1-bcd25cbc558d in namespace container-probe-550 @ 04/15/24 06:34:41.067
  Apr 15 06:34:43.118: INFO: Started pod liveness-4e81c9f3-09b6-482a-8cb1-bcd25cbc558d in namespace container-probe-550
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/15/24 06:34:43.118
  Apr 15 06:34:43.124: INFO: Initial restart count of pod liveness-4e81c9f3-09b6-482a-8cb1-bcd25cbc558d is 0
  Apr 15 06:35:03.231: INFO: Restart count of pod container-probe-550/liveness-4e81c9f3-09b6-482a-8cb1-bcd25cbc558d is now 1 (20.106082038s elapsed)
  Apr 15 06:35:03.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 06:35:03.245
  STEP: Destroying namespace "container-probe-550" for this suite. @ 04/15/24 06:35:03.276
• [22.303 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 04/15/24 06:35:03.307
  Apr 15 06:35:03.307: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename endpointslicemirroring @ 04/15/24 06:35:03.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:35:03.344
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:35:03.35
  STEP: mirroring a new custom Endpoint @ 04/15/24 06:35:03.381
  Apr 15 06:35:03.417: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 04/15/24 06:35:05.428
  Apr 15 06:35:05.457: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  STEP: mirroring deletion of a custom Endpoint @ 04/15/24 06:35:07.467
  Apr 15 06:35:07.490: INFO: Waiting for 0 EndpointSlices to exist, got 1
  Apr 15 06:35:09.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-8765" for this suite. @ 04/15/24 06:35:09.514
• [6.240 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 04/15/24 06:35:09.558
  Apr 15 06:35:09.558: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename init-container @ 04/15/24 06:35:09.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:35:09.604
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:35:09.612
  STEP: creating the pod @ 04/15/24 06:35:09.62
  Apr 15 06:35:09.621: INFO: PodSpec: initContainers in spec.initContainers
  Apr 15 06:35:14.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3476" for this suite. @ 04/15/24 06:35:14.251
• [4.709 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 04/15/24 06:35:14.264
  Apr 15 06:35:14.264: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename webhook @ 04/15/24 06:35:14.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:35:14.302
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:35:14.308
  STEP: Setting up server cert @ 04/15/24 06:35:14.357
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 06:35:15.465
  STEP: Deploying the webhook pod @ 04/15/24 06:35:15.484
  STEP: Wait for the deployment to be ready @ 04/15/24 06:35:15.516
  Apr 15 06:35:15.543: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/15/24 06:35:17.562
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 06:35:17.583
  Apr 15 06:35:18.584: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 04/15/24 06:35:18.593
  STEP: create a pod @ 04/15/24 06:35:18.635
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 04/15/24 06:35:20.671
  Apr 15 06:35:20.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=webhook-4500 attach --namespace=webhook-4500 to-be-attached-pod -i -c=container1'
  Apr 15 06:35:20.917: INFO: rc: 1
  Apr 15 06:35:20.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4500" for this suite. @ 04/15/24 06:35:21.122
  STEP: Destroying namespace "webhook-markers-8100" for this suite. @ 04/15/24 06:35:21.147
• [6.904 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 04/15/24 06:35:21.173
  Apr 15 06:35:21.174: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename proxy @ 04/15/24 06:35:21.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:35:21.205
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:35:21.211
  Apr 15 06:35:21.215: INFO: Creating pod...
  Apr 15 06:35:23.251: INFO: Creating service...
  Apr 15 06:35:23.274: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6866/pods/agnhost/proxy/some/path/with/DELETE
  Apr 15 06:35:23.294: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 15 06:35:23.294: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6866/pods/agnhost/proxy/some/path/with/GET
  Apr 15 06:35:23.303: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Apr 15 06:35:23.304: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6866/pods/agnhost/proxy/some/path/with/HEAD
  Apr 15 06:35:23.313: INFO: http.Client request:HEAD | StatusCode:200
  Apr 15 06:35:23.313: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6866/pods/agnhost/proxy/some/path/with/OPTIONS
  Apr 15 06:35:23.326: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 15 06:35:23.327: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6866/pods/agnhost/proxy/some/path/with/PATCH
  Apr 15 06:35:23.334: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 15 06:35:23.335: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6866/pods/agnhost/proxy/some/path/with/POST
  Apr 15 06:35:23.343: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 15 06:35:23.343: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6866/pods/agnhost/proxy/some/path/with/PUT
  Apr 15 06:35:23.351: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 15 06:35:23.353: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6866/services/test-service/proxy/some/path/with/DELETE
  Apr 15 06:35:23.367: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 15 06:35:23.368: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6866/services/test-service/proxy/some/path/with/GET
  Apr 15 06:35:23.386: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Apr 15 06:35:23.387: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6866/services/test-service/proxy/some/path/with/HEAD
  Apr 15 06:35:23.402: INFO: http.Client request:HEAD | StatusCode:200
  Apr 15 06:35:23.402: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6866/services/test-service/proxy/some/path/with/OPTIONS
  Apr 15 06:35:23.414: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 15 06:35:23.414: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6866/services/test-service/proxy/some/path/with/PATCH
  Apr 15 06:35:23.425: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 15 06:35:23.425: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6866/services/test-service/proxy/some/path/with/POST
  Apr 15 06:35:23.436: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 15 06:35:23.437: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6866/services/test-service/proxy/some/path/with/PUT
  Apr 15 06:35:23.446: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 15 06:35:23.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-6866" for this suite. @ 04/15/24 06:35:23.464
• [2.308 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 04/15/24 06:35:23.497
  Apr 15 06:35:23.497: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 04/15/24 06:35:23.502
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:35:23.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:35:23.544
  STEP: creating a target pod @ 04/15/24 06:35:23.554
  STEP: adding an ephemeral container @ 04/15/24 06:35:25.603
  STEP: checking pod container endpoints @ 04/15/24 06:35:29.724
  Apr 15 06:35:29.725: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-8904 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 06:35:29.726: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 06:35:29.730: INFO: ExecWithOptions: Clientset creation
  Apr 15 06:35:29.731: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-8904/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Apr 15 06:35:29.855: INFO: Exec stderr: ""
  Apr 15 06:35:29.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-8904" for this suite. @ 04/15/24 06:35:29.889
• [6.405 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 04/15/24 06:35:29.909
  Apr 15 06:35:29.909: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-runtime @ 04/15/24 06:35:29.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:35:29.946
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:35:29.952
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 04/15/24 06:35:29.979
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 04/15/24 06:35:48.182
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 04/15/24 06:35:48.191
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 04/15/24 06:35:48.209
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 04/15/24 06:35:48.209
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 04/15/24 06:35:48.263
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 04/15/24 06:35:52.311
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 04/15/24 06:35:53.329
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 04/15/24 06:35:53.361
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 04/15/24 06:35:53.361
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 04/15/24 06:35:53.452
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 04/15/24 06:35:54.469
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 04/15/24 06:35:56.522
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 04/15/24 06:35:56.546
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 04/15/24 06:35:56.547
  Apr 15 06:35:56.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3472" for this suite. @ 04/15/24 06:35:56.629
• [26.734 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 04/15/24 06:35:56.646
  Apr 15 06:35:56.646: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 06:35:56.649
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:35:56.679
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:35:56.685
  STEP: creating all guestbook components @ 04/15/24 06:35:56.691
  Apr 15 06:35:56.692: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Apr 15 06:35:56.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-5694 create -f -'
  Apr 15 06:35:58.851: INFO: stderr: ""
  Apr 15 06:35:58.851: INFO: stdout: "service/agnhost-replica created\n"
  Apr 15 06:35:58.851: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Apr 15 06:35:58.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-5694 create -f -'
  Apr 15 06:35:59.590: INFO: stderr: ""
  Apr 15 06:35:59.590: INFO: stdout: "service/agnhost-primary created\n"
  Apr 15 06:35:59.592: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Apr 15 06:35:59.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-5694 create -f -'
  Apr 15 06:36:00.272: INFO: stderr: ""
  Apr 15 06:36:00.272: INFO: stdout: "service/frontend created\n"
  Apr 15 06:36:00.273: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.47
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Apr 15 06:36:00.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-5694 create -f -'
  Apr 15 06:36:00.903: INFO: stderr: ""
  Apr 15 06:36:00.903: INFO: stdout: "deployment.apps/frontend created\n"
  Apr 15 06:36:00.904: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.47
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Apr 15 06:36:00.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-5694 create -f -'
  Apr 15 06:36:01.899: INFO: stderr: ""
  Apr 15 06:36:01.899: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Apr 15 06:36:01.899: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.47
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Apr 15 06:36:01.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-5694 create -f -'
  Apr 15 06:36:02.862: INFO: stderr: ""
  Apr 15 06:36:02.862: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 04/15/24 06:36:02.862
  Apr 15 06:36:02.863: INFO: Waiting for all frontend pods to be Running.
  Apr 15 06:36:07.914: INFO: Waiting for frontend to serve content.
  Apr 15 06:36:07.955: INFO: Trying to add a new entry to the guestbook.
  Apr 15 06:36:07.994: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 04/15/24 06:36:08.025
  Apr 15 06:36:08.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-5694 delete --grace-period=0 --force -f -'
  Apr 15 06:36:08.260: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 15 06:36:08.261: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 04/15/24 06:36:08.261
  Apr 15 06:36:08.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-5694 delete --grace-period=0 --force -f -'
  Apr 15 06:36:08.526: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 15 06:36:08.526: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 04/15/24 06:36:08.526
  Apr 15 06:36:08.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-5694 delete --grace-period=0 --force -f -'
  Apr 15 06:36:08.740: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 15 06:36:08.740: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 04/15/24 06:36:08.74
  Apr 15 06:36:08.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-5694 delete --grace-period=0 --force -f -'
  Apr 15 06:36:08.896: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 15 06:36:08.897: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 04/15/24 06:36:08.897
  Apr 15 06:36:08.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-5694 delete --grace-period=0 --force -f -'
  Apr 15 06:36:09.172: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 15 06:36:09.172: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 04/15/24 06:36:09.172
  Apr 15 06:36:09.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-5694 delete --grace-period=0 --force -f -'
  Apr 15 06:36:09.427: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 15 06:36:09.427: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Apr 15 06:36:09.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5694" for this suite. @ 04/15/24 06:36:09.446
• [12.826 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 04/15/24 06:36:09.473
  Apr 15 06:36:09.473: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 06:36:09.476
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:36:09.562
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:36:09.57
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 06:36:09.581
  STEP: Saw pod success @ 04/15/24 06:36:13.683
  Apr 15 06:36:13.688: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downwardapi-volume-bf2c1ebf-fb4c-418c-a94a-5dcd3159ea1c container client-container: <nil>
  STEP: delete the pod @ 04/15/24 06:36:13.705
  Apr 15 06:36:13.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3155" for this suite. @ 04/15/24 06:36:13.75
• [4.289 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 04/15/24 06:36:13.765
  Apr 15 06:36:13.765: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename sched-pred @ 04/15/24 06:36:13.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:36:13.794
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:36:13.804
  Apr 15 06:36:13.809: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 15 06:36:13.832: INFO: Waiting for terminating namespaces to be deleted...
  Apr 15 06:36:13.841: INFO: 
  Logging pods the apiserver thinks is on node zaigh3ewotoh-1 before test
  Apr 15 06:36:13.864: INFO: coredns-5d78c9869d-t6h7j from kube-system started at 2024-04-15 06:16:15 +0000 UTC (1 container statuses recorded)
  Apr 15 06:36:13.864: INFO: 	Container coredns ready: true, restart count 0
  Apr 15 06:36:13.864: INFO: kube-addon-manager-zaigh3ewotoh-1 from kube-system started at 2024-04-15 06:11:57 +0000 UTC (1 container statuses recorded)
  Apr 15 06:36:13.864: INFO: 	Container kube-addon-manager ready: true, restart count 4
  Apr 15 06:36:13.864: INFO: kube-apiserver-zaigh3ewotoh-1 from kube-system started at 2024-04-15 06:11:57 +0000 UTC (1 container statuses recorded)
  Apr 15 06:36:13.864: INFO: 	Container kube-apiserver ready: true, restart count 4
  Apr 15 06:36:13.864: INFO: kube-controller-manager-zaigh3ewotoh-1 from kube-system started at 2024-04-15 06:11:57 +0000 UTC (1 container statuses recorded)
  Apr 15 06:36:13.864: INFO: 	Container kube-controller-manager ready: true, restart count 4
  Apr 15 06:36:13.864: INFO: kube-flannel-ds-wwbkw from kube-system started at 2024-04-15 06:16:17 +0000 UTC (1 container statuses recorded)
  Apr 15 06:36:13.864: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 06:36:13.864: INFO: kube-proxy-v9dxk from kube-system started at 2024-04-15 06:16:16 +0000 UTC (1 container statuses recorded)
  Apr 15 06:36:13.864: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 06:36:13.864: INFO: kube-scheduler-zaigh3ewotoh-1 from kube-system started at 2024-04-15 06:11:57 +0000 UTC (1 container statuses recorded)
  Apr 15 06:36:13.864: INFO: 	Container kube-scheduler ready: true, restart count 4
  Apr 15 06:36:13.864: INFO: sonobuoy-systemd-logs-daemon-set-409cd1623c554ca6-k6p5l from sonobuoy started at 2024-04-15 06:16:23 +0000 UTC (2 container statuses recorded)
  Apr 15 06:36:13.864: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 06:36:13.864: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 15 06:36:13.864: INFO: 
  Logging pods the apiserver thinks is on node zaigh3ewotoh-2 before test
  Apr 15 06:36:13.881: INFO: kube-addon-manager-zaigh3ewotoh-2 from kube-system started at 2024-04-15 06:07:13 +0000 UTC (1 container statuses recorded)
  Apr 15 06:36:13.882: INFO: 	Container kube-addon-manager ready: true, restart count 4
  Apr 15 06:36:13.883: INFO: kube-apiserver-zaigh3ewotoh-2 from kube-system started at 2024-04-15 06:07:13 +0000 UTC (1 container statuses recorded)
  Apr 15 06:36:13.883: INFO: 	Container kube-apiserver ready: true, restart count 4
  Apr 15 06:36:13.884: INFO: kube-controller-manager-zaigh3ewotoh-2 from kube-system started at 2024-04-15 06:07:13 +0000 UTC (1 container statuses recorded)
  Apr 15 06:36:13.884: INFO: 	Container kube-controller-manager ready: true, restart count 4
  Apr 15 06:36:13.885: INFO: kube-flannel-ds-gs792 from kube-system started at 2024-04-15 06:16:17 +0000 UTC (1 container statuses recorded)
  Apr 15 06:36:13.885: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 06:36:13.886: INFO: kube-proxy-sqtk9 from kube-system started at 2024-04-15 06:16:16 +0000 UTC (1 container statuses recorded)
  Apr 15 06:36:13.886: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 06:36:13.887: INFO: kube-scheduler-zaigh3ewotoh-2 from kube-system started at 2024-04-15 06:07:13 +0000 UTC (1 container statuses recorded)
  Apr 15 06:36:13.887: INFO: 	Container kube-scheduler ready: true, restart count 4
  Apr 15 06:36:13.888: INFO: sonobuoy-systemd-logs-daemon-set-409cd1623c554ca6-pp7qg from sonobuoy started at 2024-04-15 06:16:23 +0000 UTC (2 container statuses recorded)
  Apr 15 06:36:13.888: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 06:36:13.889: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 15 06:36:13.890: INFO: 
  Logging pods the apiserver thinks is on node zaigh3ewotoh-3 before test
  Apr 15 06:36:13.908: INFO: coredns-5d78c9869d-4w296 from kube-system started at 2024-04-15 06:16:15 +0000 UTC (1 container statuses recorded)
  Apr 15 06:36:13.908: INFO: 	Container coredns ready: true, restart count 0
  Apr 15 06:36:13.908: INFO: kube-flannel-ds-6xxbt from kube-system started at 2024-04-15 06:16:17 +0000 UTC (1 container statuses recorded)
  Apr 15 06:36:13.908: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 06:36:13.909: INFO: kube-proxy-mxvh9 from kube-system started at 2024-04-15 06:16:16 +0000 UTC (1 container statuses recorded)
  Apr 15 06:36:13.909: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 06:36:13.909: INFO: sonobuoy from sonobuoy started at 2024-04-15 06:16:22 +0000 UTC (1 container statuses recorded)
  Apr 15 06:36:13.909: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 15 06:36:13.910: INFO: sonobuoy-e2e-job-7bb47b0523524b58 from sonobuoy started at 2024-04-15 06:16:23 +0000 UTC (2 container statuses recorded)
  Apr 15 06:36:13.910: INFO: 	Container e2e ready: true, restart count 0
  Apr 15 06:36:13.910: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 06:36:13.910: INFO: sonobuoy-systemd-logs-daemon-set-409cd1623c554ca6-9fkvf from sonobuoy started at 2024-04-15 06:16:23 +0000 UTC (2 container statuses recorded)
  Apr 15 06:36:13.910: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 06:36:13.910: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/15/24 06:36:13.911
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/15/24 06:36:15.969
  STEP: Trying to apply a random label on the found node. @ 04/15/24 06:36:15.994
  STEP: verifying the node has the label kubernetes.io/e2e-f999fa39-c479-4c63-b6e3-ae9faa5db006 42 @ 04/15/24 06:36:16.018
  STEP: Trying to relaunch the pod, now with labels. @ 04/15/24 06:36:16.033
  STEP: removing the label kubernetes.io/e2e-f999fa39-c479-4c63-b6e3-ae9faa5db006 off the node zaigh3ewotoh-3 @ 04/15/24 06:36:18.09
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-f999fa39-c479-4c63-b6e3-ae9faa5db006 @ 04/15/24 06:36:18.138
  Apr 15 06:36:18.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-3201" for this suite. @ 04/15/24 06:36:18.16
• [4.418 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 04/15/24 06:36:18.187
  Apr 15 06:36:18.193: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename services @ 04/15/24 06:36:18.198
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:36:18.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:36:18.247
  STEP: creating service in namespace services-6964 @ 04/15/24 06:36:18.257
  STEP: creating service affinity-nodeport-transition in namespace services-6964 @ 04/15/24 06:36:18.257
  STEP: creating replication controller affinity-nodeport-transition in namespace services-6964 @ 04/15/24 06:36:18.323
  I0415 06:36:18.354453      13 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-6964, replica count: 3
  I0415 06:36:21.409566      13 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 15 06:36:21.456: INFO: Creating new exec pod
  Apr 15 06:36:24.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-6964 exec execpod-affinity4fw2x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Apr 15 06:36:24.983: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Apr 15 06:36:24.984: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 06:36:24.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-6964 exec execpod-affinity4fw2x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.53.242 80'
  Apr 15 06:36:25.330: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.53.242 80\nConnection to 10.233.53.242 80 port [tcp/http] succeeded!\n"
  Apr 15 06:36:25.330: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 06:36:25.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-6964 exec execpod-affinity4fw2x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.27 31990'
  Apr 15 06:36:25.618: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.27 31990\nConnection to 192.168.121.27 31990 port [tcp/*] succeeded!\n"
  Apr 15 06:36:25.618: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 06:36:25.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-6964 exec execpod-affinity4fw2x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.131 31990'
  Apr 15 06:36:25.879: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.131 31990\nConnection to 192.168.121.131 31990 port [tcp/*] succeeded!\n"
  Apr 15 06:36:25.879: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 06:36:25.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-6964 exec execpod-affinity4fw2x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.74:31990/ ; done'
  Apr 15 06:36:26.401: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n"
  Apr 15 06:36:26.401: INFO: stdout: "\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-cdnb4\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-xpr7d\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-cdnb4\naffinity-nodeport-transition-cdnb4\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-xpr7d\naffinity-nodeport-transition-xpr7d\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-xpr7d\naffinity-nodeport-transition-xpr7d\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-cdnb4"
  Apr 15 06:36:26.401: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.401: INFO: Received response from host: affinity-nodeport-transition-cdnb4
  Apr 15 06:36:26.401: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.401: INFO: Received response from host: affinity-nodeport-transition-xpr7d
  Apr 15 06:36:26.401: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.401: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.401: INFO: Received response from host: affinity-nodeport-transition-cdnb4
  Apr 15 06:36:26.401: INFO: Received response from host: affinity-nodeport-transition-cdnb4
  Apr 15 06:36:26.401: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.401: INFO: Received response from host: affinity-nodeport-transition-xpr7d
  Apr 15 06:36:26.401: INFO: Received response from host: affinity-nodeport-transition-xpr7d
  Apr 15 06:36:26.401: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.401: INFO: Received response from host: affinity-nodeport-transition-xpr7d
  Apr 15 06:36:26.402: INFO: Received response from host: affinity-nodeport-transition-xpr7d
  Apr 15 06:36:26.402: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.402: INFO: Received response from host: affinity-nodeport-transition-cdnb4
  Apr 15 06:36:26.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-6964 exec execpod-affinity4fw2x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.74:31990/ ; done'
  Apr 15 06:36:26.935: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:31990/\n"
  Apr 15 06:36:26.936: INFO: stdout: "\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-5lnft\naffinity-nodeport-transition-5lnft"
  Apr 15 06:36:26.936: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.936: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.936: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.936: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.936: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.936: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.936: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.936: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.936: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.936: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.936: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.936: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.936: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.936: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.936: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.936: INFO: Received response from host: affinity-nodeport-transition-5lnft
  Apr 15 06:36:26.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 06:36:26.951: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6964, will wait for the garbage collector to delete the pods @ 04/15/24 06:36:26.978
  Apr 15 06:36:27.054: INFO: Deleting ReplicationController affinity-nodeport-transition took: 16.34311ms
  Apr 15 06:36:27.156: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.297584ms
  STEP: Destroying namespace "services-6964" for this suite. @ 04/15/24 06:36:29.617
• [11.449 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 04/15/24 06:36:29.642
  Apr 15 06:36:29.642: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename var-expansion @ 04/15/24 06:36:29.652
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:36:29.699
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:36:29.705
  STEP: Creating a pod to test substitution in container's args @ 04/15/24 06:36:29.714
  STEP: Saw pod success @ 04/15/24 06:36:33.774
  Apr 15 06:36:33.780: INFO: Trying to get logs from node zaigh3ewotoh-3 pod var-expansion-e52814ab-142b-4231-9883-46ce6fe5151d container dapi-container: <nil>
  STEP: delete the pod @ 04/15/24 06:36:33.797
  Apr 15 06:36:33.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-20" for this suite. @ 04/15/24 06:36:33.844
• [4.216 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 04/15/24 06:36:33.86
  Apr 15 06:36:33.860: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename podtemplate @ 04/15/24 06:36:33.869
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:36:33.9
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:36:33.905
  STEP: Create set of pod templates @ 04/15/24 06:36:33.912
  Apr 15 06:36:33.927: INFO: created test-podtemplate-1
  Apr 15 06:36:33.936: INFO: created test-podtemplate-2
  Apr 15 06:36:33.947: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 04/15/24 06:36:33.948
  STEP: delete collection of pod templates @ 04/15/24 06:36:33.956
  Apr 15 06:36:33.957: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 04/15/24 06:36:33.997
  Apr 15 06:36:33.997: INFO: requesting list of pod templates to confirm quantity
  Apr 15 06:36:34.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-2156" for this suite. @ 04/15/24 06:36:34.017
• [0.168 seconds]
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 04/15/24 06:36:34.029
  Apr 15 06:36:34.029: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename gc @ 04/15/24 06:36:34.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:36:34.061
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:36:34.068
  STEP: create the rc @ 04/15/24 06:36:34.075
  W0415 06:36:34.086672      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 04/15/24 06:36:39.097
  STEP: wait for all pods to be garbage collected @ 04/15/24 06:36:39.113
  STEP: Gathering metrics @ 04/15/24 06:36:44.132
  Apr 15 06:36:44.400: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 15 06:36:44.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4230" for this suite. @ 04/15/24 06:36:44.421
• [10.407 seconds]
------------------------------
S
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 04/15/24 06:36:44.438
  Apr 15 06:36:44.438: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename cronjob @ 04/15/24 06:36:44.441
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:36:44.477
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:36:44.485
  STEP: Creating a cronjob @ 04/15/24 06:36:44.501
  STEP: creating @ 04/15/24 06:36:44.502
  STEP: getting @ 04/15/24 06:36:44.515
  STEP: listing @ 04/15/24 06:36:44.547
  STEP: watching @ 04/15/24 06:36:44.557
  Apr 15 06:36:44.558: INFO: starting watch
  STEP: cluster-wide listing @ 04/15/24 06:36:44.561
  STEP: cluster-wide watching @ 04/15/24 06:36:44.571
  Apr 15 06:36:44.571: INFO: starting watch
  STEP: patching @ 04/15/24 06:36:44.576
  STEP: updating @ 04/15/24 06:36:44.596
  Apr 15 06:36:44.618: INFO: waiting for watch events with expected annotations
  Apr 15 06:36:44.618: INFO: saw patched and updated annotations
  STEP: patching /status @ 04/15/24 06:36:44.619
  STEP: updating /status @ 04/15/24 06:36:44.635
  STEP: get /status @ 04/15/24 06:36:44.656
  STEP: deleting @ 04/15/24 06:36:44.666
  STEP: deleting a collection @ 04/15/24 06:36:44.71
  Apr 15 06:36:44.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9033" for this suite. @ 04/15/24 06:36:44.751
• [0.337 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 04/15/24 06:36:44.79
  Apr 15 06:36:44.790: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename replicaset @ 04/15/24 06:36:44.793
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:36:44.822
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:36:44.828
  STEP: Create a Replicaset @ 04/15/24 06:36:44.845
  STEP: Verify that the required pods have come up. @ 04/15/24 06:36:44.863
  Apr 15 06:36:44.875: INFO: Pod name sample-pod: Found 0 pods out of 1
  Apr 15 06:36:49.889: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/15/24 06:36:49.889
  STEP: Getting /status @ 04/15/24 06:36:49.889
  Apr 15 06:36:49.901: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 04/15/24 06:36:49.902
  Apr 15 06:36:49.927: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 04/15/24 06:36:49.927
  Apr 15 06:36:49.949: INFO: Observed &ReplicaSet event: ADDED
  Apr 15 06:36:49.950: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 15 06:36:49.951: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 15 06:36:49.951: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 15 06:36:49.951: INFO: Found replicaset test-rs in namespace replicaset-7449 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 15 06:36:49.951: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 04/15/24 06:36:49.952
  Apr 15 06:36:49.952: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 15 06:36:49.972: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 04/15/24 06:36:49.973
  Apr 15 06:36:49.978: INFO: Observed &ReplicaSet event: ADDED
  Apr 15 06:36:49.979: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 15 06:36:49.980: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 15 06:36:49.981: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 15 06:36:49.983: INFO: Observed replicaset test-rs in namespace replicaset-7449 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 15 06:36:49.985: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 15 06:36:49.985: INFO: Found replicaset test-rs in namespace replicaset-7449 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Apr 15 06:36:49.985: INFO: Replicaset test-rs has a patched status
  Apr 15 06:36:49.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7449" for this suite. @ 04/15/24 06:36:50.011
• [5.238 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 04/15/24 06:36:50.03
  Apr 15 06:36:50.031: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 06:36:50.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:36:50.07
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:36:50.077
  STEP: Creating configMap with name projected-configmap-test-volume-c6ccc11c-6a53-43bf-ad02-9d56ca8f01b5 @ 04/15/24 06:36:50.087
  STEP: Creating a pod to test consume configMaps @ 04/15/24 06:36:50.1
  STEP: Saw pod success @ 04/15/24 06:36:54.16
  Apr 15 06:36:54.172: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-projected-configmaps-c5da19f6-d594-422e-a40d-706cf5d08b8c container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 06:36:54.19
  Apr 15 06:36:54.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9287" for this suite. @ 04/15/24 06:36:54.235
• [4.219 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 04/15/24 06:36:54.251
  Apr 15 06:36:54.251: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename gc @ 04/15/24 06:36:54.253
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:36:54.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:36:54.304
  STEP: create the deployment @ 04/15/24 06:36:54.311
  W0415 06:36:54.330522      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 04/15/24 06:36:54.33
  STEP: delete the deployment @ 04/15/24 06:36:54.858
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 04/15/24 06:36:54.885
  STEP: Gathering metrics @ 04/15/24 06:36:55.472
  Apr 15 06:36:55.723: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 15 06:36:55.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8957" for this suite. @ 04/15/24 06:36:55.761
• [1.530 seconds]
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 04/15/24 06:36:55.782
  Apr 15 06:36:55.782: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename sched-preemption @ 04/15/24 06:36:55.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:36:55.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:36:55.833
  Apr 15 06:36:55.885: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 15 06:37:55.974: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 04/15/24 06:37:55.994
  Apr 15 06:37:56.066: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Apr 15 06:37:56.081: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Apr 15 06:37:56.127: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Apr 15 06:37:56.140: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Apr 15 06:37:56.286: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Apr 15 06:37:56.311: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 04/15/24 06:37:56.311
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 04/15/24 06:37:58.379
  Apr 15 06:38:02.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-7226" for this suite. @ 04/15/24 06:38:02.672
• [66.904 seconds]
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 04/15/24 06:38:02.686
  Apr 15 06:38:02.686: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename var-expansion @ 04/15/24 06:38:02.691
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:38:02.726
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:38:02.732
  STEP: creating the pod with failed condition @ 04/15/24 06:38:02.738
  STEP: updating the pod @ 04/15/24 06:40:02.765
  Apr 15 06:40:03.299: INFO: Successfully updated pod "var-expansion-2a2acc73-cff6-47cf-8d9a-bb409bf1f46a"
  STEP: waiting for pod running @ 04/15/24 06:40:03.299
  STEP: deleting the pod gracefully @ 04/15/24 06:40:05.339
  Apr 15 06:40:05.340: INFO: Deleting pod "var-expansion-2a2acc73-cff6-47cf-8d9a-bb409bf1f46a" in namespace "var-expansion-2081"
  Apr 15 06:40:05.362: INFO: Wait up to 5m0s for pod "var-expansion-2a2acc73-cff6-47cf-8d9a-bb409bf1f46a" to be fully deleted
  Apr 15 06:40:37.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2081" for this suite. @ 04/15/24 06:40:37.579
• [154.914 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 04/15/24 06:40:37.609
  Apr 15 06:40:37.609: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename services @ 04/15/24 06:40:37.611
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:40:37.652
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:40:37.661
  STEP: creating service endpoint-test2 in namespace services-4287 @ 04/15/24 06:40:37.666
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4287 to expose endpoints map[] @ 04/15/24 06:40:37.704
  Apr 15 06:40:37.742: INFO: successfully validated that service endpoint-test2 in namespace services-4287 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-4287 @ 04/15/24 06:40:37.742
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4287 to expose endpoints map[pod1:[80]] @ 04/15/24 06:40:39.83
  Apr 15 06:40:39.854: INFO: successfully validated that service endpoint-test2 in namespace services-4287 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 04/15/24 06:40:39.854
  Apr 15 06:40:39.854: INFO: Creating new exec pod
  Apr 15 06:40:42.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-4287 exec execpodf47cl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 15 06:40:43.402: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 15 06:40:43.402: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 06:40:43.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-4287 exec execpodf47cl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.21.223 80'
  Apr 15 06:40:43.701: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.21.223 80\nConnection to 10.233.21.223 80 port [tcp/http] succeeded!\n"
  Apr 15 06:40:43.701: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-4287 @ 04/15/24 06:40:43.701
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4287 to expose endpoints map[pod1:[80] pod2:[80]] @ 04/15/24 06:40:45.75
  Apr 15 06:40:45.790: INFO: successfully validated that service endpoint-test2 in namespace services-4287 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 04/15/24 06:40:45.791
  Apr 15 06:40:46.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-4287 exec execpodf47cl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 15 06:40:47.119: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 15 06:40:47.119: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 06:40:47.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-4287 exec execpodf47cl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.21.223 80'
  Apr 15 06:40:47.374: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 10.233.21.223 80\nConnection to 10.233.21.223 80 port [tcp/http] succeeded!\n"
  Apr 15 06:40:47.375: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-4287 @ 04/15/24 06:40:47.375
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4287 to expose endpoints map[pod2:[80]] @ 04/15/24 06:40:47.432
  Apr 15 06:40:47.477: INFO: successfully validated that service endpoint-test2 in namespace services-4287 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 04/15/24 06:40:47.477
  Apr 15 06:40:48.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-4287 exec execpodf47cl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 15 06:40:48.802: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 15 06:40:48.803: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 06:40:48.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-4287 exec execpodf47cl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.21.223 80'
  Apr 15 06:40:49.146: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.21.223 80\nConnection to 10.233.21.223 80 port [tcp/http] succeeded!\n"
  Apr 15 06:40:49.146: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-4287 @ 04/15/24 06:40:49.147
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4287 to expose endpoints map[] @ 04/15/24 06:40:49.239
  Apr 15 06:40:49.285: INFO: successfully validated that service endpoint-test2 in namespace services-4287 exposes endpoints map[]
  Apr 15 06:40:49.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4287" for this suite. @ 04/15/24 06:40:49.393
• [11.824 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 04/15/24 06:40:49.446
  Apr 15 06:40:49.446: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename namespaces @ 04/15/24 06:40:49.453
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:40:49.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:40:49.536
  STEP: Creating a test namespace @ 04/15/24 06:40:49.543
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:40:49.613
  STEP: Creating a service in the namespace @ 04/15/24 06:40:49.629
  STEP: Deleting the namespace @ 04/15/24 06:40:49.677
  STEP: Waiting for the namespace to be removed. @ 04/15/24 06:40:49.75
  STEP: Recreating the namespace @ 04/15/24 06:40:55.768
  STEP: Verifying there is no service in the namespace @ 04/15/24 06:40:55.813
  Apr 15 06:40:55.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6469" for this suite. @ 04/15/24 06:40:55.845
  STEP: Destroying namespace "nsdeletetest-593" for this suite. @ 04/15/24 06:40:55.874
  Apr 15 06:40:55.883: INFO: Namespace nsdeletetest-593 was already deleted
  STEP: Destroying namespace "nsdeletetest-3" for this suite. @ 04/15/24 06:40:55.884
• [6.465 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 04/15/24 06:40:55.913
  Apr 15 06:40:55.913: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 06:40:55.92
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:40:55.954
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:40:55.964
  STEP: Creating a ResourceQuota with best effort scope @ 04/15/24 06:40:55.972
  STEP: Ensuring ResourceQuota status is calculated @ 04/15/24 06:40:55.985
  STEP: Creating a ResourceQuota with not best effort scope @ 04/15/24 06:40:57.996
  STEP: Ensuring ResourceQuota status is calculated @ 04/15/24 06:40:58.016
  STEP: Creating a best-effort pod @ 04/15/24 06:41:00.029
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 04/15/24 06:41:00.073
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 04/15/24 06:41:02.084
  STEP: Deleting the pod @ 04/15/24 06:41:04.094
  STEP: Ensuring resource quota status released the pod usage @ 04/15/24 06:41:04.124
  STEP: Creating a not best-effort pod @ 04/15/24 06:41:06.133
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 04/15/24 06:41:06.158
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 04/15/24 06:41:08.168
  STEP: Deleting the pod @ 04/15/24 06:41:10.177
  STEP: Ensuring resource quota status released the pod usage @ 04/15/24 06:41:10.21
  Apr 15 06:41:12.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5989" for this suite. @ 04/15/24 06:41:12.236
• [16.344 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 04/15/24 06:41:12.26
  Apr 15 06:41:12.260: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename secrets @ 04/15/24 06:41:12.264
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:41:12.31
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:41:12.318
  STEP: creating a secret @ 04/15/24 06:41:12.325
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 04/15/24 06:41:12.34
  STEP: patching the secret @ 04/15/24 06:41:12.347
  STEP: deleting the secret using a LabelSelector @ 04/15/24 06:41:12.37
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 04/15/24 06:41:12.388
  Apr 15 06:41:12.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5714" for this suite. @ 04/15/24 06:41:12.41
• [0.170 seconds]
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 04/15/24 06:41:12.431
  Apr 15 06:41:12.431: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename services @ 04/15/24 06:41:12.435
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:41:12.469
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:41:12.478
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-5895 @ 04/15/24 06:41:12.488
  STEP: changing the ExternalName service to type=ClusterIP @ 04/15/24 06:41:12.505
  STEP: creating replication controller externalname-service in namespace services-5895 @ 04/15/24 06:41:12.545
  I0415 06:41:12.586961      13 runners.go:194] Created replication controller with name: externalname-service, namespace: services-5895, replica count: 2
  I0415 06:41:15.639831      13 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 15 06:41:15.640: INFO: Creating new exec pod
  Apr 15 06:41:18.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-5895 exec execpod7k2rx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 15 06:41:19.222: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 15 06:41:19.222: INFO: stdout: "externalname-service-xvpvf"
  Apr 15 06:41:19.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-5895 exec execpod7k2rx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.43.132 80'
  Apr 15 06:41:19.548: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.43.132 80\nConnection to 10.233.43.132 80 port [tcp/http] succeeded!\n"
  Apr 15 06:41:19.548: INFO: stdout: "externalname-service-xvpvf"
  Apr 15 06:41:19.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 06:41:19.561: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-5895" for this suite. @ 04/15/24 06:41:19.615
• [7.202 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 04/15/24 06:41:19.643
  Apr 15 06:41:19.644: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 06:41:19.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:41:19.677
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:41:19.688
  STEP: Creating projection with secret that has name projected-secret-test-map-bdf643e8-d4ef-4fec-b74b-49a732684c22 @ 04/15/24 06:41:19.7
  STEP: Creating a pod to test consume secrets @ 04/15/24 06:41:19.712
  STEP: Saw pod success @ 04/15/24 06:41:23.778
  Apr 15 06:41:23.787: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-projected-secrets-6dbcc502-a4a8-4369-a222-240bd59e2c51 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 06:41:23.849
  Apr 15 06:41:23.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7835" for this suite. @ 04/15/24 06:41:23.904
• [4.283 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 04/15/24 06:41:23.932
  Apr 15 06:41:23.932: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 06:41:23.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:41:23.973
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:41:24.013
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 04/15/24 06:41:24.022
  STEP: Saw pod success @ 04/15/24 06:41:28.066
  Apr 15 06:41:28.073: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-44217797-aff0-4d82-bca1-b32d29ac0f4e container test-container: <nil>
  STEP: delete the pod @ 04/15/24 06:41:28.092
  Apr 15 06:41:28.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7809" for this suite. @ 04/15/24 06:41:28.135
• [4.222 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 04/15/24 06:41:28.157
  Apr 15 06:41:28.157: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 06:41:28.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:41:28.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:41:28.209
  STEP: creating Agnhost RC @ 04/15/24 06:41:28.218
  Apr 15 06:41:28.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-9891 create -f -'
  Apr 15 06:41:29.153: INFO: stderr: ""
  Apr 15 06:41:29.153: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/15/24 06:41:29.153
  Apr 15 06:41:30.163: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 06:41:30.163: INFO: Found 0 / 1
  Apr 15 06:41:31.162: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 06:41:31.162: INFO: Found 1 / 1
  Apr 15 06:41:31.162: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 04/15/24 06:41:31.163
  Apr 15 06:41:31.171: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 06:41:31.172: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 15 06:41:31.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-9891 patch pod agnhost-primary-456mq -p {"metadata":{"annotations":{"x":"y"}}}'
  Apr 15 06:41:31.358: INFO: stderr: ""
  Apr 15 06:41:31.358: INFO: stdout: "pod/agnhost-primary-456mq patched\n"
  STEP: checking annotations @ 04/15/24 06:41:31.358
  Apr 15 06:41:31.367: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 06:41:31.367: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 15 06:41:31.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9891" for this suite. @ 04/15/24 06:41:31.383
• [3.242 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 04/15/24 06:41:31.4
  Apr 15 06:41:31.400: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename configmap @ 04/15/24 06:41:31.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:41:31.431
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:41:31.436
  STEP: Creating configMap with name configmap-test-volume-map-828fb98e-a58b-48b2-a5f1-eb75898f8268 @ 04/15/24 06:41:31.442
  STEP: Creating a pod to test consume configMaps @ 04/15/24 06:41:31.455
  STEP: Saw pod success @ 04/15/24 06:41:35.506
  Apr 15 06:41:35.515: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-configmaps-b04156e5-9403-4a1e-8fb4-4500cc4f095d container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 06:41:35.537
  Apr 15 06:41:35.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1633" for this suite. @ 04/15/24 06:41:35.596
• [4.217 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 04/15/24 06:41:35.621
  Apr 15 06:41:35.621: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename services @ 04/15/24 06:41:35.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:41:35.681
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:41:35.697
  STEP: creating service in namespace services-5534 @ 04/15/24 06:41:35.713
  STEP: creating service affinity-clusterip in namespace services-5534 @ 04/15/24 06:41:35.713
  STEP: creating replication controller affinity-clusterip in namespace services-5534 @ 04/15/24 06:41:35.745
  I0415 06:41:35.777980      13 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-5534, replica count: 3
  I0415 06:41:38.830003      13 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 15 06:41:38.853: INFO: Creating new exec pod
  Apr 15 06:41:41.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-5534 exec execpod-affinitywhks5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Apr 15 06:41:42.575: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Apr 15 06:41:42.575: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 06:41:42.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-5534 exec execpod-affinitywhks5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.21.214 80'
  Apr 15 06:41:42.948: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.21.214 80\nConnection to 10.233.21.214 80 port [tcp/http] succeeded!\n"
  Apr 15 06:41:42.949: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 06:41:42.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-5534 exec execpod-affinitywhks5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.21.214:80/ ; done'
  Apr 15 06:41:43.582: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.214:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.214:80/\n"
  Apr 15 06:41:43.583: INFO: stdout: "\naffinity-clusterip-xsvxc\naffinity-clusterip-xsvxc\naffinity-clusterip-xsvxc\naffinity-clusterip-xsvxc\naffinity-clusterip-xsvxc\naffinity-clusterip-xsvxc\naffinity-clusterip-xsvxc\naffinity-clusterip-xsvxc\naffinity-clusterip-xsvxc\naffinity-clusterip-xsvxc\naffinity-clusterip-xsvxc\naffinity-clusterip-xsvxc\naffinity-clusterip-xsvxc\naffinity-clusterip-xsvxc\naffinity-clusterip-xsvxc\naffinity-clusterip-xsvxc"
  Apr 15 06:41:43.583: INFO: Received response from host: affinity-clusterip-xsvxc
  Apr 15 06:41:43.583: INFO: Received response from host: affinity-clusterip-xsvxc
  Apr 15 06:41:43.583: INFO: Received response from host: affinity-clusterip-xsvxc
  Apr 15 06:41:43.583: INFO: Received response from host: affinity-clusterip-xsvxc
  Apr 15 06:41:43.583: INFO: Received response from host: affinity-clusterip-xsvxc
  Apr 15 06:41:43.583: INFO: Received response from host: affinity-clusterip-xsvxc
  Apr 15 06:41:43.583: INFO: Received response from host: affinity-clusterip-xsvxc
  Apr 15 06:41:43.583: INFO: Received response from host: affinity-clusterip-xsvxc
  Apr 15 06:41:43.583: INFO: Received response from host: affinity-clusterip-xsvxc
  Apr 15 06:41:43.583: INFO: Received response from host: affinity-clusterip-xsvxc
  Apr 15 06:41:43.583: INFO: Received response from host: affinity-clusterip-xsvxc
  Apr 15 06:41:43.583: INFO: Received response from host: affinity-clusterip-xsvxc
  Apr 15 06:41:43.583: INFO: Received response from host: affinity-clusterip-xsvxc
  Apr 15 06:41:43.583: INFO: Received response from host: affinity-clusterip-xsvxc
  Apr 15 06:41:43.583: INFO: Received response from host: affinity-clusterip-xsvxc
  Apr 15 06:41:43.583: INFO: Received response from host: affinity-clusterip-xsvxc
  Apr 15 06:41:43.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 06:41:43.599: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-5534, will wait for the garbage collector to delete the pods @ 04/15/24 06:41:43.631
  Apr 15 06:41:43.713: INFO: Deleting ReplicationController affinity-clusterip took: 13.831046ms
  Apr 15 06:41:43.814: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.161789ms
  STEP: Destroying namespace "services-5534" for this suite. @ 04/15/24 06:41:46.295
• [10.692 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 04/15/24 06:41:46.357
  Apr 15 06:41:46.357: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename pods @ 04/15/24 06:41:46.366
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:41:46.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:41:46.411
  STEP: creating the pod @ 04/15/24 06:41:46.419
  STEP: submitting the pod to kubernetes @ 04/15/24 06:41:46.419
  STEP: verifying QOS class is set on the pod @ 04/15/24 06:41:46.444
  Apr 15 06:41:46.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1319" for this suite. @ 04/15/24 06:41:46.474
• [0.135 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 04/15/24 06:41:46.496
  Apr 15 06:41:46.501: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 06:41:46.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:41:46.537
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:41:46.543
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 04/15/24 06:41:46.55
  STEP: Saw pod success @ 04/15/24 06:41:50.599
  Apr 15 06:41:50.606: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-4c44d1df-0b45-4025-bb3d-0101c631b3ad container test-container: <nil>
  STEP: delete the pod @ 04/15/24 06:41:50.622
  Apr 15 06:41:50.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1274" for this suite. @ 04/15/24 06:41:50.681
• [4.203 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 04/15/24 06:41:50.703
  Apr 15 06:41:50.703: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 06:41:50.706
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:41:50.749
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:41:50.76
  STEP: Creating configMap with name projected-configmap-test-volume-map-bc5c4fa0-ad61-446b-8d37-f6e20e33f21e @ 04/15/24 06:41:50.768
  STEP: Creating a pod to test consume configMaps @ 04/15/24 06:41:50.81
  STEP: Saw pod success @ 04/15/24 06:41:54.872
  Apr 15 06:41:54.885: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-projected-configmaps-ffecfa72-640b-4f6f-b2e4-6048f4d1b80f container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 06:41:54.905
  Apr 15 06:41:54.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1871" for this suite. @ 04/15/24 06:41:54.963
• [4.276 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 04/15/24 06:41:54.981
  Apr 15 06:41:54.981: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/15/24 06:41:54.984
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:41:55.045
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:41:55.067
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 04/15/24 06:41:55.075
  Apr 15 06:41:55.077: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 06:41:58.001: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 06:42:05.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2815" for this suite. @ 04/15/24 06:42:05.557
• [10.594 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 04/15/24 06:42:05.582
  Apr 15 06:42:05.582: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename init-container @ 04/15/24 06:42:05.584
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:42:05.616
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:42:05.622
  STEP: creating the pod @ 04/15/24 06:42:05.632
  Apr 15 06:42:05.632: INFO: PodSpec: initContainers in spec.initContainers
  Apr 15 06:42:08.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-9517" for this suite. @ 04/15/24 06:42:08.758
• [3.196 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:959
  STEP: Creating a kubernetes client @ 04/15/24 06:42:08.78
  Apr 15 06:42:08.781: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename statefulset @ 04/15/24 06:42:08.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:42:08.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:42:08.881
  STEP: Creating service test in namespace statefulset-9171 @ 04/15/24 06:42:08.89
  Apr 15 06:42:08.944: INFO: Found 0 stateful pods, waiting for 1
  Apr 15 06:42:18.957: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 04/15/24 06:42:18.975
  W0415 06:42:19.007878      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 15 06:42:19.028: INFO: Found 1 stateful pods, waiting for 2
  Apr 15 06:42:29.041: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 06:42:29.041: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 04/15/24 06:42:29.057
  STEP: Delete all of the StatefulSets @ 04/15/24 06:42:29.067
  STEP: Verify that StatefulSets have been deleted @ 04/15/24 06:42:29.093
  Apr 15 06:42:29.129: INFO: Deleting all statefulset in ns statefulset-9171
  Apr 15 06:42:29.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9171" for this suite. @ 04/15/24 06:42:29.252
• [20.542 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 04/15/24 06:42:29.337
  Apr 15 06:42:29.337: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/15/24 06:42:29.341
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:42:29.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:42:29.411
  Apr 15 06:42:29.422: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/15/24 06:42:31.667
  Apr 15 06:42:31.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4926 --namespace=crd-publish-openapi-4926 create -f -'
  Apr 15 06:42:33.617: INFO: stderr: ""
  Apr 15 06:42:33.617: INFO: stdout: "e2e-test-crd-publish-openapi-6571-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Apr 15 06:42:33.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4926 --namespace=crd-publish-openapi-4926 delete e2e-test-crd-publish-openapi-6571-crds test-cr'
  Apr 15 06:42:33.969: INFO: stderr: ""
  Apr 15 06:42:33.970: INFO: stdout: "e2e-test-crd-publish-openapi-6571-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Apr 15 06:42:33.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4926 --namespace=crd-publish-openapi-4926 apply -f -'
  Apr 15 06:42:35.666: INFO: stderr: ""
  Apr 15 06:42:35.666: INFO: stdout: "e2e-test-crd-publish-openapi-6571-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Apr 15 06:42:35.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4926 --namespace=crd-publish-openapi-4926 delete e2e-test-crd-publish-openapi-6571-crds test-cr'
  Apr 15 06:42:35.925: INFO: stderr: ""
  Apr 15 06:42:35.925: INFO: stdout: "e2e-test-crd-publish-openapi-6571-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 04/15/24 06:42:35.926
  Apr 15 06:42:35.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4926 explain e2e-test-crd-publish-openapi-6571-crds'
  Apr 15 06:42:36.604: INFO: stderr: ""
  Apr 15 06:42:36.604: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-6571-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Apr 15 06:42:38.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4926" for this suite. @ 04/15/24 06:42:38.699
• [9.375 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 04/15/24 06:42:38.721
  Apr 15 06:42:38.721: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename job @ 04/15/24 06:42:38.725
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:42:38.768
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:42:38.777
  STEP: Creating a suspended job @ 04/15/24 06:42:38.798
  STEP: Patching the Job @ 04/15/24 06:42:38.821
  STEP: Watching for Job to be patched @ 04/15/24 06:42:38.893
  Apr 15 06:42:38.896: INFO: Event ADDED observed for Job e2e-27fqs in namespace job-6087 with labels: map[e2e-job-label:e2e-27fqs] and annotations: map[batch.kubernetes.io/job-tracking:]
  Apr 15 06:42:38.897: INFO: Event MODIFIED observed for Job e2e-27fqs in namespace job-6087 with labels: map[e2e-job-label:e2e-27fqs] and annotations: map[batch.kubernetes.io/job-tracking:]
  Apr 15 06:42:38.897: INFO: Event MODIFIED found for Job e2e-27fqs in namespace job-6087 with labels: map[e2e-27fqs:patched e2e-job-label:e2e-27fqs] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 04/15/24 06:42:38.898
  STEP: Watching for Job to be updated @ 04/15/24 06:42:38.931
  Apr 15 06:42:38.937: INFO: Event MODIFIED found for Job e2e-27fqs in namespace job-6087 with labels: map[e2e-27fqs:patched e2e-job-label:e2e-27fqs] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 15 06:42:38.937: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 04/15/24 06:42:38.937
  Apr 15 06:42:38.945: INFO: Job: e2e-27fqs as labels: map[e2e-27fqs:patched e2e-job-label:e2e-27fqs]
  STEP: Waiting for job to complete @ 04/15/24 06:42:38.945
  STEP: Delete a job collection with a labelselector @ 04/15/24 06:42:48.961
  STEP: Watching for Job to be deleted @ 04/15/24 06:42:48.989
  Apr 15 06:42:48.998: INFO: Event MODIFIED observed for Job e2e-27fqs in namespace job-6087 with labels: map[e2e-27fqs:patched e2e-job-label:e2e-27fqs] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 15 06:42:48.998: INFO: Event MODIFIED observed for Job e2e-27fqs in namespace job-6087 with labels: map[e2e-27fqs:patched e2e-job-label:e2e-27fqs] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 15 06:42:48.998: INFO: Event MODIFIED observed for Job e2e-27fqs in namespace job-6087 with labels: map[e2e-27fqs:patched e2e-job-label:e2e-27fqs] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 15 06:42:48.999: INFO: Event MODIFIED observed for Job e2e-27fqs in namespace job-6087 with labels: map[e2e-27fqs:patched e2e-job-label:e2e-27fqs] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 15 06:42:48.999: INFO: Event MODIFIED observed for Job e2e-27fqs in namespace job-6087 with labels: map[e2e-27fqs:patched e2e-job-label:e2e-27fqs] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 15 06:42:48.999: INFO: Event DELETED found for Job e2e-27fqs in namespace job-6087 with labels: map[e2e-27fqs:patched e2e-job-label:e2e-27fqs] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 04/15/24 06:42:48.999
  Apr 15 06:42:49.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6087" for this suite. @ 04/15/24 06:42:49.028
• [10.367 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 04/15/24 06:42:49.094
  Apr 15 06:42:49.094: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 06:42:49.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:42:49.145
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:42:49.151
  STEP: Creating configMap with name projected-configmap-test-volume-723efa59-37ad-454f-b3cb-69fa2a98a76a @ 04/15/24 06:42:49.159
  STEP: Creating a pod to test consume configMaps @ 04/15/24 06:42:49.17
  STEP: Saw pod success @ 04/15/24 06:42:53.234
  Apr 15 06:42:53.241: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-projected-configmaps-803126ef-a976-4eb9-bc99-24ce75265158 container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 06:42:53.275
  Apr 15 06:42:53.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9027" for this suite. @ 04/15/24 06:42:53.321
• [4.243 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 04/15/24 06:42:53.341
  Apr 15 06:42:53.342: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename gc @ 04/15/24 06:42:53.344
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:42:53.395
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:42:53.405
  STEP: create the rc @ 04/15/24 06:42:53.44
  W0415 06:42:53.462877      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 04/15/24 06:42:59.522
  STEP: wait for the rc to be deleted @ 04/15/24 06:42:59.603
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 04/15/24 06:43:04.627
  STEP: Gathering metrics @ 04/15/24 06:43:34.682
  Apr 15 06:43:35.088: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 15 06:43:35.092: INFO: Deleting pod "simpletest.rc-24f4t" in namespace "gc-6644"
  Apr 15 06:43:35.124: INFO: Deleting pod "simpletest.rc-2729p" in namespace "gc-6644"
  Apr 15 06:43:35.159: INFO: Deleting pod "simpletest.rc-2kdwm" in namespace "gc-6644"
  Apr 15 06:43:35.297: INFO: Deleting pod "simpletest.rc-2mbf5" in namespace "gc-6644"
  Apr 15 06:43:35.405: INFO: Deleting pod "simpletest.rc-2x7f5" in namespace "gc-6644"
  Apr 15 06:43:35.451: INFO: Deleting pod "simpletest.rc-45xnn" in namespace "gc-6644"
  Apr 15 06:43:35.520: INFO: Deleting pod "simpletest.rc-4b52l" in namespace "gc-6644"
  Apr 15 06:43:35.612: INFO: Deleting pod "simpletest.rc-4bmwl" in namespace "gc-6644"
  Apr 15 06:43:35.695: INFO: Deleting pod "simpletest.rc-4vb4k" in namespace "gc-6644"
  Apr 15 06:43:35.819: INFO: Deleting pod "simpletest.rc-4vq4l" in namespace "gc-6644"
  Apr 15 06:43:35.925: INFO: Deleting pod "simpletest.rc-564p9" in namespace "gc-6644"
  Apr 15 06:43:36.000: INFO: Deleting pod "simpletest.rc-57hc9" in namespace "gc-6644"
  Apr 15 06:43:36.142: INFO: Deleting pod "simpletest.rc-5krs9" in namespace "gc-6644"
  Apr 15 06:43:36.215: INFO: Deleting pod "simpletest.rc-5lscz" in namespace "gc-6644"
  Apr 15 06:43:36.377: INFO: Deleting pod "simpletest.rc-5pk7h" in namespace "gc-6644"
  Apr 15 06:43:36.498: INFO: Deleting pod "simpletest.rc-5sg45" in namespace "gc-6644"
  Apr 15 06:43:36.731: INFO: Deleting pod "simpletest.rc-5xmpm" in namespace "gc-6644"
  Apr 15 06:43:36.895: INFO: Deleting pod "simpletest.rc-5zh87" in namespace "gc-6644"
  Apr 15 06:43:37.043: INFO: Deleting pod "simpletest.rc-64zc7" in namespace "gc-6644"
  Apr 15 06:43:37.278: INFO: Deleting pod "simpletest.rc-67tjx" in namespace "gc-6644"
  Apr 15 06:43:37.353: INFO: Deleting pod "simpletest.rc-6b2vt" in namespace "gc-6644"
  Apr 15 06:43:37.417: INFO: Deleting pod "simpletest.rc-6c6bh" in namespace "gc-6644"
  Apr 15 06:43:37.529: INFO: Deleting pod "simpletest.rc-6grbp" in namespace "gc-6644"
  Apr 15 06:43:37.613: INFO: Deleting pod "simpletest.rc-7rxwx" in namespace "gc-6644"
  Apr 15 06:43:37.670: INFO: Deleting pod "simpletest.rc-7vvfn" in namespace "gc-6644"
  Apr 15 06:43:37.723: INFO: Deleting pod "simpletest.rc-82dx8" in namespace "gc-6644"
  Apr 15 06:43:37.769: INFO: Deleting pod "simpletest.rc-82jw9" in namespace "gc-6644"
  Apr 15 06:43:37.810: INFO: Deleting pod "simpletest.rc-8bs5m" in namespace "gc-6644"
  Apr 15 06:43:37.861: INFO: Deleting pod "simpletest.rc-8t46j" in namespace "gc-6644"
  Apr 15 06:43:37.914: INFO: Deleting pod "simpletest.rc-8t4fs" in namespace "gc-6644"
  Apr 15 06:43:37.978: INFO: Deleting pod "simpletest.rc-b76ff" in namespace "gc-6644"
  Apr 15 06:43:38.074: INFO: Deleting pod "simpletest.rc-bfffw" in namespace "gc-6644"
  Apr 15 06:43:38.152: INFO: Deleting pod "simpletest.rc-bnjn2" in namespace "gc-6644"
  Apr 15 06:43:38.277: INFO: Deleting pod "simpletest.rc-br2b5" in namespace "gc-6644"
  Apr 15 06:43:38.340: INFO: Deleting pod "simpletest.rc-bs277" in namespace "gc-6644"
  Apr 15 06:43:38.402: INFO: Deleting pod "simpletest.rc-bz2s2" in namespace "gc-6644"
  Apr 15 06:43:38.478: INFO: Deleting pod "simpletest.rc-clk4n" in namespace "gc-6644"
  Apr 15 06:43:38.564: INFO: Deleting pod "simpletest.rc-cmgnp" in namespace "gc-6644"
  Apr 15 06:43:38.613: INFO: Deleting pod "simpletest.rc-dmjq6" in namespace "gc-6644"
  Apr 15 06:43:38.678: INFO: Deleting pod "simpletest.rc-gnd5f" in namespace "gc-6644"
  Apr 15 06:43:38.891: INFO: Deleting pod "simpletest.rc-hgkbt" in namespace "gc-6644"
  Apr 15 06:43:39.078: INFO: Deleting pod "simpletest.rc-hmz95" in namespace "gc-6644"
  Apr 15 06:43:39.142: INFO: Deleting pod "simpletest.rc-jw787" in namespace "gc-6644"
  Apr 15 06:43:39.234: INFO: Deleting pod "simpletest.rc-k97mq" in namespace "gc-6644"
  Apr 15 06:43:39.315: INFO: Deleting pod "simpletest.rc-kt9pb" in namespace "gc-6644"
  Apr 15 06:43:39.447: INFO: Deleting pod "simpletest.rc-kvln8" in namespace "gc-6644"
  Apr 15 06:43:39.492: INFO: Deleting pod "simpletest.rc-kz6cn" in namespace "gc-6644"
  Apr 15 06:43:39.574: INFO: Deleting pod "simpletest.rc-l7fwd" in namespace "gc-6644"
  Apr 15 06:43:39.671: INFO: Deleting pod "simpletest.rc-ld7mz" in namespace "gc-6644"
  Apr 15 06:43:39.739: INFO: Deleting pod "simpletest.rc-m4jf5" in namespace "gc-6644"
  Apr 15 06:43:39.807: INFO: Deleting pod "simpletest.rc-mn8sp" in namespace "gc-6644"
  Apr 15 06:43:39.854: INFO: Deleting pod "simpletest.rc-mqmlw" in namespace "gc-6644"
  Apr 15 06:43:39.923: INFO: Deleting pod "simpletest.rc-mtlch" in namespace "gc-6644"
  Apr 15 06:43:39.964: INFO: Deleting pod "simpletest.rc-mv688" in namespace "gc-6644"
  Apr 15 06:43:40.035: INFO: Deleting pod "simpletest.rc-mwjvd" in namespace "gc-6644"
  Apr 15 06:43:40.145: INFO: Deleting pod "simpletest.rc-mzp79" in namespace "gc-6644"
  Apr 15 06:43:40.273: INFO: Deleting pod "simpletest.rc-nbsml" in namespace "gc-6644"
  Apr 15 06:43:40.360: INFO: Deleting pod "simpletest.rc-ngvkq" in namespace "gc-6644"
  Apr 15 06:43:40.460: INFO: Deleting pod "simpletest.rc-nsvn2" in namespace "gc-6644"
  Apr 15 06:43:40.729: INFO: Deleting pod "simpletest.rc-nw926" in namespace "gc-6644"
  Apr 15 06:43:40.821: INFO: Deleting pod "simpletest.rc-nzdw6" in namespace "gc-6644"
  Apr 15 06:43:40.913: INFO: Deleting pod "simpletest.rc-pc8ld" in namespace "gc-6644"
  Apr 15 06:43:41.121: INFO: Deleting pod "simpletest.rc-pcnkr" in namespace "gc-6644"
  Apr 15 06:43:41.261: INFO: Deleting pod "simpletest.rc-q7rmj" in namespace "gc-6644"
  Apr 15 06:43:41.387: INFO: Deleting pod "simpletest.rc-q7sgc" in namespace "gc-6644"
  Apr 15 06:43:41.509: INFO: Deleting pod "simpletest.rc-qfcv4" in namespace "gc-6644"
  Apr 15 06:43:41.579: INFO: Deleting pod "simpletest.rc-qr7qh" in namespace "gc-6644"
  Apr 15 06:43:41.674: INFO: Deleting pod "simpletest.rc-r45jd" in namespace "gc-6644"
  Apr 15 06:43:41.744: INFO: Deleting pod "simpletest.rc-r6xtg" in namespace "gc-6644"
  Apr 15 06:43:41.780: INFO: Deleting pod "simpletest.rc-rclhw" in namespace "gc-6644"
  Apr 15 06:43:41.834: INFO: Deleting pod "simpletest.rc-rgczx" in namespace "gc-6644"
  Apr 15 06:43:41.960: INFO: Deleting pod "simpletest.rc-rpfr9" in namespace "gc-6644"
  Apr 15 06:43:42.052: INFO: Deleting pod "simpletest.rc-rr5bw" in namespace "gc-6644"
  Apr 15 06:43:42.142: INFO: Deleting pod "simpletest.rc-rwph6" in namespace "gc-6644"
  Apr 15 06:43:42.269: INFO: Deleting pod "simpletest.rc-sbvnl" in namespace "gc-6644"
  Apr 15 06:43:42.363: INFO: Deleting pod "simpletest.rc-sfhwd" in namespace "gc-6644"
  Apr 15 06:43:42.467: INFO: Deleting pod "simpletest.rc-smhb9" in namespace "gc-6644"
  Apr 15 06:43:42.522: INFO: Deleting pod "simpletest.rc-spbxh" in namespace "gc-6644"
  Apr 15 06:43:42.652: INFO: Deleting pod "simpletest.rc-sssrw" in namespace "gc-6644"
  Apr 15 06:43:42.753: INFO: Deleting pod "simpletest.rc-sw5hh" in namespace "gc-6644"
  Apr 15 06:43:42.895: INFO: Deleting pod "simpletest.rc-szqlq" in namespace "gc-6644"
  Apr 15 06:43:43.018: INFO: Deleting pod "simpletest.rc-t9sbc" in namespace "gc-6644"
  Apr 15 06:43:43.050: INFO: Deleting pod "simpletest.rc-tlc6v" in namespace "gc-6644"
  Apr 15 06:43:43.098: INFO: Deleting pod "simpletest.rc-tr2nw" in namespace "gc-6644"
  Apr 15 06:43:43.285: INFO: Deleting pod "simpletest.rc-twdlj" in namespace "gc-6644"
  Apr 15 06:43:43.344: INFO: Deleting pod "simpletest.rc-v27vl" in namespace "gc-6644"
  Apr 15 06:43:43.437: INFO: Deleting pod "simpletest.rc-v5zvm" in namespace "gc-6644"
  Apr 15 06:43:43.558: INFO: Deleting pod "simpletest.rc-vfvff" in namespace "gc-6644"
  Apr 15 06:43:43.681: INFO: Deleting pod "simpletest.rc-vtzd5" in namespace "gc-6644"
  Apr 15 06:43:43.773: INFO: Deleting pod "simpletest.rc-w2pnh" in namespace "gc-6644"
  Apr 15 06:43:43.904: INFO: Deleting pod "simpletest.rc-w8npd" in namespace "gc-6644"
  Apr 15 06:43:43.987: INFO: Deleting pod "simpletest.rc-wdc9s" in namespace "gc-6644"
  Apr 15 06:43:44.101: INFO: Deleting pod "simpletest.rc-wxl9j" in namespace "gc-6644"
  Apr 15 06:43:44.178: INFO: Deleting pod "simpletest.rc-wzhl4" in namespace "gc-6644"
  Apr 15 06:43:44.281: INFO: Deleting pod "simpletest.rc-xp5pc" in namespace "gc-6644"
  Apr 15 06:43:44.369: INFO: Deleting pod "simpletest.rc-z2c2h" in namespace "gc-6644"
  Apr 15 06:43:44.464: INFO: Deleting pod "simpletest.rc-zblv7" in namespace "gc-6644"
  Apr 15 06:43:44.663: INFO: Deleting pod "simpletest.rc-zjn7l" in namespace "gc-6644"
  Apr 15 06:43:44.729: INFO: Deleting pod "simpletest.rc-zl5n4" in namespace "gc-6644"
  Apr 15 06:43:44.821: INFO: Deleting pod "simpletest.rc-zmvj7" in namespace "gc-6644"
  Apr 15 06:43:44.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6644" for this suite. @ 04/15/24 06:43:44.904
• [51.612 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 04/15/24 06:43:44.96
  Apr 15 06:43:44.960: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename configmap @ 04/15/24 06:43:44.98
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:43:45.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:43:45.137
  STEP: Creating configMap with name cm-test-opt-del-eb287396-a1f5-4dec-a35b-d48b1025029f @ 04/15/24 06:43:45.174
  STEP: Creating configMap with name cm-test-opt-upd-14b32884-f8b4-444b-b3f6-859187282527 @ 04/15/24 06:43:45.227
  STEP: Creating the pod @ 04/15/24 06:43:45.275
  STEP: Deleting configmap cm-test-opt-del-eb287396-a1f5-4dec-a35b-d48b1025029f @ 04/15/24 06:43:49.456
  STEP: Updating configmap cm-test-opt-upd-14b32884-f8b4-444b-b3f6-859187282527 @ 04/15/24 06:43:49.476
  STEP: Creating configMap with name cm-test-opt-create-b4b3391e-4f9e-43ed-a985-9de2e75f6961 @ 04/15/24 06:43:49.494
  STEP: waiting to observe update in volume @ 04/15/24 06:43:49.507
  Apr 15 06:45:00.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6849" for this suite. @ 04/15/24 06:45:00.422
• [75.493 seconds]
------------------------------
S
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 04/15/24 06:45:00.452
  Apr 15 06:45:00.452: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename events @ 04/15/24 06:45:00.454
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:45:00.532
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:45:00.539
  STEP: creating a test event @ 04/15/24 06:45:00.545
  STEP: listing all events in all namespaces @ 04/15/24 06:45:00.565
  STEP: patching the test event @ 04/15/24 06:45:00.614
  STEP: fetching the test event @ 04/15/24 06:45:00.634
  STEP: updating the test event @ 04/15/24 06:45:00.644
  STEP: getting the test event @ 04/15/24 06:45:00.679
  STEP: deleting the test event @ 04/15/24 06:45:00.689
  STEP: listing all events in all namespaces @ 04/15/24 06:45:00.713
  Apr 15 06:45:00.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-2021" for this suite. @ 04/15/24 06:45:00.792
• [0.361 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 04/15/24 06:45:00.827
  Apr 15 06:45:00.827: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename emptydir-wrapper @ 04/15/24 06:45:00.831
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:45:00.879
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:45:00.886
  STEP: Creating 50 configmaps @ 04/15/24 06:45:00.893
  STEP: Creating RC which spawns configmap-volume pods @ 04/15/24 06:45:01.714
  Apr 15 06:45:01.758: INFO: Pod name wrapped-volume-race-2ae4b3cc-ae12-4cfa-8c39-ea23d8ec9623: Found 0 pods out of 5
  Apr 15 06:45:06.796: INFO: Pod name wrapped-volume-race-2ae4b3cc-ae12-4cfa-8c39-ea23d8ec9623: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/15/24 06:45:06.799
  STEP: Creating RC which spawns configmap-volume pods @ 04/15/24 06:45:08.895
  Apr 15 06:45:08.962: INFO: Pod name wrapped-volume-race-a9cbbb06-70af-4739-8772-5f22d9bd374c: Found 0 pods out of 5
  Apr 15 06:45:14.008: INFO: Pod name wrapped-volume-race-a9cbbb06-70af-4739-8772-5f22d9bd374c: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/15/24 06:45:14.008
  STEP: Creating RC which spawns configmap-volume pods @ 04/15/24 06:45:14.089
  Apr 15 06:45:14.176: INFO: Pod name wrapped-volume-race-2f1a7dd7-9470-4c65-9cd5-341f119c3300: Found 1 pods out of 5
  Apr 15 06:45:19.215: INFO: Pod name wrapped-volume-race-2f1a7dd7-9470-4c65-9cd5-341f119c3300: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/15/24 06:45:19.215
  Apr 15 06:45:19.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-2f1a7dd7-9470-4c65-9cd5-341f119c3300 in namespace emptydir-wrapper-5581, will wait for the garbage collector to delete the pods @ 04/15/24 06:45:19.279
  Apr 15 06:45:19.365: INFO: Deleting ReplicationController wrapped-volume-race-2f1a7dd7-9470-4c65-9cd5-341f119c3300 took: 23.109002ms
  Apr 15 06:45:19.567: INFO: Terminating ReplicationController wrapped-volume-race-2f1a7dd7-9470-4c65-9cd5-341f119c3300 pods took: 201.722121ms
  STEP: deleting ReplicationController wrapped-volume-race-a9cbbb06-70af-4739-8772-5f22d9bd374c in namespace emptydir-wrapper-5581, will wait for the garbage collector to delete the pods @ 04/15/24 06:45:21.368
  Apr 15 06:45:21.455: INFO: Deleting ReplicationController wrapped-volume-race-a9cbbb06-70af-4739-8772-5f22d9bd374c took: 22.247922ms
  Apr 15 06:45:21.556: INFO: Terminating ReplicationController wrapped-volume-race-a9cbbb06-70af-4739-8772-5f22d9bd374c pods took: 100.862697ms
  STEP: deleting ReplicationController wrapped-volume-race-2ae4b3cc-ae12-4cfa-8c39-ea23d8ec9623 in namespace emptydir-wrapper-5581, will wait for the garbage collector to delete the pods @ 04/15/24 06:45:23.457
  Apr 15 06:45:23.535: INFO: Deleting ReplicationController wrapped-volume-race-2ae4b3cc-ae12-4cfa-8c39-ea23d8ec9623 took: 16.777688ms
  Apr 15 06:45:23.736: INFO: Terminating ReplicationController wrapped-volume-race-2ae4b3cc-ae12-4cfa-8c39-ea23d8ec9623 pods took: 200.396755ms
  STEP: Cleaning up the configMaps @ 04/15/24 06:45:26.337
  STEP: Destroying namespace "emptydir-wrapper-5581" for this suite. @ 04/15/24 06:45:26.986
• [26.173 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 04/15/24 06:45:27.002
  Apr 15 06:45:27.002: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 06:45:27.004
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:45:27.035
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:45:27.039
  STEP: Counting existing ResourceQuota @ 04/15/24 06:45:27.043
  STEP: Creating a ResourceQuota @ 04/15/24 06:45:32.049
  STEP: Ensuring resource quota status is calculated @ 04/15/24 06:45:32.063
  STEP: Creating a Service @ 04/15/24 06:45:34.074
  STEP: Creating a NodePort Service @ 04/15/24 06:45:34.12
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 04/15/24 06:45:34.165
  STEP: Ensuring resource quota status captures service creation @ 04/15/24 06:45:34.25
  STEP: Deleting Services @ 04/15/24 06:45:36.263
  STEP: Ensuring resource quota status released usage @ 04/15/24 06:45:36.353
  Apr 15 06:45:38.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6142" for this suite. @ 04/15/24 06:45:38.376
• [11.392 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 04/15/24 06:45:38.413
  Apr 15 06:45:38.414: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename replication-controller @ 04/15/24 06:45:38.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:45:38.451
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:45:38.457
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 04/15/24 06:45:38.463
  STEP: When a replication controller with a matching selector is created @ 04/15/24 06:45:40.507
  STEP: Then the orphan pod is adopted @ 04/15/24 06:45:40.518
  Apr 15 06:45:41.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4376" for this suite. @ 04/15/24 06:45:41.553
• [3.158 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 04/15/24 06:45:41.578
  Apr 15 06:45:41.578: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename webhook @ 04/15/24 06:45:41.582
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:45:41.623
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:45:41.629
  STEP: Setting up server cert @ 04/15/24 06:45:41.686
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 06:45:42.673
  STEP: Deploying the webhook pod @ 04/15/24 06:45:42.7
  STEP: Wait for the deployment to be ready @ 04/15/24 06:45:42.746
  Apr 15 06:45:42.848: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/15/24 06:45:44.868
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 06:45:44.905
  Apr 15 06:45:45.906: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 15 06:45:45.916: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9002-crds.webhook.example.com via the AdmissionRegistration API @ 04/15/24 06:45:46.481
  STEP: Creating a custom resource while v1 is storage version @ 04/15/24 06:45:46.578
  STEP: Patching Custom Resource Definition to set v2 as storage @ 04/15/24 06:45:48.91
  STEP: Patching the custom resource while v2 is storage version @ 04/15/24 06:45:48.976
  Apr 15 06:45:49.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2685" for this suite. @ 04/15/24 06:45:49.913
  STEP: Destroying namespace "webhook-markers-7727" for this suite. @ 04/15/24 06:45:49.931
• [8.368 seconds]
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 04/15/24 06:45:49.95
  Apr 15 06:45:49.950: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename events @ 04/15/24 06:45:49.957
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:45:50.006
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:45:50.011
  STEP: Create set of events @ 04/15/24 06:45:50.018
  STEP: get a list of Events with a label in the current namespace @ 04/15/24 06:45:50.062
  STEP: delete a list of events @ 04/15/24 06:45:50.075
  Apr 15 06:45:50.075: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 04/15/24 06:45:50.156
  Apr 15 06:45:50.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-6279" for this suite. @ 04/15/24 06:45:50.188
• [0.261 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 04/15/24 06:45:50.229
  Apr 15 06:45:50.229: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename secrets @ 04/15/24 06:45:50.232
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:45:50.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:45:50.284
  STEP: creating secret secrets-9960/secret-test-aa0d86a7-4ed5-4019-96f2-2574eb908845 @ 04/15/24 06:45:50.292
  STEP: Creating a pod to test consume secrets @ 04/15/24 06:45:50.302
  STEP: Saw pod success @ 04/15/24 06:45:54.365
  Apr 15 06:45:54.373: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-configmaps-f7d5fa95-513d-41e8-885e-c4d6de400435 container env-test: <nil>
  STEP: delete the pod @ 04/15/24 06:45:54.395
  Apr 15 06:45:54.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9960" for this suite. @ 04/15/24 06:45:54.448
• [4.236 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 04/15/24 06:45:54.466
  Apr 15 06:45:54.466: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename deployment @ 04/15/24 06:45:54.47
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:45:54.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:45:54.538
  STEP: creating a Deployment @ 04/15/24 06:45:54.553
  Apr 15 06:45:54.553: INFO: Creating simple deployment test-deployment-s7wjg
  Apr 15 06:45:54.597: INFO: deployment "test-deployment-s7wjg" doesn't have the required revision set
  STEP: Getting /status @ 04/15/24 06:45:56.636
  Apr 15 06:45:56.646: INFO: Deployment test-deployment-s7wjg has Conditions: [{Available True 2024-04-15 06:45:56 +0000 UTC 2024-04-15 06:45:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2024-04-15 06:45:56 +0000 UTC 2024-04-15 06:45:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s7wjg-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 04/15/24 06:45:56.647
  Apr 15 06:45:56.681: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 6, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 6, 45, 56, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 6, 45, 56, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 6, 45, 54, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-s7wjg-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 04/15/24 06:45:56.682
  Apr 15 06:45:56.690: INFO: Observed &Deployment event: ADDED
  Apr 15 06:45:56.691: INFO: Observed Deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 06:45:54 +0000 UTC 2024-04-15 06:45:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s7wjg-5994cf9475"}
  Apr 15 06:45:56.692: INFO: Observed &Deployment event: MODIFIED
  Apr 15 06:45:56.693: INFO: Observed Deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 06:45:54 +0000 UTC 2024-04-15 06:45:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s7wjg-5994cf9475"}
  Apr 15 06:45:56.694: INFO: Observed Deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-04-15 06:45:54 +0000 UTC 2024-04-15 06:45:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 15 06:45:56.695: INFO: Observed &Deployment event: MODIFIED
  Apr 15 06:45:56.696: INFO: Observed Deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-04-15 06:45:54 +0000 UTC 2024-04-15 06:45:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 15 06:45:56.696: INFO: Observed Deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 06:45:54 +0000 UTC 2024-04-15 06:45:54 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-s7wjg-5994cf9475" is progressing.}
  Apr 15 06:45:56.697: INFO: Observed &Deployment event: MODIFIED
  Apr 15 06:45:56.698: INFO: Observed Deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-04-15 06:45:56 +0000 UTC 2024-04-15 06:45:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 15 06:45:56.698: INFO: Observed Deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 06:45:56 +0000 UTC 2024-04-15 06:45:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s7wjg-5994cf9475" has successfully progressed.}
  Apr 15 06:45:56.700: INFO: Observed &Deployment event: MODIFIED
  Apr 15 06:45:56.701: INFO: Observed Deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-04-15 06:45:56 +0000 UTC 2024-04-15 06:45:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 15 06:45:56.701: INFO: Observed Deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 06:45:56 +0000 UTC 2024-04-15 06:45:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s7wjg-5994cf9475" has successfully progressed.}
  Apr 15 06:45:56.702: INFO: Found Deployment test-deployment-s7wjg in namespace deployment-588 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 15 06:45:56.703: INFO: Deployment test-deployment-s7wjg has an updated status
  STEP: patching the Statefulset Status @ 04/15/24 06:45:56.703
  Apr 15 06:45:56.704: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 15 06:45:56.722: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 04/15/24 06:45:56.723
  Apr 15 06:45:56.730: INFO: Observed &Deployment event: ADDED
  Apr 15 06:45:56.733: INFO: Observed deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 06:45:54 +0000 UTC 2024-04-15 06:45:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s7wjg-5994cf9475"}
  Apr 15 06:45:56.736: INFO: Observed &Deployment event: MODIFIED
  Apr 15 06:45:56.737: INFO: Observed deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 06:45:54 +0000 UTC 2024-04-15 06:45:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s7wjg-5994cf9475"}
  Apr 15 06:45:56.738: INFO: Observed deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-04-15 06:45:54 +0000 UTC 2024-04-15 06:45:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 15 06:45:56.739: INFO: Observed &Deployment event: MODIFIED
  Apr 15 06:45:56.739: INFO: Observed deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-04-15 06:45:54 +0000 UTC 2024-04-15 06:45:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 15 06:45:56.740: INFO: Observed deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 06:45:54 +0000 UTC 2024-04-15 06:45:54 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-s7wjg-5994cf9475" is progressing.}
  Apr 15 06:45:56.742: INFO: Observed &Deployment event: MODIFIED
  Apr 15 06:45:56.742: INFO: Observed deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-04-15 06:45:56 +0000 UTC 2024-04-15 06:45:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 15 06:45:56.743: INFO: Observed deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 06:45:56 +0000 UTC 2024-04-15 06:45:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s7wjg-5994cf9475" has successfully progressed.}
  Apr 15 06:45:56.744: INFO: Observed &Deployment event: MODIFIED
  Apr 15 06:45:56.745: INFO: Observed deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-04-15 06:45:56 +0000 UTC 2024-04-15 06:45:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 15 06:45:56.746: INFO: Observed deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 06:45:56 +0000 UTC 2024-04-15 06:45:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s7wjg-5994cf9475" has successfully progressed.}
  Apr 15 06:45:56.746: INFO: Observed deployment test-deployment-s7wjg in namespace deployment-588 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 15 06:45:56.747: INFO: Observed &Deployment event: MODIFIED
  Apr 15 06:45:56.748: INFO: Found deployment test-deployment-s7wjg in namespace deployment-588 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Apr 15 06:45:56.748: INFO: Deployment test-deployment-s7wjg has a patched status
  Apr 15 06:45:56.762: INFO: Deployment "test-deployment-s7wjg":
  &Deployment{ObjectMeta:{test-deployment-s7wjg  deployment-588  e135741e-b5f1-49ec-8093-23b67c014892 151816 1 2024-04-15 06:45:54 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2024-04-15 06:45:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2024-04-15 06:45:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2024-04-15 06:45:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00396ae88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-s7wjg-5994cf9475",LastUpdateTime:2024-04-15 06:45:56 +0000 UTC,LastTransitionTime:2024-04-15 06:45:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 15 06:45:56.773: INFO: New ReplicaSet "test-deployment-s7wjg-5994cf9475" of Deployment "test-deployment-s7wjg":
  &ReplicaSet{ObjectMeta:{test-deployment-s7wjg-5994cf9475  deployment-588  57c5703b-efa2-46c7-b714-305ae175ec6c 151812 1 2024-04-15 06:45:54 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-s7wjg e135741e-b5f1-49ec-8093-23b67c014892 0xc00396b280 0xc00396b281}] [] [{kube-controller-manager Update apps/v1 2024-04-15 06:45:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e135741e-b5f1-49ec-8093-23b67c014892\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 06:45:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00396b328 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 06:45:56.792: INFO: Pod "test-deployment-s7wjg-5994cf9475-qznh5" is available:
  &Pod{ObjectMeta:{test-deployment-s7wjg-5994cf9475-qznh5 test-deployment-s7wjg-5994cf9475- deployment-588  ff3f2332-699b-48d8-bf3b-c947032b9726 151811 0 2024-04-15 06:45:54 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-s7wjg-5994cf9475 57c5703b-efa2-46c7-b714-305ae175ec6c 0xc0041a9460 0xc0041a9461}] [] [{kube-controller-manager Update v1 2024-04-15 06:45:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"57c5703b-efa2-46c7-b714-305ae175ec6c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 06:45:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5xp6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5xp6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 06:45:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 06:45:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 06:45:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 06:45:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.131,PodIP:10.233.66.45,StartTime:2024-04-15 06:45:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 06:45:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://fe09a529c3fa5ee8fdf10a20236600c580852a3b97e9d62f87021719fa67bb5c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.45,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 06:45:56.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-588" for this suite. @ 04/15/24 06:45:56.81
• [2.367 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 04/15/24 06:45:56.848
  Apr 15 06:45:56.848: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 06:45:56.852
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:45:56.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:45:56.899
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 06:45:56.908
  STEP: Saw pod success @ 04/15/24 06:46:00.973
  Apr 15 06:46:00.982: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downwardapi-volume-a8d05802-46a9-461f-a264-75c6bc4d090d container client-container: <nil>
  STEP: delete the pod @ 04/15/24 06:46:00.997
  Apr 15 06:46:01.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-182" for this suite. @ 04/15/24 06:46:01.092
• [4.274 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 04/15/24 06:46:01.124
  Apr 15 06:46:01.125: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename gc @ 04/15/24 06:46:01.128
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:46:01.171
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:46:01.181
  STEP: create the rc1 @ 04/15/24 06:46:01.2
  STEP: create the rc2 @ 04/15/24 06:46:01.214
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 04/15/24 06:46:07.292
  STEP: delete the rc simpletest-rc-to-be-deleted @ 04/15/24 06:46:10.596
  STEP: wait for the rc to be deleted @ 04/15/24 06:46:10.649
  Apr 15 06:46:15.708: INFO: 71 pods remaining
  Apr 15 06:46:15.709: INFO: 71 pods has nil DeletionTimestamp
  Apr 15 06:46:15.710: INFO: 
  STEP: Gathering metrics @ 04/15/24 06:46:20.746
  Apr 15 06:46:21.330: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 15 06:46:21.330: INFO: Deleting pod "simpletest-rc-to-be-deleted-29bqz" in namespace "gc-6561"
  Apr 15 06:46:21.491: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fh4k" in namespace "gc-6561"
  Apr 15 06:46:21.619: INFO: Deleting pod "simpletest-rc-to-be-deleted-2tjz2" in namespace "gc-6561"
  Apr 15 06:46:21.795: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vflk" in namespace "gc-6561"
  Apr 15 06:46:22.017: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xzx7" in namespace "gc-6561"
  Apr 15 06:46:22.117: INFO: Deleting pod "simpletest-rc-to-be-deleted-459xp" in namespace "gc-6561"
  Apr 15 06:46:22.176: INFO: Deleting pod "simpletest-rc-to-be-deleted-4dwpn" in namespace "gc-6561"
  Apr 15 06:46:22.206: INFO: Deleting pod "simpletest-rc-to-be-deleted-52ch2" in namespace "gc-6561"
  Apr 15 06:46:22.253: INFO: Deleting pod "simpletest-rc-to-be-deleted-556jv" in namespace "gc-6561"
  Apr 15 06:46:22.417: INFO: Deleting pod "simpletest-rc-to-be-deleted-57zmd" in namespace "gc-6561"
  Apr 15 06:46:22.461: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xxnr" in namespace "gc-6561"
  Apr 15 06:46:22.547: INFO: Deleting pod "simpletest-rc-to-be-deleted-64gsz" in namespace "gc-6561"
  Apr 15 06:46:22.613: INFO: Deleting pod "simpletest-rc-to-be-deleted-6cwww" in namespace "gc-6561"
  Apr 15 06:46:22.733: INFO: Deleting pod "simpletest-rc-to-be-deleted-6drmw" in namespace "gc-6561"
  Apr 15 06:46:22.895: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qf76" in namespace "gc-6561"
  Apr 15 06:46:23.043: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qsxl" in namespace "gc-6561"
  Apr 15 06:46:23.204: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tb4q" in namespace "gc-6561"
  Apr 15 06:46:23.318: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vw4q" in namespace "gc-6561"
  Apr 15 06:46:23.404: INFO: Deleting pod "simpletest-rc-to-be-deleted-6xx5k" in namespace "gc-6561"
  Apr 15 06:46:23.547: INFO: Deleting pod "simpletest-rc-to-be-deleted-7grng" in namespace "gc-6561"
  Apr 15 06:46:23.619: INFO: Deleting pod "simpletest-rc-to-be-deleted-7h89s" in namespace "gc-6561"
  Apr 15 06:46:23.688: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jwr4" in namespace "gc-6561"
  Apr 15 06:46:23.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-7sjkb" in namespace "gc-6561"
  Apr 15 06:46:23.976: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wkwt" in namespace "gc-6561"
  Apr 15 06:46:24.078: INFO: Deleting pod "simpletest-rc-to-be-deleted-8glvq" in namespace "gc-6561"
  Apr 15 06:46:24.160: INFO: Deleting pod "simpletest-rc-to-be-deleted-8h7mc" in namespace "gc-6561"
  Apr 15 06:46:24.214: INFO: Deleting pod "simpletest-rc-to-be-deleted-8k9xh" in namespace "gc-6561"
  Apr 15 06:46:24.273: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qsbx" in namespace "gc-6561"
  Apr 15 06:46:24.320: INFO: Deleting pod "simpletest-rc-to-be-deleted-8tbwj" in namespace "gc-6561"
  Apr 15 06:46:24.374: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xqxm" in namespace "gc-6561"
  Apr 15 06:46:24.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xl4x" in namespace "gc-6561"
  Apr 15 06:46:24.475: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2j99" in namespace "gc-6561"
  Apr 15 06:46:24.638: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5dvq" in namespace "gc-6561"
  Apr 15 06:46:24.715: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9xtx" in namespace "gc-6561"
  Apr 15 06:46:24.857: INFO: Deleting pod "simpletest-rc-to-be-deleted-clhsg" in namespace "gc-6561"
  Apr 15 06:46:24.942: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqrnx" in namespace "gc-6561"
  Apr 15 06:46:25.033: INFO: Deleting pod "simpletest-rc-to-be-deleted-czbpj" in namespace "gc-6561"
  Apr 15 06:46:25.225: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7pjt" in namespace "gc-6561"
  Apr 15 06:46:25.338: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtws2" in namespace "gc-6561"
  Apr 15 06:46:25.408: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4wnk" in namespace "gc-6561"
  Apr 15 06:46:25.484: INFO: Deleting pod "simpletest-rc-to-be-deleted-fd2mn" in namespace "gc-6561"
  Apr 15 06:46:25.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsq5s" in namespace "gc-6561"
  Apr 15 06:46:25.602: INFO: Deleting pod "simpletest-rc-to-be-deleted-fw548" in namespace "gc-6561"
  Apr 15 06:46:25.674: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8cts" in namespace "gc-6561"
  Apr 15 06:46:25.766: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggvgh" in namespace "gc-6561"
  Apr 15 06:46:25.873: INFO: Deleting pod "simpletest-rc-to-be-deleted-glnrs" in namespace "gc-6561"
  Apr 15 06:46:25.921: INFO: Deleting pod "simpletest-rc-to-be-deleted-gltws" in namespace "gc-6561"
  Apr 15 06:46:25.975: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjwgb" in namespace "gc-6561"
  Apr 15 06:46:26.052: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6srq" in namespace "gc-6561"
  Apr 15 06:46:26.096: INFO: Deleting pod "simpletest-rc-to-be-deleted-jj24k" in namespace "gc-6561"
  Apr 15 06:46:26.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6561" for this suite. @ 04/15/24 06:46:26.186
• [25.094 seconds]
------------------------------
SSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 04/15/24 06:46:26.221
  Apr 15 06:46:26.222: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename hostport @ 04/15/24 06:46:26.23
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:46:26.301
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:46:26.308
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 04/15/24 06:46:26.334
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.74 on the node which pod1 resides and expect scheduled @ 04/15/24 06:46:30.424
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.74 but use UDP protocol on the node which pod2 resides @ 04/15/24 06:46:42.512
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 04/15/24 06:46:46.619
  Apr 15 06:46:46.619: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.74 http://127.0.0.1:54323/hostname] Namespace:hostport-8539 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 06:46:46.619: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 06:46:46.621: INFO: ExecWithOptions: Clientset creation
  Apr 15 06:46:46.621: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-8539/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.74+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.74, port: 54323 @ 04/15/24 06:46:46.917
  Apr 15 06:46:46.918: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.74:54323/hostname] Namespace:hostport-8539 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 06:46:46.919: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 06:46:46.921: INFO: ExecWithOptions: Clientset creation
  Apr 15 06:46:46.921: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-8539/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.74%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.74, port: 54323 UDP @ 04/15/24 06:46:47.094
  Apr 15 06:46:47.095: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.121.74 54323] Namespace:hostport-8539 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 06:46:47.095: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 06:46:47.098: INFO: ExecWithOptions: Clientset creation
  Apr 15 06:46:47.099: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-8539/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.121.74+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  Apr 15 06:46:52.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-8539" for this suite. @ 04/15/24 06:46:52.248
• [26.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 04/15/24 06:46:52.299
  Apr 15 06:46:52.300: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 06:46:52.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:46:52.375
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:46:52.384
  STEP: create deployment with httpd image @ 04/15/24 06:46:52.39
  Apr 15 06:46:52.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-1379 create -f -'
  Apr 15 06:46:53.431: INFO: stderr: ""
  Apr 15 06:46:53.431: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 04/15/24 06:46:53.431
  Apr 15 06:46:53.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-1379 diff -f -'
  Apr 15 06:46:55.565: INFO: rc: 1
  Apr 15 06:46:55.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-1379 delete -f -'
  Apr 15 06:46:55.758: INFO: stderr: ""
  Apr 15 06:46:55.758: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Apr 15 06:46:55.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1379" for this suite. @ 04/15/24 06:46:55.777
• [3.504 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 04/15/24 06:46:55.804
  Apr 15 06:46:55.804: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename webhook @ 04/15/24 06:46:55.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:46:55.865
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:46:55.87
  STEP: Setting up server cert @ 04/15/24 06:46:55.926
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 06:46:56.505
  STEP: Deploying the webhook pod @ 04/15/24 06:46:56.56
  STEP: Wait for the deployment to be ready @ 04/15/24 06:46:56.59
  Apr 15 06:46:56.635: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/15/24 06:46:58.664
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 06:46:58.695
  Apr 15 06:46:59.698: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 04/15/24 06:46:59.867
  STEP: Creating a configMap that should be mutated @ 04/15/24 06:46:59.908
  STEP: Deleting the collection of validation webhooks @ 04/15/24 06:46:59.996
  STEP: Creating a configMap that should not be mutated @ 04/15/24 06:47:00.217
  Apr 15 06:47:00.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2253" for this suite. @ 04/15/24 06:47:00.444
  STEP: Destroying namespace "webhook-markers-1869" for this suite. @ 04/15/24 06:47:00.46
• [4.708 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 04/15/24 06:47:00.514
  Apr 15 06:47:00.514: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 06:47:00.52
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:47:00.556
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:47:00.577
  STEP: Creating resourceQuota "e2e-rq-status-wv9bd" @ 04/15/24 06:47:00.6
  Apr 15 06:47:00.631: INFO: Resource quota "e2e-rq-status-wv9bd" reports spec: hard cpu limit of 500m
  Apr 15 06:47:00.632: INFO: Resource quota "e2e-rq-status-wv9bd" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-wv9bd" /status @ 04/15/24 06:47:00.632
  STEP: Confirm /status for "e2e-rq-status-wv9bd" resourceQuota via watch @ 04/15/24 06:47:00.663
  Apr 15 06:47:00.669: INFO: observed resourceQuota "e2e-rq-status-wv9bd" in namespace "resourcequota-4570" with hard status: v1.ResourceList(nil)
  Apr 15 06:47:00.670: INFO: Found resourceQuota "e2e-rq-status-wv9bd" in namespace "resourcequota-4570" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Apr 15 06:47:00.670: INFO: ResourceQuota "e2e-rq-status-wv9bd" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 04/15/24 06:47:00.683
  Apr 15 06:47:00.701: INFO: Resource quota "e2e-rq-status-wv9bd" reports spec: hard cpu limit of 1
  Apr 15 06:47:00.702: INFO: Resource quota "e2e-rq-status-wv9bd" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-wv9bd" /status @ 04/15/24 06:47:00.702
  STEP: Confirm /status for "e2e-rq-status-wv9bd" resourceQuota via watch @ 04/15/24 06:47:00.715
  Apr 15 06:47:00.719: INFO: observed resourceQuota "e2e-rq-status-wv9bd" in namespace "resourcequota-4570" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Apr 15 06:47:00.719: INFO: Found resourceQuota "e2e-rq-status-wv9bd" in namespace "resourcequota-4570" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Apr 15 06:47:00.719: INFO: ResourceQuota "e2e-rq-status-wv9bd" /status was patched
  STEP: Get "e2e-rq-status-wv9bd" /status @ 04/15/24 06:47:00.72
  Apr 15 06:47:00.739: INFO: Resourcequota "e2e-rq-status-wv9bd" reports status: hard cpu of 1
  Apr 15 06:47:00.740: INFO: Resourcequota "e2e-rq-status-wv9bd" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-wv9bd" /status before checking Spec is unchanged @ 04/15/24 06:47:00.747
  Apr 15 06:47:00.765: INFO: Resourcequota "e2e-rq-status-wv9bd" reports status: hard cpu of 2
  Apr 15 06:47:00.765: INFO: Resourcequota "e2e-rq-status-wv9bd" reports status: hard memory of 2Gi
  Apr 15 06:47:00.770: INFO: Found resourceQuota "e2e-rq-status-wv9bd" in namespace "resourcequota-4570" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  Apr 15 06:51:25.787: INFO: ResourceQuota "e2e-rq-status-wv9bd" Spec was unchanged and /status reset
  Apr 15 06:51:25.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4570" for this suite. @ 04/15/24 06:51:25.8
• [265.307 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 04/15/24 06:51:25.83
  Apr 15 06:51:25.830: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename disruption @ 04/15/24 06:51:25.832
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:51:25.882
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:51:25.888
  STEP: Waiting for the pdb to be processed @ 04/15/24 06:51:25.914
  STEP: Waiting for all pods to be running @ 04/15/24 06:51:27.994
  Apr 15 06:51:28.016: INFO: running pods: 0 < 3
  Apr 15 06:51:30.028: INFO: running pods: 0 < 3
  Apr 15 06:51:32.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9301" for this suite. @ 04/15/24 06:51:32.06
• [6.251 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 04/15/24 06:51:32.085
  Apr 15 06:51:32.085: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename deployment @ 04/15/24 06:51:32.087
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:51:32.137
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:51:32.144
  Apr 15 06:51:32.155: INFO: Creating simple deployment test-new-deployment
  Apr 15 06:51:32.198: INFO: deployment "test-new-deployment" doesn't have the required revision set
  STEP: getting scale subresource @ 04/15/24 06:51:34.241
  STEP: updating a scale subresource @ 04/15/24 06:51:34.251
  STEP: verifying the deployment Spec.Replicas was modified @ 04/15/24 06:51:34.27
  STEP: Patch a scale subresource @ 04/15/24 06:51:34.281
  Apr 15 06:51:34.352: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-5363  64244d2f-5e85-4f01-ab45-5dc5d842b1b4 154470 3 2024-04-15 06:51:32 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2024-04-15 06:51:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 06:51:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041db5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-04-15 06:51:34 +0000 UTC,LastTransitionTime:2024-04-15 06:51:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2024-04-15 06:51:34 +0000 UTC,LastTransitionTime:2024-04-15 06:51:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 15 06:51:34.364: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-5363  92057307-a69a-457e-876e-bc6756d1ef72 154475 2 2024-04-15 06:51:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 64244d2f-5e85-4f01-ab45-5dc5d842b1b4 0xc001d772e7 0xc001d772e8}] [] [{kube-controller-manager Update apps/v1 2024-04-15 06:51:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"64244d2f-5e85-4f01-ab45-5dc5d842b1b4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 06:51:34 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d77378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 06:51:34.379: INFO: Pod "test-new-deployment-67bd4bf6dc-6kt8k" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-6kt8k test-new-deployment-67bd4bf6dc- deployment-5363  afc40db3-79a7-440f-8c44-fe765ac2a472 154476 0 2024-04-15 06:51:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 92057307-a69a-457e-876e-bc6756d1ef72 0xc0041dba27 0xc0041dba28}] [] [{kube-controller-manager Update v1 2024-04-15 06:51:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"92057307-a69a-457e-876e-bc6756d1ef72\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 06:51:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-54kkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-54kkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 06:51:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 06:51:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 06:51:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 06:51:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.131,PodIP:,StartTime:2024-04-15 06:51:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 06:51:34.380: INFO: Pod "test-new-deployment-67bd4bf6dc-dzljv" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-dzljv test-new-deployment-67bd4bf6dc- deployment-5363  7b8a8a80-9b1e-4e79-9c1e-f603c5355594 154461 0 2024-04-15 06:51:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 92057307-a69a-457e-876e-bc6756d1ef72 0xc0041dbbf7 0xc0041dbbf8}] [] [{kube-controller-manager Update v1 2024-04-15 06:51:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"92057307-a69a-457e-876e-bc6756d1ef72\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 06:51:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rc7jr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rc7jr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 06:51:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 06:51:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 06:51:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 06:51:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.27,PodIP:10.233.65.140,StartTime:2024-04-15 06:51:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 06:51:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://bf753d8340b71c016909bc1eb56f0ac8502fcc71ee4678a7b5f6300c7540e824,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.140,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 06:51:34.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5363" for this suite. @ 04/15/24 06:51:34.402
• [2.340 seconds]
------------------------------
S
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 04/15/24 06:51:34.428
  Apr 15 06:51:34.428: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename svc-latency @ 04/15/24 06:51:34.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:51:34.536
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:51:34.543
  Apr 15 06:51:34.550: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-3285 @ 04/15/24 06:51:34.553
  I0415 06:51:34.574690      13 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3285, replica count: 1
  I0415 06:51:35.628281      13 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 15 06:51:35.758: INFO: Created: latency-svc-8mqnd
  Apr 15 06:51:35.779: INFO: Got endpoints: latency-svc-8mqnd [50.169282ms]
  Apr 15 06:51:35.810: INFO: Created: latency-svc-dkjfq
  Apr 15 06:51:35.831: INFO: Got endpoints: latency-svc-dkjfq [50.848292ms]
  Apr 15 06:51:35.852: INFO: Created: latency-svc-2wpg6
  Apr 15 06:51:35.869: INFO: Got endpoints: latency-svc-2wpg6 [88.888834ms]
  Apr 15 06:51:35.870: INFO: Created: latency-svc-d72tk
  Apr 15 06:51:35.879: INFO: Created: latency-svc-9gbbz
  Apr 15 06:51:35.903: INFO: Got endpoints: latency-svc-d72tk [118.752731ms]
  Apr 15 06:51:35.916: INFO: Created: latency-svc-wpn9p
  Apr 15 06:51:35.918: INFO: Got endpoints: latency-svc-9gbbz [131.963071ms]
  Apr 15 06:51:35.946: INFO: Got endpoints: latency-svc-wpn9p [160.391177ms]
  Apr 15 06:51:35.946: INFO: Created: latency-svc-jtlkb
  Apr 15 06:51:35.957: INFO: Created: latency-svc-kpnfd
  Apr 15 06:51:35.976: INFO: Got endpoints: latency-svc-jtlkb [188.7661ms]
  Apr 15 06:51:35.981: INFO: Created: latency-svc-tsx6b
  Apr 15 06:51:35.991: INFO: Created: latency-svc-5xblp
  Apr 15 06:51:36.008: INFO: Got endpoints: latency-svc-kpnfd [221.358393ms]
  Apr 15 06:51:36.025: INFO: Created: latency-svc-xtz9h
  Apr 15 06:51:36.026: INFO: Got endpoints: latency-svc-tsx6b [239.890561ms]
  Apr 15 06:51:36.042: INFO: Created: latency-svc-gg28x
  Apr 15 06:51:36.054: INFO: Got endpoints: latency-svc-5xblp [268.261346ms]
  Apr 15 06:51:36.062: INFO: Got endpoints: latency-svc-xtz9h [272.921899ms]
  Apr 15 06:51:36.091: INFO: Got endpoints: latency-svc-gg28x [303.00141ms]
  Apr 15 06:51:36.360: INFO: Created: latency-svc-z5c6b
  Apr 15 06:51:36.373: INFO: Created: latency-svc-kn8p6
  Apr 15 06:51:36.376: INFO: Created: latency-svc-lgnhm
  Apr 15 06:51:36.377: INFO: Created: latency-svc-khktr
  Apr 15 06:51:36.378: INFO: Created: latency-svc-vdcc8
  Apr 15 06:51:36.379: INFO: Created: latency-svc-whptn
  Apr 15 06:51:36.379: INFO: Created: latency-svc-kn7gn
  Apr 15 06:51:36.380: INFO: Created: latency-svc-lhtrm
  Apr 15 06:51:36.381: INFO: Created: latency-svc-zlr9p
  Apr 15 06:51:36.384: INFO: Created: latency-svc-spz6l
  Apr 15 06:51:36.387: INFO: Created: latency-svc-hswl7
  Apr 15 06:51:36.390: INFO: Created: latency-svc-fs82h
  Apr 15 06:51:36.390: INFO: Created: latency-svc-zw9vf
  Apr 15 06:51:36.390: INFO: Created: latency-svc-xkxvv
  Apr 15 06:51:36.402: INFO: Created: latency-svc-9cv84
  Apr 15 06:51:36.405: INFO: Got endpoints: latency-svc-z5c6b [343.578609ms]
  Apr 15 06:51:36.418: INFO: Got endpoints: latency-svc-kn7gn [548.099585ms]
  Apr 15 06:51:36.436: INFO: Got endpoints: latency-svc-zw9vf [646.380686ms]
  Apr 15 06:51:36.465: INFO: Created: latency-svc-znfm9
  Apr 15 06:51:36.465: INFO: Got endpoints: latency-svc-kn8p6 [373.275532ms]
  Apr 15 06:51:36.499: INFO: Created: latency-svc-h94m2
  Apr 15 06:51:36.518: INFO: Created: latency-svc-fsdm2
  Apr 15 06:51:36.535: INFO: Created: latency-svc-qr4xx
  Apr 15 06:51:36.549: INFO: Got endpoints: latency-svc-spz6l [522.788735ms]
  Apr 15 06:51:36.550: INFO: Got endpoints: latency-svc-fs82h [603.816728ms]
  Apr 15 06:51:36.554: INFO: Got endpoints: latency-svc-zlr9p [765.332772ms]
  Apr 15 06:51:36.575: INFO: Got endpoints: latency-svc-xkxvv [786.221575ms]
  Apr 15 06:51:36.581: INFO: Created: latency-svc-zzc4h
  Apr 15 06:51:36.610: INFO: Created: latency-svc-rjw6b
  Apr 15 06:51:36.623: INFO: Created: latency-svc-8wlhx
  Apr 15 06:51:36.638: INFO: Created: latency-svc-ztlq2
  Apr 15 06:51:36.649: INFO: Got endpoints: latency-svc-lgnhm [859.517617ms]
  Apr 15 06:51:36.649: INFO: Got endpoints: latency-svc-khktr [594.927137ms]
  Apr 15 06:51:36.650: INFO: Got endpoints: latency-svc-whptn [818.172793ms]
  Apr 15 06:51:36.678: INFO: Got endpoints: latency-svc-vdcc8 [774.691256ms]
  Apr 15 06:51:36.683: INFO: Got endpoints: latency-svc-lhtrm [707.115892ms]
  Apr 15 06:51:36.702: INFO: Got endpoints: latency-svc-hswl7 [783.490294ms]
  Apr 15 06:51:36.713: INFO: Created: latency-svc-56vlx
  Apr 15 06:51:36.732: INFO: Created: latency-svc-mz7k2
  Apr 15 06:51:36.739: INFO: Got endpoints: latency-svc-znfm9 [333.791824ms]
  Apr 15 06:51:36.748: INFO: Got endpoints: latency-svc-h94m2 [329.90968ms]
  Apr 15 06:51:36.757: INFO: Created: latency-svc-zxmf9
  Apr 15 06:51:36.771: INFO: Got endpoints: latency-svc-fsdm2 [334.81275ms]
  Apr 15 06:51:36.771: INFO: Got endpoints: latency-svc-9cv84 [762.853065ms]
  Apr 15 06:51:36.782: INFO: Created: latency-svc-4ckmd
  Apr 15 06:51:36.784: INFO: Got endpoints: latency-svc-qr4xx [318.288992ms]
  Apr 15 06:51:36.803: INFO: Got endpoints: latency-svc-zzc4h [253.613946ms]
  Apr 15 06:51:36.810: INFO: Created: latency-svc-z7kfv
  Apr 15 06:51:36.811: INFO: Got endpoints: latency-svc-rjw6b [260.528598ms]
  Apr 15 06:51:36.834: INFO: Got endpoints: latency-svc-8wlhx [279.374958ms]
  Apr 15 06:51:36.839: INFO: Created: latency-svc-pjxs2
  Apr 15 06:51:36.848: INFO: Got endpoints: latency-svc-ztlq2 [272.707003ms]
  Apr 15 06:51:36.864: INFO: Got endpoints: latency-svc-zxmf9 [213.818189ms]
  Apr 15 06:51:36.869: INFO: Created: latency-svc-mxb86
  Apr 15 06:51:36.894: INFO: Got endpoints: latency-svc-mz7k2 [244.259103ms]
  Apr 15 06:51:36.894: INFO: Got endpoints: latency-svc-56vlx [245.09366ms]
  Apr 15 06:51:36.895: INFO: Got endpoints: latency-svc-z7kfv [211.53405ms]
  Apr 15 06:51:36.904: INFO: Got endpoints: latency-svc-4ckmd [225.539296ms]
  Apr 15 06:51:36.905: INFO: Got endpoints: latency-svc-pjxs2 [203.26201ms]
  Apr 15 06:51:36.906: INFO: Created: latency-svc-z6lr7
  Apr 15 06:51:36.919: INFO: Created: latency-svc-hp4d9
  Apr 15 06:51:36.933: INFO: Got endpoints: latency-svc-z6lr7 [184.964764ms]
  Apr 15 06:51:36.935: INFO: Got endpoints: latency-svc-mxb86 [194.330289ms]
  Apr 15 06:51:36.954: INFO: Created: latency-svc-l2qsc
  Apr 15 06:51:36.962: INFO: Got endpoints: latency-svc-hp4d9 [190.564554ms]
  Apr 15 06:51:36.977: INFO: Got endpoints: latency-svc-l2qsc [204.768951ms]
  Apr 15 06:51:36.978: INFO: Created: latency-svc-nbvl5
  Apr 15 06:51:36.996: INFO: Created: latency-svc-j9k6t
  Apr 15 06:51:37.035: INFO: Got endpoints: latency-svc-nbvl5 [248.715757ms]
  Apr 15 06:51:37.056: INFO: Got endpoints: latency-svc-j9k6t [252.77086ms]
  Apr 15 06:51:37.085: INFO: Created: latency-svc-8k6hw
  Apr 15 06:51:37.101: INFO: Got endpoints: latency-svc-8k6hw [289.099385ms]
  Apr 15 06:51:37.113: INFO: Created: latency-svc-jx9tz
  Apr 15 06:51:37.127: INFO: Created: latency-svc-zxcx7
  Apr 15 06:51:37.152: INFO: Got endpoints: latency-svc-zxcx7 [303.891502ms]
  Apr 15 06:51:37.162: INFO: Created: latency-svc-nngzd
  Apr 15 06:51:37.163: INFO: Got endpoints: latency-svc-jx9tz [328.677029ms]
  Apr 15 06:51:37.165: INFO: Created: latency-svc-kmrlt
  Apr 15 06:51:37.188: INFO: Created: latency-svc-4kmbg
  Apr 15 06:51:37.190: INFO: Got endpoints: latency-svc-nngzd [325.944082ms]
  Apr 15 06:51:37.203: INFO: Created: latency-svc-mfjzp
  Apr 15 06:51:37.227: INFO: Created: latency-svc-lsrnl
  Apr 15 06:51:37.228: INFO: Got endpoints: latency-svc-kmrlt [333.960676ms]
  Apr 15 06:51:37.237: INFO: Got endpoints: latency-svc-4kmbg [342.763565ms]
  Apr 15 06:51:37.251: INFO: Got endpoints: latency-svc-mfjzp [355.414839ms]
  Apr 15 06:51:37.251: INFO: Got endpoints: latency-svc-lsrnl [346.991626ms]
  Apr 15 06:51:37.267: INFO: Created: latency-svc-bl2xl
  Apr 15 06:51:37.280: INFO: Created: latency-svc-8d5d5
  Apr 15 06:51:37.313: INFO: Created: latency-svc-g9rfj
  Apr 15 06:51:37.324: INFO: Got endpoints: latency-svc-bl2xl [419.139959ms]
  Apr 15 06:51:37.333: INFO: Created: latency-svc-qzrgm
  Apr 15 06:51:37.334: INFO: Got endpoints: latency-svc-8d5d5 [398.031696ms]
  Apr 15 06:51:37.357: INFO: Created: latency-svc-b9ndm
  Apr 15 06:51:37.367: INFO: Created: latency-svc-8pkkv
  Apr 15 06:51:37.407: INFO: Created: latency-svc-wz6vt
  Apr 15 06:51:37.417: INFO: Got endpoints: latency-svc-g9rfj [482.665801ms]
  Apr 15 06:51:37.417: INFO: Got endpoints: latency-svc-8pkkv [381.360909ms]
  Apr 15 06:51:37.417: INFO: Got endpoints: latency-svc-b9ndm [439.265965ms]
  Apr 15 06:51:37.418: INFO: Got endpoints: latency-svc-qzrgm [454.938318ms]
  Apr 15 06:51:37.446: INFO: Created: latency-svc-q8g2z
  Apr 15 06:51:37.447: INFO: Got endpoints: latency-svc-wz6vt [389.907483ms]
  Apr 15 06:51:37.452: INFO: Created: latency-svc-d4nmt
  Apr 15 06:51:37.477: INFO: Got endpoints: latency-svc-d4nmt [323.893594ms]
  Apr 15 06:51:37.479: INFO: Got endpoints: latency-svc-q8g2z [377.718122ms]
  Apr 15 06:51:37.663: INFO: Created: latency-svc-2vcnb
  Apr 15 06:51:37.669: INFO: Created: latency-svc-7sxj2
  Apr 15 06:51:37.670: INFO: Created: latency-svc-qbfsx
  Apr 15 06:51:37.678: INFO: Created: latency-svc-4pdcc
  Apr 15 06:51:37.698: INFO: Created: latency-svc-pq6kr
  Apr 15 06:51:37.702: INFO: Created: latency-svc-ndt6s
  Apr 15 06:51:37.705: INFO: Created: latency-svc-bzjbj
  Apr 15 06:51:37.709: INFO: Created: latency-svc-rmwpc
  Apr 15 06:51:37.710: INFO: Created: latency-svc-xb9rv
  Apr 15 06:51:37.710: INFO: Created: latency-svc-cs24t
  Apr 15 06:51:37.710: INFO: Created: latency-svc-2sr7n
  Apr 15 06:51:37.712: INFO: Created: latency-svc-b5k88
  Apr 15 06:51:37.713: INFO: Created: latency-svc-vsb9l
  Apr 15 06:51:37.713: INFO: Created: latency-svc-nkp54
  Apr 15 06:51:37.713: INFO: Created: latency-svc-9797j
  Apr 15 06:51:37.730: INFO: Got endpoints: latency-svc-7sxj2 [252.484422ms]
  Apr 15 06:51:37.739: INFO: Got endpoints: latency-svc-9797j [501.910682ms]
  Apr 15 06:51:37.742: INFO: Got endpoints: latency-svc-2vcnb [514.614326ms]
  Apr 15 06:51:37.750: INFO: Got endpoints: latency-svc-rmwpc [425.613397ms]
  Apr 15 06:51:37.768: INFO: Got endpoints: latency-svc-cs24t [516.100357ms]
  Apr 15 06:51:37.781: INFO: Created: latency-svc-bvnk6
  Apr 15 06:51:37.789: INFO: Got endpoints: latency-svc-xb9rv [341.649712ms]
  Apr 15 06:51:37.794: INFO: Created: latency-svc-fspzk
  Apr 15 06:51:37.810: INFO: Created: latency-svc-c9wsb
  Apr 15 06:51:37.820: INFO: Created: latency-svc-44j7r
  Apr 15 06:51:37.835: INFO: Got endpoints: latency-svc-qbfsx [417.496268ms]
  Apr 15 06:51:37.852: INFO: Created: latency-svc-8bdh9
  Apr 15 06:51:37.873: INFO: Created: latency-svc-cjvw2
  Apr 15 06:51:37.873: INFO: Got endpoints: latency-svc-pq6kr [709.232118ms]
  Apr 15 06:51:37.909: INFO: Created: latency-svc-4r8nx
  Apr 15 06:51:37.917: INFO: Created: latency-svc-ntmjt
  Apr 15 06:51:37.937: INFO: Got endpoints: latency-svc-vsb9l [746.665848ms]
  Apr 15 06:51:37.963: INFO: Created: latency-svc-ctdrt
  Apr 15 06:51:37.985: INFO: Got endpoints: latency-svc-4pdcc [568.435565ms]
  Apr 15 06:51:38.017: INFO: Created: latency-svc-x4m8h
  Apr 15 06:51:38.030: INFO: Got endpoints: latency-svc-ndt6s [612.057771ms]
  Apr 15 06:51:38.072: INFO: Created: latency-svc-rnm9n
  Apr 15 06:51:38.087: INFO: Got endpoints: latency-svc-bzjbj [607.630163ms]
  Apr 15 06:51:38.122: INFO: Created: latency-svc-5r64s
  Apr 15 06:51:38.135: INFO: Got endpoints: latency-svc-b5k88 [718.509778ms]
  Apr 15 06:51:38.175: INFO: Created: latency-svc-dh5bf
  Apr 15 06:51:38.178: INFO: Got endpoints: latency-svc-nkp54 [844.47178ms]
  Apr 15 06:51:38.215: INFO: Created: latency-svc-89vsf
  Apr 15 06:51:38.227: INFO: Got endpoints: latency-svc-2sr7n [974.121371ms]
  Apr 15 06:51:38.260: INFO: Created: latency-svc-k4zx4
  Apr 15 06:51:38.281: INFO: Got endpoints: latency-svc-bvnk6 [551.118352ms]
  Apr 15 06:51:38.318: INFO: Created: latency-svc-26cd5
  Apr 15 06:51:38.330: INFO: Got endpoints: latency-svc-fspzk [589.961259ms]
  Apr 15 06:51:38.369: INFO: Created: latency-svc-hs6t2
  Apr 15 06:51:38.375: INFO: Got endpoints: latency-svc-c9wsb [631.943822ms]
  Apr 15 06:51:38.398: INFO: Created: latency-svc-rrm5r
  Apr 15 06:51:38.424: INFO: Got endpoints: latency-svc-44j7r [673.134404ms]
  Apr 15 06:51:38.491: INFO: Created: latency-svc-htmgs
  Apr 15 06:51:38.503: INFO: Got endpoints: latency-svc-8bdh9 [733.258162ms]
  Apr 15 06:51:38.538: INFO: Got endpoints: latency-svc-cjvw2 [748.426184ms]
  Apr 15 06:51:38.563: INFO: Created: latency-svc-4qsfc
  Apr 15 06:51:38.578: INFO: Got endpoints: latency-svc-4r8nx [742.738675ms]
  Apr 15 06:51:38.581: INFO: Created: latency-svc-sk84l
  Apr 15 06:51:38.614: INFO: Created: latency-svc-sn4jz
  Apr 15 06:51:38.628: INFO: Got endpoints: latency-svc-ntmjt [755.332217ms]
  Apr 15 06:51:38.661: INFO: Created: latency-svc-7jc7x
  Apr 15 06:51:38.687: INFO: Got endpoints: latency-svc-ctdrt [749.913949ms]
  Apr 15 06:51:38.713: INFO: Created: latency-svc-75j44
  Apr 15 06:51:38.726: INFO: Got endpoints: latency-svc-x4m8h [740.384252ms]
  Apr 15 06:51:38.785: INFO: Created: latency-svc-8bknj
  Apr 15 06:51:38.803: INFO: Got endpoints: latency-svc-rnm9n [772.924976ms]
  Apr 15 06:51:38.834: INFO: Got endpoints: latency-svc-5r64s [746.669244ms]
  Apr 15 06:51:38.885: INFO: Created: latency-svc-p2mqw
  Apr 15 06:51:38.886: INFO: Got endpoints: latency-svc-dh5bf [745.516137ms]
  Apr 15 06:51:38.939: INFO: Created: latency-svc-l5hth
  Apr 15 06:51:38.941: INFO: Got endpoints: latency-svc-89vsf [761.612128ms]
  Apr 15 06:51:38.978: INFO: Created: latency-svc-pmpp8
  Apr 15 06:51:38.999: INFO: Got endpoints: latency-svc-k4zx4 [771.664827ms]
  Apr 15 06:51:39.028: INFO: Created: latency-svc-t8cx5
  Apr 15 06:51:39.040: INFO: Got endpoints: latency-svc-26cd5 [758.501581ms]
  Apr 15 06:51:39.043: INFO: Created: latency-svc-nt9kv
  Apr 15 06:51:39.085: INFO: Got endpoints: latency-svc-hs6t2 [754.731283ms]
  Apr 15 06:51:39.086: INFO: Created: latency-svc-rm6l5
  Apr 15 06:51:39.115: INFO: Created: latency-svc-4vhh5
  Apr 15 06:51:39.133: INFO: Got endpoints: latency-svc-rrm5r [758.234931ms]
  Apr 15 06:51:39.166: INFO: Created: latency-svc-d84tq
  Apr 15 06:51:39.188: INFO: Got endpoints: latency-svc-htmgs [764.000743ms]
  Apr 15 06:51:39.213: INFO: Created: latency-svc-fr4bg
  Apr 15 06:51:39.224: INFO: Got endpoints: latency-svc-4qsfc [720.415215ms]
  Apr 15 06:51:39.262: INFO: Created: latency-svc-glfcv
  Apr 15 06:51:39.289: INFO: Got endpoints: latency-svc-sk84l [751.158435ms]
  Apr 15 06:51:39.328: INFO: Got endpoints: latency-svc-sn4jz [749.935991ms]
  Apr 15 06:51:39.337: INFO: Created: latency-svc-6j8lk
  Apr 15 06:51:39.381: INFO: Got endpoints: latency-svc-7jc7x [753.128198ms]
  Apr 15 06:51:39.389: INFO: Created: latency-svc-hbzv6
  Apr 15 06:51:39.443: INFO: Got endpoints: latency-svc-75j44 [756.386968ms]
  Apr 15 06:51:39.447: INFO: Created: latency-svc-lsfzm
  Apr 15 06:51:39.480: INFO: Got endpoints: latency-svc-8bknj [752.989614ms]
  Apr 15 06:51:39.537: INFO: Created: latency-svc-864hl
  Apr 15 06:51:39.547: INFO: Got endpoints: latency-svc-p2mqw [743.518875ms]
  Apr 15 06:51:39.575: INFO: Created: latency-svc-rvlmb
  Apr 15 06:51:39.593: INFO: Created: latency-svc-xr7r2
  Apr 15 06:51:39.596: INFO: Got endpoints: latency-svc-l5hth [760.911274ms]
  Apr 15 06:51:39.627: INFO: Got endpoints: latency-svc-pmpp8 [740.152914ms]
  Apr 15 06:51:39.647: INFO: Created: latency-svc-wx22n
  Apr 15 06:51:39.667: INFO: Created: latency-svc-8ltj8
  Apr 15 06:51:39.680: INFO: Got endpoints: latency-svc-t8cx5 [739.372868ms]
  Apr 15 06:51:39.727: INFO: Created: latency-svc-lc9l4
  Apr 15 06:51:39.745: INFO: Got endpoints: latency-svc-nt9kv [746.321051ms]
  Apr 15 06:51:39.790: INFO: Got endpoints: latency-svc-rm6l5 [748.634486ms]
  Apr 15 06:51:39.812: INFO: Created: latency-svc-xgzvd
  Apr 15 06:51:39.863: INFO: Got endpoints: latency-svc-4vhh5 [778.157364ms]
  Apr 15 06:51:39.893: INFO: Got endpoints: latency-svc-d84tq [760.074012ms]
  Apr 15 06:51:39.906: INFO: Created: latency-svc-w4t99
  Apr 15 06:51:39.990: INFO: Got endpoints: latency-svc-fr4bg [802.487739ms]
  Apr 15 06:51:39.992: INFO: Created: latency-svc-n4jtx
  Apr 15 06:51:40.033: INFO: Got endpoints: latency-svc-glfcv [808.967307ms]
  Apr 15 06:51:40.062: INFO: Got endpoints: latency-svc-6j8lk [772.346789ms]
  Apr 15 06:51:40.085: INFO: Created: latency-svc-dz4vv
  Apr 15 06:51:40.120: INFO: Got endpoints: latency-svc-hbzv6 [791.546244ms]
  Apr 15 06:51:40.123: INFO: Created: latency-svc-kj6jv
  Apr 15 06:51:40.148: INFO: Got endpoints: latency-svc-lsfzm [767.048784ms]
  Apr 15 06:51:40.149: INFO: Created: latency-svc-rrpdb
  Apr 15 06:51:40.171: INFO: Created: latency-svc-g4dx7
  Apr 15 06:51:40.202: INFO: Created: latency-svc-8hfrc
  Apr 15 06:51:40.204: INFO: Got endpoints: latency-svc-864hl [760.851436ms]
  Apr 15 06:51:40.212: INFO: Created: latency-svc-dwsht
  Apr 15 06:51:40.232: INFO: Created: latency-svc-lbxgs
  Apr 15 06:51:40.257: INFO: Got endpoints: latency-svc-rvlmb [776.918812ms]
  Apr 15 06:51:40.282: INFO: Got endpoints: latency-svc-xr7r2 [733.972564ms]
  Apr 15 06:51:40.289: INFO: Created: latency-svc-xf8tq
  Apr 15 06:51:40.319: INFO: Created: latency-svc-24x9b
  Apr 15 06:51:40.344: INFO: Got endpoints: latency-svc-wx22n [747.316171ms]
  Apr 15 06:51:40.385: INFO: Created: latency-svc-jqz9r
  Apr 15 06:51:40.395: INFO: Got endpoints: latency-svc-8ltj8 [767.244195ms]
  Apr 15 06:51:40.423: INFO: Created: latency-svc-bsbgv
  Apr 15 06:51:40.443: INFO: Got endpoints: latency-svc-lc9l4 [762.032962ms]
  Apr 15 06:51:40.535: INFO: Got endpoints: latency-svc-xgzvd [790.071683ms]
  Apr 15 06:51:40.556: INFO: Got endpoints: latency-svc-w4t99 [765.188505ms]
  Apr 15 06:51:40.587: INFO: Created: latency-svc-m7hf2
  Apr 15 06:51:40.636: INFO: Got endpoints: latency-svc-n4jtx [772.599949ms]
  Apr 15 06:51:40.698: INFO: Got endpoints: latency-svc-dz4vv [803.386376ms]
  Apr 15 06:51:40.706: INFO: Created: latency-svc-vrr9c
  Apr 15 06:51:40.723: INFO: Got endpoints: latency-svc-kj6jv [732.190782ms]
  Apr 15 06:51:40.748: INFO: Got endpoints: latency-svc-rrpdb [714.216077ms]
  Apr 15 06:51:40.787: INFO: Got endpoints: latency-svc-g4dx7 [725.313269ms]
  Apr 15 06:51:40.804: INFO: Created: latency-svc-dp4ws
  Apr 15 06:51:40.868: INFO: Got endpoints: latency-svc-8hfrc [747.019176ms]
  Apr 15 06:51:40.874: INFO: Created: latency-svc-pf2cq
  Apr 15 06:51:40.898: INFO: Got endpoints: latency-svc-dwsht [748.340121ms]
  Apr 15 06:51:40.947: INFO: Created: latency-svc-5zxrd
  Apr 15 06:51:40.971: INFO: Got endpoints: latency-svc-lbxgs [766.847585ms]
  Apr 15 06:51:40.986: INFO: Created: latency-svc-dg4mc
  Apr 15 06:51:41.058: INFO: Got endpoints: latency-svc-xf8tq [799.064015ms]
  Apr 15 06:51:41.257: INFO: Got endpoints: latency-svc-24x9b [974.962293ms]
  Apr 15 06:51:41.257: INFO: Got endpoints: latency-svc-jqz9r [912.463003ms]
  Apr 15 06:51:41.292: INFO: Created: latency-svc-gmff2
  Apr 15 06:51:41.299: INFO: Got endpoints: latency-svc-bsbgv [903.532913ms]
  Apr 15 06:51:41.299: INFO: Got endpoints: latency-svc-m7hf2 [855.724744ms]
  Apr 15 06:51:41.311: INFO: Got endpoints: latency-svc-vrr9c [775.618943ms]
  Apr 15 06:51:41.321: INFO: Got endpoints: latency-svc-dp4ws [765.112056ms]
  Apr 15 06:51:41.343: INFO: Got endpoints: latency-svc-pf2cq [707.208303ms]
  Apr 15 06:51:41.383: INFO: Created: latency-svc-n68jf
  Apr 15 06:51:41.404: INFO: Got endpoints: latency-svc-5zxrd [705.335251ms]
  Apr 15 06:51:41.428: INFO: Got endpoints: latency-svc-dg4mc [704.205384ms]
  Apr 15 06:51:41.481: INFO: Got endpoints: latency-svc-gmff2 [733.362545ms]
  Apr 15 06:51:41.531: INFO: Got endpoints: latency-svc-n68jf [739.698119ms]
  Apr 15 06:51:41.652: INFO: Created: latency-svc-mwsrs
  Apr 15 06:51:41.666: INFO: Created: latency-svc-gs2f6
  Apr 15 06:51:41.666: INFO: Created: latency-svc-q7rs6
  Apr 15 06:51:41.667: INFO: Created: latency-svc-x4fnl
  Apr 15 06:51:41.667: INFO: Created: latency-svc-84qc7
  Apr 15 06:51:41.667: INFO: Created: latency-svc-m9fws
  Apr 15 06:51:41.668: INFO: Created: latency-svc-kzwf9
  Apr 15 06:51:41.684: INFO: Created: latency-svc-d6h2w
  Apr 15 06:51:41.681: INFO: Created: latency-svc-kz6wv
  Apr 15 06:51:41.689: INFO: Created: latency-svc-6h82f
  Apr 15 06:51:41.690: INFO: Created: latency-svc-k59m8
  Apr 15 06:51:41.691: INFO: Created: latency-svc-tnpsj
  Apr 15 06:51:41.692: INFO: Created: latency-svc-6jsdt
  Apr 15 06:51:41.692: INFO: Created: latency-svc-xw7xb
  Apr 15 06:51:41.693: INFO: Created: latency-svc-hzfvh
  Apr 15 06:51:41.698: INFO: Got endpoints: latency-svc-mwsrs [165.675772ms]
  Apr 15 06:51:41.733: INFO: Got endpoints: latency-svc-kzwf9 [476.300913ms]
  Apr 15 06:51:41.735: INFO: Created: latency-svc-vw2pc
  Apr 15 06:51:41.739: INFO: Got endpoints: latency-svc-x4fnl [257.663621ms]
  Apr 15 06:51:41.757: INFO: Created: latency-svc-vjh7m
  Apr 15 06:51:41.777: INFO: Got endpoints: latency-svc-gs2f6 [477.545015ms]
  Apr 15 06:51:41.796: INFO: Got endpoints: latency-svc-84qc7 [452.410707ms]
  Apr 15 06:51:41.799: INFO: Created: latency-svc-wxj54
  Apr 15 06:51:41.836: INFO: Created: latency-svc-sdct7
  Apr 15 06:51:41.839: INFO: Created: latency-svc-xd8xd
  Apr 15 06:51:41.928: INFO: Got endpoints: latency-svc-m9fws [1.060834024s]
  Apr 15 06:51:41.962: INFO: Created: latency-svc-qvxph
  Apr 15 06:51:42.022: INFO: Got endpoints: latency-svc-q7rs6 [722.717796ms]
  Apr 15 06:51:42.022: INFO: Got endpoints: latency-svc-k59m8 [764.378097ms]
  Apr 15 06:51:42.071: INFO: Created: latency-svc-z9fsg
  Apr 15 06:51:42.100: INFO: Created: latency-svc-t5wkl
  Apr 15 06:51:42.106: INFO: Got endpoints: latency-svc-kz6wv [677.821842ms]
  Apr 15 06:51:42.107: INFO: Got endpoints: latency-svc-xw7xb [1.134345323s]
  Apr 15 06:51:42.152: INFO: Got endpoints: latency-svc-d6h2w [830.673405ms]
  Apr 15 06:51:42.197: INFO: Got endpoints: latency-svc-tnpsj [886.26975ms]
  Apr 15 06:51:42.208: INFO: Got endpoints: latency-svc-hzfvh [1.148992077s]
  Apr 15 06:51:42.211: INFO: Created: latency-svc-fnpjz
  Apr 15 06:51:42.238: INFO: Got endpoints: latency-svc-6h82f [1.338801581s]
  Apr 15 06:51:42.243: INFO: Created: latency-svc-8pg8t
  Apr 15 06:51:42.255: INFO: Created: latency-svc-7ghcs
  Apr 15 06:51:42.274: INFO: Created: latency-svc-mgbzx
  Apr 15 06:51:42.281: INFO: Created: latency-svc-thqk8
  Apr 15 06:51:42.294: INFO: Got endpoints: latency-svc-6jsdt [889.816752ms]
  Apr 15 06:51:42.299: INFO: Created: latency-svc-w97tr
  Apr 15 06:51:42.321: INFO: Created: latency-svc-sw85p
  Apr 15 06:51:42.340: INFO: Got endpoints: latency-svc-vw2pc [642.313721ms]
  Apr 15 06:51:42.371: INFO: Created: latency-svc-c2n5n
  Apr 15 06:51:42.406: INFO: Got endpoints: latency-svc-vjh7m [672.47891ms]
  Apr 15 06:51:42.430: INFO: Created: latency-svc-tslxp
  Apr 15 06:51:42.451: INFO: Got endpoints: latency-svc-wxj54 [712.154484ms]
  Apr 15 06:51:42.478: INFO: Got endpoints: latency-svc-sdct7 [692.466305ms]
  Apr 15 06:51:42.485: INFO: Created: latency-svc-pghc8
  Apr 15 06:51:42.507: INFO: Created: latency-svc-9r86z
  Apr 15 06:51:42.529: INFO: Got endpoints: latency-svc-xd8xd [731.9275ms]
  Apr 15 06:51:42.550: INFO: Created: latency-svc-rfws7
  Apr 15 06:51:42.582: INFO: Got endpoints: latency-svc-qvxph [653.367088ms]
  Apr 15 06:51:42.608: INFO: Created: latency-svc-4lbbb
  Apr 15 06:51:42.631: INFO: Got endpoints: latency-svc-z9fsg [607.437657ms]
  Apr 15 06:51:42.659: INFO: Created: latency-svc-xzh74
  Apr 15 06:51:42.676: INFO: Got endpoints: latency-svc-t5wkl [654.240849ms]
  Apr 15 06:51:42.703: INFO: Created: latency-svc-wfqbv
  Apr 15 06:51:42.735: INFO: Got endpoints: latency-svc-fnpjz [628.906886ms]
  Apr 15 06:51:42.764: INFO: Created: latency-svc-frl48
  Apr 15 06:51:42.780: INFO: Got endpoints: latency-svc-8pg8t [672.484298ms]
  Apr 15 06:51:42.860: INFO: Got endpoints: latency-svc-7ghcs [707.281328ms]
  Apr 15 06:51:42.868: INFO: Created: latency-svc-kdbsj
  Apr 15 06:51:42.872: INFO: Got endpoints: latency-svc-mgbzx [675.029165ms]
  Apr 15 06:51:42.897: INFO: Created: latency-svc-vxg8k
  Apr 15 06:51:42.922: INFO: Created: latency-svc-49k4w
  Apr 15 06:51:42.936: INFO: Got endpoints: latency-svc-thqk8 [727.850737ms]
  Apr 15 06:51:42.969: INFO: Created: latency-svc-96bvx
  Apr 15 06:51:42.989: INFO: Got endpoints: latency-svc-w97tr [750.605009ms]
  Apr 15 06:51:43.011: INFO: Created: latency-svc-k7hsn
  Apr 15 06:51:43.035: INFO: Got endpoints: latency-svc-sw85p [741.172792ms]
  Apr 15 06:51:43.065: INFO: Created: latency-svc-lqggv
  Apr 15 06:51:43.082: INFO: Got endpoints: latency-svc-c2n5n [741.154687ms]
  Apr 15 06:51:43.117: INFO: Created: latency-svc-726zt
  Apr 15 06:51:43.127: INFO: Got endpoints: latency-svc-tslxp [720.738998ms]
  Apr 15 06:51:43.148: INFO: Created: latency-svc-ltbq7
  Apr 15 06:51:43.179: INFO: Got endpoints: latency-svc-pghc8 [726.617028ms]
  Apr 15 06:51:43.210: INFO: Created: latency-svc-6q6w4
  Apr 15 06:51:43.226: INFO: Got endpoints: latency-svc-9r86z [748.458776ms]
  Apr 15 06:51:43.255: INFO: Created: latency-svc-k7zww
  Apr 15 06:51:43.281: INFO: Got endpoints: latency-svc-rfws7 [751.791413ms]
  Apr 15 06:51:43.311: INFO: Created: latency-svc-tsc4q
  Apr 15 06:51:43.326: INFO: Got endpoints: latency-svc-4lbbb [742.693374ms]
  Apr 15 06:51:43.355: INFO: Created: latency-svc-25ggz
  Apr 15 06:51:43.385: INFO: Got endpoints: latency-svc-xzh74 [753.051252ms]
  Apr 15 06:51:43.418: INFO: Created: latency-svc-pzqlr
  Apr 15 06:51:43.428: INFO: Got endpoints: latency-svc-wfqbv [751.950423ms]
  Apr 15 06:51:43.466: INFO: Created: latency-svc-6xxpf
  Apr 15 06:51:43.479: INFO: Got endpoints: latency-svc-frl48 [743.911719ms]
  Apr 15 06:51:43.506: INFO: Created: latency-svc-psnbn
  Apr 15 06:51:43.528: INFO: Got endpoints: latency-svc-kdbsj [746.509024ms]
  Apr 15 06:51:43.556: INFO: Created: latency-svc-979x9
  Apr 15 06:51:43.579: INFO: Got endpoints: latency-svc-vxg8k [718.497709ms]
  Apr 15 06:51:43.607: INFO: Created: latency-svc-fslkl
  Apr 15 06:51:43.622: INFO: Got endpoints: latency-svc-49k4w [749.804212ms]
  Apr 15 06:51:43.688: INFO: Got endpoints: latency-svc-96bvx [750.994157ms]
  Apr 15 06:51:43.730: INFO: Got endpoints: latency-svc-k7hsn [741.188056ms]
  Apr 15 06:51:43.783: INFO: Got endpoints: latency-svc-lqggv [747.250617ms]
  Apr 15 06:51:43.835: INFO: Got endpoints: latency-svc-726zt [752.976901ms]
  Apr 15 06:51:43.879: INFO: Got endpoints: latency-svc-ltbq7 [751.974216ms]
  Apr 15 06:51:43.923: INFO: Got endpoints: latency-svc-6q6w4 [743.676045ms]
  Apr 15 06:51:43.975: INFO: Got endpoints: latency-svc-k7zww [749.019793ms]
  Apr 15 06:51:44.031: INFO: Got endpoints: latency-svc-tsc4q [748.778313ms]
  Apr 15 06:51:44.075: INFO: Got endpoints: latency-svc-25ggz [747.991523ms]
  Apr 15 06:51:44.122: INFO: Got endpoints: latency-svc-pzqlr [737.348666ms]
  Apr 15 06:51:44.175: INFO: Got endpoints: latency-svc-6xxpf [747.430893ms]
  Apr 15 06:51:44.229: INFO: Got endpoints: latency-svc-psnbn [749.527745ms]
  Apr 15 06:51:44.275: INFO: Got endpoints: latency-svc-979x9 [746.268032ms]
  Apr 15 06:51:44.330: INFO: Got endpoints: latency-svc-fslkl [749.751911ms]
  Apr 15 06:51:44.330: INFO: Latencies: [50.848292ms 88.888834ms 118.752731ms 131.963071ms 160.391177ms 165.675772ms 184.964764ms 188.7661ms 190.564554ms 194.330289ms 203.26201ms 204.768951ms 211.53405ms 213.818189ms 221.358393ms 225.539296ms 239.890561ms 244.259103ms 245.09366ms 248.715757ms 252.484422ms 252.77086ms 253.613946ms 257.663621ms 260.528598ms 268.261346ms 272.707003ms 272.921899ms 279.374958ms 289.099385ms 303.00141ms 303.891502ms 318.288992ms 323.893594ms 325.944082ms 328.677029ms 329.90968ms 333.791824ms 333.960676ms 334.81275ms 341.649712ms 342.763565ms 343.578609ms 346.991626ms 355.414839ms 373.275532ms 377.718122ms 381.360909ms 389.907483ms 398.031696ms 417.496268ms 419.139959ms 425.613397ms 439.265965ms 452.410707ms 454.938318ms 476.300913ms 477.545015ms 482.665801ms 501.910682ms 514.614326ms 516.100357ms 522.788735ms 548.099585ms 551.118352ms 568.435565ms 589.961259ms 594.927137ms 603.816728ms 607.437657ms 607.630163ms 612.057771ms 628.906886ms 631.943822ms 642.313721ms 646.380686ms 653.367088ms 654.240849ms 672.47891ms 672.484298ms 673.134404ms 675.029165ms 677.821842ms 692.466305ms 704.205384ms 705.335251ms 707.115892ms 707.208303ms 707.281328ms 709.232118ms 712.154484ms 714.216077ms 718.497709ms 718.509778ms 720.415215ms 720.738998ms 722.717796ms 725.313269ms 726.617028ms 727.850737ms 731.9275ms 732.190782ms 733.258162ms 733.362545ms 733.972564ms 737.348666ms 739.372868ms 739.698119ms 740.152914ms 740.384252ms 741.154687ms 741.172792ms 741.188056ms 742.693374ms 742.738675ms 743.518875ms 743.676045ms 743.911719ms 745.516137ms 746.268032ms 746.321051ms 746.509024ms 746.665848ms 746.669244ms 747.019176ms 747.250617ms 747.316171ms 747.430893ms 747.991523ms 748.340121ms 748.426184ms 748.458776ms 748.634486ms 748.778313ms 749.019793ms 749.527745ms 749.751911ms 749.804212ms 749.913949ms 749.935991ms 750.605009ms 750.994157ms 751.158435ms 751.791413ms 751.950423ms 751.974216ms 752.976901ms 752.989614ms 753.051252ms 753.128198ms 754.731283ms 755.332217ms 756.386968ms 758.234931ms 758.501581ms 760.074012ms 760.851436ms 760.911274ms 761.612128ms 762.032962ms 762.853065ms 764.000743ms 764.378097ms 765.112056ms 765.188505ms 765.332772ms 766.847585ms 767.048784ms 767.244195ms 771.664827ms 772.346789ms 772.599949ms 772.924976ms 774.691256ms 775.618943ms 776.918812ms 778.157364ms 783.490294ms 786.221575ms 790.071683ms 791.546244ms 799.064015ms 802.487739ms 803.386376ms 808.967307ms 818.172793ms 830.673405ms 844.47178ms 855.724744ms 859.517617ms 886.26975ms 889.816752ms 903.532913ms 912.463003ms 974.121371ms 974.962293ms 1.060834024s 1.134345323s 1.148992077s 1.338801581s]
  Apr 15 06:51:44.331: INFO: 50 %ile: 731.9275ms
  Apr 15 06:51:44.331: INFO: 90 %ile: 791.546244ms
  Apr 15 06:51:44.331: INFO: 99 %ile: 1.148992077s
  Apr 15 06:51:44.331: INFO: Total sample count: 200
  Apr 15 06:51:44.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-3285" for this suite. @ 04/15/24 06:51:44.348
• [9.945 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 04/15/24 06:51:44.375
  Apr 15 06:51:44.375: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename dns @ 04/15/24 06:51:44.379
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:51:44.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:51:44.444
  STEP: Creating a test headless service @ 04/15/24 06:51:44.451
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8942.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8942.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8942.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8942.svc.cluster.local;sleep 1; done
   @ 04/15/24 06:51:44.471
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8942.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8942.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8942.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8942.svc.cluster.local;sleep 1; done
   @ 04/15/24 06:51:44.472
  STEP: creating a pod to probe DNS @ 04/15/24 06:51:44.472
  STEP: submitting the pod to kubernetes @ 04/15/24 06:51:44.473
  STEP: retrieving the pod @ 04/15/24 06:51:48.542
  STEP: looking for the results for each expected name from probers @ 04/15/24 06:51:48.551
  Apr 15 06:51:48.573: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:48.583: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:48.593: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:48.602: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:48.611: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:48.626: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:48.639: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:48.652: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:48.653: INFO: Lookups using dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8942.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8942.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local jessie_udp@dns-test-service-2.dns-8942.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8942.svc.cluster.local]

  Apr 15 06:51:53.672: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:53.685: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:53.697: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:53.714: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:53.724: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:53.738: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:53.758: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:53.769: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:53.769: INFO: Lookups using dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8942.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8942.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local jessie_udp@dns-test-service-2.dns-8942.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8942.svc.cluster.local]

  Apr 15 06:51:58.663: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:58.679: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:58.696: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:58.705: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:58.715: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:58.726: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:58.740: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:58.762: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:51:58.762: INFO: Lookups using dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8942.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8942.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local jessie_udp@dns-test-service-2.dns-8942.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8942.svc.cluster.local]

  Apr 15 06:52:03.666: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:03.680: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:03.691: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:03.701: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:03.713: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:03.724: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:03.734: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:03.745: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:03.745: INFO: Lookups using dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8942.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8942.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local jessie_udp@dns-test-service-2.dns-8942.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8942.svc.cluster.local]

  Apr 15 06:52:08.672: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:08.684: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:08.699: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:08.708: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:08.717: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:08.726: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:08.737: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:08.749: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:08.750: INFO: Lookups using dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8942.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8942.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local jessie_udp@dns-test-service-2.dns-8942.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8942.svc.cluster.local]

  Apr 15 06:52:13.670: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:13.687: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:13.700: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:13.715: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:13.729: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:13.743: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:13.755: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:13.765: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8942.svc.cluster.local from pod dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1: the server could not find the requested resource (get pods dns-test-281566ef-5b89-448a-8bca-be85330160d1)
  Apr 15 06:52:13.765: INFO: Lookups using dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8942.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8942.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8942.svc.cluster.local jessie_udp@dns-test-service-2.dns-8942.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8942.svc.cluster.local]

  Apr 15 06:52:18.739: INFO: DNS probes using dns-8942/dns-test-281566ef-5b89-448a-8bca-be85330160d1 succeeded

  Apr 15 06:52:18.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 06:52:18.757
  STEP: deleting the test headless service @ 04/15/24 06:52:18.826
  STEP: Destroying namespace "dns-8942" for this suite. @ 04/15/24 06:52:18.894
• [34.545 seconds]
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 04/15/24 06:52:18.921
  Apr 15 06:52:18.922: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename var-expansion @ 04/15/24 06:52:18.929
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:52:18.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:52:18.995
  STEP: Creating a pod to test substitution in volume subpath @ 04/15/24 06:52:19.005
  STEP: Saw pod success @ 04/15/24 06:52:23.064
  Apr 15 06:52:23.073: INFO: Trying to get logs from node zaigh3ewotoh-3 pod var-expansion-c9e5932a-c571-4fa0-8293-abd891f58ba8 container dapi-container: <nil>
  STEP: delete the pod @ 04/15/24 06:52:23.124
  Apr 15 06:52:23.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3300" for this suite. @ 04/15/24 06:52:23.174
• [4.268 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 04/15/24 06:52:23.19
  Apr 15 06:52:23.190: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename cronjob @ 04/15/24 06:52:23.193
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:52:23.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:52:23.243
  STEP: Creating a cronjob @ 04/15/24 06:52:23.252
  STEP: Ensuring more than one job is running at a time @ 04/15/24 06:52:23.267
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 04/15/24 06:54:01.276
  STEP: Removing cronjob @ 04/15/24 06:54:01.29
  Apr 15 06:54:01.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8360" for this suite. @ 04/15/24 06:54:01.341
• [98.175 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 04/15/24 06:54:01.38
  Apr 15 06:54:01.381: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename field-validation @ 04/15/24 06:54:01.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:54:01.47
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:54:01.481
  Apr 15 06:54:01.487: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  W0415 06:54:04.311604      13 warnings.go:70] unknown field "alpha"
  W0415 06:54:04.311737      13 warnings.go:70] unknown field "beta"
  W0415 06:54:04.311752      13 warnings.go:70] unknown field "delta"
  W0415 06:54:04.311764      13 warnings.go:70] unknown field "epsilon"
  W0415 06:54:04.311774      13 warnings.go:70] unknown field "gamma"
  Apr 15 06:54:04.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8501" for this suite. @ 04/15/24 06:54:04.944
• [3.581 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 04/15/24 06:54:04.962
  Apr 15 06:54:04.962: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/15/24 06:54:04.964
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:54:05.03
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:54:05.04
  STEP: create the container to handle the HTTPGet hook request. @ 04/15/24 06:54:05.06
  STEP: create the pod with lifecycle hook @ 04/15/24 06:54:09.169
  STEP: check poststart hook @ 04/15/24 06:54:11.231
  STEP: delete the pod with lifecycle hook @ 04/15/24 06:54:11.281
  Apr 15 06:54:13.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8327" for this suite. @ 04/15/24 06:54:13.347
• [8.424 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 04/15/24 06:54:13.394
  Apr 15 06:54:13.394: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/15/24 06:54:13.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:54:13.449
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:54:13.459
  Apr 15 06:54:13.469: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 04/15/24 06:54:15.756
  Apr 15 06:54:15.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4732 --namespace=crd-publish-openapi-4732 create -f -'
  Apr 15 06:54:17.752: INFO: stderr: ""
  Apr 15 06:54:17.752: INFO: stdout: "e2e-test-crd-publish-openapi-4082-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Apr 15 06:54:17.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4732 --namespace=crd-publish-openapi-4732 delete e2e-test-crd-publish-openapi-4082-crds test-foo'
  Apr 15 06:54:17.995: INFO: stderr: ""
  Apr 15 06:54:17.995: INFO: stdout: "e2e-test-crd-publish-openapi-4082-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Apr 15 06:54:17.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4732 --namespace=crd-publish-openapi-4732 apply -f -'
  Apr 15 06:54:19.618: INFO: stderr: ""
  Apr 15 06:54:19.618: INFO: stdout: "e2e-test-crd-publish-openapi-4082-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Apr 15 06:54:19.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4732 --namespace=crd-publish-openapi-4732 delete e2e-test-crd-publish-openapi-4082-crds test-foo'
  Apr 15 06:54:19.817: INFO: stderr: ""
  Apr 15 06:54:19.817: INFO: stdout: "e2e-test-crd-publish-openapi-4082-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 04/15/24 06:54:19.817
  Apr 15 06:54:19.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4732 --namespace=crd-publish-openapi-4732 create -f -'
  Apr 15 06:54:20.383: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 04/15/24 06:54:20.384
  Apr 15 06:54:20.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4732 --namespace=crd-publish-openapi-4732 create -f -'
  Apr 15 06:54:20.917: INFO: rc: 1
  Apr 15 06:54:20.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4732 --namespace=crd-publish-openapi-4732 apply -f -'
  Apr 15 06:54:21.506: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 04/15/24 06:54:21.506
  Apr 15 06:54:21.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4732 --namespace=crd-publish-openapi-4732 create -f -'
  Apr 15 06:54:22.113: INFO: rc: 1
  Apr 15 06:54:22.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4732 --namespace=crd-publish-openapi-4732 apply -f -'
  Apr 15 06:54:22.686: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 04/15/24 06:54:22.686
  Apr 15 06:54:22.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4732 explain e2e-test-crd-publish-openapi-4082-crds'
  Apr 15 06:54:23.303: INFO: stderr: ""
  Apr 15 06:54:23.303: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4082-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 04/15/24 06:54:23.304
  Apr 15 06:54:23.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4732 explain e2e-test-crd-publish-openapi-4082-crds.metadata'
  Apr 15 06:54:23.879: INFO: stderr: ""
  Apr 15 06:54:23.879: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4082-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Apr 15 06:54:23.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4732 explain e2e-test-crd-publish-openapi-4082-crds.spec'
  Apr 15 06:54:24.382: INFO: stderr: ""
  Apr 15 06:54:24.382: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4082-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Apr 15 06:54:24.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4732 explain e2e-test-crd-publish-openapi-4082-crds.spec.bars'
  Apr 15 06:54:24.957: INFO: stderr: ""
  Apr 15 06:54:24.957: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4082-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 04/15/24 06:54:24.958
  Apr 15 06:54:24.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-4732 explain e2e-test-crd-publish-openapi-4082-crds.spec.bars2'
  Apr 15 06:54:25.489: INFO: rc: 1
  Apr 15 06:54:27.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4732" for this suite. @ 04/15/24 06:54:27.497
• [14.125 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 04/15/24 06:54:27.522
  Apr 15 06:54:27.522: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 06:54:27.528
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:54:27.574
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:54:27.579
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 06:54:27.591
  STEP: Saw pod success @ 04/15/24 06:54:31.652
  Apr 15 06:54:31.659: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downwardapi-volume-5d27e403-4762-4bec-9d34-8751eb266c36 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 06:54:31.697
  Apr 15 06:54:31.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4728" for this suite. @ 04/15/24 06:54:31.744
• [4.243 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 04/15/24 06:54:31.767
  Apr 15 06:54:31.767: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename subpath @ 04/15/24 06:54:31.77
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:54:31.815
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:54:31.821
  STEP: Setting up data @ 04/15/24 06:54:31.835
  STEP: Creating pod pod-subpath-test-downwardapi-xbvb @ 04/15/24 06:54:31.861
  STEP: Creating a pod to test atomic-volume-subpath @ 04/15/24 06:54:31.862
  STEP: Saw pod success @ 04/15/24 06:54:56.019
  Apr 15 06:54:56.029: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-subpath-test-downwardapi-xbvb container test-container-subpath-downwardapi-xbvb: <nil>
  STEP: delete the pod @ 04/15/24 06:54:56.048
  STEP: Deleting pod pod-subpath-test-downwardapi-xbvb @ 04/15/24 06:54:56.084
  Apr 15 06:54:56.084: INFO: Deleting pod "pod-subpath-test-downwardapi-xbvb" in namespace "subpath-2322"
  Apr 15 06:54:56.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-2322" for this suite. @ 04/15/24 06:54:56.112
• [24.371 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:474
  STEP: Creating a kubernetes client @ 04/15/24 06:54:56.146
  Apr 15 06:54:56.146: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename field-validation @ 04/15/24 06:54:56.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:54:56.203
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:54:56.212
  Apr 15 06:54:56.220: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  W0415 06:54:59.052824      13 warnings.go:70] unknown field "alpha"
  W0415 06:54:59.052999      13 warnings.go:70] unknown field "beta"
  W0415 06:54:59.053218      13 warnings.go:70] unknown field "delta"
  W0415 06:54:59.053324      13 warnings.go:70] unknown field "epsilon"
  W0415 06:54:59.053430      13 warnings.go:70] unknown field "gamma"
  Apr 15 06:54:59.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2775" for this suite. @ 04/15/24 06:54:59.655
• [3.528 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 04/15/24 06:54:59.674
  Apr 15 06:54:59.674: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 06:54:59.677
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:54:59.718
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:54:59.722
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 06:54:59.727
  STEP: Saw pod success @ 04/15/24 06:55:03.772
  Apr 15 06:55:03.780: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downwardapi-volume-9ed8ad6e-09b7-4c2d-b8fb-22e5139b90ed container client-container: <nil>
  STEP: delete the pod @ 04/15/24 06:55:03.803
  Apr 15 06:55:03.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3141" for this suite. @ 04/15/24 06:55:03.865
• [4.213 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 04/15/24 06:55:03.892
  Apr 15 06:55:03.893: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename security-context @ 04/15/24 06:55:03.895
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:55:03.938
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:55:03.943
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 04/15/24 06:55:03.949
  STEP: Saw pod success @ 04/15/24 06:55:08.014
  Apr 15 06:55:08.021: INFO: Trying to get logs from node zaigh3ewotoh-3 pod security-context-d01e5f11-dd46-4ebc-a51a-bf550bbcef3a container test-container: <nil>
  STEP: delete the pod @ 04/15/24 06:55:08.037
  Apr 15 06:55:08.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-1284" for this suite. @ 04/15/24 06:55:08.085
• [4.207 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 04/15/24 06:55:08.104
  Apr 15 06:55:08.104: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 06:55:08.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:55:08.155
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:55:08.164
  STEP: Counting existing ResourceQuota @ 04/15/24 06:55:25.213
  STEP: Creating a ResourceQuota @ 04/15/24 06:55:30.227
  STEP: Ensuring resource quota status is calculated @ 04/15/24 06:55:30.247
  STEP: Creating a ConfigMap @ 04/15/24 06:55:32.258
  STEP: Ensuring resource quota status captures configMap creation @ 04/15/24 06:55:32.281
  STEP: Deleting a ConfigMap @ 04/15/24 06:55:34.293
  STEP: Ensuring resource quota status released usage @ 04/15/24 06:55:34.312
  Apr 15 06:55:36.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2785" for this suite. @ 04/15/24 06:55:36.337
• [28.250 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 04/15/24 06:55:36.366
  Apr 15 06:55:36.366: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename disruption @ 04/15/24 06:55:36.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:55:36.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:55:36.417
  STEP: Creating a pdb that targets all three pods in a test replica set @ 04/15/24 06:55:36.423
  STEP: Waiting for the pdb to be processed @ 04/15/24 06:55:36.44
  STEP: First trying to evict a pod which shouldn't be evictable @ 04/15/24 06:55:38.49
  STEP: Waiting for all pods to be running @ 04/15/24 06:55:38.49
  Apr 15 06:55:38.500: INFO: pods: 0 < 3
  Apr 15 06:55:40.532: INFO: running pods: 2 < 3
  STEP: locating a running pod @ 04/15/24 06:55:42.512
  STEP: Updating the pdb to allow a pod to be evicted @ 04/15/24 06:55:42.55
  STEP: Waiting for the pdb to be processed @ 04/15/24 06:55:42.577
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 04/15/24 06:55:44.6
  STEP: Waiting for all pods to be running @ 04/15/24 06:55:44.6
  STEP: Waiting for the pdb to observed all healthy pods @ 04/15/24 06:55:44.611
  STEP: Patching the pdb to disallow a pod to be evicted @ 04/15/24 06:55:44.71
  STEP: Waiting for the pdb to be processed @ 04/15/24 06:55:44.828
  STEP: Waiting for all pods to be running @ 04/15/24 06:55:44.868
  Apr 15 06:55:44.912: INFO: running pods: 2 < 3
  STEP: locating a running pod @ 04/15/24 06:55:46.929
  STEP: Deleting the pdb to allow a pod to be evicted @ 04/15/24 06:55:46.959
  STEP: Waiting for the pdb to be deleted @ 04/15/24 06:55:46.976
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 04/15/24 06:55:46.983
  STEP: Waiting for all pods to be running @ 04/15/24 06:55:46.983
  Apr 15 06:55:47.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-1435" for this suite. @ 04/15/24 06:55:47.049
• [10.802 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 04/15/24 06:55:47.168
  Apr 15 06:55:47.168: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename subpath @ 04/15/24 06:55:47.172
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:55:47.269
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:55:47.277
  STEP: Setting up data @ 04/15/24 06:55:47.287
  STEP: Creating pod pod-subpath-test-configmap-tdv4 @ 04/15/24 06:55:47.316
  STEP: Creating a pod to test atomic-volume-subpath @ 04/15/24 06:55:47.318
  STEP: Saw pod success @ 04/15/24 06:56:11.52
  Apr 15 06:56:11.530: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-subpath-test-configmap-tdv4 container test-container-subpath-configmap-tdv4: <nil>
  STEP: delete the pod @ 04/15/24 06:56:11.554
  STEP: Deleting pod pod-subpath-test-configmap-tdv4 @ 04/15/24 06:56:11.591
  Apr 15 06:56:11.591: INFO: Deleting pod "pod-subpath-test-configmap-tdv4" in namespace "subpath-7220"
  Apr 15 06:56:11.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7220" for this suite. @ 04/15/24 06:56:11.614
• [24.458 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:385
  STEP: Creating a kubernetes client @ 04/15/24 06:56:11.64
  Apr 15 06:56:11.640: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename daemonsets @ 04/15/24 06:56:11.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:56:11.702
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:56:11.707
  Apr 15 06:56:11.764: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/15/24 06:56:11.779
  Apr 15 06:56:11.803: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 06:56:11.804: INFO: Node zaigh3ewotoh-1 is running 0 daemon pod, expected 1
  Apr 15 06:56:12.850: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 06:56:12.850: INFO: Node zaigh3ewotoh-1 is running 0 daemon pod, expected 1
  Apr 15 06:56:13.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 15 06:56:13.827: INFO: Node zaigh3ewotoh-1 is running 0 daemon pod, expected 1
  Apr 15 06:56:14.832: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 15 06:56:14.832: INFO: Node zaigh3ewotoh-2 is running 0 daemon pod, expected 1
  Apr 15 06:56:15.840: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 15 06:56:15.840: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 04/15/24 06:56:15.9
  STEP: Check that daemon pods images are updated. @ 04/15/24 06:56:15.94
  Apr 15 06:56:15.953: INFO: Wrong image for pod: daemon-set-6gcs6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.47, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 15 06:56:15.954: INFO: Wrong image for pod: daemon-set-rsxpd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.47, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 15 06:56:15.954: INFO: Wrong image for pod: daemon-set-wxhjp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.47, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 15 06:56:17.073: INFO: Pod daemon-set-qcnh9 is not available
  Apr 15 06:56:17.074: INFO: Wrong image for pod: daemon-set-rsxpd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.47, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 15 06:56:17.074: INFO: Wrong image for pod: daemon-set-wxhjp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.47, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 15 06:56:17.989: INFO: Wrong image for pod: daemon-set-rsxpd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.47, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 15 06:56:17.989: INFO: Wrong image for pod: daemon-set-wxhjp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.47, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 15 06:56:18.994: INFO: Pod daemon-set-q9s7b is not available
  Apr 15 06:56:18.994: INFO: Wrong image for pod: daemon-set-rsxpd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.47, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 15 06:56:20.997: INFO: Pod daemon-set-c2cgl is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 04/15/24 06:56:21.032
  Apr 15 06:56:21.100: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 15 06:56:21.100: INFO: Node zaigh3ewotoh-2 is running 0 daemon pod, expected 1
  Apr 15 06:56:22.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 15 06:56:22.127: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/15/24 06:56:22.174
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7884, will wait for the garbage collector to delete the pods @ 04/15/24 06:56:22.174
  Apr 15 06:56:22.247: INFO: Deleting DaemonSet.extensions daemon-set took: 14.795938ms
  Apr 15 06:56:22.349: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.134246ms
  Apr 15 06:56:23.659: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 06:56:23.659: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 15 06:56:23.676: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"157404"},"items":null}

  Apr 15 06:56:23.686: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"157404"},"items":null}

  Apr 15 06:56:23.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7884" for this suite. @ 04/15/24 06:56:23.735
• [12.110 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 04/15/24 06:56:23.753
  Apr 15 06:56:23.753: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename runtimeclass @ 04/15/24 06:56:23.757
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:56:23.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:56:23.806
  Apr 15 06:56:23.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8059" for this suite. @ 04/15/24 06:56:23.838
• [0.103 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 04/15/24 06:56:23.858
  Apr 15 06:56:23.858: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 06:56:23.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:56:23.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:56:23.907
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 06:56:23.913
  STEP: Saw pod success @ 04/15/24 06:56:27.972
  Apr 15 06:56:27.979: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downwardapi-volume-adf3cb3f-2322-49f0-88c3-9c5cd15138fe container client-container: <nil>
  STEP: delete the pod @ 04/15/24 06:56:27.995
  Apr 15 06:56:28.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3911" for this suite. @ 04/15/24 06:56:28.056
• [4.226 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 04/15/24 06:56:28.089
  Apr 15 06:56:28.090: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename gc @ 04/15/24 06:56:28.093
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:56:28.144
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:56:28.151
  STEP: create the deployment @ 04/15/24 06:56:28.156
  W0415 06:56:28.169075      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 04/15/24 06:56:28.169
  STEP: delete the deployment @ 04/15/24 06:56:28.699
  STEP: wait for all rs to be garbage collected @ 04/15/24 06:56:28.718
  STEP: expected 0 rs, got 1 rs @ 04/15/24 06:56:28.736
  STEP: expected 0 pods, got 2 pods @ 04/15/24 06:56:28.748
  STEP: Gathering metrics @ 04/15/24 06:56:29.32
  Apr 15 06:56:29.680: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 15 06:56:29.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4183" for this suite. @ 04/15/24 06:56:29.692
• [1.616 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 04/15/24 06:56:29.728
  Apr 15 06:56:29.729: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename configmap @ 04/15/24 06:56:29.734
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:56:29.795
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:56:29.8
  STEP: Creating configMap with name configmap-test-volume-b7435ac3-37ec-4909-980c-ff8c29335045 @ 04/15/24 06:56:29.805
  STEP: Creating a pod to test consume configMaps @ 04/15/24 06:56:29.815
  STEP: Saw pod success @ 04/15/24 06:56:33.858
  Apr 15 06:56:33.866: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-configmaps-2bee230a-e2fe-40f4-a14b-51ecc71ba2bf container configmap-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 06:56:33.879
  Apr 15 06:56:33.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7701" for this suite. @ 04/15/24 06:56:33.918
• [4.207 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 04/15/24 06:56:33.935
  Apr 15 06:56:33.936: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 06:56:33.939
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:56:33.977
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:56:33.985
  Apr 15 06:56:33.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-1443 version'
  Apr 15 06:56:34.141: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Apr 15 06:56:34.141: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.12\", GitCommit:\"12031002905c0410706974560cbdf2dad9278919\", GitTreeState:\"clean\", BuildDate:\"2024-03-15T02:15:31Z\", GoVersion:\"go1.21.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.12\", GitCommit:\"12031002905c0410706974560cbdf2dad9278919\", GitTreeState:\"clean\", BuildDate:\"2024-03-15T02:06:14Z\", GoVersion:\"go1.21.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Apr 15 06:56:34.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1443" for this suite. @ 04/15/24 06:56:34.158
• [0.243 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 04/15/24 06:56:34.181
  Apr 15 06:56:34.181: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 06:56:34.185
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:56:34.228
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:56:34.24
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 06:56:34.255
  STEP: Saw pod success @ 04/15/24 06:56:38.312
  Apr 15 06:56:38.323: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downwardapi-volume-a5507bfa-688f-4146-9d2f-3ae1f07cef38 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 06:56:38.338
  Apr 15 06:56:38.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1456" for this suite. @ 04/15/24 06:56:38.386
• [4.220 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 04/15/24 06:56:38.409
  Apr 15 06:56:38.410: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 06:56:38.414
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:56:38.461
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:56:38.473
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 06:56:38.481
  STEP: Saw pod success @ 04/15/24 06:56:42.553
  Apr 15 06:56:42.568: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downwardapi-volume-166762d2-40b1-4aa5-9d09-459ab2130379 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 06:56:42.59
  Apr 15 06:56:42.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7317" for this suite. @ 04/15/24 06:56:42.648
• [4.256 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 04/15/24 06:56:42.674
  Apr 15 06:56:42.674: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename containers @ 04/15/24 06:56:42.678
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:56:42.721
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:56:42.727
  Apr 15 06:56:44.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-4772" for this suite. @ 04/15/24 06:56:44.88
• [2.220 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 04/15/24 06:56:44.906
  Apr 15 06:56:44.907: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename secrets @ 04/15/24 06:56:44.91
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:56:44.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:56:44.948
  STEP: Creating secret with name secret-test-f560525e-5fb6-471b-acfd-e50ba91c5cb4 @ 04/15/24 06:56:44.955
  STEP: Creating a pod to test consume secrets @ 04/15/24 06:56:44.971
  STEP: Saw pod success @ 04/15/24 06:56:49.059
  Apr 15 06:56:49.070: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-secrets-bf647dde-296f-4477-9fe0-94864737d9bb container secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 06:56:49.088
  Apr 15 06:56:49.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6773" for this suite. @ 04/15/24 06:56:49.142
• [4.249 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 04/15/24 06:56:49.157
  Apr 15 06:56:49.157: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 06:56:49.159
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:56:49.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:56:49.196
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 04/15/24 06:56:49.203
  STEP: Saw pod success @ 04/15/24 06:56:53.265
  Apr 15 06:56:53.274: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-240f20fe-1c38-4f7c-92c6-64b867e50090 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 06:56:53.29
  Apr 15 06:56:53.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7791" for this suite. @ 04/15/24 06:56:53.35
• [4.226 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 04/15/24 06:56:53.389
  Apr 15 06:56:53.389: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 06:56:53.396
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:56:53.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:56:53.448
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 06:56:53.454
  STEP: Saw pod success @ 04/15/24 06:56:57.504
  Apr 15 06:56:57.511: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downwardapi-volume-b9e6fb95-8543-402c-a293-026b85e657ad container client-container: <nil>
  STEP: delete the pod @ 04/15/24 06:56:57.523
  Apr 15 06:56:57.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3793" for this suite. @ 04/15/24 06:56:57.564
• [4.190 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 04/15/24 06:56:57.582
  Apr 15 06:56:57.583: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename sched-pred @ 04/15/24 06:56:57.586
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:56:57.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:56:57.631
  Apr 15 06:56:57.636: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 15 06:56:57.662: INFO: Waiting for terminating namespaces to be deleted...
  Apr 15 06:56:57.674: INFO: 
  Logging pods the apiserver thinks is on node zaigh3ewotoh-1 before test
  Apr 15 06:56:57.690: INFO: coredns-5d78c9869d-t6h7j from kube-system started at 2024-04-15 06:16:15 +0000 UTC (1 container statuses recorded)
  Apr 15 06:56:57.691: INFO: 	Container coredns ready: true, restart count 0
  Apr 15 06:56:57.691: INFO: kube-addon-manager-zaigh3ewotoh-1 from kube-system started at 2024-04-15 06:11:57 +0000 UTC (1 container statuses recorded)
  Apr 15 06:56:57.691: INFO: 	Container kube-addon-manager ready: true, restart count 4
  Apr 15 06:56:57.691: INFO: kube-apiserver-zaigh3ewotoh-1 from kube-system started at 2024-04-15 06:11:57 +0000 UTC (1 container statuses recorded)
  Apr 15 06:56:57.692: INFO: 	Container kube-apiserver ready: true, restart count 4
  Apr 15 06:56:57.692: INFO: kube-controller-manager-zaigh3ewotoh-1 from kube-system started at 2024-04-15 06:11:57 +0000 UTC (1 container statuses recorded)
  Apr 15 06:56:57.692: INFO: 	Container kube-controller-manager ready: true, restart count 4
  Apr 15 06:56:57.692: INFO: kube-flannel-ds-wwbkw from kube-system started at 2024-04-15 06:16:17 +0000 UTC (1 container statuses recorded)
  Apr 15 06:56:57.692: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 06:56:57.692: INFO: kube-proxy-v9dxk from kube-system started at 2024-04-15 06:16:16 +0000 UTC (1 container statuses recorded)
  Apr 15 06:56:57.693: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 06:56:57.693: INFO: kube-scheduler-zaigh3ewotoh-1 from kube-system started at 2024-04-15 06:11:57 +0000 UTC (1 container statuses recorded)
  Apr 15 06:56:57.693: INFO: 	Container kube-scheduler ready: true, restart count 4
  Apr 15 06:56:57.693: INFO: sonobuoy-systemd-logs-daemon-set-409cd1623c554ca6-k6p5l from sonobuoy started at 2024-04-15 06:16:23 +0000 UTC (2 container statuses recorded)
  Apr 15 06:56:57.693: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 06:56:57.694: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 15 06:56:57.694: INFO: 
  Logging pods the apiserver thinks is on node zaigh3ewotoh-2 before test
  Apr 15 06:56:57.709: INFO: rs-lc4zf from disruption-1435 started at 2024-04-15 06:55:47 +0000 UTC (1 container statuses recorded)
  Apr 15 06:56:57.709: INFO: 	Container donothing ready: false, restart count 0
  Apr 15 06:56:57.710: INFO: kube-addon-manager-zaigh3ewotoh-2 from kube-system started at 2024-04-15 06:07:13 +0000 UTC (1 container statuses recorded)
  Apr 15 06:56:57.710: INFO: 	Container kube-addon-manager ready: true, restart count 4
  Apr 15 06:56:57.710: INFO: kube-apiserver-zaigh3ewotoh-2 from kube-system started at 2024-04-15 06:07:13 +0000 UTC (1 container statuses recorded)
  Apr 15 06:56:57.710: INFO: 	Container kube-apiserver ready: true, restart count 4
  Apr 15 06:56:57.710: INFO: kube-controller-manager-zaigh3ewotoh-2 from kube-system started at 2024-04-15 06:07:13 +0000 UTC (1 container statuses recorded)
  Apr 15 06:56:57.710: INFO: 	Container kube-controller-manager ready: true, restart count 4
  Apr 15 06:56:57.711: INFO: kube-flannel-ds-gs792 from kube-system started at 2024-04-15 06:16:17 +0000 UTC (1 container statuses recorded)
  Apr 15 06:56:57.711: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 06:56:57.711: INFO: kube-proxy-sqtk9 from kube-system started at 2024-04-15 06:16:16 +0000 UTC (1 container statuses recorded)
  Apr 15 06:56:57.712: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 06:56:57.713: INFO: kube-scheduler-zaigh3ewotoh-2 from kube-system started at 2024-04-15 06:07:13 +0000 UTC (1 container statuses recorded)
  Apr 15 06:56:57.713: INFO: 	Container kube-scheduler ready: true, restart count 4
  Apr 15 06:56:57.714: INFO: sonobuoy-systemd-logs-daemon-set-409cd1623c554ca6-pp7qg from sonobuoy started at 2024-04-15 06:16:23 +0000 UTC (2 container statuses recorded)
  Apr 15 06:56:57.715: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 06:56:57.715: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 15 06:56:57.715: INFO: 
  Logging pods the apiserver thinks is on node zaigh3ewotoh-3 before test
  Apr 15 06:56:57.736: INFO: coredns-5d78c9869d-4w296 from kube-system started at 2024-04-15 06:16:15 +0000 UTC (1 container statuses recorded)
  Apr 15 06:56:57.736: INFO: 	Container coredns ready: true, restart count 0
  Apr 15 06:56:57.736: INFO: kube-flannel-ds-6xxbt from kube-system started at 2024-04-15 06:16:17 +0000 UTC (1 container statuses recorded)
  Apr 15 06:56:57.736: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 06:56:57.736: INFO: kube-proxy-mxvh9 from kube-system started at 2024-04-15 06:16:16 +0000 UTC (1 container statuses recorded)
  Apr 15 06:56:57.736: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 06:56:57.736: INFO: sonobuoy from sonobuoy started at 2024-04-15 06:16:22 +0000 UTC (1 container statuses recorded)
  Apr 15 06:56:57.736: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 15 06:56:57.736: INFO: sonobuoy-e2e-job-7bb47b0523524b58 from sonobuoy started at 2024-04-15 06:16:23 +0000 UTC (2 container statuses recorded)
  Apr 15 06:56:57.736: INFO: 	Container e2e ready: true, restart count 0
  Apr 15 06:56:57.736: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 06:56:57.736: INFO: sonobuoy-systemd-logs-daemon-set-409cd1623c554ca6-9fkvf from sonobuoy started at 2024-04-15 06:16:23 +0000 UTC (2 container statuses recorded)
  Apr 15 06:56:57.736: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 06:56:57.736: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node zaigh3ewotoh-1 @ 04/15/24 06:56:57.797
  STEP: verifying the node has the label node zaigh3ewotoh-2 @ 04/15/24 06:56:57.85
  STEP: verifying the node has the label node zaigh3ewotoh-3 @ 04/15/24 06:56:57.886
  Apr 15 06:56:57.929: INFO: Pod rs-lc4zf requesting resource cpu=0m on Node zaigh3ewotoh-2
  Apr 15 06:56:57.931: INFO: Pod coredns-5d78c9869d-4w296 requesting resource cpu=100m on Node zaigh3ewotoh-3
  Apr 15 06:56:57.932: INFO: Pod coredns-5d78c9869d-t6h7j requesting resource cpu=100m on Node zaigh3ewotoh-1
  Apr 15 06:56:57.933: INFO: Pod kube-addon-manager-zaigh3ewotoh-1 requesting resource cpu=5m on Node zaigh3ewotoh-1
  Apr 15 06:56:57.934: INFO: Pod kube-addon-manager-zaigh3ewotoh-2 requesting resource cpu=5m on Node zaigh3ewotoh-2
  Apr 15 06:56:57.936: INFO: Pod kube-apiserver-zaigh3ewotoh-1 requesting resource cpu=250m on Node zaigh3ewotoh-1
  Apr 15 06:56:57.937: INFO: Pod kube-apiserver-zaigh3ewotoh-2 requesting resource cpu=250m on Node zaigh3ewotoh-2
  Apr 15 06:56:57.939: INFO: Pod kube-controller-manager-zaigh3ewotoh-1 requesting resource cpu=200m on Node zaigh3ewotoh-1
  Apr 15 06:56:57.939: INFO: Pod kube-controller-manager-zaigh3ewotoh-2 requesting resource cpu=200m on Node zaigh3ewotoh-2
  Apr 15 06:56:57.940: INFO: Pod kube-flannel-ds-6xxbt requesting resource cpu=100m on Node zaigh3ewotoh-3
  Apr 15 06:56:57.942: INFO: Pod kube-flannel-ds-gs792 requesting resource cpu=100m on Node zaigh3ewotoh-2
  Apr 15 06:56:57.944: INFO: Pod kube-flannel-ds-wwbkw requesting resource cpu=100m on Node zaigh3ewotoh-1
  Apr 15 06:56:57.946: INFO: Pod kube-proxy-mxvh9 requesting resource cpu=0m on Node zaigh3ewotoh-3
  Apr 15 06:56:57.946: INFO: Pod kube-proxy-sqtk9 requesting resource cpu=0m on Node zaigh3ewotoh-2
  Apr 15 06:56:57.947: INFO: Pod kube-proxy-v9dxk requesting resource cpu=0m on Node zaigh3ewotoh-1
  Apr 15 06:56:57.947: INFO: Pod kube-scheduler-zaigh3ewotoh-1 requesting resource cpu=100m on Node zaigh3ewotoh-1
  Apr 15 06:56:57.948: INFO: Pod kube-scheduler-zaigh3ewotoh-2 requesting resource cpu=100m on Node zaigh3ewotoh-2
  Apr 15 06:56:57.950: INFO: Pod sonobuoy requesting resource cpu=0m on Node zaigh3ewotoh-3
  Apr 15 06:56:57.953: INFO: Pod sonobuoy-e2e-job-7bb47b0523524b58 requesting resource cpu=0m on Node zaigh3ewotoh-3
  Apr 15 06:56:57.955: INFO: Pod sonobuoy-systemd-logs-daemon-set-409cd1623c554ca6-9fkvf requesting resource cpu=0m on Node zaigh3ewotoh-3
  Apr 15 06:56:57.959: INFO: Pod sonobuoy-systemd-logs-daemon-set-409cd1623c554ca6-k6p5l requesting resource cpu=0m on Node zaigh3ewotoh-1
  Apr 15 06:56:57.961: INFO: Pod sonobuoy-systemd-logs-daemon-set-409cd1623c554ca6-pp7qg requesting resource cpu=0m on Node zaigh3ewotoh-2
  STEP: Starting Pods to consume most of the cluster CPU. @ 04/15/24 06:56:57.964
  Apr 15 06:56:57.965: INFO: Creating a pod which consumes cpu=591m on Node zaigh3ewotoh-1
  Apr 15 06:56:57.990: INFO: Creating a pod which consumes cpu=661m on Node zaigh3ewotoh-2
  Apr 15 06:56:58.040: INFO: Creating a pod which consumes cpu=980m on Node zaigh3ewotoh-3
  STEP: Creating another pod that requires unavailable amount of CPU. @ 04/15/24 06:57:00.092
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6443efc0-b7e4-4257-bf83-14ac9edcaf97.17c661c79411c64b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3389/filler-pod-6443efc0-b7e4-4257-bf83-14ac9edcaf97 to zaigh3ewotoh-2] @ 04/15/24 06:57:00.108
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6443efc0-b7e4-4257-bf83-14ac9edcaf97.17c661c7b92c3a51], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/15/24 06:57:00.108
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6443efc0-b7e4-4257-bf83-14ac9edcaf97.17c661c7c551fe4d], Reason = [Created], Message = [Created container filler-pod-6443efc0-b7e4-4257-bf83-14ac9edcaf97] @ 04/15/24 06:57:00.108
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6443efc0-b7e4-4257-bf83-14ac9edcaf97.17c661c7c93988b5], Reason = [Started], Message = [Started container filler-pod-6443efc0-b7e4-4257-bf83-14ac9edcaf97] @ 04/15/24 06:57:00.109
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-68f8171e-62f9-4ef8-9001-b2712d8cfb90.17c661c78fd5c7d2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3389/filler-pod-68f8171e-62f9-4ef8-9001-b2712d8cfb90 to zaigh3ewotoh-1] @ 04/15/24 06:57:00.109
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-68f8171e-62f9-4ef8-9001-b2712d8cfb90.17c661c7ba8047bd], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/15/24 06:57:00.109
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-68f8171e-62f9-4ef8-9001-b2712d8cfb90.17c661c7c7bf38e8], Reason = [Created], Message = [Created container filler-pod-68f8171e-62f9-4ef8-9001-b2712d8cfb90] @ 04/15/24 06:57:00.109
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-68f8171e-62f9-4ef8-9001-b2712d8cfb90.17c661c7ca553d3e], Reason = [Started], Message = [Started container filler-pod-68f8171e-62f9-4ef8-9001-b2712d8cfb90] @ 04/15/24 06:57:00.109
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a5a9f7da-3089-43f4-977f-10a9411f2feb.17c661c7966da04b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3389/filler-pod-a5a9f7da-3089-43f4-977f-10a9411f2feb to zaigh3ewotoh-3] @ 04/15/24 06:57:00.109
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a5a9f7da-3089-43f4-977f-10a9411f2feb.17c661c7b6227305], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/15/24 06:57:00.109
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a5a9f7da-3089-43f4-977f-10a9411f2feb.17c661c7c0233c38], Reason = [Created], Message = [Created container filler-pod-a5a9f7da-3089-43f4-977f-10a9411f2feb] @ 04/15/24 06:57:00.109
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a5a9f7da-3089-43f4-977f-10a9411f2feb.17c661c7c290fd9a], Reason = [Started], Message = [Started container filler-pod-a5a9f7da-3089-43f4-977f-10a9411f2feb] @ 04/15/24 06:57:00.109
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.17c661c80eb2adc1], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] @ 04/15/24 06:57:00.145
  STEP: removing the label node off the node zaigh3ewotoh-1 @ 04/15/24 06:57:01.143
  STEP: verifying the node doesn't have the label node @ 04/15/24 06:57:01.173
  STEP: removing the label node off the node zaigh3ewotoh-2 @ 04/15/24 06:57:01.182
  STEP: verifying the node doesn't have the label node @ 04/15/24 06:57:01.213
  STEP: removing the label node off the node zaigh3ewotoh-3 @ 04/15/24 06:57:01.223
  STEP: verifying the node doesn't have the label node @ 04/15/24 06:57:01.263
  Apr 15 06:57:01.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-3389" for this suite. @ 04/15/24 06:57:01.302
• [3.736 seconds]
------------------------------
SS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 04/15/24 06:57:01.319
  Apr 15 06:57:01.319: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename runtimeclass @ 04/15/24 06:57:01.324
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:57:01.367
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:57:01.384
  Apr 15 06:57:01.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-4543" for this suite. @ 04/15/24 06:57:01.533
• [0.233 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:350
  STEP: Creating a kubernetes client @ 04/15/24 06:57:01.557
  Apr 15 06:57:01.557: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename field-validation @ 04/15/24 06:57:01.561
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:57:01.596
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:57:01.604
  Apr 15 06:57:01.616: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  W0415 06:57:01.618677      13 field_validation.go:423] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc003e68610 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  W0415 06:57:04.477768      13 warnings.go:70] unknown field "alpha"
  W0415 06:57:04.477825      13 warnings.go:70] unknown field "beta"
  W0415 06:57:04.477850      13 warnings.go:70] unknown field "delta"
  W0415 06:57:04.477865      13 warnings.go:70] unknown field "epsilon"
  W0415 06:57:04.477879      13 warnings.go:70] unknown field "gamma"
  Apr 15 06:57:05.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2767" for this suite. @ 04/15/24 06:57:05.133
• [3.591 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 04/15/24 06:57:05.158
  Apr 15 06:57:05.158: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-runtime @ 04/15/24 06:57:05.162
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:57:05.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:57:05.202
  STEP: create the container @ 04/15/24 06:57:05.207
  W0415 06:57:05.227436      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 04/15/24 06:57:05.228
  STEP: get the container status @ 04/15/24 06:57:08.266
  STEP: the container should be terminated @ 04/15/24 06:57:08.273
  STEP: the termination message should be set @ 04/15/24 06:57:08.274
  Apr 15 06:57:08.274: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 04/15/24 06:57:08.275
  Apr 15 06:57:08.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9567" for this suite. @ 04/15/24 06:57:08.315
• [3.168 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 04/15/24 06:57:08.328
  Apr 15 06:57:08.328: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename pods @ 04/15/24 06:57:08.33
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:57:08.366
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:57:08.373
  Apr 15 06:57:08.379: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: creating the pod @ 04/15/24 06:57:08.381
  STEP: submitting the pod to kubernetes @ 04/15/24 06:57:08.382
  Apr 15 06:57:10.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2709" for this suite. @ 04/15/24 06:57:10.491
• [2.184 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 04/15/24 06:57:10.514
  Apr 15 06:57:10.514: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 06:57:10.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:57:10.549
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:57:10.559
  STEP: Creating projection with secret that has name projected-secret-test-map-f1b8460f-33e5-4169-9ae8-952f5f5d3a9a @ 04/15/24 06:57:10.568
  STEP: Creating a pod to test consume secrets @ 04/15/24 06:57:10.582
  STEP: Saw pod success @ 04/15/24 06:57:14.629
  Apr 15 06:57:14.641: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-projected-secrets-bfeb092b-6b0e-4110-a78e-fda33ac39f5f container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 06:57:14.657
  Apr 15 06:57:14.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5297" for this suite. @ 04/15/24 06:57:14.714
• [4.222 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 04/15/24 06:57:14.736
  Apr 15 06:57:14.736: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename secrets @ 04/15/24 06:57:14.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:57:14.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:57:14.8
  STEP: Creating projection with secret that has name secret-emptykey-test-884e00aa-fad5-4f0d-9400-78571d3d5474 @ 04/15/24 06:57:14.808
  Apr 15 06:57:14.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1231" for this suite. @ 04/15/24 06:57:14.822
• [0.124 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 04/15/24 06:57:14.865
  Apr 15 06:57:14.866: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 06:57:14.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:57:14.906
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:57:14.913
  STEP: Counting existing ResourceQuota @ 04/15/24 06:57:14.923
  STEP: Creating a ResourceQuota @ 04/15/24 06:57:19.932
  STEP: Ensuring resource quota status is calculated @ 04/15/24 06:57:19.946
  STEP: Creating a ReplicaSet @ 04/15/24 06:57:21.956
  STEP: Ensuring resource quota status captures replicaset creation @ 04/15/24 06:57:21.988
  STEP: Deleting a ReplicaSet @ 04/15/24 06:57:23.998
  STEP: Ensuring resource quota status released usage @ 04/15/24 06:57:24.013
  Apr 15 06:57:26.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7761" for this suite. @ 04/15/24 06:57:26.039
• [11.190 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 04/15/24 06:57:26.06
  Apr 15 06:57:26.060: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename pods @ 04/15/24 06:57:26.063
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:57:26.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:57:26.111
  STEP: creating a Pod with a static label @ 04/15/24 06:57:26.131
  STEP: watching for Pod to be ready @ 04/15/24 06:57:26.153
  Apr 15 06:57:26.157: INFO: observed Pod pod-test in namespace pods-1620 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Apr 15 06:57:26.176: INFO: observed Pod pod-test in namespace pods-1620 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:57:26 +0000 UTC  }]
  Apr 15 06:57:26.203: INFO: observed Pod pod-test in namespace pods-1620 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:57:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:57:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:57:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:57:26 +0000 UTC  }]
  Apr 15 06:57:27.821: INFO: Found Pod pod-test in namespace pods-1620 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:57:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:57:27 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:57:27 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:57:26 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 04/15/24 06:57:27.828
  STEP: getting the Pod and ensuring that it's patched @ 04/15/24 06:57:27.849
  STEP: replacing the Pod's status Ready condition to False @ 04/15/24 06:57:27.857
  STEP: check the Pod again to ensure its Ready conditions are False @ 04/15/24 06:57:27.886
  STEP: deleting the Pod via a Collection with a LabelSelector @ 04/15/24 06:57:27.887
  STEP: watching for the Pod to be deleted @ 04/15/24 06:57:27.911
  Apr 15 06:57:27.928: INFO: observed event type MODIFIED
  Apr 15 06:57:29.850: INFO: observed event type MODIFIED
  Apr 15 06:57:30.195: INFO: observed event type MODIFIED
  Apr 15 06:57:30.862: INFO: observed event type MODIFIED
  Apr 15 06:57:30.893: INFO: observed event type MODIFIED
  Apr 15 06:57:30.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1620" for this suite. @ 04/15/24 06:57:30.917
• [4.871 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 04/15/24 06:57:30.934
  Apr 15 06:57:30.934: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename pods @ 04/15/24 06:57:30.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:57:30.976
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:57:30.984
  Apr 15 06:57:30.994: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: creating the pod @ 04/15/24 06:57:30.996
  STEP: submitting the pod to kubernetes @ 04/15/24 06:57:30.997
  Apr 15 06:57:33.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9996" for this suite. @ 04/15/24 06:57:33.236
• [2.323 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:790
  STEP: Creating a kubernetes client @ 04/15/24 06:57:33.26
  Apr 15 06:57:33.260: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename statefulset @ 04/15/24 06:57:33.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:57:33.313
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:57:33.319
  STEP: Creating service test in namespace statefulset-4744 @ 04/15/24 06:57:33.324
  STEP: Looking for a node to schedule stateful set and pod @ 04/15/24 06:57:33.341
  STEP: Creating pod with conflicting port in namespace statefulset-4744 @ 04/15/24 06:57:33.355
  STEP: Waiting until pod test-pod will start running in namespace statefulset-4744 @ 04/15/24 06:57:33.387
  STEP: Creating statefulset with conflicting port in namespace statefulset-4744 @ 04/15/24 06:57:35.413
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4744 @ 04/15/24 06:57:35.427
  Apr 15 06:57:35.465: INFO: Observed stateful pod in namespace: statefulset-4744, name: ss-0, uid: f2ac01a3-10fa-43eb-a637-5849390bc28b, status phase: Pending. Waiting for statefulset controller to delete.
  Apr 15 06:57:35.510: INFO: Observed stateful pod in namespace: statefulset-4744, name: ss-0, uid: f2ac01a3-10fa-43eb-a637-5849390bc28b, status phase: Failed. Waiting for statefulset controller to delete.
  Apr 15 06:57:35.546: INFO: Observed stateful pod in namespace: statefulset-4744, name: ss-0, uid: f2ac01a3-10fa-43eb-a637-5849390bc28b, status phase: Failed. Waiting for statefulset controller to delete.
  Apr 15 06:57:35.559: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4744
  STEP: Removing pod with conflicting port in namespace statefulset-4744 @ 04/15/24 06:57:35.559
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4744 and will be in running state @ 04/15/24 06:57:35.584
  Apr 15 06:57:37.606: INFO: Deleting all statefulset in ns statefulset-4744
  Apr 15 06:57:37.616: INFO: Scaling statefulset ss to 0
  Apr 15 06:57:47.659: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 06:57:47.671: INFO: Deleting statefulset ss
  Apr 15 06:57:47.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4744" for this suite. @ 04/15/24 06:57:47.72
• [14.485 seconds]
------------------------------
SS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 04/15/24 06:57:47.747
  Apr 15 06:57:47.748: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename job @ 04/15/24 06:57:47.752
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:57:47.786
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:57:47.793
  STEP: Creating a job @ 04/15/24 06:57:47.797
  STEP: Ensure pods equal to parallelism count is attached to the job @ 04/15/24 06:57:47.808
  STEP: patching /status @ 04/15/24 06:57:51.821
  STEP: updating /status @ 04/15/24 06:57:51.843
  STEP: get /status @ 04/15/24 06:57:51.919
  Apr 15 06:57:51.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8398" for this suite. @ 04/15/24 06:57:51.942
• [4.218 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 04/15/24 06:57:51.985
  Apr 15 06:57:51.985: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename configmap @ 04/15/24 06:57:51.993
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:57:52.096
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:57:52.106
  STEP: Creating configMap configmap-6481/configmap-test-b7e926d1-d49d-46aa-9c4a-f9cb8e115113 @ 04/15/24 06:57:52.113
  STEP: Creating a pod to test consume configMaps @ 04/15/24 06:57:52.123
  STEP: Saw pod success @ 04/15/24 06:57:56.168
  Apr 15 06:57:56.174: INFO: Trying to get logs from node zaigh3ewotoh-2 pod pod-configmaps-88a0a769-daa9-4c53-89eb-81fb3edc37fc container env-test: <nil>
  STEP: delete the pod @ 04/15/24 06:57:56.195
  Apr 15 06:57:56.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6481" for this suite. @ 04/15/24 06:57:56.247
• [4.280 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 04/15/24 06:57:56.274
  Apr 15 06:57:56.274: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename dns @ 04/15/24 06:57:56.28
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:57:56.312
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:57:56.318
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 04/15/24 06:57:56.326
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 04/15/24 06:57:56.326
  STEP: creating a pod to probe DNS @ 04/15/24 06:57:56.326
  STEP: submitting the pod to kubernetes @ 04/15/24 06:57:56.327
  STEP: retrieving the pod @ 04/15/24 06:58:00.39
  STEP: looking for the results for each expected name from probers @ 04/15/24 06:58:00.399
  Apr 15 06:58:00.449: INFO: DNS probes using dns-7630/dns-test-b0950766-35d9-4d04-9cd8-7aa200c997b3 succeeded

  Apr 15 06:58:00.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 06:58:00.463
  STEP: Destroying namespace "dns-7630" for this suite. @ 04/15/24 06:58:00.524
• [4.268 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 04/15/24 06:58:00.543
  Apr 15 06:58:00.543: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename webhook @ 04/15/24 06:58:00.546
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:58:00.61
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:58:00.617
  STEP: Setting up server cert @ 04/15/24 06:58:00.689
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 06:58:01.821
  STEP: Deploying the webhook pod @ 04/15/24 06:58:01.846
  STEP: Wait for the deployment to be ready @ 04/15/24 06:58:01.881
  Apr 15 06:58:01.903: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/15/24 06:58:03.932
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 06:58:03.955
  Apr 15 06:58:04.957: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 04/15/24 06:58:04.967
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/15/24 06:58:05.017
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 04/15/24 06:58:05.052
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/15/24 06:58:05.084
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 04/15/24 06:58:05.121
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/15/24 06:58:05.145
  Apr 15 06:58:05.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2286" for this suite. @ 04/15/24 06:58:05.317
  STEP: Destroying namespace "webhook-markers-3991" for this suite. @ 04/15/24 06:58:05.343
• [4.823 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:748
  STEP: Creating a kubernetes client @ 04/15/24 06:58:05.428
  Apr 15 06:58:05.428: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename statefulset @ 04/15/24 06:58:05.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:58:05.469
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:58:05.478
  STEP: Creating service test in namespace statefulset-6272 @ 04/15/24 06:58:05.484
  STEP: Creating stateful set ss in namespace statefulset-6272 @ 04/15/24 06:58:05.496
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6272 @ 04/15/24 06:58:05.51
  Apr 15 06:58:05.519: INFO: Found 0 stateful pods, waiting for 1
  Apr 15 06:58:15.533: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 04/15/24 06:58:15.534
  Apr 15 06:58:15.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-6272 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 06:58:15.965: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 06:58:15.965: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 06:58:15.965: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 15 06:58:15.977: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Apr 15 06:58:25.988: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 15 06:58:25.989: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 06:58:26.029: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Apr 15 06:58:26.030: INFO: ss-0  zaigh3ewotoh-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:05 +0000 UTC  }]
  Apr 15 06:58:26.030: INFO: 
  Apr 15 06:58:26.031: INFO: StatefulSet ss has not reached scale 3, at 1
  Apr 15 06:58:27.046: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989183132s
  Apr 15 06:58:28.060: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.974226155s
  Apr 15 06:58:29.075: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.960357248s
  Apr 15 06:58:30.089: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.944876388s
  Apr 15 06:58:31.101: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.931233518s
  Apr 15 06:58:32.112: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.91969841s
  Apr 15 06:58:33.123: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.909304366s
  Apr 15 06:58:34.132: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.897796784s
  Apr 15 06:58:35.141: INFO: Verifying statefulset ss doesn't scale past 3 for another 888.875611ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6272 @ 04/15/24 06:58:36.142
  Apr 15 06:58:36.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-6272 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 15 06:58:36.617: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 15 06:58:36.617: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 15 06:58:36.617: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 15 06:58:36.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-6272 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 15 06:58:36.915: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Apr 15 06:58:36.915: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 15 06:58:36.915: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 15 06:58:36.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-6272 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 15 06:58:37.285: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Apr 15 06:58:37.285: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 15 06:58:37.285: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 15 06:58:37.295: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 06:58:37.295: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 06:58:37.295: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 04/15/24 06:58:37.296
  Apr 15 06:58:37.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-6272 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 06:58:37.639: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 06:58:37.639: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 06:58:37.639: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 15 06:58:37.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-6272 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 06:58:37.931: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 06:58:37.931: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 06:58:37.931: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 15 06:58:37.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-6272 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 06:58:38.244: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 06:58:38.244: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 06:58:38.244: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 15 06:58:38.244: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 06:58:38.254: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  Apr 15 06:58:48.270: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 15 06:58:48.270: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Apr 15 06:58:48.270: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Apr 15 06:58:48.301: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Apr 15 06:58:48.301: INFO: ss-0  zaigh3ewotoh-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:05 +0000 UTC  }]
  Apr 15 06:58:48.301: INFO: ss-1  zaigh3ewotoh-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:26 +0000 UTC  }]
  Apr 15 06:58:48.302: INFO: ss-2  zaigh3ewotoh-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:26 +0000 UTC  }]
  Apr 15 06:58:48.302: INFO: 
  Apr 15 06:58:48.302: INFO: StatefulSet ss has not reached scale 0, at 3
  Apr 15 06:58:49.333: INFO: POD   NODE            PHASE      GRACE  CONDITIONS
  Apr 15 06:58:49.333: INFO: ss-0  zaigh3ewotoh-2  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:05 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:37 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:37 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:05 +0000 UTC  }]
  Apr 15 06:58:49.334: INFO: ss-1  zaigh3ewotoh-3  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:26 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:38 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:38 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 06:58:26 +0000 UTC  }]
  Apr 15 06:58:49.334: INFO: 
  Apr 15 06:58:49.335: INFO: StatefulSet ss has not reached scale 0, at 2
  Apr 15 06:58:50.345: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.954011607s
  Apr 15 06:58:51.358: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.943624222s
  Apr 15 06:58:52.369: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.93110165s
  Apr 15 06:58:53.381: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.920314048s
  Apr 15 06:58:54.393: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.907739942s
  Apr 15 06:58:55.408: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.895717584s
  Apr 15 06:58:56.418: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.881006471s
  Apr 15 06:58:57.428: INFO: Verifying statefulset ss doesn't scale past 0 for another 871.166149ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6272 @ 04/15/24 06:58:58.428
  Apr 15 06:58:58.447: INFO: Scaling statefulset ss to 0
  Apr 15 06:58:58.487: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 06:58:58.497: INFO: Deleting all statefulset in ns statefulset-6272
  Apr 15 06:58:58.506: INFO: Scaling statefulset ss to 0
  Apr 15 06:58:58.540: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 06:58:58.547: INFO: Deleting statefulset ss
  Apr 15 06:58:58.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6272" for this suite. @ 04/15/24 06:58:58.602
• [53.196 seconds]
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 04/15/24 06:58:58.625
  Apr 15 06:58:58.625: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename security-context @ 04/15/24 06:58:58.627
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:58:58.664
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:58:58.673
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 04/15/24 06:58:58.68
  STEP: Saw pod success @ 04/15/24 06:59:02.75
  Apr 15 06:59:02.758: INFO: Trying to get logs from node zaigh3ewotoh-3 pod security-context-e92ab3f5-b5ad-432d-bae4-d780ac387c20 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 06:59:02.794
  Apr 15 06:59:02.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-9090" for this suite. @ 04/15/24 06:59:02.894
• [4.284 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 04/15/24 06:59:02.915
  Apr 15 06:59:02.915: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename dns @ 04/15/24 06:59:02.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:59:02.984
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:59:02.993
  STEP: Creating a test headless service @ 04/15/24 06:59:02.998
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4575.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4575.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4575.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4575.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4575.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4575.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4575.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4575.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4575.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4575.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 44.23.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.23.44_udp@PTR;check="$$(dig +tcp +noall +answer +search 44.23.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.23.44_tcp@PTR;sleep 1; done
   @ 04/15/24 06:59:03.039
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4575.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4575.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4575.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4575.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4575.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4575.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4575.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4575.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4575.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4575.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 44.23.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.23.44_udp@PTR;check="$$(dig +tcp +noall +answer +search 44.23.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.23.44_tcp@PTR;sleep 1; done
   @ 04/15/24 06:59:03.039
  STEP: creating a pod to probe DNS @ 04/15/24 06:59:03.04
  STEP: submitting the pod to kubernetes @ 04/15/24 06:59:03.04
  STEP: retrieving the pod @ 04/15/24 06:59:07.146
  STEP: looking for the results for each expected name from probers @ 04/15/24 06:59:07.165
  Apr 15 06:59:07.213: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:07.220: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:07.275: INFO: Unable to read jessie_udp@dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:07.297: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:07.310: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:07.352: INFO: Lookups using dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local jessie_udp@dns-test-service.dns-4575.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local]

  Apr 15 06:59:12.392: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:12.402: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:12.517: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:12.556: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:12.641: INFO: Lookups using dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local]

  Apr 15 06:59:17.388: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:17.398: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:17.475: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:17.486: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:17.527: INFO: Lookups using dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local]

  Apr 15 06:59:22.388: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:22.399: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:22.479: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:22.492: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:22.554: INFO: Lookups using dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local]

  Apr 15 06:59:27.406: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:27.422: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:27.511: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:27.520: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:27.580: INFO: Lookups using dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local]

  Apr 15 06:59:32.392: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:32.405: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:32.504: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:32.529: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local from pod dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada: the server could not find the requested resource (get pods dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada)
  Apr 15 06:59:32.590: INFO: Lookups using dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4575.svc.cluster.local]

  Apr 15 06:59:37.570: INFO: DNS probes using dns-4575/dns-test-35dafa6e-8487-4015-b4e6-cbea30e33ada succeeded

  Apr 15 06:59:37.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 06:59:37.588
  STEP: deleting the test service @ 04/15/24 06:59:37.696
  STEP: deleting the test headless service @ 04/15/24 06:59:37.777
  STEP: Destroying namespace "dns-4575" for this suite. @ 04/15/24 06:59:37.83
• [34.942 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 04/15/24 06:59:37.878
  Apr 15 06:59:37.878: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename services @ 04/15/24 06:59:37.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:59:37.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:59:37.935
  STEP: creating a collection of services @ 04/15/24 06:59:37.943
  Apr 15 06:59:37.945: INFO: Creating e2e-svc-a-9j78m
  Apr 15 06:59:37.969: INFO: Creating e2e-svc-b-c6jfc
  Apr 15 06:59:37.995: INFO: Creating e2e-svc-c-4qnhv
  STEP: deleting service collection @ 04/15/24 06:59:38.025
  Apr 15 06:59:38.110: INFO: Collection of services has been deleted
  Apr 15 06:59:38.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8707" for this suite. @ 04/15/24 06:59:38.122
• [0.258 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 04/15/24 06:59:38.144
  Apr 15 06:59:38.144: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename field-validation @ 04/15/24 06:59:38.149
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:59:38.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:59:38.196
  STEP: apply creating a deployment @ 04/15/24 06:59:38.203
  Apr 15 06:59:38.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6422" for this suite. @ 04/15/24 06:59:38.256
• [0.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 04/15/24 06:59:38.297
  Apr 15 06:59:38.297: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename emptydir-wrapper @ 04/15/24 06:59:38.3
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:59:38.344
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:59:38.353
  Apr 15 06:59:40.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 04/15/24 06:59:40.491
  STEP: Cleaning up the configmap @ 04/15/24 06:59:40.517
  STEP: Cleaning up the pod @ 04/15/24 06:59:40.569
  STEP: Destroying namespace "emptydir-wrapper-9668" for this suite. @ 04/15/24 06:59:40.642
• [2.382 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 04/15/24 06:59:40.687
  Apr 15 06:59:40.687: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename watch @ 04/15/24 06:59:40.694
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 06:59:40.73
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 06:59:40.736
  STEP: creating a watch on configmaps with label A @ 04/15/24 06:59:40.741
  STEP: creating a watch on configmaps with label B @ 04/15/24 06:59:40.743
  STEP: creating a watch on configmaps with label A or B @ 04/15/24 06:59:40.747
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 04/15/24 06:59:40.749
  Apr 15 06:59:40.762: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5341  6f81161a-3fe8-46d5-9bb8-77e9e2acccc7 158886 0 2024-04-15 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-04-15 06:59:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 06:59:40.764: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5341  6f81161a-3fe8-46d5-9bb8-77e9e2acccc7 158886 0 2024-04-15 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-04-15 06:59:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 04/15/24 06:59:40.765
  Apr 15 06:59:40.791: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5341  6f81161a-3fe8-46d5-9bb8-77e9e2acccc7 158887 0 2024-04-15 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-04-15 06:59:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 06:59:40.792: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5341  6f81161a-3fe8-46d5-9bb8-77e9e2acccc7 158887 0 2024-04-15 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-04-15 06:59:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 04/15/24 06:59:40.793
  Apr 15 06:59:40.831: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5341  6f81161a-3fe8-46d5-9bb8-77e9e2acccc7 158888 0 2024-04-15 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-04-15 06:59:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 06:59:40.832: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5341  6f81161a-3fe8-46d5-9bb8-77e9e2acccc7 158888 0 2024-04-15 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-04-15 06:59:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 04/15/24 06:59:40.833
  Apr 15 06:59:40.862: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5341  6f81161a-3fe8-46d5-9bb8-77e9e2acccc7 158889 0 2024-04-15 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-04-15 06:59:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 06:59:40.862: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5341  6f81161a-3fe8-46d5-9bb8-77e9e2acccc7 158889 0 2024-04-15 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-04-15 06:59:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 04/15/24 06:59:40.862
  Apr 15 06:59:40.889: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5341  01d5d009-9b1f-4cd0-953c-4e9b48c6311d 158890 0 2024-04-15 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-04-15 06:59:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 06:59:40.890: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5341  01d5d009-9b1f-4cd0-953c-4e9b48c6311d 158890 0 2024-04-15 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-04-15 06:59:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 04/15/24 06:59:50.892
  Apr 15 06:59:50.909: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5341  01d5d009-9b1f-4cd0-953c-4e9b48c6311d 158944 0 2024-04-15 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-04-15 06:59:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 06:59:50.909: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5341  01d5d009-9b1f-4cd0-953c-4e9b48c6311d 158944 0 2024-04-15 06:59:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-04-15 06:59:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 07:00:00.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5341" for this suite. @ 04/15/24 07:00:00.926
• [20.261 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 04/15/24 07:00:00.951
  Apr 15 07:00:00.951: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/15/24 07:00:00.953
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:00:01.046
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:00:01.057
  STEP: create the container to handle the HTTPGet hook request. @ 04/15/24 07:00:01.081
  STEP: create the pod with lifecycle hook @ 04/15/24 07:00:03.129
  STEP: check poststart hook @ 04/15/24 07:00:05.179
  STEP: delete the pod with lifecycle hook @ 04/15/24 07:00:05.203
  Apr 15 07:00:07.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8812" for this suite. @ 04/15/24 07:00:07.266
• [6.333 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 04/15/24 07:00:07.29
  Apr 15 07:00:07.290: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 07:00:07.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:00:07.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:00:07.353
  STEP: Creating pod test-grpc-97f84668-27c6-4cf0-8f92-796c2154f42d in namespace container-probe-6507 @ 04/15/24 07:00:07.365
  Apr 15 07:00:09.424: INFO: Started pod test-grpc-97f84668-27c6-4cf0-8f92-796c2154f42d in namespace container-probe-6507
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/15/24 07:00:09.424
  Apr 15 07:00:09.438: INFO: Initial restart count of pod test-grpc-97f84668-27c6-4cf0-8f92-796c2154f42d is 0
  Apr 15 07:01:25.905: INFO: Restart count of pod container-probe-6507/test-grpc-97f84668-27c6-4cf0-8f92-796c2154f42d is now 1 (1m16.466106024s elapsed)
  Apr 15 07:01:25.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 07:01:25.92
  STEP: Destroying namespace "container-probe-6507" for this suite. @ 04/15/24 07:01:25.956
• [78.690 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 04/15/24 07:01:25.982
  Apr 15 07:01:25.982: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename csiinlinevolumes @ 04/15/24 07:01:25.984
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:01:26.051
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:01:26.057
  STEP: creating @ 04/15/24 07:01:26.062
  STEP: getting @ 04/15/24 07:01:26.16
  STEP: listing in namespace @ 04/15/24 07:01:26.173
  STEP: patching @ 04/15/24 07:01:26.183
  STEP: deleting @ 04/15/24 07:01:26.205
  Apr 15 07:01:26.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-3115" for this suite. @ 04/15/24 07:01:26.244
• [0.280 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 04/15/24 07:01:26.266
  Apr 15 07:01:26.266: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename svcaccounts @ 04/15/24 07:01:26.268
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:01:26.315
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:01:26.322
  Apr 15 07:01:26.335: INFO: Got root ca configmap in namespace "svcaccounts-8928"
  Apr 15 07:01:26.351: INFO: Deleted root ca configmap in namespace "svcaccounts-8928"
  STEP: waiting for a new root ca configmap created @ 04/15/24 07:01:26.852
  Apr 15 07:01:26.860: INFO: Recreated root ca configmap in namespace "svcaccounts-8928"
  Apr 15 07:01:26.873: INFO: Updated root ca configmap in namespace "svcaccounts-8928"
  STEP: waiting for the root ca configmap reconciled @ 04/15/24 07:01:27.374
  Apr 15 07:01:27.385: INFO: Reconciled root ca configmap in namespace "svcaccounts-8928"
  Apr 15 07:01:27.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8928" for this suite. @ 04/15/24 07:01:27.401
• [1.157 seconds]
------------------------------
S
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 04/15/24 07:01:27.423
  Apr 15 07:01:27.424: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename job @ 04/15/24 07:01:27.427
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:01:27.47
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:01:27.477
  STEP: Creating a job @ 04/15/24 07:01:27.482
  STEP: Ensuring active pods == parallelism @ 04/15/24 07:01:27.5
  STEP: delete a job @ 04/15/24 07:01:29.511
  STEP: deleting Job.batch foo in namespace job-9522, will wait for the garbage collector to delete the pods @ 04/15/24 07:01:29.512
  Apr 15 07:01:29.589: INFO: Deleting Job.batch foo took: 17.965483ms
  Apr 15 07:01:29.690: INFO: Terminating Job.batch foo pods took: 101.126207ms
  STEP: Ensuring job was deleted @ 04/15/24 07:02:01.491
  Apr 15 07:02:01.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9522" for this suite. @ 04/15/24 07:02:01.513
• [34.106 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 04/15/24 07:02:01.53
  Apr 15 07:02:01.530: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 07:02:01.533
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:02:01.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:02:01.578
  STEP: Creating pod busybox-3526bc9e-81fc-45d6-8d0a-659b0f2ad6ab in namespace container-probe-6475 @ 04/15/24 07:02:01.583
  Apr 15 07:02:03.626: INFO: Started pod busybox-3526bc9e-81fc-45d6-8d0a-659b0f2ad6ab in namespace container-probe-6475
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/15/24 07:02:03.626
  Apr 15 07:02:03.634: INFO: Initial restart count of pod busybox-3526bc9e-81fc-45d6-8d0a-659b0f2ad6ab is 0
  Apr 15 07:06:05.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 07:06:05.089
  STEP: Destroying namespace "container-probe-6475" for this suite. @ 04/15/24 07:06:05.133
• [243.627 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 04/15/24 07:06:05.159
  Apr 15 07:06:05.160: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename replicaset @ 04/15/24 07:06:05.165
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:06:05.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:06:05.248
  STEP: Create a ReplicaSet @ 04/15/24 07:06:05.256
  STEP: Verify that the required pods have come up @ 04/15/24 07:06:05.274
  Apr 15 07:06:05.285: INFO: Pod name sample-pod: Found 0 pods out of 3
  Apr 15 07:06:10.300: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 04/15/24 07:06:10.3
  Apr 15 07:06:10.310: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 04/15/24 07:06:10.31
  STEP: DeleteCollection of the ReplicaSets @ 04/15/24 07:06:10.32
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 04/15/24 07:06:10.347
  Apr 15 07:06:10.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8724" for this suite. @ 04/15/24 07:06:10.368
• [5.241 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 04/15/24 07:06:10.404
  Apr 15 07:06:10.404: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 07:06:10.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:06:10.535
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:06:10.545
  STEP: Creating the pod @ 04/15/24 07:06:10.554
  Apr 15 07:06:13.293: INFO: Successfully updated pod "labelsupdate18a96d40-f3cb-4c4c-8bab-424dcb0ad082"
  Apr 15 07:06:15.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5332" for this suite. @ 04/15/24 07:06:15.339
• [4.960 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 04/15/24 07:06:15.364
  Apr 15 07:06:15.364: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 07:06:15.37
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:06:15.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:06:15.421
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 04/15/24 07:06:15.431
  STEP: Saw pod success @ 04/15/24 07:06:19.505
  Apr 15 07:06:19.538: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-9907f8dd-e425-4d29-a714-569e9f702ed0 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 07:06:19.564
  Apr 15 07:06:19.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5608" for this suite. @ 04/15/24 07:06:19.622
• [4.275 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 04/15/24 07:06:19.645
  Apr 15 07:06:19.645: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename security-context-test @ 04/15/24 07:06:19.647
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:06:19.699
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:06:19.705
  Apr 15 07:06:23.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6665" for this suite. @ 04/15/24 07:06:23.795
• [4.168 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 04/15/24 07:06:23.813
  Apr 15 07:06:23.813: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename dns @ 04/15/24 07:06:23.819
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:06:23.857
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:06:23.862
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1327.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1327.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 04/15/24 07:06:23.869
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1327.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1327.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 04/15/24 07:06:23.87
  STEP: creating a pod to probe /etc/hosts @ 04/15/24 07:06:23.87
  STEP: submitting the pod to kubernetes @ 04/15/24 07:06:23.87
  STEP: retrieving the pod @ 04/15/24 07:06:25.921
  STEP: looking for the results for each expected name from probers @ 04/15/24 07:06:25.93
  Apr 15 07:06:25.978: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-1327/dns-test-53d18f92-f85b-4d21-853e-f1a8a0b86da5: the server could not find the requested resource (get pods dns-test-53d18f92-f85b-4d21-853e-f1a8a0b86da5)
  Apr 15 07:06:25.978: INFO: Lookups using dns-1327/dns-test-53d18f92-f85b-4d21-853e-f1a8a0b86da5 failed for: [jessie_hosts@dns-querier-1]

  Apr 15 07:06:31.030: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-1327/dns-test-53d18f92-f85b-4d21-853e-f1a8a0b86da5: the server could not find the requested resource (get pods dns-test-53d18f92-f85b-4d21-853e-f1a8a0b86da5)
  Apr 15 07:06:31.030: INFO: Lookups using dns-1327/dns-test-53d18f92-f85b-4d21-853e-f1a8a0b86da5 failed for: [jessie_hosts@dns-querier-1]

  Apr 15 07:06:36.031: INFO: DNS probes using dns-1327/dns-test-53d18f92-f85b-4d21-853e-f1a8a0b86da5 succeeded

  Apr 15 07:06:36.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 07:06:36.046
  STEP: Destroying namespace "dns-1327" for this suite. @ 04/15/24 07:06:36.087
• [12.296 seconds]
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 04/15/24 07:06:36.11
  Apr 15 07:06:36.110: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename secrets @ 04/15/24 07:06:36.12
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:06:36.163
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:06:36.172
  Apr 15 07:06:36.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2933" for this suite. @ 04/15/24 07:06:36.327
• [0.235 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 04/15/24 07:06:36.346
  Apr 15 07:06:36.346: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 07:06:36.349
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:06:36.393
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:06:36.401
  STEP: Creating pod liveness-945661db-b83d-44da-a764-dcccd8f4c63c in namespace container-probe-4094 @ 04/15/24 07:06:36.411
  Apr 15 07:06:38.479: INFO: Started pod liveness-945661db-b83d-44da-a764-dcccd8f4c63c in namespace container-probe-4094
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/15/24 07:06:38.48
  Apr 15 07:06:38.492: INFO: Initial restart count of pod liveness-945661db-b83d-44da-a764-dcccd8f4c63c is 0
  Apr 15 07:10:39.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 07:10:40.011
  STEP: Destroying namespace "container-probe-4094" for this suite. @ 04/15/24 07:10:40.085
• [243.781 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 04/15/24 07:10:40.129
  Apr 15 07:10:40.129: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename namespaces @ 04/15/24 07:10:40.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:10:40.204
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:10:40.213
  STEP: Creating namespace "e2e-ns-z5p8f" @ 04/15/24 07:10:40.241
  Apr 15 07:10:40.293: INFO: Namespace "e2e-ns-z5p8f-9240" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-z5p8f-9240" @ 04/15/24 07:10:40.293
  Apr 15 07:10:40.331: INFO: Namespace "e2e-ns-z5p8f-9240" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-z5p8f-9240" @ 04/15/24 07:10:40.332
  Apr 15 07:10:40.358: INFO: Namespace "e2e-ns-z5p8f-9240" has []v1.FinalizerName{"kubernetes"}
  Apr 15 07:10:40.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3931" for this suite. @ 04/15/24 07:10:40.379
  STEP: Destroying namespace "e2e-ns-z5p8f-9240" for this suite. @ 04/15/24 07:10:40.395
• [0.293 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 04/15/24 07:10:40.437
  Apr 15 07:10:40.439: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 07:10:40.443
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:10:40.503
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:10:40.518
  STEP: Creating pod test-grpc-acded5a8-3a3c-4fcc-a165-daaec4dcb93c in namespace container-probe-1199 @ 04/15/24 07:10:40.533
  Apr 15 07:10:42.601: INFO: Started pod test-grpc-acded5a8-3a3c-4fcc-a165-daaec4dcb93c in namespace container-probe-1199
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/15/24 07:10:42.601
  Apr 15 07:10:42.610: INFO: Initial restart count of pod test-grpc-acded5a8-3a3c-4fcc-a165-daaec4dcb93c is 0
  Apr 15 07:14:43.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 07:14:44.006
  STEP: Destroying namespace "container-probe-1199" for this suite. @ 04/15/24 07:14:44.05
• [243.636 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 04/15/24 07:14:44.079
  Apr 15 07:14:44.079: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename field-validation @ 04/15/24 07:14:44.095
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:14:44.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:14:44.176
  STEP: apply creating a deployment @ 04/15/24 07:14:44.182
  Apr 15 07:14:44.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9216" for this suite. @ 04/15/24 07:14:44.244
• [0.182 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 04/15/24 07:14:44.277
  Apr 15 07:14:44.277: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 07:14:44.279
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:14:44.329
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:14:44.335
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 07:14:44.341
  STEP: Saw pod success @ 04/15/24 07:14:48.401
  Apr 15 07:14:48.410: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downwardapi-volume-c87d26bf-f84e-4453-8833-2ccee0a28d53 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 07:14:48.46
  Apr 15 07:14:48.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3158" for this suite. @ 04/15/24 07:14:48.573
• [4.315 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 04/15/24 07:14:48.593
  Apr 15 07:14:48.593: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/15/24 07:14:48.599
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:14:48.64
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:14:48.648
  Apr 15 07:14:48.655: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:14:52.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8816" for this suite. @ 04/15/24 07:14:52.301
• [3.723 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 04/15/24 07:14:52.322
  Apr 15 07:14:52.322: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename secrets @ 04/15/24 07:14:52.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:14:52.367
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:14:52.372
  STEP: Creating secret with name secret-test-c7095272-d73f-439f-9126-d1067ac78f0a @ 04/15/24 07:14:52.423
  STEP: Creating a pod to test consume secrets @ 04/15/24 07:14:52.433
  STEP: Saw pod success @ 04/15/24 07:14:56.494
  Apr 15 07:14:56.509: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-secrets-7bfc1f78-a502-49a3-83ad-6f430bca1dd1 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 07:14:56.556
  Apr 15 07:14:56.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-253" for this suite. @ 04/15/24 07:14:56.604
  STEP: Destroying namespace "secret-namespace-333" for this suite. @ 04/15/24 07:14:56.617
• [4.308 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 04/15/24 07:14:56.633
  Apr 15 07:14:56.633: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 07:14:56.635
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:14:56.679
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:14:56.685
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-c77c4e3e-d1e9-456e-886b-5edfda969d2f @ 04/15/24 07:14:56.708
  STEP: Creating the pod @ 04/15/24 07:14:56.727
  STEP: Updating configmap projected-configmap-test-upd-c77c4e3e-d1e9-456e-886b-5edfda969d2f @ 04/15/24 07:14:58.842
  STEP: waiting to observe update in volume @ 04/15/24 07:14:58.859
  Apr 15 07:16:03.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8664" for this suite. @ 04/15/24 07:16:03.621
• [67.003 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:177
  STEP: Creating a kubernetes client @ 04/15/24 07:16:03.644
  Apr 15 07:16:03.644: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename daemonsets @ 04/15/24 07:16:03.646
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:16:03.708
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:16:03.714
  STEP: Creating simple DaemonSet "daemon-set" @ 04/15/24 07:16:03.78
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/15/24 07:16:03.795
  Apr 15 07:16:03.815: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:16:03.815: INFO: Node zaigh3ewotoh-1 is running 0 daemon pod, expected 1
  Apr 15 07:16:04.848: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:16:04.848: INFO: Node zaigh3ewotoh-1 is running 0 daemon pod, expected 1
  Apr 15 07:16:05.840: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 15 07:16:05.841: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 04/15/24 07:16:05.851
  Apr 15 07:16:05.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 15 07:16:05.901: INFO: Node zaigh3ewotoh-2 is running 0 daemon pod, expected 1
  Apr 15 07:16:06.923: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 15 07:16:06.923: INFO: Node zaigh3ewotoh-2 is running 0 daemon pod, expected 1
  Apr 15 07:16:07.925: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 15 07:16:07.925: INFO: Node zaigh3ewotoh-2 is running 0 daemon pod, expected 1
  Apr 15 07:16:08.931: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 15 07:16:08.931: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/15/24 07:16:08.95
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1883, will wait for the garbage collector to delete the pods @ 04/15/24 07:16:08.95
  Apr 15 07:16:09.044: INFO: Deleting DaemonSet.extensions daemon-set took: 27.912064ms
  Apr 15 07:16:09.245: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.927837ms
  Apr 15 07:16:11.354: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:16:11.354: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 15 07:16:11.363: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"161265"},"items":null}

  Apr 15 07:16:11.377: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"161265"},"items":null}

  Apr 15 07:16:11.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1883" for this suite. @ 04/15/24 07:16:11.431
• [7.801 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 04/15/24 07:16:11.45
  Apr 15 07:16:11.450: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename pod-network-test @ 04/15/24 07:16:11.452
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:16:11.484
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:16:11.492
  STEP: Performing setup for networking test in namespace pod-network-test-9974 @ 04/15/24 07:16:11.497
  STEP: creating a selector @ 04/15/24 07:16:11.497
  STEP: Creating the service pods in kubernetes @ 04/15/24 07:16:11.497
  Apr 15 07:16:11.498: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 04/15/24 07:16:33.755
  Apr 15 07:16:35.824: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 15 07:16:35.824: INFO: Going to poll 10.233.64.194 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Apr 15 07:16:35.830: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.194:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9974 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 07:16:35.830: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:16:35.832: INFO: ExecWithOptions: Clientset creation
  Apr 15 07:16:35.833: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9974/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.194%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 15 07:16:35.990: INFO: Found all 1 expected endpoints: [netserver-0]
  Apr 15 07:16:35.990: INFO: Going to poll 10.233.65.153 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Apr 15 07:16:35.995: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.153:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9974 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 07:16:35.996: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:16:35.997: INFO: ExecWithOptions: Clientset creation
  Apr 15 07:16:35.997: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9974/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.65.153%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 15 07:16:36.114: INFO: Found all 1 expected endpoints: [netserver-1]
  Apr 15 07:16:36.114: INFO: Going to poll 10.233.66.141 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Apr 15 07:16:36.122: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.141:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9974 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 07:16:36.123: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:16:36.124: INFO: ExecWithOptions: Clientset creation
  Apr 15 07:16:36.124: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9974/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.141%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 15 07:16:36.246: INFO: Found all 1 expected endpoints: [netserver-2]
  Apr 15 07:16:36.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-9974" for this suite. @ 04/15/24 07:16:36.257
• [24.820 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 04/15/24 07:16:36.272
  Apr 15 07:16:36.272: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename replicaset @ 04/15/24 07:16:36.275
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:16:36.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:16:36.327
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 04/15/24 07:16:36.332
  STEP: When a replicaset with a matching selector is created @ 04/15/24 07:16:38.373
  STEP: Then the orphan pod is adopted @ 04/15/24 07:16:38.388
  STEP: When the matched label of one of its pods change @ 04/15/24 07:16:39.404
  Apr 15 07:16:39.416: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 04/15/24 07:16:39.439
  Apr 15 07:16:40.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6798" for this suite. @ 04/15/24 07:16:40.471
• [4.227 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 04/15/24 07:16:40.504
  Apr 15 07:16:40.504: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 07:16:40.51
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:16:40.562
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:16:40.569
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 07:16:40.575
  STEP: Saw pod success @ 04/15/24 07:16:44.623
  Apr 15 07:16:44.632: INFO: Trying to get logs from node zaigh3ewotoh-2 pod downwardapi-volume-1198dd70-fc3e-4922-b101-5b823d680d9b container client-container: <nil>
  STEP: delete the pod @ 04/15/24 07:16:44.665
  Apr 15 07:16:44.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3576" for this suite. @ 04/15/24 07:16:44.734
• [4.258 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 04/15/24 07:16:44.768
  Apr 15 07:16:44.768: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename services @ 04/15/24 07:16:44.771
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:16:44.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:16:44.813
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-978 @ 04/15/24 07:16:44.819
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 04/15/24 07:16:44.843
  STEP: creating service externalsvc in namespace services-978 @ 04/15/24 07:16:44.844
  STEP: creating replication controller externalsvc in namespace services-978 @ 04/15/24 07:16:44.879
  I0415 07:16:44.898328      13 runners.go:194] Created replication controller with name: externalsvc, namespace: services-978, replica count: 2
  I0415 07:16:47.949388      13 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 04/15/24 07:16:47.958
  Apr 15 07:16:47.994: INFO: Creating new exec pod
  Apr 15 07:16:50.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-978 exec execpodkvkql -- /bin/sh -x -c nslookup clusterip-service.services-978.svc.cluster.local'
  Apr 15 07:16:50.466: INFO: stderr: "+ nslookup clusterip-service.services-978.svc.cluster.local\n"
  Apr 15 07:16:50.466: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-978.svc.cluster.local\tcanonical name = externalsvc.services-978.svc.cluster.local.\nName:\texternalsvc.services-978.svc.cluster.local\nAddress: 10.233.58.197\n\n"
  Apr 15 07:16:50.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-978, will wait for the garbage collector to delete the pods @ 04/15/24 07:16:50.477
  Apr 15 07:16:50.551: INFO: Deleting ReplicationController externalsvc took: 16.149074ms
  Apr 15 07:16:50.651: INFO: Terminating ReplicationController externalsvc pods took: 100.742927ms
  Apr 15 07:16:52.691: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-978" for this suite. @ 04/15/24 07:16:52.727
• [7.975 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 04/15/24 07:16:52.747
  Apr 15 07:16:52.747: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename configmap @ 04/15/24 07:16:52.749
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:16:52.782
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:16:52.788
  STEP: Creating configMap configmap-9968/configmap-test-7822114b-c47a-4cca-8c09-b3778551443e @ 04/15/24 07:16:52.794
  STEP: Creating a pod to test consume configMaps @ 04/15/24 07:16:52.804
  STEP: Saw pod success @ 04/15/24 07:16:56.849
  Apr 15 07:16:56.859: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-configmaps-e196d8ed-4247-47e7-af0e-cdb5f24a6a94 container env-test: <nil>
  STEP: delete the pod @ 04/15/24 07:16:56.877
  Apr 15 07:16:56.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9968" for this suite. @ 04/15/24 07:16:56.919
• [4.194 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 04/15/24 07:16:56.95
  Apr 15 07:16:56.951: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 07:16:56.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:16:57.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:16:57.041
  STEP: validating api versions @ 04/15/24 07:16:57.049
  Apr 15 07:16:57.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-1631 api-versions'
  Apr 15 07:16:57.181: INFO: stderr: ""
  Apr 15 07:16:57.181: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Apr 15 07:16:57.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1631" for this suite. @ 04/15/24 07:16:57.192
• [0.253 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 04/15/24 07:16:57.205
  Apr 15 07:16:57.205: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/15/24 07:16:57.209
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:16:57.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:16:57.254
  STEP: set up a multi version CRD @ 04/15/24 07:16:57.264
  Apr 15 07:16:57.265: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: mark a version not serverd @ 04/15/24 07:17:01.902
  STEP: check the unserved version gets removed @ 04/15/24 07:17:01.935
  STEP: check the other version is not changed @ 04/15/24 07:17:03.584
  Apr 15 07:17:07.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3042" for this suite. @ 04/15/24 07:17:07.199
• [10.008 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 04/15/24 07:17:07.213
  Apr 15 07:17:07.213: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 07:17:07.215
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:17:07.245
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:17:07.252
  STEP: Creating pod test-webserver-fe9b6e47-7757-4890-86e4-f0895cb55749 in namespace container-probe-3915 @ 04/15/24 07:17:07.258
  Apr 15 07:17:09.291: INFO: Started pod test-webserver-fe9b6e47-7757-4890-86e4-f0895cb55749 in namespace container-probe-3915
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/15/24 07:17:09.291
  Apr 15 07:17:09.296: INFO: Initial restart count of pod test-webserver-fe9b6e47-7757-4890-86e4-f0895cb55749 is 0
  Apr 15 07:21:10.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 07:21:10.677
  STEP: Destroying namespace "container-probe-3915" for this suite. @ 04/15/24 07:21:10.715
• [243.568 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 04/15/24 07:21:10.787
  Apr 15 07:21:10.787: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename deployment @ 04/15/24 07:21:10.816
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:21:10.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:21:10.909
  Apr 15 07:21:10.915: INFO: Creating deployment "test-recreate-deployment"
  Apr 15 07:21:10.934: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Apr 15 07:21:10.949: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
  Apr 15 07:21:12.971: INFO: Waiting deployment "test-recreate-deployment" to complete
  Apr 15 07:21:12.993: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Apr 15 07:21:13.040: INFO: Updating deployment test-recreate-deployment
  Apr 15 07:21:13.041: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Apr 15 07:21:13.270: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-6926  850f9502-7c75-47dc-93c6-7ccf0dac4ce8 162185 2 2024-04-15 07:21:10 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-04-15 07:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 07:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d769e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-04-15 07:21:13 +0000 UTC,LastTransitionTime:2024-04-15 07:21:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2024-04-15 07:21:13 +0000 UTC,LastTransitionTime:2024-04-15 07:21:10 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Apr 15 07:21:13.287: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-6926  abfb8cfb-d0c8-4904-a09d-13b7ce8e82bd 162182 1 2024-04-15 07:21:13 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 850f9502-7c75-47dc-93c6-7ccf0dac4ce8 0xc00429dd87 0xc00429dd88}] [] [{kube-controller-manager Update apps/v1 2024-04-15 07:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"850f9502-7c75-47dc-93c6-7ccf0dac4ce8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 07:21:13 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00429de38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 07:21:13.287: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Apr 15 07:21:13.288: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6fcf456ccb  deployment-6926  e344de1c-8ef8-47a2-bf3b-c038d89d6c0b 162173 2 2024-04-15 07:21:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6fcf456ccb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 850f9502-7c75-47dc-93c6-7ccf0dac4ce8 0xc00429dea7 0xc00429dea8}] [] [{kube-controller-manager Update apps/v1 2024-04-15 07:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"850f9502-7c75-47dc-93c6-7ccf0dac4ce8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 07:21:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6fcf456ccb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6fcf456ccb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.47 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00429df68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 07:21:13.322: INFO: Pod "test-recreate-deployment-54757ffd6c-8vmc2" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-8vmc2 test-recreate-deployment-54757ffd6c- deployment-6926  2443b085-0d62-4233-a7cb-5885da87276a 162183 0 2024-04-15 07:21:13 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c abfb8cfb-d0c8-4904-a09d-13b7ce8e82bd 0xc005f243f7 0xc005f243f8}] [] [{kube-controller-manager Update v1 2024-04-15 07:21:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"abfb8cfb-d0c8-4904-a09d-13b7ce8e82bd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:21:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pcv9j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pcv9j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:21:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:21:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:21:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:21:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.131,PodIP:,StartTime:2024-04-15 07:21:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:21:13.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6926" for this suite. @ 04/15/24 07:21:13.348
• [2.608 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 04/15/24 07:21:13.417
  Apr 15 07:21:13.418: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 07:21:13.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:21:13.477
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:21:13.486
  STEP: Creating a pod to test downward api env vars @ 04/15/24 07:21:13.506
  STEP: Saw pod success @ 04/15/24 07:21:17.644
  Apr 15 07:21:17.662: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downward-api-440968f7-0202-4b7a-9d62-ba7d8d5c8c53 container dapi-container: <nil>
  STEP: delete the pod @ 04/15/24 07:21:17.712
  Apr 15 07:21:17.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1093" for this suite. @ 04/15/24 07:21:17.762
• [4.362 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 04/15/24 07:21:17.788
  Apr 15 07:21:17.789: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 07:21:17.79
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:21:17.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:21:17.833
  STEP: Creating a pod to test downward api env vars @ 04/15/24 07:21:17.84
  STEP: Saw pod success @ 04/15/24 07:21:21.912
  Apr 15 07:21:21.921: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downward-api-fbef2847-82fb-440f-b735-ab1cf341f531 container dapi-container: <nil>
  STEP: delete the pod @ 04/15/24 07:21:21.942
  Apr 15 07:21:21.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2042" for this suite. @ 04/15/24 07:21:21.992
• [4.221 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 04/15/24 07:21:22.011
  Apr 15 07:21:22.012: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/15/24 07:21:22.014
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:21:22.067
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:21:22.075
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 04/15/24 07:21:22.084
  Apr 15 07:21:22.087: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:21:24.193: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:21:32.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8529" for this suite. @ 04/15/24 07:21:33.018
• [11.027 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 04/15/24 07:21:33.047
  Apr 15 07:21:33.047: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename webhook @ 04/15/24 07:21:33.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:21:33.1
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:21:33.111
  STEP: Setting up server cert @ 04/15/24 07:21:33.175
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 07:21:33.808
  STEP: Deploying the webhook pod @ 04/15/24 07:21:33.826
  STEP: Wait for the deployment to be ready @ 04/15/24 07:21:33.852
  Apr 15 07:21:33.868: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 04/15/24 07:21:35.894
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 07:21:35.926
  Apr 15 07:21:36.926: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 15 07:21:36.939: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2911-crds.webhook.example.com via the AdmissionRegistration API @ 04/15/24 07:21:37.467
  STEP: Creating a custom resource that should be mutated by the webhook @ 04/15/24 07:21:37.519
  Apr 15 07:21:39.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8853" for this suite. @ 04/15/24 07:21:40.534
  STEP: Destroying namespace "webhook-markers-2410" for this suite. @ 04/15/24 07:21:40.549
• [7.528 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 04/15/24 07:21:40.583
  Apr 15 07:21:40.583: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename endpointslice @ 04/15/24 07:21:40.585
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:21:40.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:21:40.639
  Apr 15 07:21:42.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5938" for this suite. @ 04/15/24 07:21:42.859
• [2.294 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 04/15/24 07:21:42.878
  Apr 15 07:21:42.878: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 07:21:42.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:21:42.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:21:42.922
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 04/15/24 07:21:42.928
  STEP: Saw pod success @ 04/15/24 07:21:46.974
  Apr 15 07:21:46.982: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-2d0dc78e-5ee1-4a56-8a30-0f021fa6d85e container test-container: <nil>
  STEP: delete the pod @ 04/15/24 07:21:47.005
  Apr 15 07:21:47.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8649" for this suite. @ 04/15/24 07:21:47.068
• [4.215 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 04/15/24 07:21:47.098
  Apr 15 07:21:47.098: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 07:21:47.101
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:21:47.143
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:21:47.149
  STEP: creating the pod @ 04/15/24 07:21:47.155
  Apr 15 07:21:47.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3830 create -f -'
  Apr 15 07:21:48.499: INFO: stderr: ""
  Apr 15 07:21:48.501: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 04/15/24 07:21:50.54
  Apr 15 07:21:50.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3830 label pods pause testing-label=testing-label-value'
  Apr 15 07:21:50.756: INFO: stderr: ""
  Apr 15 07:21:50.756: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 04/15/24 07:21:50.756
  Apr 15 07:21:50.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3830 get pod pause -L testing-label'
  Apr 15 07:21:50.951: INFO: stderr: ""
  Apr 15 07:21:50.952: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 04/15/24 07:21:50.952
  Apr 15 07:21:50.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3830 label pods pause testing-label-'
  Apr 15 07:21:51.166: INFO: stderr: ""
  Apr 15 07:21:51.166: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 04/15/24 07:21:51.166
  Apr 15 07:21:51.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3830 get pod pause -L testing-label'
  Apr 15 07:21:51.351: INFO: stderr: ""
  Apr 15 07:21:51.351: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 04/15/24 07:21:51.351
  Apr 15 07:21:51.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3830 delete --grace-period=0 --force -f -'
  Apr 15 07:21:51.516: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 15 07:21:51.516: INFO: stdout: "pod \"pause\" force deleted\n"
  Apr 15 07:21:51.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3830 get rc,svc -l name=pause --no-headers'
  Apr 15 07:21:51.737: INFO: stderr: "No resources found in kubectl-3830 namespace.\n"
  Apr 15 07:21:51.738: INFO: stdout: ""
  Apr 15 07:21:51.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3830 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 15 07:21:51.883: INFO: stderr: ""
  Apr 15 07:21:51.883: INFO: stdout: ""
  Apr 15 07:21:51.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3830" for this suite. @ 04/15/24 07:21:51.895
• [4.815 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 04/15/24 07:21:51.918
  Apr 15 07:21:51.918: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename job @ 04/15/24 07:21:51.921
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:21:51.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:21:51.966
  STEP: Creating a job @ 04/15/24 07:21:51.975
  STEP: Ensuring job reaches completions @ 04/15/24 07:21:51.993
  Apr 15 07:22:02.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1305" for this suite. @ 04/15/24 07:22:02.031
• [10.135 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 04/15/24 07:22:02.064
  Apr 15 07:22:02.064: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 07:22:02.072
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:22:02.114
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:22:02.124
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 07:22:02.133
  STEP: Saw pod success @ 04/15/24 07:22:06.201
  Apr 15 07:22:06.211: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downwardapi-volume-e605196b-a8b3-497f-bd34-6829e25ce887 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 07:22:06.227
  Apr 15 07:22:06.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2288" for this suite. @ 04/15/24 07:22:06.306
• [4.261 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 04/15/24 07:22:06.326
  Apr 15 07:22:06.326: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 07:22:06.33
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:22:06.368
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:22:06.378
  STEP: Creating the pod @ 04/15/24 07:22:06.388
  Apr 15 07:22:08.984: INFO: Successfully updated pod "annotationupdated37586d8-8db6-4e2e-9435-72b4fd7836ba"
  Apr 15 07:22:11.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9206" for this suite. @ 04/15/24 07:22:11.032
• [4.718 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 04/15/24 07:22:11.048
  Apr 15 07:22:11.048: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubelet-test @ 04/15/24 07:22:11.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:22:11.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:22:11.09
  STEP: Waiting for pod completion @ 04/15/24 07:22:11.126
  Apr 15 07:22:15.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1279" for this suite. @ 04/15/24 07:22:15.201
• [4.175 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 04/15/24 07:22:15.24
  Apr 15 07:22:15.240: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename services @ 04/15/24 07:22:15.246
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:22:15.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:22:15.295
  Apr 15 07:22:15.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5877" for this suite. @ 04/15/24 07:22:15.326
• [0.105 seconds]
------------------------------
S
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 04/15/24 07:22:15.345
  Apr 15 07:22:15.345: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 07:22:15.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:22:15.397
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:22:15.404
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 07:22:15.413
  STEP: Saw pod success @ 04/15/24 07:22:19.481
  Apr 15 07:22:19.489: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downwardapi-volume-451f17fa-e353-454a-96f4-44aa4f4ed4b2 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 07:22:19.507
  Apr 15 07:22:19.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5578" for this suite. @ 04/15/24 07:22:19.565
• [4.236 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 04/15/24 07:22:19.585
  Apr 15 07:22:19.585: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 07:22:19.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:22:19.622
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:22:19.628
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 07:22:19.636
  STEP: Saw pod success @ 04/15/24 07:22:23.699
  Apr 15 07:22:23.706: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downwardapi-volume-375f8182-0fee-4fdc-9eac-eb210b4aa991 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 07:22:23.72
  Apr 15 07:22:23.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4050" for this suite. @ 04/15/24 07:22:23.778
• [4.210 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 04/15/24 07:22:23.804
  Apr 15 07:22:23.805: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename csiinlinevolumes @ 04/15/24 07:22:23.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:22:23.855
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:22:23.862
  STEP: creating @ 04/15/24 07:22:23.868
  STEP: getting @ 04/15/24 07:22:23.907
  STEP: listing @ 04/15/24 07:22:23.921
  STEP: deleting @ 04/15/24 07:22:23.93
  Apr 15 07:22:23.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-543" for this suite. @ 04/15/24 07:22:23.987
• [0.199 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 04/15/24 07:22:24.015
  Apr 15 07:22:24.015: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename crd-webhook @ 04/15/24 07:22:24.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:22:24.056
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:22:24.063
  STEP: Setting up server cert @ 04/15/24 07:22:24.071
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 04/15/24 07:22:24.878
  STEP: Deploying the custom resource conversion webhook pod @ 04/15/24 07:22:24.909
  STEP: Wait for the deployment to be ready @ 04/15/24 07:22:24.957
  Apr 15 07:22:25.012: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/15/24 07:22:27.038
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 07:22:27.087
  Apr 15 07:22:28.087: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Apr 15 07:22:28.099: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Creating a v1 custom resource @ 04/15/24 07:22:31.029
  STEP: v2 custom resource should be converted @ 04/15/24 07:22:31.048
  Apr 15 07:22:31.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-5135" for this suite. @ 04/15/24 07:22:31.74
• [7.740 seconds]
------------------------------
S
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 04/15/24 07:22:31.762
  Apr 15 07:22:31.762: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename endpointslice @ 04/15/24 07:22:31.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:22:31.834
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:22:31.846
  Apr 15 07:22:31.880: INFO: Endpoints addresses: [192.168.121.27 192.168.121.74] , ports: [6443]
  Apr 15 07:22:31.881: INFO: EndpointSlices addresses: [192.168.121.27 192.168.121.74] , ports: [6443]
  Apr 15 07:22:31.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-9496" for this suite. @ 04/15/24 07:22:31.892
• [0.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 04/15/24 07:22:31.91
  Apr 15 07:22:31.911: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename replication-controller @ 04/15/24 07:22:31.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:22:31.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:22:31.974
  STEP: Creating replication controller my-hostname-basic-824c12f8-ba3e-4396-9523-2d1bc0c5fa15 @ 04/15/24 07:22:31.98
  Apr 15 07:22:31.999: INFO: Pod name my-hostname-basic-824c12f8-ba3e-4396-9523-2d1bc0c5fa15: Found 0 pods out of 1
  Apr 15 07:22:37.017: INFO: Pod name my-hostname-basic-824c12f8-ba3e-4396-9523-2d1bc0c5fa15: Found 1 pods out of 1
  Apr 15 07:22:37.017: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-824c12f8-ba3e-4396-9523-2d1bc0c5fa15" are running
  Apr 15 07:22:37.035: INFO: Pod "my-hostname-basic-824c12f8-ba3e-4396-9523-2d1bc0c5fa15-4bl64" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-04-15 07:22:32 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-04-15 07:22:35 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-04-15 07:22:35 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-04-15 07:22:32 +0000 UTC Reason: Message:}])
  Apr 15 07:22:37.035: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 04/15/24 07:22:37.035
  Apr 15 07:22:37.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1410" for this suite. @ 04/15/24 07:22:37.085
• [5.192 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 04/15/24 07:22:37.11
  Apr 15 07:22:37.110: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename secrets @ 04/15/24 07:22:37.113
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:22:37.179
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:22:37.187
  STEP: Creating secret with name secret-test-e42a9176-040b-4685-a0cc-2d8c0c78bd45 @ 04/15/24 07:22:37.195
  STEP: Creating a pod to test consume secrets @ 04/15/24 07:22:37.205
  STEP: Saw pod success @ 04/15/24 07:22:41.273
  Apr 15 07:22:41.284: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-secrets-e8f848db-6143-423d-9982-64f10d790e37 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 07:22:41.305
  Apr 15 07:22:41.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9059" for this suite. @ 04/15/24 07:22:41.355
• [4.274 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 04/15/24 07:22:41.386
  Apr 15 07:22:41.386: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename pods @ 04/15/24 07:22:41.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:22:41.428
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:22:41.434
  STEP: Create set of pods @ 04/15/24 07:22:41.439
  Apr 15 07:22:41.458: INFO: created test-pod-1
  Apr 15 07:22:41.477: INFO: created test-pod-2
  Apr 15 07:22:41.495: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 04/15/24 07:22:41.496
  STEP: waiting for all pods to be deleted @ 04/15/24 07:22:45.677
  Apr 15 07:22:45.709: INFO: Pod quantity 3 is different from expected quantity 0
  Apr 15 07:22:46.732: INFO: Pod quantity 2 is different from expected quantity 0
  Apr 15 07:22:47.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3544" for this suite. @ 04/15/24 07:22:47.738
• [6.372 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 04/15/24 07:22:47.839
  Apr 15 07:22:47.839: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename csistoragecapacity @ 04/15/24 07:22:47.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:22:47.882
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:22:47.898
  STEP: getting /apis @ 04/15/24 07:22:47.904
  STEP: getting /apis/storage.k8s.io @ 04/15/24 07:22:47.915
  STEP: getting /apis/storage.k8s.io/v1 @ 04/15/24 07:22:47.928
  STEP: creating @ 04/15/24 07:22:47.934
  STEP: watching @ 04/15/24 07:22:47.968
  Apr 15 07:22:47.968: INFO: starting watch
  STEP: getting @ 04/15/24 07:22:47.99
  STEP: listing in namespace @ 04/15/24 07:22:47.998
  STEP: listing across namespaces @ 04/15/24 07:22:48.008
  STEP: patching @ 04/15/24 07:22:48.017
  STEP: updating @ 04/15/24 07:22:48.028
  Apr 15 07:22:48.042: INFO: waiting for watch events with expected annotations in namespace
  Apr 15 07:22:48.043: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 04/15/24 07:22:48.043
  STEP: deleting a collection @ 04/15/24 07:22:48.075
  Apr 15 07:22:48.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-3235" for this suite. @ 04/15/24 07:22:48.151
• [0.328 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 04/15/24 07:22:48.17
  Apr 15 07:22:48.171: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename secrets @ 04/15/24 07:22:48.174
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:22:48.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:22:48.225
  STEP: Creating secret with name s-test-opt-del-769e2d1b-8f37-46f0-9419-d17709233c4b @ 04/15/24 07:22:48.244
  STEP: Creating secret with name s-test-opt-upd-e7615719-ebfe-4e11-9233-1ef56f4aa246 @ 04/15/24 07:22:48.258
  STEP: Creating the pod @ 04/15/24 07:22:48.275
  STEP: Deleting secret s-test-opt-del-769e2d1b-8f37-46f0-9419-d17709233c4b @ 04/15/24 07:22:52.397
  STEP: Updating secret s-test-opt-upd-e7615719-ebfe-4e11-9233-1ef56f4aa246 @ 04/15/24 07:22:52.414
  STEP: Creating secret with name s-test-opt-create-aa375f57-9dcd-426b-a0dc-af6fb667e6fd @ 04/15/24 07:22:52.423
  STEP: waiting to observe update in volume @ 04/15/24 07:22:52.434
  Apr 15 07:24:13.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5644" for this suite. @ 04/15/24 07:24:13.646
• [85.515 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 04/15/24 07:24:13.691
  Apr 15 07:24:13.691: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename watch @ 04/15/24 07:24:13.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:24:13.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:24:13.771
  STEP: creating a watch on configmaps with a certain label @ 04/15/24 07:24:13.788
  STEP: creating a new configmap @ 04/15/24 07:24:13.793
  STEP: modifying the configmap once @ 04/15/24 07:24:13.81
  STEP: changing the label value of the configmap @ 04/15/24 07:24:13.836
  STEP: Expecting to observe a delete notification for the watched object @ 04/15/24 07:24:13.861
  Apr 15 07:24:13.861: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7827  d9b98459-4306-436a-aa9d-1e6aa4fc8623 163232 0 2024-04-15 07:24:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-04-15 07:24:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 07:24:13.864: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7827  d9b98459-4306-436a-aa9d-1e6aa4fc8623 163233 0 2024-04-15 07:24:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-04-15 07:24:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 07:24:13.865: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7827  d9b98459-4306-436a-aa9d-1e6aa4fc8623 163234 0 2024-04-15 07:24:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-04-15 07:24:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 04/15/24 07:24:13.865
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 04/15/24 07:24:13.893
  STEP: changing the label value of the configmap back @ 04/15/24 07:24:23.895
  STEP: modifying the configmap a third time @ 04/15/24 07:24:23.918
  STEP: deleting the configmap @ 04/15/24 07:24:23.935
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 04/15/24 07:24:23.956
  Apr 15 07:24:23.956: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7827  d9b98459-4306-436a-aa9d-1e6aa4fc8623 163273 0 2024-04-15 07:24:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-04-15 07:24:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 07:24:23.957: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7827  d9b98459-4306-436a-aa9d-1e6aa4fc8623 163274 0 2024-04-15 07:24:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-04-15 07:24:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 07:24:23.957: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7827  d9b98459-4306-436a-aa9d-1e6aa4fc8623 163275 0 2024-04-15 07:24:13 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-04-15 07:24:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 07:24:23.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7827" for this suite. @ 04/15/24 07:24:23.972
• [10.298 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 04/15/24 07:24:23.995
  Apr 15 07:24:23.995: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename secrets @ 04/15/24 07:24:23.998
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:24:24.044
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:24:24.052
  STEP: Creating secret with name secret-test-map-c9b49351-086c-4b9d-8cae-31647aa14bf1 @ 04/15/24 07:24:24.059
  STEP: Creating a pod to test consume secrets @ 04/15/24 07:24:24.074
  STEP: Saw pod success @ 04/15/24 07:24:28.136
  Apr 15 07:24:28.143: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-secrets-9b37b37f-f5a0-41fd-bc55-d318631e75e9 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 07:24:28.159
  Apr 15 07:24:28.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8286" for this suite. @ 04/15/24 07:24:28.21
• [4.229 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:899
  STEP: Creating a kubernetes client @ 04/15/24 07:24:28.228
  Apr 15 07:24:28.228: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename statefulset @ 04/15/24 07:24:28.23
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:24:28.264
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:24:28.27
  STEP: Creating service test in namespace statefulset-5446 @ 04/15/24 07:24:28.278
  STEP: Creating statefulset ss in namespace statefulset-5446 @ 04/15/24 07:24:28.29
  Apr 15 07:24:28.311: INFO: Found 0 stateful pods, waiting for 1
  Apr 15 07:24:38.334: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 04/15/24 07:24:38.358
  STEP: updating a scale subresource @ 04/15/24 07:24:38.368
  STEP: verifying the statefulset Spec.Replicas was modified @ 04/15/24 07:24:38.382
  STEP: Patch a scale subresource @ 04/15/24 07:24:38.39
  STEP: verifying the statefulset Spec.Replicas was modified @ 04/15/24 07:24:38.411
  Apr 15 07:24:38.422: INFO: Deleting all statefulset in ns statefulset-5446
  Apr 15 07:24:38.435: INFO: Scaling statefulset ss to 0
  Apr 15 07:24:48.523: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 07:24:48.553: INFO: Deleting statefulset ss
  Apr 15 07:24:48.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5446" for this suite. @ 04/15/24 07:24:48.682
• [20.476 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 04/15/24 07:24:48.706
  Apr 15 07:24:48.706: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 07:24:48.712
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:24:48.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:24:48.771
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 04/15/24 07:24:48.779
  STEP: Saw pod success @ 04/15/24 07:24:52.838
  Apr 15 07:24:52.849: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-0c5c33bf-9402-4865-a20a-1e8d53e977cb container test-container: <nil>
  STEP: delete the pod @ 04/15/24 07:24:52.875
  Apr 15 07:24:52.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5581" for this suite. @ 04/15/24 07:24:52.963
• [4.271 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 04/15/24 07:24:52.988
  Apr 15 07:24:52.988: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename webhook @ 04/15/24 07:24:52.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:24:53.093
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:24:53.109
  STEP: Setting up server cert @ 04/15/24 07:24:53.171
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 07:24:54.184
  STEP: Deploying the webhook pod @ 04/15/24 07:24:54.213
  STEP: Wait for the deployment to be ready @ 04/15/24 07:24:54.262
  Apr 15 07:24:54.290: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/15/24 07:24:56.313
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 07:24:56.336
  Apr 15 07:24:57.337: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 04/15/24 07:24:57.35
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 04/15/24 07:24:57.354
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 04/15/24 07:24:57.354
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 04/15/24 07:24:57.354
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 04/15/24 07:24:57.36
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 04/15/24 07:24:57.36
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 04/15/24 07:24:57.374
  Apr 15 07:24:57.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1127" for this suite. @ 04/15/24 07:24:57.594
  STEP: Destroying namespace "webhook-markers-8911" for this suite. @ 04/15/24 07:24:57.615
• [4.644 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 04/15/24 07:24:57.647
  Apr 15 07:24:57.647: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename secrets @ 04/15/24 07:24:57.649
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:24:57.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:24:57.72
  STEP: Creating secret with name secret-test-c0238a35-d020-45ca-aa99-2d2f9416516d @ 04/15/24 07:24:57.731
  STEP: Creating a pod to test consume secrets @ 04/15/24 07:24:57.753
  STEP: Saw pod success @ 04/15/24 07:25:01.815
  Apr 15 07:25:01.835: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-secrets-6011bba5-83a4-4723-b4a3-24cd24b63abc container secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 07:25:01.853
  Apr 15 07:25:01.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5035" for this suite. @ 04/15/24 07:25:01.911
• [4.280 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 04/15/24 07:25:01.93
  Apr 15 07:25:01.930: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename svcaccounts @ 04/15/24 07:25:01.933
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:25:01.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:25:01.985
  Apr 15 07:25:02.049: INFO: created pod pod-service-account-defaultsa
  Apr 15 07:25:02.049: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Apr 15 07:25:02.065: INFO: created pod pod-service-account-mountsa
  Apr 15 07:25:02.065: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Apr 15 07:25:02.075: INFO: created pod pod-service-account-nomountsa
  Apr 15 07:25:02.075: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Apr 15 07:25:02.092: INFO: created pod pod-service-account-defaultsa-mountspec
  Apr 15 07:25:02.093: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Apr 15 07:25:02.109: INFO: created pod pod-service-account-mountsa-mountspec
  Apr 15 07:25:02.109: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Apr 15 07:25:02.127: INFO: created pod pod-service-account-nomountsa-mountspec
  Apr 15 07:25:02.128: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Apr 15 07:25:02.150: INFO: created pod pod-service-account-defaultsa-nomountspec
  Apr 15 07:25:02.150: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Apr 15 07:25:02.168: INFO: created pod pod-service-account-mountsa-nomountspec
  Apr 15 07:25:02.168: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Apr 15 07:25:02.177: INFO: created pod pod-service-account-nomountsa-nomountspec
  Apr 15 07:25:02.178: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Apr 15 07:25:02.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2603" for this suite. @ 04/15/24 07:25:02.216
• [0.385 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 04/15/24 07:25:02.339
  Apr 15 07:25:02.339: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename disruption @ 04/15/24 07:25:02.342
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:25:02.406
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:25:02.411
  STEP: Waiting for the pdb to be processed @ 04/15/24 07:25:02.428
  STEP: Updating PodDisruptionBudget status @ 04/15/24 07:25:04.448
  STEP: Waiting for all pods to be running @ 04/15/24 07:25:04.467
  Apr 15 07:25:04.482: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 04/15/24 07:25:06.492
  STEP: Waiting for the pdb to be processed @ 04/15/24 07:25:06.518
  STEP: Patching PodDisruptionBudget status @ 04/15/24 07:25:06.539
  STEP: Waiting for the pdb to be processed @ 04/15/24 07:25:06.561
  Apr 15 07:25:06.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-1538" for this suite. @ 04/15/24 07:25:06.585
• [4.262 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 04/15/24 07:25:06.601
  Apr 15 07:25:06.601: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename services @ 04/15/24 07:25:06.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:25:06.649
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:25:06.659
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-1968 @ 04/15/24 07:25:06.668
  STEP: changing the ExternalName service to type=NodePort @ 04/15/24 07:25:06.688
  STEP: creating replication controller externalname-service in namespace services-1968 @ 04/15/24 07:25:06.742
  I0415 07:25:06.775746      13 runners.go:194] Created replication controller with name: externalname-service, namespace: services-1968, replica count: 2
  I0415 07:25:09.829259      13 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 15 07:25:09.830: INFO: Creating new exec pod
  Apr 15 07:25:12.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-1968 exec execpodcgcvf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 15 07:25:13.329: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 15 07:25:13.329: INFO: stdout: ""
  Apr 15 07:25:14.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-1968 exec execpodcgcvf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 15 07:25:14.684: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 15 07:25:14.684: INFO: stdout: ""
  Apr 15 07:25:15.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-1968 exec execpodcgcvf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 15 07:25:15.662: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 15 07:25:15.662: INFO: stdout: "externalname-service-ds4kj"
  Apr 15 07:25:15.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-1968 exec execpodcgcvf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.12.128 80'
  Apr 15 07:25:15.997: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.12.128 80\nConnection to 10.233.12.128 80 port [tcp/http] succeeded!\n"
  Apr 15 07:25:15.997: INFO: stdout: "externalname-service-ds4kj"
  Apr 15 07:25:15.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-1968 exec execpodcgcvf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.27 31461'
  Apr 15 07:25:16.314: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.27 31461\nConnection to 192.168.121.27 31461 port [tcp/*] succeeded!\n"
  Apr 15 07:25:16.314: INFO: stdout: "externalname-service-pvllh"
  Apr 15 07:25:16.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-1968 exec execpodcgcvf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.131 31461'
  Apr 15 07:25:16.639: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.131 31461\nConnection to 192.168.121.131 31461 port [tcp/*] succeeded!\n"
  Apr 15 07:25:16.639: INFO: stdout: "externalname-service-pvllh"
  Apr 15 07:25:16.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 07:25:16.651: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-1968" for this suite. @ 04/15/24 07:25:16.721
• [10.140 seconds]
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:443
  STEP: Creating a kubernetes client @ 04/15/24 07:25:16.742
  Apr 15 07:25:16.742: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename daemonsets @ 04/15/24 07:25:16.747
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:25:16.794
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:25:16.806
  Apr 15 07:25:16.885: INFO: Create a RollingUpdate DaemonSet
  Apr 15 07:25:16.901: INFO: Check that daemon pods launch on every node of the cluster
  Apr 15 07:25:16.930: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:25:16.931: INFO: Node zaigh3ewotoh-1 is running 0 daemon pod, expected 1
  Apr 15 07:25:17.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:25:17.951: INFO: Node zaigh3ewotoh-1 is running 0 daemon pod, expected 1
  Apr 15 07:25:18.951: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 15 07:25:18.951: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Apr 15 07:25:18.951: INFO: Update the DaemonSet to trigger a rollout
  Apr 15 07:25:18.974: INFO: Updating DaemonSet daemon-set
  Apr 15 07:25:22.099: INFO: Roll back the DaemonSet before rollout is complete
  Apr 15 07:25:22.129: INFO: Updating DaemonSet daemon-set
  Apr 15 07:25:22.130: INFO: Make sure DaemonSet rollback is complete
  Apr 15 07:25:22.142: INFO: Wrong image for pod: daemon-set-ff7sf. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Apr 15 07:25:22.143: INFO: Pod daemon-set-ff7sf is not available
  Apr 15 07:25:29.161: INFO: Pod daemon-set-97r5p is not available
  STEP: Deleting DaemonSet "daemon-set" @ 04/15/24 07:25:29.182
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-442, will wait for the garbage collector to delete the pods @ 04/15/24 07:25:29.183
  Apr 15 07:25:29.251: INFO: Deleting DaemonSet.extensions daemon-set took: 12.686832ms
  Apr 15 07:25:29.352: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.432584ms
  Apr 15 07:25:31.071: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:25:31.071: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 15 07:25:31.077: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"163930"},"items":null}

  Apr 15 07:25:31.090: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"163930"},"items":null}

  Apr 15 07:25:31.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-442" for this suite. @ 04/15/24 07:25:31.139
• [14.410 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 04/15/24 07:25:31.167
  Apr 15 07:25:31.167: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename services @ 04/15/24 07:25:31.171
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:25:31.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:25:31.228
  STEP: creating service nodeport-test with type=NodePort in namespace services-1379 @ 04/15/24 07:25:31.233
  STEP: creating replication controller nodeport-test in namespace services-1379 @ 04/15/24 07:25:31.268
  I0415 07:25:31.289676      13 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-1379, replica count: 2
  I0415 07:25:34.340948      13 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 15 07:25:34.341: INFO: Creating new exec pod
  Apr 15 07:25:37.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-1379 exec execpod2qb9d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Apr 15 07:25:37.726: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Apr 15 07:25:37.726: INFO: stdout: "nodeport-test-6xzmr"
  Apr 15 07:25:37.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-1379 exec execpod2qb9d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.44.96 80'
  Apr 15 07:25:38.050: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.44.96 80\nConnection to 10.233.44.96 80 port [tcp/http] succeeded!\n"
  Apr 15 07:25:38.050: INFO: stdout: "nodeport-test-zxn9s"
  Apr 15 07:25:38.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-1379 exec execpod2qb9d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.131 31837'
  Apr 15 07:25:38.289: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.131 31837\nConnection to 192.168.121.131 31837 port [tcp/*] succeeded!\n"
  Apr 15 07:25:38.289: INFO: stdout: "nodeport-test-zxn9s"
  Apr 15 07:25:38.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-1379 exec execpod2qb9d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.27 31837'
  Apr 15 07:25:38.579: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.27 31837\nConnection to 192.168.121.27 31837 port [tcp/*] succeeded!\n"
  Apr 15 07:25:38.579: INFO: stdout: "nodeport-test-zxn9s"
  Apr 15 07:25:38.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1379" for this suite. @ 04/15/24 07:25:38.592
• [7.441 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 04/15/24 07:25:38.61
  Apr 15 07:25:38.610: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/15/24 07:25:38.613
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:25:38.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:25:38.664
  Apr 15 07:25:38.678: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/15/24 07:25:40.656
  Apr 15 07:25:40.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-670 --namespace=crd-publish-openapi-670 create -f -'
  Apr 15 07:25:42.150: INFO: stderr: ""
  Apr 15 07:25:42.150: INFO: stdout: "e2e-test-crd-publish-openapi-4603-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Apr 15 07:25:42.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-670 --namespace=crd-publish-openapi-670 delete e2e-test-crd-publish-openapi-4603-crds test-cr'
  Apr 15 07:25:42.304: INFO: stderr: ""
  Apr 15 07:25:42.304: INFO: stdout: "e2e-test-crd-publish-openapi-4603-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Apr 15 07:25:42.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-670 --namespace=crd-publish-openapi-670 apply -f -'
  Apr 15 07:25:43.754: INFO: stderr: ""
  Apr 15 07:25:43.754: INFO: stdout: "e2e-test-crd-publish-openapi-4603-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Apr 15 07:25:43.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-670 --namespace=crd-publish-openapi-670 delete e2e-test-crd-publish-openapi-4603-crds test-cr'
  Apr 15 07:25:44.053: INFO: stderr: ""
  Apr 15 07:25:44.053: INFO: stdout: "e2e-test-crd-publish-openapi-4603-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 04/15/24 07:25:44.053
  Apr 15 07:25:44.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=crd-publish-openapi-670 explain e2e-test-crd-publish-openapi-4603-crds'
  Apr 15 07:25:44.691: INFO: stderr: ""
  Apr 15 07:25:44.691: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-4603-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Apr 15 07:25:46.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-670" for this suite. @ 04/15/24 07:25:46.53
• [7.931 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 04/15/24 07:25:46.546
  Apr 15 07:25:46.546: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename proxy @ 04/15/24 07:25:46.549
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:25:46.573
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:25:46.579
  Apr 15 07:25:46.584: INFO: Creating pod...
  Apr 15 07:25:48.620: INFO: Creating service...
  Apr 15 07:25:48.644: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5539/pods/agnhost/proxy?method=DELETE
  Apr 15 07:26:00.011: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 15 07:26:00.013: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5539/pods/agnhost/proxy?method=OPTIONS
  Apr 15 07:26:00.027: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 15 07:26:00.027: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5539/pods/agnhost/proxy?method=PATCH
  Apr 15 07:26:00.037: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 15 07:26:00.037: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5539/pods/agnhost/proxy?method=POST
  Apr 15 07:26:00.050: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 15 07:26:00.050: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5539/pods/agnhost/proxy?method=PUT
  Apr 15 07:26:00.058: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 15 07:26:00.058: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5539/services/e2e-proxy-test-service/proxy?method=DELETE
  Apr 15 07:26:00.071: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 15 07:26:00.071: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5539/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Apr 15 07:26:00.084: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 15 07:26:00.084: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5539/services/e2e-proxy-test-service/proxy?method=PATCH
  Apr 15 07:26:00.095: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 15 07:26:00.096: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5539/services/e2e-proxy-test-service/proxy?method=POST
  Apr 15 07:26:00.108: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 15 07:26:00.108: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5539/services/e2e-proxy-test-service/proxy?method=PUT
  Apr 15 07:26:00.120: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 15 07:26:00.121: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5539/pods/agnhost/proxy?method=GET
  Apr 15 07:26:00.131: INFO: http.Client request:GET StatusCode:301
  Apr 15 07:26:00.131: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5539/services/e2e-proxy-test-service/proxy?method=GET
  Apr 15 07:26:00.144: INFO: http.Client request:GET StatusCode:301
  Apr 15 07:26:00.145: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5539/pods/agnhost/proxy?method=HEAD
  Apr 15 07:26:00.153: INFO: http.Client request:HEAD StatusCode:301
  Apr 15 07:26:00.154: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-5539/services/e2e-proxy-test-service/proxy?method=HEAD
  Apr 15 07:26:00.163: INFO: http.Client request:HEAD StatusCode:301
  Apr 15 07:26:00.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-5539" for this suite. @ 04/15/24 07:26:00.182
• [13.658 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 04/15/24 07:26:00.233
  Apr 15 07:26:00.234: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename webhook @ 04/15/24 07:26:00.239
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:26:00.286
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:26:00.298
  STEP: Setting up server cert @ 04/15/24 07:26:00.358
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 07:26:01.482
  STEP: Deploying the webhook pod @ 04/15/24 07:26:01.501
  STEP: Wait for the deployment to be ready @ 04/15/24 07:26:01.532
  Apr 15 07:26:01.553: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  Apr 15 07:26:03.583: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6d58c8c59c\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 07:26:05.593: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6d58c8c59c\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 07:26:07.595: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6d58c8c59c\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 07:26:09.595: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6d58c8c59c\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 07:26:11.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 26, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6d58c8c59c\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 04/15/24 07:26:13.594
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 07:26:13.627
  Apr 15 07:26:14.627: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 04/15/24 07:26:14.635
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 04/15/24 07:26:14.68
  STEP: Creating a dummy validating-webhook-configuration object @ 04/15/24 07:26:14.715
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 04/15/24 07:26:14.749
  STEP: Creating a dummy mutating-webhook-configuration object @ 04/15/24 07:26:14.766
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 04/15/24 07:26:14.796
  Apr 15 07:26:14.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9922" for this suite. @ 04/15/24 07:26:15.025
  STEP: Destroying namespace "webhook-markers-1565" for this suite. @ 04/15/24 07:26:15.053
• [14.838 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:205
  STEP: Creating a kubernetes client @ 04/15/24 07:26:15.08
  Apr 15 07:26:15.082: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename daemonsets @ 04/15/24 07:26:15.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:26:15.135
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:26:15.142
  Apr 15 07:26:15.198: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 04/15/24 07:26:15.214
  Apr 15 07:26:15.225: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:26:15.225: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 04/15/24 07:26:15.225
  Apr 15 07:26:15.288: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:26:15.289: INFO: Node zaigh3ewotoh-2 is running 0 daemon pod, expected 1
  Apr 15 07:26:16.299: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:26:16.299: INFO: Node zaigh3ewotoh-2 is running 0 daemon pod, expected 1
  Apr 15 07:26:17.298: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 15 07:26:17.298: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 04/15/24 07:26:17.305
  Apr 15 07:26:17.344: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 15 07:26:17.344: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  Apr 15 07:26:18.353: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:26:18.353: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 04/15/24 07:26:18.353
  Apr 15 07:26:18.384: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:26:18.385: INFO: Node zaigh3ewotoh-2 is running 0 daemon pod, expected 1
  Apr 15 07:26:19.396: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:26:19.397: INFO: Node zaigh3ewotoh-2 is running 0 daemon pod, expected 1
  Apr 15 07:26:20.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:26:20.395: INFO: Node zaigh3ewotoh-2 is running 0 daemon pod, expected 1
  Apr 15 07:26:21.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 15 07:26:21.394: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/15/24 07:26:21.408
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3748, will wait for the garbage collector to delete the pods @ 04/15/24 07:26:21.409
  Apr 15 07:26:21.482: INFO: Deleting DaemonSet.extensions daemon-set took: 16.13266ms
  Apr 15 07:26:21.583: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.189942ms
  Apr 15 07:26:24.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:26:24.394: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 15 07:26:24.400: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"164330"},"items":null}

  Apr 15 07:26:24.406: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"164330"},"items":null}

  Apr 15 07:26:24.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3748" for this suite. @ 04/15/24 07:26:24.51
• [9.456 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 04/15/24 07:26:24.543
  Apr 15 07:26:24.543: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 07:26:24.547
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:26:24.585
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:26:24.591
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 07:26:24.598
  STEP: Saw pod success @ 04/15/24 07:26:28.65
  Apr 15 07:26:28.659: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downwardapi-volume-b13da92d-5ed1-4472-bda0-ab5217551e5c container client-container: <nil>
  STEP: delete the pod @ 04/15/24 07:26:28.691
  Apr 15 07:26:28.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4469" for this suite. @ 04/15/24 07:26:28.747
• [4.221 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 04/15/24 07:26:28.766
  Apr 15 07:26:28.766: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename discovery @ 04/15/24 07:26:28.771
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:26:28.822
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:26:28.831
  STEP: Setting up server cert @ 04/15/24 07:26:28.843
  Apr 15 07:26:30.169: INFO: Checking APIGroup: apiregistration.k8s.io
  Apr 15 07:26:30.174: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Apr 15 07:26:30.174: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Apr 15 07:26:30.174: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Apr 15 07:26:30.174: INFO: Checking APIGroup: apps
  Apr 15 07:26:30.178: INFO: PreferredVersion.GroupVersion: apps/v1
  Apr 15 07:26:30.178: INFO: Versions found [{apps/v1 v1}]
  Apr 15 07:26:30.178: INFO: apps/v1 matches apps/v1
  Apr 15 07:26:30.178: INFO: Checking APIGroup: events.k8s.io
  Apr 15 07:26:30.180: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Apr 15 07:26:30.180: INFO: Versions found [{events.k8s.io/v1 v1}]
  Apr 15 07:26:30.180: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Apr 15 07:26:30.180: INFO: Checking APIGroup: authentication.k8s.io
  Apr 15 07:26:30.182: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Apr 15 07:26:30.183: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Apr 15 07:26:30.183: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Apr 15 07:26:30.184: INFO: Checking APIGroup: authorization.k8s.io
  Apr 15 07:26:30.187: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Apr 15 07:26:30.188: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Apr 15 07:26:30.188: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Apr 15 07:26:30.189: INFO: Checking APIGroup: autoscaling
  Apr 15 07:26:30.192: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Apr 15 07:26:30.193: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Apr 15 07:26:30.193: INFO: autoscaling/v2 matches autoscaling/v2
  Apr 15 07:26:30.194: INFO: Checking APIGroup: batch
  Apr 15 07:26:30.198: INFO: PreferredVersion.GroupVersion: batch/v1
  Apr 15 07:26:30.198: INFO: Versions found [{batch/v1 v1}]
  Apr 15 07:26:30.199: INFO: batch/v1 matches batch/v1
  Apr 15 07:26:30.199: INFO: Checking APIGroup: certificates.k8s.io
  Apr 15 07:26:30.204: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Apr 15 07:26:30.204: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Apr 15 07:26:30.206: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Apr 15 07:26:30.207: INFO: Checking APIGroup: networking.k8s.io
  Apr 15 07:26:30.210: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Apr 15 07:26:30.211: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Apr 15 07:26:30.211: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Apr 15 07:26:30.211: INFO: Checking APIGroup: policy
  Apr 15 07:26:30.213: INFO: PreferredVersion.GroupVersion: policy/v1
  Apr 15 07:26:30.214: INFO: Versions found [{policy/v1 v1}]
  Apr 15 07:26:30.214: INFO: policy/v1 matches policy/v1
  Apr 15 07:26:30.215: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Apr 15 07:26:30.218: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Apr 15 07:26:30.218: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Apr 15 07:26:30.219: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Apr 15 07:26:30.220: INFO: Checking APIGroup: storage.k8s.io
  Apr 15 07:26:30.224: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Apr 15 07:26:30.224: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Apr 15 07:26:30.224: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Apr 15 07:26:30.224: INFO: Checking APIGroup: admissionregistration.k8s.io
  Apr 15 07:26:30.228: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Apr 15 07:26:30.228: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Apr 15 07:26:30.228: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Apr 15 07:26:30.228: INFO: Checking APIGroup: apiextensions.k8s.io
  Apr 15 07:26:30.231: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Apr 15 07:26:30.233: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Apr 15 07:26:30.234: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Apr 15 07:26:30.235: INFO: Checking APIGroup: scheduling.k8s.io
  Apr 15 07:26:30.239: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Apr 15 07:26:30.239: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Apr 15 07:26:30.239: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Apr 15 07:26:30.239: INFO: Checking APIGroup: coordination.k8s.io
  Apr 15 07:26:30.242: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Apr 15 07:26:30.242: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Apr 15 07:26:30.242: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Apr 15 07:26:30.242: INFO: Checking APIGroup: node.k8s.io
  Apr 15 07:26:30.245: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Apr 15 07:26:30.245: INFO: Versions found [{node.k8s.io/v1 v1}]
  Apr 15 07:26:30.245: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Apr 15 07:26:30.245: INFO: Checking APIGroup: discovery.k8s.io
  Apr 15 07:26:30.248: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Apr 15 07:26:30.248: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Apr 15 07:26:30.248: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Apr 15 07:26:30.248: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Apr 15 07:26:30.251: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Apr 15 07:26:30.251: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Apr 15 07:26:30.251: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Apr 15 07:26:30.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-7031" for this suite. @ 04/15/24 07:26:30.265
• [1.515 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 04/15/24 07:26:30.282
  Apr 15 07:26:30.282: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 07:26:30.286
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:26:30.335
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:26:30.347
  STEP: Creating projection with secret that has name projected-secret-test-83c95ec9-4038-4c46-bf71-bd378f367d50 @ 04/15/24 07:26:30.357
  STEP: Creating a pod to test consume secrets @ 04/15/24 07:26:30.376
  STEP: Saw pod success @ 04/15/24 07:26:34.436
  Apr 15 07:26:34.443: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-projected-secrets-7298107c-8b85-4333-8ef9-d0280b579844 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 07:26:34.457
  Apr 15 07:26:34.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1450" for this suite. @ 04/15/24 07:26:34.503
• [4.237 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 04/15/24 07:26:34.532
  Apr 15 07:26:34.532: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 07:26:34.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:26:34.571
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:26:34.58
  STEP: Creating a pod to test downward api env vars @ 04/15/24 07:26:34.585
  STEP: Saw pod success @ 04/15/24 07:26:38.645
  Apr 15 07:26:38.654: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downward-api-cb1af93f-125e-4079-9de8-9ecaa0eb5bd0 container dapi-container: <nil>
  STEP: delete the pod @ 04/15/24 07:26:38.674
  Apr 15 07:26:38.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6593" for this suite. @ 04/15/24 07:26:38.718
• [4.204 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 04/15/24 07:26:38.739
  Apr 15 07:26:38.739: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 07:26:38.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:26:38.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:26:38.789
  STEP: creating a replication controller @ 04/15/24 07:26:38.806
  Apr 15 07:26:38.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3132 create -f -'
  Apr 15 07:26:39.739: INFO: stderr: ""
  Apr 15 07:26:39.739: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/15/24 07:26:39.739
  Apr 15 07:26:39.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3132 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 15 07:26:39.990: INFO: stderr: ""
  Apr 15 07:26:39.990: INFO: stdout: "update-demo-nautilus-79gbs update-demo-nautilus-wsqmf "
  Apr 15 07:26:39.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3132 get pods update-demo-nautilus-79gbs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 15 07:26:40.252: INFO: stderr: ""
  Apr 15 07:26:40.252: INFO: stdout: ""
  Apr 15 07:26:40.252: INFO: update-demo-nautilus-79gbs is created but not running
  Apr 15 07:26:45.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3132 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 15 07:26:45.449: INFO: stderr: ""
  Apr 15 07:26:45.449: INFO: stdout: "update-demo-nautilus-79gbs update-demo-nautilus-wsqmf "
  Apr 15 07:26:45.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3132 get pods update-demo-nautilus-79gbs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 15 07:26:45.614: INFO: stderr: ""
  Apr 15 07:26:45.614: INFO: stdout: "true"
  Apr 15 07:26:45.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3132 get pods update-demo-nautilus-79gbs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 15 07:26:45.825: INFO: stderr: ""
  Apr 15 07:26:45.825: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 15 07:26:45.825: INFO: validating pod update-demo-nautilus-79gbs
  Apr 15 07:26:45.852: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 15 07:26:45.852: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 15 07:26:45.852: INFO: update-demo-nautilus-79gbs is verified up and running
  Apr 15 07:26:45.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3132 get pods update-demo-nautilus-wsqmf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 15 07:26:46.005: INFO: stderr: ""
  Apr 15 07:26:46.005: INFO: stdout: "true"
  Apr 15 07:26:46.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3132 get pods update-demo-nautilus-wsqmf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 15 07:26:46.200: INFO: stderr: ""
  Apr 15 07:26:46.200: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 15 07:26:46.200: INFO: validating pod update-demo-nautilus-wsqmf
  Apr 15 07:26:46.232: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 15 07:26:46.233: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 15 07:26:46.233: INFO: update-demo-nautilus-wsqmf is verified up and running
  STEP: using delete to clean up resources @ 04/15/24 07:26:46.233
  Apr 15 07:26:46.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3132 delete --grace-period=0 --force -f -'
  Apr 15 07:26:46.415: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 15 07:26:46.416: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Apr 15 07:26:46.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3132 get rc,svc -l name=update-demo --no-headers'
  Apr 15 07:26:46.687: INFO: stderr: "No resources found in kubectl-3132 namespace.\n"
  Apr 15 07:26:46.687: INFO: stdout: ""
  Apr 15 07:26:46.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3132 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 15 07:26:47.025: INFO: stderr: ""
  Apr 15 07:26:47.025: INFO: stdout: ""
  Apr 15 07:26:47.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3132" for this suite. @ 04/15/24 07:26:47.037
• [8.322 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 04/15/24 07:26:47.064
  Apr 15 07:26:47.064: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename server-version @ 04/15/24 07:26:47.067
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:26:47.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:26:47.129
  STEP: Request ServerVersion @ 04/15/24 07:26:47.137
  STEP: Confirm major version @ 04/15/24 07:26:47.139
  Apr 15 07:26:47.139: INFO: Major version: 1
  STEP: Confirm minor version @ 04/15/24 07:26:47.139
  Apr 15 07:26:47.139: INFO: cleanMinorVersion: 27
  Apr 15 07:26:47.139: INFO: Minor version: 27
  Apr 15 07:26:47.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-3100" for this suite. @ 04/15/24 07:26:47.154
• [0.108 seconds]
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 04/15/24 07:26:47.172
  Apr 15 07:26:47.172: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 07:26:47.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:26:47.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:26:47.218
  STEP: Creating a pod to test downward api env vars @ 04/15/24 07:26:47.225
  STEP: Saw pod success @ 04/15/24 07:26:51.277
  Apr 15 07:26:51.288: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downward-api-e8e7a21a-440b-431e-8caa-32c991508708 container dapi-container: <nil>
  STEP: delete the pod @ 04/15/24 07:26:51.314
  Apr 15 07:26:51.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8159" for this suite. @ 04/15/24 07:26:51.358
• [4.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 04/15/24 07:26:51.399
  Apr 15 07:26:51.399: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 07:26:51.401
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:26:51.448
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:26:51.455
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 07:26:51.461
  STEP: Saw pod success @ 04/15/24 07:26:55.53
  Apr 15 07:26:55.541: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downwardapi-volume-9013a4a2-6fbb-416a-80c1-7e5c66757248 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 07:26:55.558
  Apr 15 07:26:55.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4379" for this suite. @ 04/15/24 07:26:55.621
• [4.245 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 04/15/24 07:26:55.653
  Apr 15 07:26:55.653: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename dns @ 04/15/24 07:26:55.656
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:26:55.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:26:55.713
  STEP: Creating a test headless service @ 04/15/24 07:26:55.723
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-151.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-151.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 04/15/24 07:26:55.736
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-151.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-151.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 04/15/24 07:26:55.736
  STEP: creating a pod to probe DNS @ 04/15/24 07:26:55.736
  STEP: submitting the pod to kubernetes @ 04/15/24 07:26:55.736
  STEP: retrieving the pod @ 04/15/24 07:26:59.829
  STEP: looking for the results for each expected name from probers @ 04/15/24 07:26:59.841
  Apr 15 07:26:59.897: INFO: Unable to read jessie_hosts@dns-querier-2 from pod dns-151/dns-test-26f64dd8-599d-4cb0-8c80-dd59a73f6a78: the server could not find the requested resource (get pods dns-test-26f64dd8-599d-4cb0-8c80-dd59a73f6a78)
  Apr 15 07:26:59.897: INFO: Lookups using dns-151/dns-test-26f64dd8-599d-4cb0-8c80-dd59a73f6a78 failed for: [jessie_hosts@dns-querier-2]

  Apr 15 07:27:04.953: INFO: DNS probes using dns-151/dns-test-26f64dd8-599d-4cb0-8c80-dd59a73f6a78 succeeded

  Apr 15 07:27:04.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 07:27:04.965
  STEP: deleting the test headless service @ 04/15/24 07:27:05.033
  STEP: Destroying namespace "dns-151" for this suite. @ 04/15/24 07:27:05.103
• [9.476 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 04/15/24 07:27:05.132
  Apr 15 07:27:05.133: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename webhook @ 04/15/24 07:27:05.138
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:27:05.184
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:27:05.195
  STEP: Setting up server cert @ 04/15/24 07:27:05.273
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 07:27:06.37
  STEP: Deploying the webhook pod @ 04/15/24 07:27:06.398
  STEP: Wait for the deployment to be ready @ 04/15/24 07:27:06.442
  Apr 15 07:27:06.509: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/15/24 07:27:08.564
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 07:27:08.588
  Apr 15 07:27:09.590: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 04/15/24 07:27:09.599
  STEP: Creating a custom resource definition that should be denied by the webhook @ 04/15/24 07:27:09.643
  Apr 15 07:27:09.644: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:27:09.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8714" for this suite. @ 04/15/24 07:27:09.845
  STEP: Destroying namespace "webhook-markers-1008" for this suite. @ 04/15/24 07:27:09.876
• [4.762 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 04/15/24 07:27:09.904
  Apr 15 07:27:09.904: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename sched-preemption @ 04/15/24 07:27:09.913
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:27:09.959
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:27:09.973
  Apr 15 07:27:10.034: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 15 07:28:10.120: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 04/15/24 07:28:10.13
  Apr 15 07:28:10.130: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename sched-preemption-path @ 04/15/24 07:28:10.134
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:28:10.188
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:28:10.195
  STEP: Finding an available node @ 04/15/24 07:28:10.2
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/15/24 07:28:10.2
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/15/24 07:28:12.251
  Apr 15 07:28:12.281: INFO: found a healthy node: zaigh3ewotoh-3
  Apr 15 07:28:18.517: INFO: pods created so far: [1 1 1]
  Apr 15 07:28:18.518: INFO: length of pods created so far: 3
  Apr 15 07:28:20.546: INFO: pods created so far: [2 2 1]
  Apr 15 07:28:27.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 07:28:27.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-5413" for this suite. @ 04/15/24 07:28:27.74
  STEP: Destroying namespace "sched-preemption-2439" for this suite. @ 04/15/24 07:28:27.754
• [77.865 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 04/15/24 07:28:27.774
  Apr 15 07:28:27.775: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 07:28:27.777
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:28:27.825
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:28:27.836
  STEP: Creating a ResourceQuota with terminating scope @ 04/15/24 07:28:27.84
  STEP: Ensuring ResourceQuota status is calculated @ 04/15/24 07:28:27.852
  STEP: Creating a ResourceQuota with not terminating scope @ 04/15/24 07:28:29.864
  STEP: Ensuring ResourceQuota status is calculated @ 04/15/24 07:28:29.877
  STEP: Creating a long running pod @ 04/15/24 07:28:31.887
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 04/15/24 07:28:31.926
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 04/15/24 07:28:33.938
  STEP: Deleting the pod @ 04/15/24 07:28:35.952
  STEP: Ensuring resource quota status released the pod usage @ 04/15/24 07:28:35.979
  STEP: Creating a terminating pod @ 04/15/24 07:28:37.992
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 04/15/24 07:28:38.018
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 04/15/24 07:28:40.028
  STEP: Deleting the pod @ 04/15/24 07:28:42.041
  STEP: Ensuring resource quota status released the pod usage @ 04/15/24 07:28:42.086
  Apr 15 07:28:44.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-130" for this suite. @ 04/15/24 07:28:44.121
• [16.368 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 04/15/24 07:28:44.152
  Apr 15 07:28:44.152: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename webhook @ 04/15/24 07:28:44.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:28:44.199
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:28:44.211
  STEP: Setting up server cert @ 04/15/24 07:28:44.266
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 07:28:44.932
  STEP: Deploying the webhook pod @ 04/15/24 07:28:44.968
  STEP: Wait for the deployment to be ready @ 04/15/24 07:28:45.011
  Apr 15 07:28:45.060: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  Apr 15 07:28:47.090: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6d58c8c59c\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 07:28:49.103: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6d58c8c59c\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 07:28:51.109: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6d58c8c59c\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 07:28:53.103: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6d58c8c59c\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 07:28:55.107: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 28, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6d58c8c59c\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 04/15/24 07:28:57.103
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 07:28:57.133
  Apr 15 07:28:58.133: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 04/15/24 07:28:58.144
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/15/24 07:28:58.145
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 04/15/24 07:28:58.198
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 04/15/24 07:28:59.223
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/15/24 07:28:59.223
  STEP: Having no error when timeout is longer than webhook latency @ 04/15/24 07:29:00.302
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/15/24 07:29:00.303
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 04/15/24 07:29:05.396
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/15/24 07:29:05.396
  Apr 15 07:29:10.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-207" for this suite. @ 04/15/24 07:29:10.662
  STEP: Destroying namespace "webhook-markers-6003" for this suite. @ 04/15/24 07:29:10.72
• [26.620 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 04/15/24 07:29:10.781
  Apr 15 07:29:10.782: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubectl-logs @ 04/15/24 07:29:10.791
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:29:10.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:29:10.869
  STEP: creating an pod @ 04/15/24 07:29:10.876
  Apr 15 07:29:10.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-logs-8080 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.47 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Apr 15 07:29:11.166: INFO: stderr: ""
  Apr 15 07:29:11.166: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 04/15/24 07:29:11.171
  Apr 15 07:29:11.172: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  Apr 15 07:29:13.204: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 04/15/24 07:29:13.204
  Apr 15 07:29:13.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-logs-8080 logs logs-generator logs-generator'
  Apr 15 07:29:13.499: INFO: stderr: ""
  Apr 15 07:29:13.499: INFO: stdout: "I0415 07:29:12.069001       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/2tp 362\nI0415 07:29:12.266910       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/v6m 406\nI0415 07:29:12.466482       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/vxjq 306\nI0415 07:29:12.667159       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/wjv 412\nI0415 07:29:12.866572       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/2bt 591\nI0415 07:29:13.067081       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/9hq 453\nI0415 07:29:13.267242       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/tbsd 204\nI0415 07:29:13.466655       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/pzh 521\n"
  STEP: limiting log lines @ 04/15/24 07:29:13.5
  Apr 15 07:29:13.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-logs-8080 logs logs-generator logs-generator --tail=1'
  Apr 15 07:29:13.749: INFO: stderr: ""
  Apr 15 07:29:13.750: INFO: stdout: "I0415 07:29:13.667310       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/wzq 425\n"
  Apr 15 07:29:13.750: INFO: got output "I0415 07:29:13.667310       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/wzq 425\n"
  STEP: limiting log bytes @ 04/15/24 07:29:13.75
  Apr 15 07:29:13.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-logs-8080 logs logs-generator logs-generator --limit-bytes=1'
  Apr 15 07:29:13.993: INFO: stderr: ""
  Apr 15 07:29:13.993: INFO: stdout: "I"
  Apr 15 07:29:13.993: INFO: got output "I"
  STEP: exposing timestamps @ 04/15/24 07:29:13.994
  Apr 15 07:29:13.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-logs-8080 logs logs-generator logs-generator --tail=1 --timestamps'
  Apr 15 07:29:14.209: INFO: stderr: ""
  Apr 15 07:29:14.210: INFO: stdout: "2024-04-15T07:29:14.068787410Z I0415 07:29:14.068600       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/9sf 223\n"
  Apr 15 07:29:14.210: INFO: got output "2024-04-15T07:29:14.068787410Z I0415 07:29:14.068600       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/9sf 223\n"
  STEP: restricting to a time range @ 04/15/24 07:29:14.21
  Apr 15 07:29:16.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-logs-8080 logs logs-generator logs-generator --since=1s'
  Apr 15 07:29:16.947: INFO: stderr: ""
  Apr 15 07:29:16.947: INFO: stdout: "I0415 07:29:16.066640       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/kxz 416\nI0415 07:29:16.267074       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/hvvv 580\nI0415 07:29:16.466597       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/wr6 369\nI0415 07:29:16.667023       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/default/pods/lrl 266\nI0415 07:29:16.866633       1 logs_generator.go:76] 24 POST /api/v1/namespaces/kube-system/pods/dk4 525\n"
  Apr 15 07:29:16.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-logs-8080 logs logs-generator logs-generator --since=24h'
  Apr 15 07:29:17.264: INFO: stderr: ""
  Apr 15 07:29:17.264: INFO: stdout: "I0415 07:29:12.069001       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/2tp 362\nI0415 07:29:12.266910       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/v6m 406\nI0415 07:29:12.466482       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/vxjq 306\nI0415 07:29:12.667159       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/wjv 412\nI0415 07:29:12.866572       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/2bt 591\nI0415 07:29:13.067081       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/9hq 453\nI0415 07:29:13.267242       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/tbsd 204\nI0415 07:29:13.466655       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/pzh 521\nI0415 07:29:13.667310       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/wzq 425\nI0415 07:29:13.866895       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/w6xd 419\nI0415 07:29:14.068600       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/9sf 223\nI0415 07:29:14.266156       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/jb7 505\nI0415 07:29:14.466701       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/vxw 376\nI0415 07:29:14.666153       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/mqd 477\nI0415 07:29:14.866646       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/t6p 351\nI0415 07:29:15.066121       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/tzw 320\nI0415 07:29:15.267028       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/ssm 256\nI0415 07:29:15.466322       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/dgf9 207\nI0415 07:29:15.666760       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/bk9 288\nI0415 07:29:15.866129       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/vlv 523\nI0415 07:29:16.066640       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/kxz 416\nI0415 07:29:16.267074       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/hvvv 580\nI0415 07:29:16.466597       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/wr6 369\nI0415 07:29:16.667023       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/default/pods/lrl 266\nI0415 07:29:16.866633       1 logs_generator.go:76] 24 POST /api/v1/namespaces/kube-system/pods/dk4 525\nI0415 07:29:17.066552       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/pvvn 448\n"
  Apr 15 07:29:17.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-logs-8080 delete pod logs-generator'
  Apr 15 07:29:18.032: INFO: stderr: ""
  Apr 15 07:29:18.032: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Apr 15 07:29:18.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-8080" for this suite. @ 04/15/24 07:29:18.058
• [7.314 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 04/15/24 07:29:18.101
  Apr 15 07:29:18.101: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename replication-controller @ 04/15/24 07:29:18.114
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:29:18.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:29:18.173
  Apr 15 07:29:18.188: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 04/15/24 07:29:19.226
  STEP: Checking rc "condition-test" has the desired failure condition set @ 04/15/24 07:29:19.24
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 04/15/24 07:29:20.265
  Apr 15 07:29:20.310: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 04/15/24 07:29:20.31
  Apr 15 07:29:20.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6877" for this suite. @ 04/15/24 07:29:20.348
• [2.266 seconds]
------------------------------
S
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 04/15/24 07:29:20.367
  Apr 15 07:29:20.367: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename dns @ 04/15/24 07:29:20.37
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:29:20.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:29:20.422
  STEP: Creating a test externalName service @ 04/15/24 07:29:20.433
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5300.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5300.svc.cluster.local; sleep 1; done
   @ 04/15/24 07:29:20.453
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5300.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5300.svc.cluster.local; sleep 1; done
   @ 04/15/24 07:29:20.453
  STEP: creating a pod to probe DNS @ 04/15/24 07:29:20.453
  STEP: submitting the pod to kubernetes @ 04/15/24 07:29:20.454
  STEP: retrieving the pod @ 04/15/24 07:29:22.552
  STEP: looking for the results for each expected name from probers @ 04/15/24 07:29:22.561
  Apr 15 07:29:22.598: INFO: DNS probes using dns-test-c400bcfb-0f52-4b2d-a1f4-e4ac9362f621 succeeded

  STEP: changing the externalName to bar.example.com @ 04/15/24 07:29:22.599
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5300.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5300.svc.cluster.local; sleep 1; done
   @ 04/15/24 07:29:22.627
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5300.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5300.svc.cluster.local; sleep 1; done
   @ 04/15/24 07:29:22.627
  STEP: creating a second pod to probe DNS @ 04/15/24 07:29:22.628
  STEP: submitting the pod to kubernetes @ 04/15/24 07:29:22.628
  STEP: retrieving the pod @ 04/15/24 07:29:26.673
  STEP: looking for the results for each expected name from probers @ 04/15/24 07:29:26.684
  Apr 15 07:29:26.700: INFO: File wheezy_udp@dns-test-service-3.dns-5300.svc.cluster.local from pod  dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 07:29:26.713: INFO: File jessie_udp@dns-test-service-3.dns-5300.svc.cluster.local from pod  dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 07:29:26.713: INFO: Lookups using dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 failed for: [wheezy_udp@dns-test-service-3.dns-5300.svc.cluster.local jessie_udp@dns-test-service-3.dns-5300.svc.cluster.local]

  Apr 15 07:29:31.727: INFO: File wheezy_udp@dns-test-service-3.dns-5300.svc.cluster.local from pod  dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 07:29:31.738: INFO: File jessie_udp@dns-test-service-3.dns-5300.svc.cluster.local from pod  dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 07:29:31.738: INFO: Lookups using dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 failed for: [wheezy_udp@dns-test-service-3.dns-5300.svc.cluster.local jessie_udp@dns-test-service-3.dns-5300.svc.cluster.local]

  Apr 15 07:29:36.727: INFO: File wheezy_udp@dns-test-service-3.dns-5300.svc.cluster.local from pod  dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 07:29:36.743: INFO: File jessie_udp@dns-test-service-3.dns-5300.svc.cluster.local from pod  dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 07:29:36.743: INFO: Lookups using dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 failed for: [wheezy_udp@dns-test-service-3.dns-5300.svc.cluster.local jessie_udp@dns-test-service-3.dns-5300.svc.cluster.local]

  Apr 15 07:29:41.725: INFO: File wheezy_udp@dns-test-service-3.dns-5300.svc.cluster.local from pod  dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 contains '' instead of 'bar.example.com.'
  Apr 15 07:29:41.741: INFO: File jessie_udp@dns-test-service-3.dns-5300.svc.cluster.local from pod  dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 contains '' instead of 'bar.example.com.'
  Apr 15 07:29:41.741: INFO: Lookups using dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 failed for: [wheezy_udp@dns-test-service-3.dns-5300.svc.cluster.local jessie_udp@dns-test-service-3.dns-5300.svc.cluster.local]

  Apr 15 07:29:46.730: INFO: File wheezy_udp@dns-test-service-3.dns-5300.svc.cluster.local from pod  dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 07:29:46.751: INFO: File jessie_udp@dns-test-service-3.dns-5300.svc.cluster.local from pod  dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 07:29:46.751: INFO: Lookups using dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 failed for: [wheezy_udp@dns-test-service-3.dns-5300.svc.cluster.local jessie_udp@dns-test-service-3.dns-5300.svc.cluster.local]

  Apr 15 07:29:51.725: INFO: File wheezy_udp@dns-test-service-3.dns-5300.svc.cluster.local from pod  dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 07:29:51.739: INFO: File jessie_udp@dns-test-service-3.dns-5300.svc.cluster.local from pod  dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 07:29:51.739: INFO: Lookups using dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 failed for: [wheezy_udp@dns-test-service-3.dns-5300.svc.cluster.local jessie_udp@dns-test-service-3.dns-5300.svc.cluster.local]

  Apr 15 07:29:56.735: INFO: File jessie_udp@dns-test-service-3.dns-5300.svc.cluster.local from pod  dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 contains '' instead of 'bar.example.com.'
  Apr 15 07:29:56.735: INFO: Lookups using dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 failed for: [jessie_udp@dns-test-service-3.dns-5300.svc.cluster.local]

  Apr 15 07:30:01.729: INFO: File wheezy_udp@dns-test-service-3.dns-5300.svc.cluster.local from pod  dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 contains '' instead of 'bar.example.com.'
  Apr 15 07:30:01.743: INFO: Lookups using dns-5300/dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 failed for: [wheezy_udp@dns-test-service-3.dns-5300.svc.cluster.local]

  Apr 15 07:30:06.740: INFO: DNS probes using dns-test-84674f00-d3d9-4737-b960-b210968a1bf3 succeeded

  STEP: changing the service to type=ClusterIP @ 04/15/24 07:30:06.74
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5300.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5300.svc.cluster.local; sleep 1; done
   @ 04/15/24 07:30:06.821
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5300.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5300.svc.cluster.local; sleep 1; done
   @ 04/15/24 07:30:06.821
  STEP: creating a third pod to probe DNS @ 04/15/24 07:30:06.821
  STEP: submitting the pod to kubernetes @ 04/15/24 07:30:06.838
  STEP: retrieving the pod @ 04/15/24 07:30:39.091
  STEP: looking for the results for each expected name from probers @ 04/15/24 07:30:39.099
  Apr 15 07:30:39.127: INFO: DNS probes using dns-test-982448d4-6136-4605-97c7-b10ed0b07282 succeeded

  Apr 15 07:30:39.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 07:30:39.143
  STEP: deleting the pod @ 04/15/24 07:30:39.166
  STEP: deleting the pod @ 04/15/24 07:30:39.193
  STEP: deleting the test externalName service @ 04/15/24 07:30:39.26
  STEP: Destroying namespace "dns-5300" for this suite. @ 04/15/24 07:30:39.363
• [79.029 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:329
  STEP: Creating a kubernetes client @ 04/15/24 07:30:39.399
  Apr 15 07:30:39.401: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename statefulset @ 04/15/24 07:30:39.406
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:30:39.469
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:30:39.477
  STEP: Creating service test in namespace statefulset-9374 @ 04/15/24 07:30:39.486
  STEP: Creating a new StatefulSet @ 04/15/24 07:30:39.498
  Apr 15 07:30:39.537: INFO: Found 0 stateful pods, waiting for 3
  Apr 15 07:30:49.553: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 07:30:49.553: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 07:30:49.553: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 04/15/24 07:30:49.588
  Apr 15 07:30:49.633: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 04/15/24 07:30:49.634
  STEP: Not applying an update when the partition is greater than the number of replicas @ 04/15/24 07:30:59.678
  STEP: Performing a canary update @ 04/15/24 07:30:59.679
  Apr 15 07:30:59.717: INFO: Updating stateful set ss2
  Apr 15 07:30:59.743: INFO: Waiting for Pod statefulset-9374/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 04/15/24 07:31:09.774
  Apr 15 07:31:09.946: INFO: Found 1 stateful pods, waiting for 3
  Apr 15 07:31:19.966: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 07:31:19.967: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 07:31:19.967: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 04/15/24 07:31:19.992
  Apr 15 07:31:20.028: INFO: Updating stateful set ss2
  Apr 15 07:31:20.063: INFO: Waiting for Pod statefulset-9374/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Apr 15 07:31:30.130: INFO: Updating stateful set ss2
  Apr 15 07:31:30.150: INFO: Waiting for StatefulSet statefulset-9374/ss2 to complete update
  Apr 15 07:31:30.150: INFO: Waiting for Pod statefulset-9374/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Apr 15 07:31:40.186: INFO: Deleting all statefulset in ns statefulset-9374
  Apr 15 07:31:40.196: INFO: Scaling statefulset ss2 to 0
  Apr 15 07:31:50.263: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 07:31:50.274: INFO: Deleting statefulset ss2
  Apr 15 07:31:50.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9374" for this suite. @ 04/15/24 07:31:50.343
• [70.975 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 04/15/24 07:31:50.384
  Apr 15 07:31:50.384: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 07:31:50.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:31:50.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:31:50.452
  STEP: Creating secret with name s-test-opt-del-d2854e4a-ca46-48da-85a7-4a15aaaef054 @ 04/15/24 07:31:50.478
  STEP: Creating secret with name s-test-opt-upd-c40eabd7-aa0a-48d6-97c1-df398ee986dc @ 04/15/24 07:31:50.49
  STEP: Creating the pod @ 04/15/24 07:31:50.502
  STEP: Deleting secret s-test-opt-del-d2854e4a-ca46-48da-85a7-4a15aaaef054 @ 04/15/24 07:31:52.702
  STEP: Updating secret s-test-opt-upd-c40eabd7-aa0a-48d6-97c1-df398ee986dc @ 04/15/24 07:31:52.72
  STEP: Creating secret with name s-test-opt-create-7554ff99-6d39-4010-a7d5-c5475c3941a1 @ 04/15/24 07:31:52.74
  STEP: waiting to observe update in volume @ 04/15/24 07:31:52.758
  Apr 15 07:32:55.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7849" for this suite. @ 04/15/24 07:32:55.722
• [65.366 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 04/15/24 07:32:55.777
  Apr 15 07:32:55.777: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 07:32:55.796
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:32:55.857
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:32:55.863
  STEP: Discovering how many secrets are in namespace by default @ 04/15/24 07:32:55.872
  STEP: Counting existing ResourceQuota @ 04/15/24 07:33:00.881
  STEP: Creating a ResourceQuota @ 04/15/24 07:33:05.891
  STEP: Ensuring resource quota status is calculated @ 04/15/24 07:33:05.906
  STEP: Creating a Secret @ 04/15/24 07:33:07.915
  STEP: Ensuring resource quota status captures secret creation @ 04/15/24 07:33:07.939
  STEP: Deleting a secret @ 04/15/24 07:33:09.951
  STEP: Ensuring resource quota status released usage @ 04/15/24 07:33:09.969
  Apr 15 07:33:11.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4227" for this suite. @ 04/15/24 07:33:12.007
• [16.253 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 04/15/24 07:33:12.039
  Apr 15 07:33:12.039: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/15/24 07:33:12.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:33:12.091
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:33:12.104
  Apr 15 07:33:12.114: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:33:13.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-423" for this suite. @ 04/15/24 07:33:13.218
• [1.191 seconds]
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 04/15/24 07:33:13.232
  Apr 15 07:33:13.232: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename ingress @ 04/15/24 07:33:13.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:33:13.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:33:13.282
  STEP: getting /apis @ 04/15/24 07:33:13.288
  STEP: getting /apis/networking.k8s.io @ 04/15/24 07:33:13.303
  STEP: getting /apis/networking.k8s.iov1 @ 04/15/24 07:33:13.308
  STEP: creating @ 04/15/24 07:33:13.312
  STEP: getting @ 04/15/24 07:33:13.381
  STEP: listing @ 04/15/24 07:33:13.391
  STEP: watching @ 04/15/24 07:33:13.402
  Apr 15 07:33:13.402: INFO: starting watch
  STEP: cluster-wide listing @ 04/15/24 07:33:13.405
  STEP: cluster-wide watching @ 04/15/24 07:33:13.42
  Apr 15 07:33:13.420: INFO: starting watch
  STEP: patching @ 04/15/24 07:33:13.424
  STEP: updating @ 04/15/24 07:33:13.44
  Apr 15 07:33:13.459: INFO: waiting for watch events with expected annotations
  Apr 15 07:33:13.459: INFO: saw patched and updated annotations
  STEP: patching /status @ 04/15/24 07:33:13.459
  STEP: updating /status @ 04/15/24 07:33:13.479
  STEP: get /status @ 04/15/24 07:33:13.505
  STEP: deleting @ 04/15/24 07:33:13.513
  STEP: deleting a collection @ 04/15/24 07:33:13.553
  Apr 15 07:33:13.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-9762" for this suite. @ 04/15/24 07:33:13.622
• [0.427 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 04/15/24 07:33:13.661
  Apr 15 07:33:13.662: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 07:33:13.666
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:33:13.722
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:33:13.73
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 07:33:13.739
  STEP: Saw pod success @ 04/15/24 07:33:17.81
  Apr 15 07:33:17.820: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downwardapi-volume-d1e2de30-a32b-4a32-98c7-06f18c39e293 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 07:33:17.844
  Apr 15 07:33:17.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-352" for this suite. @ 04/15/24 07:33:17.905
• [4.260 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 04/15/24 07:33:17.923
  Apr 15 07:33:17.923: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename pods @ 04/15/24 07:33:17.927
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:33:17.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:33:17.982
  STEP: creating the pod @ 04/15/24 07:33:17.989
  STEP: setting up watch @ 04/15/24 07:33:17.99
  STEP: submitting the pod to kubernetes @ 04/15/24 07:33:18.1
  STEP: verifying the pod is in kubernetes @ 04/15/24 07:33:18.126
  STEP: verifying pod creation was observed @ 04/15/24 07:33:18.136
  STEP: deleting the pod gracefully @ 04/15/24 07:33:20.166
  STEP: verifying pod deletion was observed @ 04/15/24 07:33:20.186
  Apr 15 07:33:21.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7950" for this suite. @ 04/15/24 07:33:21.467
• [3.571 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 04/15/24 07:33:21.51
  Apr 15 07:33:21.510: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename gc @ 04/15/24 07:33:21.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:33:21.551
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:33:21.56
  Apr 15 07:33:21.666: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"5febfab0-6a89-4aed-b03e-1e06fa2a5e3e", Controller:(*bool)(0xc003f36212), BlockOwnerDeletion:(*bool)(0xc003f36213)}}
  Apr 15 07:33:21.699: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"48beec74-1375-483c-a10f-3a25abbfd42c", Controller:(*bool)(0xc00454ef9a), BlockOwnerDeletion:(*bool)(0xc00454ef9b)}}
  Apr 15 07:33:21.719: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"0bfcacbb-c1d3-4102-bbb2-538b94387f79", Controller:(*bool)(0xc003f3684a), BlockOwnerDeletion:(*bool)(0xc003f3684b)}}
  Apr 15 07:33:26.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6457" for this suite. @ 04/15/24 07:33:26.77
• [5.280 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 04/15/24 07:33:26.797
  Apr 15 07:33:26.797: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 07:33:26.801
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:33:26.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:33:26.876
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 04/15/24 07:33:26.884
  STEP: Saw pod success @ 04/15/24 07:33:30.951
  Apr 15 07:33:30.961: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-e3566c9e-86c6-4362-a314-e81ea7180369 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 07:33:30.975
  Apr 15 07:33:31.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7566" for this suite. @ 04/15/24 07:33:31.042
• [4.263 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 04/15/24 07:33:31.067
  Apr 15 07:33:31.067: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename replicaset @ 04/15/24 07:33:31.07
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:33:31.114
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:33:31.122
  Apr 15 07:33:31.176: INFO: Pod name sample-pod: Found 0 pods out of 1
  Apr 15 07:33:36.187: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/15/24 07:33:36.188
  STEP: Scaling up "test-rs" replicaset  @ 04/15/24 07:33:36.188
  Apr 15 07:33:36.230: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 04/15/24 07:33:36.231
  W0415 07:33:36.255829      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 15 07:33:36.263: INFO: observed ReplicaSet test-rs in namespace replicaset-4946 with ReadyReplicas 1, AvailableReplicas 1
  Apr 15 07:33:36.316: INFO: observed ReplicaSet test-rs in namespace replicaset-4946 with ReadyReplicas 1, AvailableReplicas 1
  Apr 15 07:33:36.411: INFO: observed ReplicaSet test-rs in namespace replicaset-4946 with ReadyReplicas 1, AvailableReplicas 1
  Apr 15 07:33:36.435: INFO: observed ReplicaSet test-rs in namespace replicaset-4946 with ReadyReplicas 1, AvailableReplicas 1
  Apr 15 07:33:37.853: INFO: observed ReplicaSet test-rs in namespace replicaset-4946 with ReadyReplicas 2, AvailableReplicas 2
  Apr 15 07:33:37.935: INFO: observed Replicaset test-rs in namespace replicaset-4946 with ReadyReplicas 3 found true
  Apr 15 07:33:37.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4946" for this suite. @ 04/15/24 07:33:37.954
• [6.900 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 04/15/24 07:33:37.98
  Apr 15 07:33:37.980: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename job @ 04/15/24 07:33:37.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:33:38.024
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:33:38.029
  STEP: Creating a job @ 04/15/24 07:33:38.037
  STEP: Ensuring active pods == parallelism @ 04/15/24 07:33:38.054
  STEP: Orphaning one of the Job's Pods @ 04/15/24 07:33:40.069
  Apr 15 07:33:40.613: INFO: Successfully updated pod "adopt-release-d9b5q"
  STEP: Checking that the Job readopts the Pod @ 04/15/24 07:33:40.614
  STEP: Removing the labels from the Job's Pod @ 04/15/24 07:33:42.641
  Apr 15 07:33:43.178: INFO: Successfully updated pod "adopt-release-d9b5q"
  STEP: Checking that the Job releases the Pod @ 04/15/24 07:33:43.178
  Apr 15 07:33:45.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7649" for this suite. @ 04/15/24 07:33:45.223
• [7.264 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 04/15/24 07:33:45.249
  Apr 15 07:33:45.250: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-runtime @ 04/15/24 07:33:45.257
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:33:45.308
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:33:45.315
  STEP: create the container @ 04/15/24 07:33:45.326
  W0415 07:33:45.351885      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/15/24 07:33:45.352
  STEP: get the container status @ 04/15/24 07:33:48.397
  STEP: the container should be terminated @ 04/15/24 07:33:48.41
  STEP: the termination message should be set @ 04/15/24 07:33:48.41
  Apr 15 07:33:48.410: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 04/15/24 07:33:48.41
  Apr 15 07:33:48.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-4969" for this suite. @ 04/15/24 07:33:48.471
• [3.255 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 04/15/24 07:33:48.517
  Apr 15 07:33:48.517: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename pods @ 04/15/24 07:33:48.522
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:33:48.562
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:33:48.567
  STEP: creating pod @ 04/15/24 07:33:48.574
  Apr 15 07:33:50.666: INFO: Pod pod-hostip-da92ee05-9f8b-44cb-8c01-09220da08cd4 has hostIP: 192.168.121.27
  Apr 15 07:33:50.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6432" for this suite. @ 04/15/24 07:33:50.682
• [2.187 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 04/15/24 07:33:50.706
  Apr 15 07:33:50.706: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename replication-controller @ 04/15/24 07:33:50.715
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:33:50.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:33:50.772
  STEP: Given a ReplicationController is created @ 04/15/24 07:33:50.778
  STEP: When the matched label of one of its pods change @ 04/15/24 07:33:50.797
  Apr 15 07:33:50.803: INFO: Pod name pod-release: Found 0 pods out of 1
  Apr 15 07:33:55.825: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 04/15/24 07:33:55.851
  Apr 15 07:33:56.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3786" for this suite. @ 04/15/24 07:33:56.923
• [6.286 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 04/15/24 07:33:56.995
  Apr 15 07:33:56.995: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename replicaset @ 04/15/24 07:33:56.996
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:33:57.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:33:57.209
  Apr 15 07:33:57.217: INFO: Creating ReplicaSet my-hostname-basic-dc1a93c1-0c4b-4c25-9e82-d75bf8e7f7a8
  Apr 15 07:33:57.256: INFO: Pod name my-hostname-basic-dc1a93c1-0c4b-4c25-9e82-d75bf8e7f7a8: Found 0 pods out of 1
  Apr 15 07:34:02.268: INFO: Pod name my-hostname-basic-dc1a93c1-0c4b-4c25-9e82-d75bf8e7f7a8: Found 1 pods out of 1
  Apr 15 07:34:02.269: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-dc1a93c1-0c4b-4c25-9e82-d75bf8e7f7a8" is running
  Apr 15 07:34:02.285: INFO: Pod "my-hostname-basic-dc1a93c1-0c4b-4c25-9e82-d75bf8e7f7a8-b7lqc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-04-15 07:33:57 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-04-15 07:33:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-04-15 07:33:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-04-15 07:33:57 +0000 UTC Reason: Message:}])
  Apr 15 07:34:02.286: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 04/15/24 07:34:02.287
  Apr 15 07:34:02.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1462" for this suite. @ 04/15/24 07:34:02.374
• [5.396 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 04/15/24 07:34:02.392
  Apr 15 07:34:02.393: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename watch @ 04/15/24 07:34:02.396
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:34:02.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:34:02.442
  STEP: creating a new configmap @ 04/15/24 07:34:02.449
  STEP: modifying the configmap once @ 04/15/24 07:34:02.46
  STEP: modifying the configmap a second time @ 04/15/24 07:34:02.49
  STEP: deleting the configmap @ 04/15/24 07:34:02.511
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 04/15/24 07:34:02.535
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 04/15/24 07:34:02.539
  Apr 15 07:34:02.539: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1807  8ffdcb11-96aa-4ae5-8e6c-f2846fb93055 166763 0 2024-04-15 07:34:02 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2024-04-15 07:34:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 07:34:02.540: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1807  8ffdcb11-96aa-4ae5-8e6c-f2846fb93055 166765 0 2024-04-15 07:34:02 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2024-04-15 07:34:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 07:34:02.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1807" for this suite. @ 04/15/24 07:34:02.554
• [0.185 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 04/15/24 07:34:02.589
  Apr 15 07:34:02.590: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubelet-test @ 04/15/24 07:34:02.592
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:34:02.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:34:02.65
  Apr 15 07:34:04.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4686" for this suite. @ 04/15/24 07:34:04.779
• [2.207 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 04/15/24 07:34:04.804
  Apr 15 07:34:04.805: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 07:34:04.811
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:34:04.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:34:04.874
  STEP: Creating projection with secret that has name projected-secret-test-f1949dc2-4034-4b2b-be23-1fc41928776d @ 04/15/24 07:34:04.88
  STEP: Creating a pod to test consume secrets @ 04/15/24 07:34:04.891
  STEP: Saw pod success @ 04/15/24 07:34:06.939
  Apr 15 07:34:06.946: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-projected-secrets-0311932d-aa06-4855-b870-9706896da76b container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 07:34:06.961
  Apr 15 07:34:06.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6904" for this suite. @ 04/15/24 07:34:07.011
• [2.223 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 04/15/24 07:34:07.029
  Apr 15 07:34:07.029: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename pods @ 04/15/24 07:34:07.031
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:34:07.076
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:34:07.081
  STEP: Create a pod @ 04/15/24 07:34:07.088
  STEP: patching /status @ 04/15/24 07:34:09.152
  Apr 15 07:34:09.196: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Apr 15 07:34:09.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7332" for this suite. @ 04/15/24 07:34:09.21
• [2.208 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:875
  STEP: Creating a kubernetes client @ 04/15/24 07:34:09.244
  Apr 15 07:34:09.244: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename daemonsets @ 04/15/24 07:34:09.247
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:34:09.283
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:34:09.296
  STEP: Creating simple DaemonSet "daemon-set" @ 04/15/24 07:34:09.371
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/15/24 07:34:09.391
  Apr 15 07:34:09.422: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:34:09.423: INFO: Node zaigh3ewotoh-1 is running 0 daemon pod, expected 1
  Apr 15 07:34:10.447: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:34:10.448: INFO: Node zaigh3ewotoh-1 is running 0 daemon pod, expected 1
  Apr 15 07:34:11.442: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 15 07:34:11.442: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 04/15/24 07:34:11.449
  Apr 15 07:34:11.459: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 04/15/24 07:34:11.46
  Apr 15 07:34:11.488: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 04/15/24 07:34:11.488
  Apr 15 07:34:11.493: INFO: Observed &DaemonSet event: ADDED
  Apr 15 07:34:11.493: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 07:34:11.493: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 07:34:11.494: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 07:34:11.494: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 07:34:11.494: INFO: Found daemon set daemon-set in namespace daemonsets-5433 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 15 07:34:11.494: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 04/15/24 07:34:11.494
  STEP: watching for the daemon set status to be patched @ 04/15/24 07:34:11.51
  Apr 15 07:34:11.513: INFO: Observed &DaemonSet event: ADDED
  Apr 15 07:34:11.514: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 07:34:11.515: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 07:34:11.515: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 07:34:11.516: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 07:34:11.517: INFO: Observed daemon set daemon-set in namespace daemonsets-5433 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 15 07:34:11.518: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 07:34:11.518: INFO: Found daemon set daemon-set in namespace daemonsets-5433 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Apr 15 07:34:11.519: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 04/15/24 07:34:11.533
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5433, will wait for the garbage collector to delete the pods @ 04/15/24 07:34:11.534
  Apr 15 07:34:11.607: INFO: Deleting DaemonSet.extensions daemon-set took: 13.309852ms
  Apr 15 07:34:11.708: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.018805ms
  Apr 15 07:34:13.319: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:34:13.319: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 15 07:34:13.326: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"166925"},"items":null}

  Apr 15 07:34:13.335: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"166925"},"items":null}

  Apr 15 07:34:13.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5433" for this suite. @ 04/15/24 07:34:13.395
• [4.164 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 04/15/24 07:34:13.411
  Apr 15 07:34:13.411: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename configmap @ 04/15/24 07:34:13.415
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:34:13.447
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:34:13.452
  STEP: Creating configMap with name configmap-test-upd-cf61c73d-0565-4ea0-a0ab-9704d6db2006 @ 04/15/24 07:34:13.472
  STEP: Creating the pod @ 04/15/24 07:34:13.483
  STEP: Waiting for pod with text data @ 04/15/24 07:34:15.534
  STEP: Waiting for pod with binary data @ 04/15/24 07:34:15.565
  Apr 15 07:34:15.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8218" for this suite. @ 04/15/24 07:34:15.587
• [2.188 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 04/15/24 07:34:15.612
  Apr 15 07:34:15.612: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename pod-network-test @ 04/15/24 07:34:15.614
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:34:15.653
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:34:15.659
  STEP: Performing setup for networking test in namespace pod-network-test-2437 @ 04/15/24 07:34:15.668
  STEP: creating a selector @ 04/15/24 07:34:15.668
  STEP: Creating the service pods in kubernetes @ 04/15/24 07:34:15.668
  Apr 15 07:34:15.669: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 04/15/24 07:34:27.859
  Apr 15 07:34:29.916: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 15 07:34:29.916: INFO: Going to poll 10.233.64.207 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Apr 15 07:34:29.922: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.207 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2437 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 07:34:29.922: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:34:29.923: INFO: ExecWithOptions: Clientset creation
  Apr 15 07:34:29.923: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2437/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.207+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 15 07:34:31.086: INFO: Found all 1 expected endpoints: [netserver-0]
  Apr 15 07:34:31.086: INFO: Going to poll 10.233.65.174 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Apr 15 07:34:31.093: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.174 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2437 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 07:34:31.093: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:34:31.095: INFO: ExecWithOptions: Clientset creation
  Apr 15 07:34:31.095: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2437/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.65.174+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 15 07:34:32.214: INFO: Found all 1 expected endpoints: [netserver-1]
  Apr 15 07:34:32.214: INFO: Going to poll 10.233.66.226 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Apr 15 07:34:32.222: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.226 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2437 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 07:34:32.222: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:34:32.224: INFO: ExecWithOptions: Clientset creation
  Apr 15 07:34:32.224: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-2437/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.226+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 15 07:34:33.351: INFO: Found all 1 expected endpoints: [netserver-2]
  Apr 15 07:34:33.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-2437" for this suite. @ 04/15/24 07:34:33.366
• [17.778 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 04/15/24 07:34:33.393
  Apr 15 07:34:33.394: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename endpointslice @ 04/15/24 07:34:33.395
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:34:33.454
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:34:33.462
  STEP: getting /apis @ 04/15/24 07:34:33.468
  STEP: getting /apis/discovery.k8s.io @ 04/15/24 07:34:33.477
  STEP: getting /apis/discovery.k8s.iov1 @ 04/15/24 07:34:33.478
  STEP: creating @ 04/15/24 07:34:33.48
  STEP: getting @ 04/15/24 07:34:33.522
  STEP: listing @ 04/15/24 07:34:33.537
  STEP: watching @ 04/15/24 07:34:33.546
  Apr 15 07:34:33.546: INFO: starting watch
  STEP: cluster-wide listing @ 04/15/24 07:34:33.547
  STEP: cluster-wide watching @ 04/15/24 07:34:33.553
  Apr 15 07:34:33.553: INFO: starting watch
  STEP: patching @ 04/15/24 07:34:33.557
  STEP: updating @ 04/15/24 07:34:33.572
  Apr 15 07:34:33.591: INFO: waiting for watch events with expected annotations
  Apr 15 07:34:33.592: INFO: saw patched and updated annotations
  STEP: deleting @ 04/15/24 07:34:33.593
  STEP: deleting a collection @ 04/15/24 07:34:33.625
  Apr 15 07:34:33.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-729" for this suite. @ 04/15/24 07:34:33.678
• [0.300 seconds]
------------------------------
S
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 04/15/24 07:34:33.694
  Apr 15 07:34:33.694: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename limitrange @ 04/15/24 07:34:33.696
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:34:33.736
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:34:33.744
  STEP: Creating LimitRange "e2e-limitrange-x7hww" in namespace "limitrange-7979" @ 04/15/24 07:34:33.751
  STEP: Creating another limitRange in another namespace @ 04/15/24 07:34:33.774
  Apr 15 07:34:33.823: INFO: Namespace "e2e-limitrange-x7hww-5762" created
  Apr 15 07:34:33.823: INFO: Creating LimitRange "e2e-limitrange-x7hww" in namespace "e2e-limitrange-x7hww-5762"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-x7hww" @ 04/15/24 07:34:33.837
  Apr 15 07:34:33.845: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-x7hww" in "limitrange-7979" namespace @ 04/15/24 07:34:33.846
  Apr 15 07:34:33.877: INFO: LimitRange "e2e-limitrange-x7hww" has been patched
  STEP: Delete LimitRange "e2e-limitrange-x7hww" by Collection with labelSelector: "e2e-limitrange-x7hww=patched" @ 04/15/24 07:34:33.877
  STEP: Confirm that the limitRange "e2e-limitrange-x7hww" has been deleted @ 04/15/24 07:34:33.897
  Apr 15 07:34:33.898: INFO: Requesting list of LimitRange to confirm quantity
  Apr 15 07:34:33.906: INFO: Found 0 LimitRange with label "e2e-limitrange-x7hww=patched"
  Apr 15 07:34:33.907: INFO: LimitRange "e2e-limitrange-x7hww" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-x7hww" @ 04/15/24 07:34:33.907
  Apr 15 07:34:33.913: INFO: Found 1 limitRange
  Apr 15 07:34:33.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-7979" for this suite. @ 04/15/24 07:34:33.926
  STEP: Destroying namespace "e2e-limitrange-x7hww-5762" for this suite. @ 04/15/24 07:34:33.946
• [0.265 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 04/15/24 07:34:33.961
  Apr 15 07:34:33.961: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 07:34:33.964
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:34:34.008
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:34:34.014
  STEP: Creating secret with name projected-secret-test-0bedf506-c615-461f-a395-a7bbbc4b5622 @ 04/15/24 07:34:34.022
  STEP: Creating a pod to test consume secrets @ 04/15/24 07:34:34.035
  STEP: Saw pod success @ 04/15/24 07:34:38.114
  Apr 15 07:34:38.126: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-projected-secrets-ec5b566e-8775-4a57-a9a0-3c4651bca961 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 07:34:38.148
  Apr 15 07:34:38.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-78" for this suite. @ 04/15/24 07:34:38.194
• [4.255 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 04/15/24 07:34:38.217
  Apr 15 07:34:38.217: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename configmap @ 04/15/24 07:34:38.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:34:38.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:34:38.266
  STEP: Creating configMap with name configmap-test-volume-c19cfb39-d376-436f-85fe-f7144936b7c9 @ 04/15/24 07:34:38.274
  STEP: Creating a pod to test consume configMaps @ 04/15/24 07:34:38.289
  STEP: Saw pod success @ 04/15/24 07:34:42.371
  Apr 15 07:34:42.378: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-configmaps-dca93c61-8307-49b9-999b-09430a6deeef container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 07:34:42.396
  Apr 15 07:34:42.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4553" for this suite. @ 04/15/24 07:34:42.435
• [4.235 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 04/15/24 07:34:42.464
  Apr 15 07:34:42.464: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename containers @ 04/15/24 07:34:42.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:34:42.505
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:34:42.512
  STEP: Creating a pod to test override all @ 04/15/24 07:34:42.519
  STEP: Saw pod success @ 04/15/24 07:34:46.573
  Apr 15 07:34:46.583: INFO: Trying to get logs from node zaigh3ewotoh-3 pod client-containers-691e7814-9e83-45ec-ad87-e26802e31ded container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 07:34:46.598
  Apr 15 07:34:46.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-7068" for this suite. @ 04/15/24 07:34:46.653
• [4.214 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 04/15/24 07:34:46.693
  Apr 15 07:34:46.693: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename disruption @ 04/15/24 07:34:46.697
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:34:46.747
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:34:46.755
  STEP: creating the pdb @ 04/15/24 07:34:46.764
  STEP: Waiting for the pdb to be processed @ 04/15/24 07:34:46.776
  STEP: updating the pdb @ 04/15/24 07:34:48.8
  STEP: Waiting for the pdb to be processed @ 04/15/24 07:34:48.833
  STEP: patching the pdb @ 04/15/24 07:34:50.852
  STEP: Waiting for the pdb to be processed @ 04/15/24 07:34:50.876
  STEP: Waiting for the pdb to be deleted @ 04/15/24 07:34:52.909
  Apr 15 07:34:52.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-3033" for this suite. @ 04/15/24 07:34:52.944
• [6.267 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 04/15/24 07:34:52.962
  Apr 15 07:34:52.963: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename services @ 04/15/24 07:34:52.965
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:34:53.056
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:34:53.061
  STEP: creating service in namespace services-8026 @ 04/15/24 07:34:53.067
  STEP: creating service affinity-clusterip-transition in namespace services-8026 @ 04/15/24 07:34:53.068
  STEP: creating replication controller affinity-clusterip-transition in namespace services-8026 @ 04/15/24 07:34:53.116
  I0415 07:34:53.146367      13 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-8026, replica count: 3
  I0415 07:34:56.197842      13 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0415 07:34:59.198584      13 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 15 07:34:59.219: INFO: Creating new exec pod
  Apr 15 07:35:02.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-8026 exec execpod-affinityddntm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Apr 15 07:35:02.690: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Apr 15 07:35:02.690: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 07:35:02.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-8026 exec execpod-affinityddntm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.195 80'
  Apr 15 07:35:03.089: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.40.195 80\nConnection to 10.233.40.195 80 port [tcp/http] succeeded!\n"
  Apr 15 07:35:03.089: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 07:35:03.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-8026 exec execpod-affinityddntm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.40.195:80/ ; done'
  Apr 15 07:35:03.758: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n"
  Apr 15 07:35:03.758: INFO: stdout: "\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-k6f5f\naffinity-clusterip-transition-wn6jd\naffinity-clusterip-transition-wn6jd\naffinity-clusterip-transition-wn6jd\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-wn6jd\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-wn6jd\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-k6f5f\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-wn6jd"
  Apr 15 07:35:03.758: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:03.758: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:03.758: INFO: Received response from host: affinity-clusterip-transition-k6f5f
  Apr 15 07:35:03.758: INFO: Received response from host: affinity-clusterip-transition-wn6jd
  Apr 15 07:35:03.758: INFO: Received response from host: affinity-clusterip-transition-wn6jd
  Apr 15 07:35:03.758: INFO: Received response from host: affinity-clusterip-transition-wn6jd
  Apr 15 07:35:03.758: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:03.758: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:03.758: INFO: Received response from host: affinity-clusterip-transition-wn6jd
  Apr 15 07:35:03.758: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:03.758: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:03.758: INFO: Received response from host: affinity-clusterip-transition-wn6jd
  Apr 15 07:35:03.758: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:03.758: INFO: Received response from host: affinity-clusterip-transition-k6f5f
  Apr 15 07:35:03.758: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:03.758: INFO: Received response from host: affinity-clusterip-transition-wn6jd
  Apr 15 07:35:03.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-8026 exec execpod-affinityddntm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.40.195:80/ ; done'
  Apr 15 07:35:04.383: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.40.195:80/\n"
  Apr 15 07:35:04.383: INFO: stdout: "\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-2xcdg\naffinity-clusterip-transition-2xcdg"
  Apr 15 07:35:04.383: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:04.383: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:04.383: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:04.383: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:04.383: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:04.383: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:04.383: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:04.383: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:04.383: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:04.383: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:04.383: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:04.383: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:04.383: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:04.383: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:04.383: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:04.383: INFO: Received response from host: affinity-clusterip-transition-2xcdg
  Apr 15 07:35:04.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 07:35:04.397: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8026, will wait for the garbage collector to delete the pods @ 04/15/24 07:35:04.445
  Apr 15 07:35:04.581: INFO: Deleting ReplicationController affinity-clusterip-transition took: 67.011562ms
  Apr 15 07:35:04.682: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.956733ms
  STEP: Destroying namespace "services-8026" for this suite. @ 04/15/24 07:35:06.741
• [13.830 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 04/15/24 07:35:06.799
  Apr 15 07:35:06.799: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename subpath @ 04/15/24 07:35:06.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:35:06.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:35:06.859
  STEP: Setting up data @ 04/15/24 07:35:06.865
  STEP: Creating pod pod-subpath-test-secret-8kqh @ 04/15/24 07:35:06.885
  STEP: Creating a pod to test atomic-volume-subpath @ 04/15/24 07:35:06.886
  STEP: Saw pod success @ 04/15/24 07:35:31.071
  Apr 15 07:35:31.079: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-subpath-test-secret-8kqh container test-container-subpath-secret-8kqh: <nil>
  STEP: delete the pod @ 04/15/24 07:35:31.099
  STEP: Deleting pod pod-subpath-test-secret-8kqh @ 04/15/24 07:35:31.144
  Apr 15 07:35:31.145: INFO: Deleting pod "pod-subpath-test-secret-8kqh" in namespace "subpath-2000"
  Apr 15 07:35:31.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-2000" for this suite. @ 04/15/24 07:35:31.165
• [24.384 seconds]
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 04/15/24 07:35:31.184
  Apr 15 07:35:31.184: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/15/24 07:35:31.19
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:35:31.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:35:31.244
  Apr 15 07:35:31.258: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:35:31.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9015" for this suite. @ 04/15/24 07:35:31.89
• [0.724 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 04/15/24 07:35:31.908
  Apr 15 07:35:31.908: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 07:35:31.911
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:35:31.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:35:31.986
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 07:35:31.993
  STEP: Saw pod success @ 04/15/24 07:35:36.062
  Apr 15 07:35:36.069: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downwardapi-volume-18c01168-dee9-429e-b622-5920f8bbe361 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 07:35:36.085
  Apr 15 07:35:36.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6230" for this suite. @ 04/15/24 07:35:36.138
• [4.245 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 04/15/24 07:35:36.155
  Apr 15 07:35:36.155: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename taint-single-pod @ 04/15/24 07:35:36.157
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:35:36.196
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:35:36.203
  Apr 15 07:35:36.212: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 15 07:36:36.262: INFO: Waiting for terminating namespaces to be deleted...
  Apr 15 07:36:36.273: INFO: Starting informer...
  STEP: Starting pod... @ 04/15/24 07:36:36.273
  Apr 15 07:36:36.509: INFO: Pod is running on zaigh3ewotoh-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 04/15/24 07:36:36.51
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/15/24 07:36:36.547
  STEP: Waiting short time to make sure Pod is queued for deletion @ 04/15/24 07:36:36.603
  Apr 15 07:36:36.604: INFO: Pod wasn't evicted. Proceeding
  Apr 15 07:36:36.604: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/15/24 07:36:36.649
  STEP: Waiting some time to make sure that toleration time passed. @ 04/15/24 07:36:36.669
  Apr 15 07:37:51.670: INFO: Pod wasn't evicted. Test successful
  Apr 15 07:37:51.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-1275" for this suite. @ 04/15/24 07:37:51.688
• [135.562 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 04/15/24 07:37:51.72
  Apr 15 07:37:51.720: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename deployment @ 04/15/24 07:37:51.723
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:37:51.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:37:51.805
  STEP: creating a Deployment @ 04/15/24 07:37:51.828
  STEP: waiting for Deployment to be created @ 04/15/24 07:37:51.841
  STEP: waiting for all Replicas to be Ready @ 04/15/24 07:37:51.846
  Apr 15 07:37:51.850: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 15 07:37:51.852: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 15 07:37:51.885: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 15 07:37:51.886: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 15 07:37:51.931: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 15 07:37:51.931: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 15 07:37:52.101: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 15 07:37:52.101: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 15 07:37:53.265: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Apr 15 07:37:53.266: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Apr 15 07:37:53.418: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 04/15/24 07:37:53.418
  W0415 07:37:53.443880      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 15 07:37:53.448: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 04/15/24 07:37:53.448
  Apr 15 07:37:53.452: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 0
  Apr 15 07:37:53.452: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 0
  Apr 15 07:37:53.452: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 0
  Apr 15 07:37:53.452: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 0
  Apr 15 07:37:53.453: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 0
  Apr 15 07:37:53.453: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 0
  Apr 15 07:37:53.453: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 0
  Apr 15 07:37:53.453: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 0
  Apr 15 07:37:53.453: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 1
  Apr 15 07:37:53.453: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 1
  Apr 15 07:37:53.453: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 2
  Apr 15 07:37:53.453: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 2
  Apr 15 07:37:53.454: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 2
  Apr 15 07:37:53.454: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 2
  Apr 15 07:37:53.495: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 2
  Apr 15 07:37:53.495: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 2
  Apr 15 07:37:53.540: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 2
  Apr 15 07:37:53.540: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 2
  Apr 15 07:37:53.616: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 1
  Apr 15 07:37:53.616: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 1
  Apr 15 07:37:53.656: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 1
  Apr 15 07:37:53.657: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 1
  Apr 15 07:37:55.330: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 2
  Apr 15 07:37:55.330: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 2
  Apr 15 07:37:55.451: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 1
  STEP: listing Deployments @ 04/15/24 07:37:55.452
  Apr 15 07:37:55.460: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 04/15/24 07:37:55.461
  Apr 15 07:37:55.499: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 04/15/24 07:37:55.499
  Apr 15 07:37:55.529: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 15 07:37:55.540: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 15 07:37:55.630: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 15 07:37:55.777: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 15 07:37:57.198: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 15 07:37:57.456: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 15 07:37:57.665: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 15 07:37:57.723: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 15 07:38:00.542: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 04/15/24 07:38:00.615
  STEP: fetching the DeploymentStatus @ 04/15/24 07:38:00.631
  Apr 15 07:38:00.654: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 1
  Apr 15 07:38:00.655: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 1
  Apr 15 07:38:00.655: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 1
  Apr 15 07:38:00.655: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 1
  Apr 15 07:38:00.655: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 2
  Apr 15 07:38:00.655: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 3
  Apr 15 07:38:00.655: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 2
  Apr 15 07:38:00.656: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 2
  Apr 15 07:38:00.656: INFO: observed Deployment test-deployment in namespace deployment-3898 with ReadyReplicas 3
  STEP: deleting the Deployment @ 04/15/24 07:38:00.657
  Apr 15 07:38:00.697: INFO: observed event type MODIFIED
  Apr 15 07:38:00.698: INFO: observed event type MODIFIED
  Apr 15 07:38:00.698: INFO: observed event type MODIFIED
  Apr 15 07:38:00.700: INFO: observed event type MODIFIED
  Apr 15 07:38:00.700: INFO: observed event type MODIFIED
  Apr 15 07:38:00.701: INFO: observed event type MODIFIED
  Apr 15 07:38:00.701: INFO: observed event type MODIFIED
  Apr 15 07:38:00.702: INFO: observed event type MODIFIED
  Apr 15 07:38:00.702: INFO: observed event type MODIFIED
  Apr 15 07:38:00.703: INFO: observed event type MODIFIED
  Apr 15 07:38:00.713: INFO: Log out all the ReplicaSets if there is no deployment created
  Apr 15 07:38:00.723: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-3898  d8a8b065-25da-4b69-b0b2-1eb8b1840a5c 168058 4 2024-04-15 07:37:53 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 24a7ed23-c893-4bfa-820c-0277119107e2 0xc005cd34b7 0xc005cd34b8}] [] [{kube-controller-manager Update apps/v1 2024-04-15 07:38:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24a7ed23-c893-4bfa-820c-0277119107e2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 07:38:00 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cd3540 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Apr 15 07:38:00.733: INFO: pod: "test-deployment-5b5dcbcd95-svp2d":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-svp2d test-deployment-5b5dcbcd95- deployment-3898  85c04cb4-c6dd-4dfa-bbca-6925cac30842 168052 0 2024-04-15 07:37:55 +0000 UTC 2024-04-15 07:38:01 +0000 UTC 0xc005b01678 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 d8a8b065-25da-4b69-b0b2-1eb8b1840a5c 0xc005b016a7 0xc005b016a8}] [] [{kube-controller-manager Update v1 2024-04-15 07:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8a8b065-25da-4b69-b0b2-1eb8b1840a5c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:37:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bwtrs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bwtrs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:37:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:37:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:37:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:37:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.74,PodIP:10.233.64.209,StartTime:2024-04-15 07:37:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 07:37:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://df7dc61686db69d313ebb9a3e416942e19e16d132e0a24764eef489ac5d6ac9f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.209,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 15 07:38:00.736: INFO: ReplicaSet "test-deployment-6c79cbbc7f":
  &ReplicaSet{ObjectMeta:{test-deployment-6c79cbbc7f  deployment-3898  518904c9-65fd-4bbe-81f9-054283fb967e 167961 3 2024-04-15 07:37:51 +0000 UTC <nil> <nil> map[pod-template-hash:6c79cbbc7f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 24a7ed23-c893-4bfa-820c-0277119107e2 0xc005cd35a7 0xc005cd35a8}] [] [{kube-controller-manager Update apps/v1 2024-04-15 07:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24a7ed23-c893-4bfa-820c-0277119107e2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 07:37:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6c79cbbc7f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6c79cbbc7f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.47 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cd3630 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Apr 15 07:38:00.754: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-3898  b696237f-e567-4f10-9566-25613615dcba 168049 2 2024-04-15 07:37:55 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 24a7ed23-c893-4bfa-820c-0277119107e2 0xc005cd3697 0xc005cd3698}] [] [{kube-controller-manager Update apps/v1 2024-04-15 07:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24a7ed23-c893-4bfa-820c-0277119107e2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 07:38:00 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005cd3720 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Apr 15 07:38:00.766: INFO: pod: "test-deployment-6fc78d85c6-2dpfs":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-2dpfs test-deployment-6fc78d85c6- deployment-3898  063d83ad-09ac-40ef-906a-8063bd712ce5 168048 0 2024-04-15 07:37:57 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 b696237f-e567-4f10-9566-25613615dcba 0xc005cd39d7 0xc005cd39d8}] [] [{kube-controller-manager Update v1 2024-04-15 07:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b696237f-e567-4f10-9566-25613615dcba\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:38:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dpg57,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dpg57,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:37:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:38:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:38:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:37:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.27,PodIP:10.233.65.178,StartTime:2024-04-15 07:37:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 07:37:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://ab9025fe65aee10aec1ac3616d0feeff34eb6a387a3bfdac2ce8c7dde9edba6f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.178,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 15 07:38:00.766: INFO: pod: "test-deployment-6fc78d85c6-hpxvf":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-hpxvf test-deployment-6fc78d85c6- deployment-3898  4c922450-57a6-4914-a772-2d60679ececd 168009 0 2024-04-15 07:37:55 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 b696237f-e567-4f10-9566-25613615dcba 0xc005cd3bc7 0xc005cd3bc8}] [] [{kube-controller-manager Update v1 2024-04-15 07:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b696237f-e567-4f10-9566-25613615dcba\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:37:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.238\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bh6pv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bh6pv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:37:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:37:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:37:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:37:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.131,PodIP:10.233.66.238,StartTime:2024-04-15 07:37:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 07:37:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://2cd5c9696cdf23793ac683f2911db02144263901f41bb64225d05c0026256004,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.238,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 15 07:38:00.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3898" for this suite. @ 04/15/24 07:38:00.795
• [9.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 04/15/24 07:38:00.822
  Apr 15 07:38:00.822: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename taint-multiple-pods @ 04/15/24 07:38:00.824
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:38:00.94
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:38:00.947
  Apr 15 07:38:00.960: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 15 07:39:01.044: INFO: Waiting for terminating namespaces to be deleted...
  Apr 15 07:39:01.060: INFO: Starting informer...
  STEP: Starting pods... @ 04/15/24 07:39:01.061
  Apr 15 07:39:01.323: INFO: Pod1 is running on zaigh3ewotoh-3. Tainting Node
  Apr 15 07:39:03.587: INFO: Pod2 is running on zaigh3ewotoh-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 04/15/24 07:39:03.587
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/15/24 07:39:03.618
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 04/15/24 07:39:03.634
  Apr 15 07:39:09.729: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  Apr 15 07:39:29.832: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Apr 15 07:39:29.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/15/24 07:39:29.881
  STEP: Destroying namespace "taint-multiple-pods-6804" for this suite. @ 04/15/24 07:39:29.894
• [89.088 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 04/15/24 07:39:29.919
  Apr 15 07:39:29.920: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 07:39:29.925
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:39:30.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:39:30.015
  STEP: Creating projection with secret that has name projected-secret-test-4ca41a98-eb98-4826-9487-a0664d50b785 @ 04/15/24 07:39:30.024
  STEP: Creating a pod to test consume secrets @ 04/15/24 07:39:30.038
  STEP: Saw pod success @ 04/15/24 07:39:34.098
  Apr 15 07:39:34.107: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-projected-secrets-1db47b5d-1a5e-422e-b2e9-092f84b12d08 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 07:39:34.156
  Apr 15 07:39:34.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3438" for this suite. @ 04/15/24 07:39:34.22
• [4.324 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 04/15/24 07:39:34.249
  Apr 15 07:39:34.249: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 07:39:34.253
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:39:34.293
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:39:34.301
  STEP: Creating the pod @ 04/15/24 07:39:34.311
  Apr 15 07:39:36.928: INFO: Successfully updated pod "labelsupdate01266eef-1c78-45d5-b762-e4c78d401de7"
  Apr 15 07:39:38.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3877" for this suite. @ 04/15/24 07:39:38.981
• [4.750 seconds]
------------------------------
SSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 04/15/24 07:39:39
  Apr 15 07:39:39.001: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename subjectreview @ 04/15/24 07:39:39.004
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:39:39.046
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:39:39.059
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-8833" @ 04/15/24 07:39:39.067
  Apr 15 07:39:39.081: INFO: saUsername: "system:serviceaccount:subjectreview-8833:e2e"
  Apr 15 07:39:39.081: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-8833"}
  Apr 15 07:39:39.081: INFO: saUID: "747f986d-82f1-4215-8fac-82609ad06e00"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-8833:e2e" @ 04/15/24 07:39:39.081
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-8833:e2e" @ 04/15/24 07:39:39.082
  Apr 15 07:39:39.093: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-8833:e2e" api 'list' configmaps in "subjectreview-8833" namespace @ 04/15/24 07:39:39.093
  Apr 15 07:39:39.097: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-8833:e2e" @ 04/15/24 07:39:39.098
  Apr 15 07:39:39.106: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Apr 15 07:39:39.106: INFO: LocalSubjectAccessReview has been verified
  Apr 15 07:39:39.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-8833" for this suite. @ 04/15/24 07:39:39.119
• [0.135 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 04/15/24 07:39:39.139
  Apr 15 07:39:39.139: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 07:39:39.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:39:39.191
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:39:39.202
  STEP: Creating a ResourceQuota @ 04/15/24 07:39:39.211
  STEP: Getting a ResourceQuota @ 04/15/24 07:39:39.23
  STEP: Listing all ResourceQuotas with LabelSelector @ 04/15/24 07:39:39.245
  STEP: Patching the ResourceQuota @ 04/15/24 07:39:39.256
  STEP: Deleting a Collection of ResourceQuotas @ 04/15/24 07:39:39.27
  STEP: Verifying the deleted ResourceQuota @ 04/15/24 07:39:39.3
  Apr 15 07:39:39.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3276" for this suite. @ 04/15/24 07:39:39.327
• [0.217 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 04/15/24 07:39:39.374
  Apr 15 07:39:39.374: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename pod-network-test @ 04/15/24 07:39:39.379
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:39:39.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:39:39.45
  STEP: Performing setup for networking test in namespace pod-network-test-5292 @ 04/15/24 07:39:39.461
  STEP: creating a selector @ 04/15/24 07:39:39.461
  STEP: Creating the service pods in kubernetes @ 04/15/24 07:39:39.461
  Apr 15 07:39:39.461: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 04/15/24 07:40:01.817
  Apr 15 07:40:03.862: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 15 07:40:03.862: INFO: Breadth first check of 10.233.64.210 on host 192.168.121.74...
  Apr 15 07:40:03.870: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.244:9080/dial?request=hostname&protocol=udp&host=10.233.64.210&port=8081&tries=1'] Namespace:pod-network-test-5292 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 07:40:03.871: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:40:03.875: INFO: ExecWithOptions: Clientset creation
  Apr 15 07:40:03.875: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5292/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.244%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.210%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 15 07:40:04.107: INFO: Waiting for responses: map[]
  Apr 15 07:40:04.108: INFO: reached 10.233.64.210 after 0/1 tries
  Apr 15 07:40:04.108: INFO: Breadth first check of 10.233.65.179 on host 192.168.121.27...
  Apr 15 07:40:04.114: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.244:9080/dial?request=hostname&protocol=udp&host=10.233.65.179&port=8081&tries=1'] Namespace:pod-network-test-5292 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 07:40:04.114: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:40:04.116: INFO: ExecWithOptions: Clientset creation
  Apr 15 07:40:04.117: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5292/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.244%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.65.179%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 15 07:40:04.227: INFO: Waiting for responses: map[]
  Apr 15 07:40:04.227: INFO: reached 10.233.65.179 after 0/1 tries
  Apr 15 07:40:04.227: INFO: Breadth first check of 10.233.66.243 on host 192.168.121.131...
  Apr 15 07:40:04.234: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.244:9080/dial?request=hostname&protocol=udp&host=10.233.66.243&port=8081&tries=1'] Namespace:pod-network-test-5292 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 07:40:04.234: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:40:04.236: INFO: ExecWithOptions: Clientset creation
  Apr 15 07:40:04.236: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5292/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.244%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.243%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 15 07:40:04.336: INFO: Waiting for responses: map[]
  Apr 15 07:40:04.336: INFO: reached 10.233.66.243 after 0/1 tries
  Apr 15 07:40:04.336: INFO: Going to retry 0 out of 3 pods....
  Apr 15 07:40:04.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-5292" for this suite. @ 04/15/24 07:40:04.347
• [24.987 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 04/15/24 07:40:04.363
  Apr 15 07:40:04.363: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename pods @ 04/15/24 07:40:04.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:40:04.398
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:40:04.414
  STEP: creating the pod @ 04/15/24 07:40:04.423
  STEP: submitting the pod to kubernetes @ 04/15/24 07:40:04.424
  STEP: verifying the pod is in kubernetes @ 04/15/24 07:40:06.47
  STEP: updating the pod @ 04/15/24 07:40:06.48
  Apr 15 07:40:07.010: INFO: Successfully updated pod "pod-update-dbea70b4-2a2a-4a1d-968f-c1e25fd4c000"
  STEP: verifying the updated pod is in kubernetes @ 04/15/24 07:40:07.017
  Apr 15 07:40:07.029: INFO: Pod update OK
  Apr 15 07:40:07.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4363" for this suite. @ 04/15/24 07:40:07.041
• [2.701 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 04/15/24 07:40:07.069
  Apr 15 07:40:07.069: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 07:40:07.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:40:07.116
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:40:07.123
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 04/15/24 07:40:07.13
  STEP: Saw pod success @ 04/15/24 07:40:09.178
  Apr 15 07:40:09.186: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-d0a761d4-38f8-4fe0-b121-e0dc865c143d container test-container: <nil>
  STEP: delete the pod @ 04/15/24 07:40:09.203
  Apr 15 07:40:09.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8941" for this suite. @ 04/15/24 07:40:09.249
• [2.210 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 04/15/24 07:40:09.281
  Apr 15 07:40:09.281: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 07:40:09.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:40:09.317
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:40:09.324
  STEP: creating Agnhost RC @ 04/15/24 07:40:09.333
  Apr 15 07:40:09.333: INFO: namespace kubectl-3433
  Apr 15 07:40:09.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3433 create -f -'
  Apr 15 07:40:10.428: INFO: stderr: ""
  Apr 15 07:40:10.429: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/15/24 07:40:10.429
  Apr 15 07:40:11.436: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 07:40:11.436: INFO: Found 0 / 1
  Apr 15 07:40:12.437: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 07:40:12.437: INFO: Found 1 / 1
  Apr 15 07:40:12.438: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Apr 15 07:40:12.446: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 07:40:12.446: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 15 07:40:12.446: INFO: wait on agnhost-primary startup in kubectl-3433 
  Apr 15 07:40:12.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3433 logs agnhost-primary-jk4fz agnhost-primary'
  Apr 15 07:40:12.695: INFO: stderr: ""
  Apr 15 07:40:12.695: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 04/15/24 07:40:12.695
  Apr 15 07:40:12.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3433 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Apr 15 07:40:12.957: INFO: stderr: ""
  Apr 15 07:40:12.957: INFO: stdout: "service/rm2 exposed\n"
  Apr 15 07:40:12.968: INFO: Service rm2 in namespace kubectl-3433 found.
  STEP: exposing service @ 04/15/24 07:40:14.987
  Apr 15 07:40:14.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3433 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Apr 15 07:40:15.161: INFO: stderr: ""
  Apr 15 07:40:15.161: INFO: stdout: "service/rm3 exposed\n"
  Apr 15 07:40:15.173: INFO: Service rm3 in namespace kubectl-3433 found.
  Apr 15 07:40:17.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3433" for this suite. @ 04/15/24 07:40:17.202
• [7.941 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 04/15/24 07:40:17.231
  Apr 15 07:40:17.231: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 07:40:17.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:40:17.283
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:40:17.292
  Apr 15 07:41:17.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-422" for this suite. @ 04/15/24 07:41:17.345
• [60.142 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 04/15/24 07:41:17.376
  Apr 15 07:41:17.376: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename controllerrevisions @ 04/15/24 07:41:17.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:41:17.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:41:17.449
  STEP: Creating DaemonSet "e2e-zj6sn-daemon-set" @ 04/15/24 07:41:17.541
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/15/24 07:41:17.563
  Apr 15 07:41:17.589: INFO: Number of nodes with available pods controlled by daemonset e2e-zj6sn-daemon-set: 0
  Apr 15 07:41:17.589: INFO: Node zaigh3ewotoh-1 is running 0 daemon pod, expected 1
  Apr 15 07:41:18.629: INFO: Number of nodes with available pods controlled by daemonset e2e-zj6sn-daemon-set: 0
  Apr 15 07:41:18.630: INFO: Node zaigh3ewotoh-1 is running 0 daemon pod, expected 1
  Apr 15 07:41:19.615: INFO: Number of nodes with available pods controlled by daemonset e2e-zj6sn-daemon-set: 3
  Apr 15 07:41:19.616: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-zj6sn-daemon-set
  STEP: Confirm DaemonSet "e2e-zj6sn-daemon-set" successfully created with "daemonset-name=e2e-zj6sn-daemon-set" label @ 04/15/24 07:41:19.624
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-zj6sn-daemon-set" @ 04/15/24 07:41:19.646
  Apr 15 07:41:19.667: INFO: Located ControllerRevision: "e2e-zj6sn-daemon-set-849d79c87d"
  STEP: Patching ControllerRevision "e2e-zj6sn-daemon-set-849d79c87d" @ 04/15/24 07:41:19.676
  Apr 15 07:41:19.701: INFO: e2e-zj6sn-daemon-set-849d79c87d has been patched
  STEP: Create a new ControllerRevision @ 04/15/24 07:41:19.702
  Apr 15 07:41:19.717: INFO: Created ControllerRevision: e2e-zj6sn-daemon-set-6c7cf9cd88
  STEP: Confirm that there are two ControllerRevisions @ 04/15/24 07:41:19.717
  Apr 15 07:41:19.718: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 15 07:41:19.727: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-zj6sn-daemon-set-849d79c87d" @ 04/15/24 07:41:19.727
  STEP: Confirm that there is only one ControllerRevision @ 04/15/24 07:41:19.743
  Apr 15 07:41:19.743: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 15 07:41:19.754: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-zj6sn-daemon-set-6c7cf9cd88" @ 04/15/24 07:41:19.762
  Apr 15 07:41:19.795: INFO: e2e-zj6sn-daemon-set-6c7cf9cd88 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 04/15/24 07:41:19.795
  W0415 07:41:19.815781      13 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 04/15/24 07:41:19.816
  Apr 15 07:41:19.816: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 15 07:41:20.826: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 15 07:41:20.834: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-zj6sn-daemon-set-6c7cf9cd88=updated" @ 04/15/24 07:41:20.835
  STEP: Confirm that there is only one ControllerRevision @ 04/15/24 07:41:20.856
  Apr 15 07:41:20.856: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 15 07:41:20.864: INFO: Found 1 ControllerRevisions
  Apr 15 07:41:20.887: INFO: ControllerRevision "e2e-zj6sn-daemon-set-56db54c94f" has revision 3
  STEP: Deleting DaemonSet "e2e-zj6sn-daemon-set" @ 04/15/24 07:41:20.896
  STEP: deleting DaemonSet.extensions e2e-zj6sn-daemon-set in namespace controllerrevisions-7478, will wait for the garbage collector to delete the pods @ 04/15/24 07:41:20.896
  Apr 15 07:41:20.972: INFO: Deleting DaemonSet.extensions e2e-zj6sn-daemon-set took: 16.255637ms
  Apr 15 07:41:21.274: INFO: Terminating DaemonSet.extensions e2e-zj6sn-daemon-set pods took: 301.677902ms
  Apr 15 07:41:23.284: INFO: Number of nodes with available pods controlled by daemonset e2e-zj6sn-daemon-set: 0
  Apr 15 07:41:23.284: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-zj6sn-daemon-set
  Apr 15 07:41:23.292: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"168933"},"items":null}

  Apr 15 07:41:23.300: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"168933"},"items":null}

  Apr 15 07:41:23.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-7478" for this suite. @ 04/15/24 07:41:23.342
• [5.986 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 04/15/24 07:41:23.366
  Apr 15 07:41:23.366: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename configmap @ 04/15/24 07:41:23.37
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:41:23.426
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:41:23.431
  STEP: Creating configMap with name configmap-test-volume-452e59de-c6b9-446d-832a-968746108aee @ 04/15/24 07:41:23.438
  STEP: Creating a pod to test consume configMaps @ 04/15/24 07:41:23.453
  STEP: Saw pod success @ 04/15/24 07:41:27.535
  Apr 15 07:41:27.542: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-configmaps-bb2770cf-05a0-487c-96c6-9eb509247734 container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 07:41:27.562
  Apr 15 07:41:27.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5058" for this suite. @ 04/15/24 07:41:27.623
• [4.271 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 04/15/24 07:41:27.64
  Apr 15 07:41:27.640: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename job @ 04/15/24 07:41:27.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:41:27.753
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:41:27.765
  STEP: Creating Indexed job @ 04/15/24 07:41:27.772
  STEP: Ensuring job reaches completions @ 04/15/24 07:41:27.788
  STEP: Ensuring pods with index for job exist @ 04/15/24 07:41:37.797
  Apr 15 07:41:37.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-992" for this suite. @ 04/15/24 07:41:37.821
• [10.200 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 04/15/24 07:41:37.845
  Apr 15 07:41:37.845: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename svcaccounts @ 04/15/24 07:41:37.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:41:37.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:41:37.891
  STEP: Creating a pod to test service account token:  @ 04/15/24 07:41:37.898
  STEP: Saw pod success @ 04/15/24 07:41:41.959
  Apr 15 07:41:41.972: INFO: Trying to get logs from node zaigh3ewotoh-3 pod test-pod-419c2a69-0ef6-4910-9c60-a8323500e229 container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 07:41:41.993
  Apr 15 07:41:42.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-517" for this suite. @ 04/15/24 07:41:42.037
• [4.207 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 04/15/24 07:41:42.053
  Apr 15 07:41:42.054: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename webhook @ 04/15/24 07:41:42.055
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:41:42.105
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:41:42.114
  STEP: Setting up server cert @ 04/15/24 07:41:42.17
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 07:41:42.926
  STEP: Deploying the webhook pod @ 04/15/24 07:41:42.943
  STEP: Wait for the deployment to be ready @ 04/15/24 07:41:42.981
  Apr 15 07:41:43.007: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/15/24 07:41:45.035
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 07:41:45.061
  Apr 15 07:41:46.062: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 04/15/24 07:41:46.078
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 04/15/24 07:41:46.144
  STEP: Creating a configMap that should not be mutated @ 04/15/24 07:41:46.163
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 04/15/24 07:41:46.188
  STEP: Creating a configMap that should be mutated @ 04/15/24 07:41:46.206
  Apr 15 07:41:46.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1440" for this suite. @ 04/15/24 07:41:46.38
  STEP: Destroying namespace "webhook-markers-66" for this suite. @ 04/15/24 07:41:46.394
• [4.363 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 04/15/24 07:41:46.441
  Apr 15 07:41:46.441: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename runtimeclass @ 04/15/24 07:41:46.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:41:46.483
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:41:46.49
  Apr 15 07:41:48.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7801" for this suite. @ 04/15/24 07:41:48.594
• [2.169 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 04/15/24 07:41:48.614
  Apr 15 07:41:48.614: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename services @ 04/15/24 07:41:48.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:41:48.66
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:41:48.667
  STEP: creating service multi-endpoint-test in namespace services-8147 @ 04/15/24 07:41:48.675
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8147 to expose endpoints map[] @ 04/15/24 07:41:48.7
  Apr 15 07:41:48.779: INFO: successfully validated that service multi-endpoint-test in namespace services-8147 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-8147 @ 04/15/24 07:41:48.779
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8147 to expose endpoints map[pod1:[100]] @ 04/15/24 07:41:50.874
  Apr 15 07:41:50.906: INFO: successfully validated that service multi-endpoint-test in namespace services-8147 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-8147 @ 04/15/24 07:41:50.908
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8147 to expose endpoints map[pod1:[100] pod2:[101]] @ 04/15/24 07:41:52.969
  Apr 15 07:41:53.047: INFO: successfully validated that service multi-endpoint-test in namespace services-8147 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 04/15/24 07:41:53.048
  Apr 15 07:41:53.048: INFO: Creating new exec pod
  Apr 15 07:41:56.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-8147 exec execpodp278b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Apr 15 07:41:56.509: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Apr 15 07:41:56.509: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 07:41:56.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-8147 exec execpodp278b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.30.161 80'
  Apr 15 07:41:56.834: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.30.161 80\nConnection to 10.233.30.161 80 port [tcp/http] succeeded!\n"
  Apr 15 07:41:56.835: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 07:41:56.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-8147 exec execpodp278b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Apr 15 07:41:57.220: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Apr 15 07:41:57.220: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 07:41:57.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-8147 exec execpodp278b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.30.161 81'
  Apr 15 07:41:57.553: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.30.161 81\nConnection to 10.233.30.161 81 port [tcp/*] succeeded!\n"
  Apr 15 07:41:57.553: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-8147 @ 04/15/24 07:41:57.553
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8147 to expose endpoints map[pod2:[101]] @ 04/15/24 07:41:57.588
  Apr 15 07:41:58.629: INFO: successfully validated that service multi-endpoint-test in namespace services-8147 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-8147 @ 04/15/24 07:41:58.629
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8147 to expose endpoints map[] @ 04/15/24 07:41:58.655
  Apr 15 07:41:59.722: INFO: successfully validated that service multi-endpoint-test in namespace services-8147 exposes endpoints map[]
  Apr 15 07:41:59.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8147" for this suite. @ 04/15/24 07:41:59.828
• [11.230 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 04/15/24 07:41:59.845
  Apr 15 07:41:59.845: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename var-expansion @ 04/15/24 07:41:59.85
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:41:59.899
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:41:59.909
  Apr 15 07:42:01.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 07:42:01.978: INFO: Deleting pod "var-expansion-182f0c2a-923d-4b57-8c27-e6a8cdfe568d" in namespace "var-expansion-3458"
  Apr 15 07:42:01.997: INFO: Wait up to 5m0s for pod "var-expansion-182f0c2a-923d-4b57-8c27-e6a8cdfe568d" to be fully deleted
  STEP: Destroying namespace "var-expansion-3458" for this suite. @ 04/15/24 07:42:04.016
• [4.189 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 04/15/24 07:42:04.046
  Apr 15 07:42:04.046: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 07:42:04.049
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:42:04.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:42:04.092
  STEP: Creating configMap with name projected-configmap-test-volume-435982c8-6049-42b5-920e-f862bc8328f9 @ 04/15/24 07:42:04.103
  STEP: Creating a pod to test consume configMaps @ 04/15/24 07:42:04.115
  STEP: Saw pod success @ 04/15/24 07:42:08.164
  Apr 15 07:42:08.175: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-projected-configmaps-ec59c3d6-0c07-45e2-9755-3c7638bf605d container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 07:42:08.198
  Apr 15 07:42:08.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9419" for this suite. @ 04/15/24 07:42:08.252
• [4.234 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 04/15/24 07:42:08.292
  Apr 15 07:42:08.293: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename secrets @ 04/15/24 07:42:08.298
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:42:08.338
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:42:08.344
  STEP: Creating secret with name secret-test-316a4863-a3ca-47ed-8ffb-e6dbfd070930 @ 04/15/24 07:42:08.351
  STEP: Creating a pod to test consume secrets @ 04/15/24 07:42:08.366
  STEP: Saw pod success @ 04/15/24 07:42:12.416
  Apr 15 07:42:12.425: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-secrets-45926e4e-bc99-4a6c-af8f-ee7eb743a45e container secret-env-test: <nil>
  STEP: delete the pod @ 04/15/24 07:42:12.445
  Apr 15 07:42:12.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3650" for this suite. @ 04/15/24 07:42:12.496
• [4.221 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 04/15/24 07:42:12.529
  Apr 15 07:42:12.530: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename svcaccounts @ 04/15/24 07:42:12.543
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:42:12.584
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:42:12.592
  STEP: reading a file in the container @ 04/15/24 07:42:14.684
  Apr 15 07:42:14.685: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9261 pod-service-account-de4b8c15-459e-41ec-811e-cbdefa201403 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 04/15/24 07:42:15.052
  Apr 15 07:42:15.052: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9261 pod-service-account-de4b8c15-459e-41ec-811e-cbdefa201403 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 04/15/24 07:42:15.336
  Apr 15 07:42:15.336: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9261 pod-service-account-de4b8c15-459e-41ec-811e-cbdefa201403 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Apr 15 07:42:15.652: INFO: Got root ca configmap in namespace "svcaccounts-9261"
  Apr 15 07:42:15.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9261" for this suite. @ 04/15/24 07:42:15.671
• [3.162 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 04/15/24 07:42:15.698
  Apr 15 07:42:15.698: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename ingressclass @ 04/15/24 07:42:15.7
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:42:15.736
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:42:15.745
  STEP: getting /apis @ 04/15/24 07:42:15.754
  STEP: getting /apis/networking.k8s.io @ 04/15/24 07:42:15.769
  STEP: getting /apis/networking.k8s.iov1 @ 04/15/24 07:42:15.772
  STEP: creating @ 04/15/24 07:42:15.775
  STEP: getting @ 04/15/24 07:42:15.813
  STEP: listing @ 04/15/24 07:42:15.821
  STEP: watching @ 04/15/24 07:42:15.832
  Apr 15 07:42:15.832: INFO: starting watch
  STEP: patching @ 04/15/24 07:42:15.838
  STEP: updating @ 04/15/24 07:42:15.854
  Apr 15 07:42:15.870: INFO: waiting for watch events with expected annotations
  Apr 15 07:42:15.870: INFO: saw patched and updated annotations
  STEP: deleting @ 04/15/24 07:42:15.871
  STEP: deleting a collection @ 04/15/24 07:42:15.914
  Apr 15 07:42:15.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-2521" for this suite. @ 04/15/24 07:42:15.957
• [0.278 seconds]
------------------------------
SS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 04/15/24 07:42:15.978
  Apr 15 07:42:15.979: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename podtemplate @ 04/15/24 07:42:15.982
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:42:16.029
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:42:16.035
  Apr 15 07:42:16.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-9503" for this suite. @ 04/15/24 07:42:16.134
• [0.174 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:622
  STEP: Creating a kubernetes client @ 04/15/24 07:42:16.159
  Apr 15 07:42:16.159: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename field-validation @ 04/15/24 07:42:16.163
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:42:16.198
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:42:16.213
  Apr 15 07:42:16.221: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  W0415 07:42:19.087596      13 warnings.go:70] unknown field "alpha"
  W0415 07:42:19.088596      13 warnings.go:70] unknown field "beta"
  W0415 07:42:19.089249      13 warnings.go:70] unknown field "delta"
  W0415 07:42:19.090147      13 warnings.go:70] unknown field "epsilon"
  W0415 07:42:19.091066      13 warnings.go:70] unknown field "gamma"
  Apr 15 07:42:19.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-754" for this suite. @ 04/15/24 07:42:19.9
• [3.765 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 04/15/24 07:42:19.929
  Apr 15 07:42:19.929: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename webhook @ 04/15/24 07:42:19.933
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:42:19.977
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:42:19.981
  STEP: Setting up server cert @ 04/15/24 07:42:20.041
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 07:42:21.177
  STEP: Deploying the webhook pod @ 04/15/24 07:42:21.195
  STEP: Wait for the deployment to be ready @ 04/15/24 07:42:21.236
  Apr 15 07:42:21.257: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 04/15/24 07:42:23.284
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 07:42:23.308
  Apr 15 07:42:24.309: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 15 07:42:24.317: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-88-crds.webhook.example.com via the AdmissionRegistration API @ 04/15/24 07:42:24.842
  STEP: Creating a custom resource that should be mutated by the webhook @ 04/15/24 07:42:24.908
  Apr 15 07:42:27.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-939" for this suite. @ 04/15/24 07:42:27.758
  STEP: Destroying namespace "webhook-markers-7232" for this suite. @ 04/15/24 07:42:27.78
• [7.871 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 04/15/24 07:42:27.803
  Apr 15 07:42:27.803: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename deployment @ 04/15/24 07:42:27.81
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:42:27.845
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:42:27.852
  Apr 15 07:42:27.861: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Apr 15 07:42:27.888: INFO: Pod name sample-pod: Found 0 pods out of 1
  Apr 15 07:42:32.900: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/15/24 07:42:32.9
  Apr 15 07:42:32.901: INFO: Creating deployment "test-rolling-update-deployment"
  Apr 15 07:42:32.913: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Apr 15 07:42:32.925: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  Apr 15 07:42:34.943: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Apr 15 07:42:34.953: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Apr 15 07:42:34.981: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2054  9f199b09-4e95-4e99-a3e3-842147e1c18e 169665 1 2024-04-15 07:42:32 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2024-04-15 07:42:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 07:42:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.47 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a383b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-04-15 07:42:33 +0000 UTC,LastTransitionTime:2024-04-15 07:42:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67b9f48bb4" has successfully progressed.,LastUpdateTime:2024-04-15 07:42:34 +0000 UTC,LastTransitionTime:2024-04-15 07:42:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 15 07:42:34.990: INFO: New ReplicaSet "test-rolling-update-deployment-67b9f48bb4" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67b9f48bb4  deployment-2054  6ffc6f93-dd7e-40b1-aeb6-f805f283f87d 169655 1 2024-04-15 07:42:32 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67b9f48bb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 9f199b09-4e95-4e99-a3e3-842147e1c18e 0xc0044e0017 0xc0044e0018}] [] [{kube-controller-manager Update apps/v1 2024-04-15 07:42:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f199b09-4e95-4e99-a3e3-842147e1c18e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 07:42:34 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67b9f48bb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67b9f48bb4] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.47 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044e00c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 07:42:34.990: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Apr 15 07:42:34.990: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2054  9e938546-4425-47a1-b0fb-84923e373f96 169664 2 2024-04-15 07:42:27 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 9f199b09-4e95-4e99-a3e3-842147e1c18e 0xc004447ee7 0xc004447ee8}] [] [{e2e.test Update apps/v1 2024-04-15 07:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 07:42:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f199b09-4e95-4e99-a3e3-842147e1c18e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2024-04-15 07:42:34 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004447fa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 07:42:35.000: INFO: Pod "test-rolling-update-deployment-67b9f48bb4-ks267" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-67b9f48bb4-ks267 test-rolling-update-deployment-67b9f48bb4- deployment-2054  2e22e0b5-6311-43f6-98b7-b8a5e811349d 169654 0 2024-04-15 07:42:32 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67b9f48bb4] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-67b9f48bb4 6ffc6f93-dd7e-40b1-aeb6-f805f283f87d 0xc004a38797 0xc004a38798}] [] [{kube-controller-manager Update v1 2024-04-15 07:42:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ffc6f93-dd7e-40b1-aeb6-f805f283f87d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:42:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-btsjz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.47,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-btsjz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:42:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:42:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:42:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:42:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.131,PodIP:10.233.66.12,StartTime:2024-04-15 07:42:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 07:42:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.47,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:c9997bf8d2e223d7d2a0078dcfb11a653e9b16cf09418829ec03e1d57ca9628a,ContainerID:cri-o://311ed0cb0ac704003bdd15b02640730e06b59c2a5677bfdf4bdff4195dc0161b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.12,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:42:35.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2054" for this suite. @ 04/15/24 07:42:35.013
• [7.223 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 04/15/24 07:42:35.045
  Apr 15 07:42:35.046: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename endpointslice @ 04/15/24 07:42:35.049
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:42:35.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:42:35.09
  STEP: referencing a single matching pod @ 04/15/24 07:42:40.279
  STEP: referencing matching pods with named port @ 04/15/24 07:42:45.303
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 04/15/24 07:42:50.318
  STEP: recreating EndpointSlices after they've been deleted @ 04/15/24 07:42:55.344
  Apr 15 07:42:55.420: INFO: EndpointSlice for Service endpointslice-2073/example-named-port not found
  Apr 15 07:43:05.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-2073" for this suite. @ 04/15/24 07:43:05.495
• [30.472 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 04/15/24 07:43:05.519
  Apr 15 07:43:05.519: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename configmap @ 04/15/24 07:43:05.525
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:43:05.59
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:43:05.613
  STEP: Creating configMap with name configmap-test-upd-4adf6378-f297-4cb0-9c27-70a625316299 @ 04/15/24 07:43:05.644
  STEP: Creating the pod @ 04/15/24 07:43:05.664
  STEP: Updating configmap configmap-test-upd-4adf6378-f297-4cb0-9c27-70a625316299 @ 04/15/24 07:43:07.757
  STEP: waiting to observe update in volume @ 04/15/24 07:43:07.769
  Apr 15 07:43:09.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3201" for this suite. @ 04/15/24 07:43:09.822
• [4.327 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 04/15/24 07:43:09.853
  Apr 15 07:43:09.853: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 07:43:09.857
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:43:09.911
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:43:09.918
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 04/15/24 07:43:09.926
  STEP: Saw pod success @ 04/15/24 07:43:13.998
  Apr 15 07:43:14.007: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-3c39832c-08d5-4c70-89a1-00446f94351f container test-container: <nil>
  STEP: delete the pod @ 04/15/24 07:43:14.027
  Apr 15 07:43:14.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5413" for this suite. @ 04/15/24 07:43:14.075
• [4.238 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:836
  STEP: Creating a kubernetes client @ 04/15/24 07:43:14.098
  Apr 15 07:43:14.098: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename daemonsets @ 04/15/24 07:43:14.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:43:14.146
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:43:14.152
  STEP: Creating simple DaemonSet "daemon-set" @ 04/15/24 07:43:14.231
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/15/24 07:43:14.245
  Apr 15 07:43:14.273: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:43:14.273: INFO: Node zaigh3ewotoh-1 is running 0 daemon pod, expected 1
  Apr 15 07:43:15.330: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 07:43:15.331: INFO: Node zaigh3ewotoh-1 is running 0 daemon pod, expected 1
  Apr 15 07:43:16.317: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 15 07:43:16.317: INFO: Node zaigh3ewotoh-2 is running 0 daemon pod, expected 1
  Apr 15 07:43:17.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 15 07:43:17.296: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 04/15/24 07:43:17.305
  STEP: DeleteCollection of the DaemonSets @ 04/15/24 07:43:17.315
  STEP: Verify that ReplicaSets have been deleted @ 04/15/24 07:43:17.342
  Apr 15 07:43:17.399: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"169923"},"items":null}

  Apr 15 07:43:17.437: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"169929"},"items":[{"metadata":{"name":"daemon-set-bsfpv","generateName":"daemon-set-","namespace":"daemonsets-8295","uid":"960e704e-b30a-4910-8f73-f5bdf6cbd05f","resourceVersion":"169927","creationTimestamp":"2024-04-15T07:43:14Z","deletionTimestamp":"2024-04-15T07:43:47Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e4df6fc1-96ff-4dd2-95cc-6a3447a43759","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-04-15T07:43:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4df6fc1-96ff-4dd2-95cc-6a3447a43759\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-04-15T07:43:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-t5nzh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-t5nzh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"zaigh3ewotoh-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["zaigh3ewotoh-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T07:43:14Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T07:43:15Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T07:43:15Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T07:43:14Z"}],"hostIP":"192.168.121.131","podIP":"10.233.66.17","podIPs":[{"ip":"10.233.66.17"}],"startTime":"2024-04-15T07:43:14Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-04-15T07:43:15Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://2fec25d0521525f8cfe90ca1665806e9c3376af112f807efb40125fd52f6d0d9","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hgrr5","generateName":"daemon-set-","namespace":"daemonsets-8295","uid":"2c8b4e40-27c5-4be0-bd37-6010ad88799e","resourceVersion":"169925","creationTimestamp":"2024-04-15T07:43:14Z","deletionTimestamp":"2024-04-15T07:43:47Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e4df6fc1-96ff-4dd2-95cc-6a3447a43759","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-04-15T07:43:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4df6fc1-96ff-4dd2-95cc-6a3447a43759\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-04-15T07:43:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.214\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5dm42","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5dm42","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"zaigh3ewotoh-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["zaigh3ewotoh-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T07:43:14Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T07:43:15Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T07:43:15Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T07:43:14Z"}],"hostIP":"192.168.121.74","podIP":"10.233.64.214","podIPs":[{"ip":"10.233.64.214"}],"startTime":"2024-04-15T07:43:14Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-04-15T07:43:15Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://576c545c1fac3a62a1103b6f5c932936a573b7657a7ac021a65ac8c946e5b853","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-m22gg","generateName":"daemon-set-","namespace":"daemonsets-8295","uid":"8a6fecf0-a446-445a-88ce-7222fc0ea644","resourceVersion":"169926","creationTimestamp":"2024-04-15T07:43:14Z","deletionTimestamp":"2024-04-15T07:43:47Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e4df6fc1-96ff-4dd2-95cc-6a3447a43759","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-04-15T07:43:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4df6fc1-96ff-4dd2-95cc-6a3447a43759\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-04-15T07:43:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4klcp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4klcp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"zaigh3ewotoh-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["zaigh3ewotoh-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T07:43:14Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T07:43:17Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T07:43:17Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T07:43:14Z"}],"hostIP":"192.168.121.27","podIP":"10.233.65.181","podIPs":[{"ip":"10.233.65.181"}],"startTime":"2024-04-15T07:43:14Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-04-15T07:43:16Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://672c1fd82fcbd5dcd71853a2bd489ba50a1fc1ae564715873eb42807c4339e81","started":true}],"qosClass":"BestEffort"}}]}

  Apr 15 07:43:17.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8295" for this suite. @ 04/15/24 07:43:17.507
• [3.424 seconds]
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 04/15/24 07:43:17.521
  Apr 15 07:43:17.521: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename replication-controller @ 04/15/24 07:43:17.524
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:43:17.568
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:43:17.59
  STEP: creating a ReplicationController @ 04/15/24 07:43:17.615
  STEP: waiting for RC to be added @ 04/15/24 07:43:17.637
  STEP: waiting for available Replicas @ 04/15/24 07:43:17.637
  STEP: patching ReplicationController @ 04/15/24 07:43:18.56
  STEP: waiting for RC to be modified @ 04/15/24 07:43:18.595
  STEP: patching ReplicationController status @ 04/15/24 07:43:18.596
  STEP: waiting for RC to be modified @ 04/15/24 07:43:18.613
  STEP: waiting for available Replicas @ 04/15/24 07:43:18.613
  STEP: fetching ReplicationController status @ 04/15/24 07:43:18.638
  STEP: patching ReplicationController scale @ 04/15/24 07:43:18.655
  STEP: waiting for RC to be modified @ 04/15/24 07:43:18.676
  STEP: waiting for ReplicationController's scale to be the max amount @ 04/15/24 07:43:18.678
  STEP: fetching ReplicationController; ensuring that it's patched @ 04/15/24 07:43:19.774
  STEP: updating ReplicationController status @ 04/15/24 07:43:19.783
  STEP: waiting for RC to be modified @ 04/15/24 07:43:19.802
  STEP: listing all ReplicationControllers @ 04/15/24 07:43:19.804
  STEP: checking that ReplicationController has expected values @ 04/15/24 07:43:19.831
  STEP: deleting ReplicationControllers by collection @ 04/15/24 07:43:19.831
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 04/15/24 07:43:19.855
  Apr 15 07:43:19.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0415 07:43:19.916466      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-861" for this suite. @ 04/15/24 07:43:19.928
• [2.424 seconds]
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 04/15/24 07:43:19.946
  Apr 15 07:43:19.948: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename secrets @ 04/15/24 07:43:19.959
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:43:20.005
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:43:20.019
  STEP: Creating secret with name secret-test-eeab6235-e1b2-466a-891c-ba91a726b17c @ 04/15/24 07:43:20.026
  STEP: Creating a pod to test consume secrets @ 04/15/24 07:43:20.039
  E0415 07:43:20.917309      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:21.918456      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:22.918802      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:23.919393      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 07:43:24.126
  Apr 15 07:43:24.134: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-secrets-83392852-6efa-4e01-b6fd-21246466c0a9 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 07:43:24.154
  Apr 15 07:43:24.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7933" for this suite. @ 04/15/24 07:43:24.23
• [4.297 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 04/15/24 07:43:24.245
  Apr 15 07:43:24.245: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 07:43:24.247
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:43:24.284
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:43:24.29
  Apr 15 07:43:24.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-9095 create -f -'
  E0415 07:43:24.919291      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:43:25.194: INFO: stderr: ""
  Apr 15 07:43:25.194: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Apr 15 07:43:25.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-9095 create -f -'
  E0415 07:43:25.923640      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:43:26.137: INFO: stderr: ""
  Apr 15 07:43:26.137: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/15/24 07:43:26.137
  E0415 07:43:26.924550      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:43:27.147: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 07:43:27.147: INFO: Found 1 / 1
  Apr 15 07:43:27.147: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Apr 15 07:43:27.152: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 07:43:27.152: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 15 07:43:27.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-9095 describe pod agnhost-primary-tfwzc'
  Apr 15 07:43:27.339: INFO: stderr: ""
  Apr 15 07:43:27.339: INFO: stdout: "Name:             agnhost-primary-tfwzc\nNamespace:        kubectl-9095\nPriority:         0\nService Account:  default\nNode:             zaigh3ewotoh-3/192.168.121.131\nStart Time:       Mon, 15 Apr 2024 07:43:25 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.66.20\nIPs:\n  IP:           10.233.66.20\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://d4dbe7d596a9c241d6e856671cf335371cb548560a91939f8a56c433f5e3275a\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.47\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:c9997bf8d2e223d7d2a0078dcfb11a653e9b16cf09418829ec03e1d57ca9628a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 15 Apr 2024 07:43:26 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2m5k7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-2m5k7:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-9095/agnhost-primary-tfwzc to zaigh3ewotoh-3\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.47\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  Apr 15 07:43:27.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-9095 describe rc agnhost-primary'
  Apr 15 07:43:27.515: INFO: stderr: ""
  Apr 15 07:43:27.515: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9095\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.47\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-tfwzc\n"
  Apr 15 07:43:27.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-9095 describe service agnhost-primary'
  Apr 15 07:43:27.691: INFO: stderr: ""
  Apr 15 07:43:27.691: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9095\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.53.203\nIPs:               10.233.53.203\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.66.20:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Apr 15 07:43:27.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-9095 describe node zaigh3ewotoh-1'
  Apr 15 07:43:27.896: INFO: stderr: ""
  Apr 15 07:43:27.896: INFO: stdout: "Name:               zaigh3ewotoh-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=zaigh3ewotoh-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"1e:15:6b:54:3b:48\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.121.74\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 14 Apr 2024 14:57:53 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  zaigh3ewotoh-1\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 15 Apr 2024 07:43:27 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 15 Apr 2024 06:16:21 +0000   Mon, 15 Apr 2024 06:16:21 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Mon, 15 Apr 2024 07:40:17 +0000   Mon, 15 Apr 2024 06:12:39 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 15 Apr 2024 07:40:17 +0000   Mon, 15 Apr 2024 06:12:39 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 15 Apr 2024 07:40:17 +0000   Mon, 15 Apr 2024 06:12:39 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 15 Apr 2024 07:40:17 +0000   Mon, 15 Apr 2024 06:12:39 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.74\n  Hostname:    zaigh3ewotoh-1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  115008636Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8123552Ki\n  pods:               110\nAllocatable:\n  cpu:                1600m\n  ephemeral-storage:  111880401014\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3273888Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 9bb6030a4a224516a0c4933775f1c43b\n  System UUID:                9bb6030a-4a22-4516-a0c4-933775f1c43b\n  Boot ID:                    50eefce0-929d-4717-bb63-ea5ff4c884f0\n  Kernel Version:             6.5.0-27-generic\n  OS Image:                   Ubuntu 22.04.4 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.27.4\n  Kubelet Version:            v1.27.12\n  Kube-Proxy Version:         v1.27.12\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-5d78c9869d-t6h7j                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (5%)     87m\n  kube-system                 kube-addon-manager-zaigh3ewotoh-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         87m\n  kube-system                 kube-apiserver-zaigh3ewotoh-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         87m\n  kube-system                 kube-controller-manager-zaigh3ewotoh-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         87m\n  kube-system                 kube-flannel-ds-wwbkw                                      100m (6%)     0 (0%)      50Mi (1%)        0 (0%)         87m\n  kube-system                 kube-proxy-v9dxk                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         87m\n  kube-system                 kube-scheduler-zaigh3ewotoh-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         87m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-409cd1623c554ca6-k6p5l    0 (0%)        0 (0%)      0 (0%)           0 (0%)         87m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                755m (47%)  0 (0%)\n  memory             170Mi (5%)  170Mi (5%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason         Age                 From     Message\n  ----     ------         ----                ----     -------\n  Warning  ImageGCFailed  91s (x18 over 86m)  kubelet  failed to get imageFs info: non-existent label \"crio-images\"\n"
  Apr 15 07:43:27.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-9095 describe namespace kubectl-9095'
  E0415 07:43:27.924756      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:43:28.046: INFO: stderr: ""
  Apr 15 07:43:28.046: INFO: stdout: "Name:         kubectl-9095\nLabels:       e2e-framework=kubectl\n              e2e-run=fe35d8fe-a901-4578-84fa-433ba542a568\n              kubernetes.io/metadata.name=kubectl-9095\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Apr 15 07:43:28.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9095" for this suite. @ 04/15/24 07:43:28.055
• [3.827 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 04/15/24 07:43:28.072
  Apr 15 07:43:28.072: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename webhook @ 04/15/24 07:43:28.074
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:43:28.109
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:43:28.115
  STEP: Setting up server cert @ 04/15/24 07:43:28.166
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 07:43:28.744
  STEP: Deploying the webhook pod @ 04/15/24 07:43:28.758
  STEP: Wait for the deployment to be ready @ 04/15/24 07:43:28.793
  Apr 15 07:43:28.823: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0415 07:43:28.926020      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:29.926136      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/15/24 07:43:30.846
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 07:43:30.877
  E0415 07:43:30.926942      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:43:31.878: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 04/15/24 07:43:31.888
  E0415 07:43:31.927568      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be updated by the webhook @ 04/15/24 07:43:31.93
  Apr 15 07:43:31.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5777" for this suite. @ 04/15/24 07:43:32.106
  STEP: Destroying namespace "webhook-markers-5868" for this suite. @ 04/15/24 07:43:32.121
• [4.064 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 04/15/24 07:43:32.138
  Apr 15 07:43:32.138: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 07:43:32.141
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:43:32.175
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:43:32.181
  STEP: Counting existing ResourceQuota @ 04/15/24 07:43:32.188
  E0415 07:43:32.929215      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:33.929124      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:34.929573      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:35.930693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:36.930698      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/15/24 07:43:37.198
  STEP: Ensuring resource quota status is calculated @ 04/15/24 07:43:37.215
  E0415 07:43:37.931119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:38.931739      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 04/15/24 07:43:39.223
  STEP: Ensuring resource quota status captures replication controller creation @ 04/15/24 07:43:39.256
  E0415 07:43:39.934885      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:40.933594      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 04/15/24 07:43:41.267
  STEP: Ensuring resource quota status released usage @ 04/15/24 07:43:41.281
  E0415 07:43:41.933921      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:42.934182      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:43:43.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7432" for this suite. @ 04/15/24 07:43:43.306
• [11.183 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 04/15/24 07:43:43.336
  Apr 15 07:43:43.336: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename sysctl @ 04/15/24 07:43:43.344
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:43:43.385
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:43:43.388
  STEP: Creating a pod with one valid and two invalid sysctls @ 04/15/24 07:43:43.395
  Apr 15 07:43:43.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-9904" for this suite. @ 04/15/24 07:43:43.415
• [0.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 04/15/24 07:43:43.439
  Apr 15 07:43:43.439: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename crd-watch @ 04/15/24 07:43:43.441
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:43:43.476
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:43:43.482
  Apr 15 07:43:43.487: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  E0415 07:43:43.934497      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:44.934849      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:45.935374      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 04/15/24 07:43:46.209
  Apr 15 07:43:46.219: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-04-15T07:43:46Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-04-15T07:43:46Z]] name:name1 resourceVersion:170242 uid:7aec818a-bd1e-49df-89cb-8b428f86e9d5] num:map[num1:9223372036854775807 num2:1000000]]}
  E0415 07:43:46.936002      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:47.936651      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:48.936980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:49.937042      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:50.937206      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:51.937673      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:52.937829      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:53.938303      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:54.939501      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:55.940179      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 04/15/24 07:43:56.22
  Apr 15 07:43:56.237: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-04-15T07:43:56Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-04-15T07:43:56Z]] name:name2 resourceVersion:170270 uid:0d05d4a6-bbf2-41e1-b4b2-496cd6d9a6f1] num:map[num1:9223372036854775807 num2:1000000]]}
  E0415 07:43:56.940456      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:57.940939      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:58.941786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:43:59.942115      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:00.942282      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:01.943587      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:02.943920      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:03.944554      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:04.944879      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:05.946352      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 04/15/24 07:44:06.239
  Apr 15 07:44:06.262: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-04-15T07:43:46Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-04-15T07:44:06Z]] name:name1 resourceVersion:170288 uid:7aec818a-bd1e-49df-89cb-8b428f86e9d5] num:map[num1:9223372036854775807 num2:1000000]]}
  E0415 07:44:06.945642      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:07.946631      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:08.946874      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:09.947707      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:10.948555      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:11.948853      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:12.949743      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:13.950143      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:14.950778      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:15.951099      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 04/15/24 07:44:16.265
  Apr 15 07:44:16.282: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-04-15T07:43:56Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-04-15T07:44:16Z]] name:name2 resourceVersion:170306 uid:0d05d4a6-bbf2-41e1-b4b2-496cd6d9a6f1] num:map[num1:9223372036854775807 num2:1000000]]}
  E0415 07:44:16.952163      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:17.952840      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:18.952954      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:19.953436      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:20.953531      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:21.953766      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:22.954180      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:23.954452      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:24.954549      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:25.954725      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 04/15/24 07:44:26.282
  Apr 15 07:44:26.298: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-04-15T07:43:46Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-04-15T07:44:06Z]] name:name1 resourceVersion:170324 uid:7aec818a-bd1e-49df-89cb-8b428f86e9d5] num:map[num1:9223372036854775807 num2:1000000]]}
  E0415 07:44:26.955557      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:27.957019      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:28.956808      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:29.956971      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:30.957174      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:31.957644      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:32.957707      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:33.958065      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:34.958304      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:35.959212      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 04/15/24 07:44:36.299
  Apr 15 07:44:36.318: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-04-15T07:43:56Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-04-15T07:44:16Z]] name:name2 resourceVersion:170341 uid:0d05d4a6-bbf2-41e1-b4b2-496cd6d9a6f1] num:map[num1:9223372036854775807 num2:1000000]]}
  E0415 07:44:36.960523      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:37.960859      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:38.960877      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:39.961214      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:40.961160      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:41.962466      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:42.962233      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:43.962533      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:44.962603      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:45.962778      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:44:46.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-6457" for this suite. @ 04/15/24 07:44:46.879
• [63.455 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 04/15/24 07:44:46.898
  Apr 15 07:44:46.898: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename subpath @ 04/15/24 07:44:46.906
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:44:46.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:44:46.958
  E0415 07:44:46.963841      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up data @ 04/15/24 07:44:46.966
  STEP: Creating pod pod-subpath-test-configmap-vv89 @ 04/15/24 07:44:46.988
  STEP: Creating a pod to test atomic-volume-subpath @ 04/15/24 07:44:46.988
  E0415 07:44:47.964673      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:48.964854      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:49.965154      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:50.965627      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:51.966120      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:52.966538      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:53.967613      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:54.967851      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:55.968902      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:56.969252      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:57.969736      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:58.970308      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:44:59.971097      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:00.971997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:01.971908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:02.972549      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:03.972923      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:04.972986      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:05.973296      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:06.975742      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:07.975165      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:08.975636      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:09.976095      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:10.976174      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 07:45:11.163
  Apr 15 07:45:11.170: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-subpath-test-configmap-vv89 container test-container-subpath-configmap-vv89: <nil>
  STEP: delete the pod @ 04/15/24 07:45:11.211
  STEP: Deleting pod pod-subpath-test-configmap-vv89 @ 04/15/24 07:45:11.245
  Apr 15 07:45:11.245: INFO: Deleting pod "pod-subpath-test-configmap-vv89" in namespace "subpath-5338"
  Apr 15 07:45:11.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5338" for this suite. @ 04/15/24 07:45:11.273
• [24.409 seconds]
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 04/15/24 07:45:11.308
  Apr 15 07:45:11.308: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-runtime @ 04/15/24 07:45:11.312
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:45:11.354
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:45:11.362
  STEP: create the container @ 04/15/24 07:45:11.369
  W0415 07:45:11.400608      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/15/24 07:45:11.402
  E0415 07:45:11.976594      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:12.976840      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:13.976932      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/15/24 07:45:14.436
  STEP: the container should be terminated @ 04/15/24 07:45:14.442
  STEP: the termination message should be set @ 04/15/24 07:45:14.442
  Apr 15 07:45:14.442: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 04/15/24 07:45:14.443
  Apr 15 07:45:14.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7719" for this suite. @ 04/15/24 07:45:14.49
• [3.196 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 04/15/24 07:45:14.511
  Apr 15 07:45:14.511: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 07:45:14.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:45:14.549
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:45:14.554
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/15/24 07:45:14.559
  Apr 15 07:45:14.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-6840 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Apr 15 07:45:14.789: INFO: stderr: ""
  Apr 15 07:45:14.789: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 04/15/24 07:45:14.789
  Apr 15 07:45:14.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-6840 delete pods e2e-test-httpd-pod'
  E0415 07:45:14.977964      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:15.978234      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:16.978305      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:45:17.167: INFO: stderr: ""
  Apr 15 07:45:17.167: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 15 07:45:17.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6840" for this suite. @ 04/15/24 07:45:17.18
• [2.686 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 04/15/24 07:45:17.2
  Apr 15 07:45:17.200: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename replication-controller @ 04/15/24 07:45:17.204
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:45:17.244
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:45:17.256
  STEP: Creating ReplicationController "e2e-rc-nr226" @ 04/15/24 07:45:17.263
  Apr 15 07:45:17.274: INFO: Get Replication Controller "e2e-rc-nr226" to confirm replicas
  E0415 07:45:17.978819      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:45:18.283: INFO: Get Replication Controller "e2e-rc-nr226" to confirm replicas
  Apr 15 07:45:18.292: INFO: Found 1 replicas for "e2e-rc-nr226" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-nr226" @ 04/15/24 07:45:18.292
  STEP: Updating a scale subresource @ 04/15/24 07:45:18.3
  STEP: Verifying replicas where modified for replication controller "e2e-rc-nr226" @ 04/15/24 07:45:18.314
  Apr 15 07:45:18.314: INFO: Get Replication Controller "e2e-rc-nr226" to confirm replicas
  E0415 07:45:18.979647      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:45:19.323: INFO: Get Replication Controller "e2e-rc-nr226" to confirm replicas
  Apr 15 07:45:19.339: INFO: Found 2 replicas for "e2e-rc-nr226" replication controller
  Apr 15 07:45:19.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7412" for this suite. @ 04/15/24 07:45:19.353
• [2.166 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 04/15/24 07:45:19.374
  Apr 15 07:45:19.374: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 07:45:19.385
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:45:19.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:45:19.428
  STEP: starting the proxy server @ 04/15/24 07:45:19.436
  Apr 15 07:45:19.438: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-5696 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 04/15/24 07:45:19.617
  Apr 15 07:45:19.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5696" for this suite. @ 04/15/24 07:45:19.664
• [0.306 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 04/15/24 07:45:19.685
  Apr 15 07:45:19.685: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 07:45:19.691
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:45:19.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:45:19.742
  STEP: Starting the proxy @ 04/15/24 07:45:19.755
  Apr 15 07:45:19.758: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-5902 proxy --unix-socket=/tmp/kubectl-proxy-unix2394242216/test'
  STEP: retrieving proxy /api/ output @ 04/15/24 07:45:19.903
  Apr 15 07:45:19.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5902" for this suite. @ 04/15/24 07:45:19.917
• [0.253 seconds]
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 04/15/24 07:45:19.938
  Apr 15 07:45:19.938: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename var-expansion @ 04/15/24 07:45:19.94
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:45:19.978
  E0415 07:45:19.980804      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:45:19.985
  STEP: creating the pod @ 04/15/24 07:45:19.992
  STEP: waiting for pod running @ 04/15/24 07:45:20.01
  E0415 07:45:20.981003      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:21.982279      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 04/15/24 07:45:22.027
  Apr 15 07:45:22.035: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5305 PodName:var-expansion-8b10bc40-6162-4f60-a0bb-d23d5887fedc ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 07:45:22.036: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:45:22.038: INFO: ExecWithOptions: Clientset creation
  Apr 15 07:45:22.038: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-5305/pods/var-expansion-8b10bc40-6162-4f60-a0bb-d23d5887fedc/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 04/15/24 07:45:22.176
  Apr 15 07:45:22.187: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5305 PodName:var-expansion-8b10bc40-6162-4f60-a0bb-d23d5887fedc ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 07:45:22.187: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:45:22.190: INFO: ExecWithOptions: Clientset creation
  Apr 15 07:45:22.190: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-5305/pods/var-expansion-8b10bc40-6162-4f60-a0bb-d23d5887fedc/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 04/15/24 07:45:22.325
  Apr 15 07:45:22.848: INFO: Successfully updated pod "var-expansion-8b10bc40-6162-4f60-a0bb-d23d5887fedc"
  STEP: waiting for annotated pod running @ 04/15/24 07:45:22.848
  STEP: deleting the pod gracefully @ 04/15/24 07:45:22.855
  Apr 15 07:45:22.855: INFO: Deleting pod "var-expansion-8b10bc40-6162-4f60-a0bb-d23d5887fedc" in namespace "var-expansion-5305"
  Apr 15 07:45:22.869: INFO: Wait up to 5m0s for pod "var-expansion-8b10bc40-6162-4f60-a0bb-d23d5887fedc" to be fully deleted
  E0415 07:45:22.982301      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:23.983785      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:24.984516      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:25.985833      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:26.986211      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:27.987150      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:28.987986      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:29.988555      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:30.989321      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:31.989453      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:32.990329      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:33.990800      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:34.991027      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:35.991492      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:36.991812      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:37.991988      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:38.992876      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:39.993046      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:40.993503      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:41.994187      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:42.994398      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:43.994449      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:44.995315      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:45.995759      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:46.995851      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:47.996440      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:48.996865      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:49.997199      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:50.997458      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:51.997742      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:52.998822      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:53.998995      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:54.999834      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:45:55.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5305" for this suite. @ 04/15/24 07:45:55.097
• [35.183 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 04/15/24 07:45:55.124
  Apr 15 07:45:55.125: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename prestop @ 04/15/24 07:45:55.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:45:55.18
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:45:55.187
  STEP: Creating server pod server in namespace prestop-6072 @ 04/15/24 07:45:55.194
  STEP: Waiting for pods to come up. @ 04/15/24 07:45:55.219
  E0415 07:45:55.999816      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:57.000141      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-6072 @ 04/15/24 07:45:57.242
  E0415 07:45:58.000299      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:45:59.000648      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 04/15/24 07:45:59.275
  E0415 07:46:00.000870      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:01.001012      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:02.002178      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:03.002505      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:04.002642      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:46:04.305: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Apr 15 07:46:04.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 04/15/24 07:46:04.319
  STEP: Destroying namespace "prestop-6072" for this suite. @ 04/15/24 07:46:04.356
• [9.249 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 04/15/24 07:46:04.38
  Apr 15 07:46:04.380: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename replicaset @ 04/15/24 07:46:04.383
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:46:04.439
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:46:04.446
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 04/15/24 07:46:04.455
  Apr 15 07:46:04.503: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0415 07:46:05.003646      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:06.003885      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:07.004977      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:08.005922      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:09.006127      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:46:09.514: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/15/24 07:46:09.515
  STEP: getting scale subresource @ 04/15/24 07:46:09.516
  STEP: updating a scale subresource @ 04/15/24 07:46:09.526
  STEP: verifying the replicaset Spec.Replicas was modified @ 04/15/24 07:46:09.541
  STEP: Patch a scale subresource @ 04/15/24 07:46:09.55
  Apr 15 07:46:09.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1447" for this suite. @ 04/15/24 07:46:09.588
• [5.227 seconds]
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 04/15/24 07:46:09.608
  Apr 15 07:46:09.608: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename sched-preemption @ 04/15/24 07:46:09.615
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:46:09.775
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:46:09.783
  Apr 15 07:46:09.854: INFO: Waiting up to 1m0s for all nodes to be ready
  E0415 07:46:10.006707      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:11.007576      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:12.007740      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:13.007937      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:14.009219      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:15.009420      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:16.010479      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:17.011625      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:18.012485      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:19.012804      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:20.012997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:21.013365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:22.014135      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:23.014312      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:24.015557      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:25.015761      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:26.016547      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:27.017396      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:28.018022      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:29.018240      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:30.019199      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:31.019331      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:32.020156      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:33.020195      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:34.020856      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:35.021022      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:36.022065      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:37.022046      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:38.022755      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:39.022920      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:40.023174      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:41.023552      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:42.024075      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:43.024868      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:44.025830      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:45.026437      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:46.026947      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:47.027072      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:48.028002      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:49.028370      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:50.028762      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:51.029844      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:52.030386      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:53.030533      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:54.031937      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:55.032250      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:56.033192      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:57.034353      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:58.034693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:46:59.034760      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:00.035934      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:01.037163      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:02.037128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:03.038025      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:04.038249      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:05.038496      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:06.039487      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:07.039706      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:08.040235      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:09.040867      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:47:09.957: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 04/15/24 07:47:09.974
  Apr 15 07:47:09.975: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename sched-preemption-path @ 04/15/24 07:47:09.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:47:10.031
  E0415 07:47:10.041020      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:47:10.041
  Apr 15 07:47:10.090: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Apr 15 07:47:10.099: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Apr 15 07:47:10.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 07:47:10.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-2916" for this suite. @ 04/15/24 07:47:10.381
  STEP: Destroying namespace "sched-preemption-291" for this suite. @ 04/15/24 07:47:10.401
• [60.812 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 04/15/24 07:47:10.43
  Apr 15 07:47:10.430: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename watch @ 04/15/24 07:47:10.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:47:10.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:47:10.483
  STEP: creating a watch on configmaps @ 04/15/24 07:47:10.489
  STEP: creating a new configmap @ 04/15/24 07:47:10.492
  STEP: modifying the configmap once @ 04/15/24 07:47:10.523
  STEP: closing the watch once it receives two notifications @ 04/15/24 07:47:10.551
  Apr 15 07:47:10.551: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7111  d6311edc-14b4-4c00-9610-96068c8b81b0 170890 0 2024-04-15 07:47:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-04-15 07:47:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 07:47:10.553: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7111  d6311edc-14b4-4c00-9610-96068c8b81b0 170892 0 2024-04-15 07:47:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-04-15 07:47:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 04/15/24 07:47:10.553
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 04/15/24 07:47:10.579
  STEP: deleting the configmap @ 04/15/24 07:47:10.584
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 04/15/24 07:47:10.601
  Apr 15 07:47:10.602: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7111  d6311edc-14b4-4c00-9610-96068c8b81b0 170893 0 2024-04-15 07:47:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-04-15 07:47:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 07:47:10.602: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7111  d6311edc-14b4-4c00-9610-96068c8b81b0 170894 0 2024-04-15 07:47:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-04-15 07:47:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 07:47:10.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7111" for this suite. @ 04/15/24 07:47:10.618
• [0.215 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 04/15/24 07:47:10.648
  Apr 15 07:47:10.648: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename sched-preemption @ 04/15/24 07:47:10.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:47:10.701
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:47:10.715
  Apr 15 07:47:10.767: INFO: Waiting up to 1m0s for all nodes to be ready
  E0415 07:47:11.041389      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:12.042281      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:13.042983      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:14.043727      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:15.044938      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:16.044900      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:17.045498      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:18.046256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:19.046353      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:20.047198      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:21.047535      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:22.047744      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:23.048578      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:24.048791      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:25.049266      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:26.050096      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:27.051069      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:28.052973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:29.053153      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:30.054222      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:31.054664      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:32.054904      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:33.054877      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:34.055156      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:35.056204      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:36.056379      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:37.056822      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:38.057154      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:39.057742      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:40.058359      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:41.058937      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:42.060121      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:43.060772      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:44.061034      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:45.061918      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:46.062618      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:47.062392      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:48.063258      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:49.063658      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:50.063757      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:51.064960      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:52.065060      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:53.065689      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:54.065953      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:55.067156      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:56.067974      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:57.068812      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:58.068795      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:47:59.068951      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:00.069418      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:01.070020      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:02.071119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:03.071217      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:04.071515      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:05.072840      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:06.073700      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:07.074070      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:08.074171      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:09.074905      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:10.075915      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:48:10.845: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 04/15/24 07:48:10.857
  Apr 15 07:48:10.928: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Apr 15 07:48:10.946: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  E0415 07:48:11.077296      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:48:11.077: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Apr 15 07:48:11.135: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Apr 15 07:48:11.352: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Apr 15 07:48:11.391: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 04/15/24 07:48:11.391
  E0415 07:48:12.078111      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:13.078948      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 04/15/24 07:48:13.455
  E0415 07:48:14.080057      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:15.081038      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:16.081978      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:17.083549      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:48:17.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-1131" for this suite. @ 04/15/24 07:48:17.668
• [67.033 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 04/15/24 07:48:17.687
  Apr 15 07:48:17.687: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename security-context-test @ 04/15/24 07:48:17.691
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:48:17.726
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:48:17.73
  E0415 07:48:18.082956      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:19.083410      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:20.084286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:21.085085      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:48:21.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-3487" for this suite. @ 04/15/24 07:48:21.842
• [4.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 04/15/24 07:48:21.867
  Apr 15 07:48:21.867: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 07:48:21.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:48:21.91
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:48:21.916
  STEP: Counting existing ResourceQuota @ 04/15/24 07:48:21.922
  E0415 07:48:22.087032      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:23.087752      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:24.087968      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:25.088645      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:26.089665      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/15/24 07:48:26.932
  STEP: Ensuring resource quota status is calculated @ 04/15/24 07:48:26.945
  E0415 07:48:27.090957      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:28.091695      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:48:28.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9241" for this suite. @ 04/15/24 07:48:28.978
• [7.132 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 04/15/24 07:48:29.002
  Apr 15 07:48:29.002: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename configmap @ 04/15/24 07:48:29.01
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:48:29.062
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:48:29.069
  STEP: Creating configMap with name configmap-test-volume-9fea0d23-eff6-495a-bba4-f5cc8f038f9b @ 04/15/24 07:48:29.078
  E0415 07:48:29.091791      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a pod to test consume configMaps @ 04/15/24 07:48:29.093
  E0415 07:48:30.091747      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:31.092111      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:32.092582      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:33.093018      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 07:48:33.175
  Apr 15 07:48:33.187: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-configmaps-bd694450-d438-4137-8d85-9871f7207b50 container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 07:48:33.202
  Apr 15 07:48:33.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9664" for this suite. @ 04/15/24 07:48:33.271
• [4.289 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 04/15/24 07:48:33.301
  Apr 15 07:48:33.301: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename webhook @ 04/15/24 07:48:33.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:48:33.348
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:48:33.353
  STEP: Setting up server cert @ 04/15/24 07:48:33.41
  E0415 07:48:34.094011      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 07:48:34.334
  STEP: Deploying the webhook pod @ 04/15/24 07:48:34.359
  STEP: Wait for the deployment to be ready @ 04/15/24 07:48:34.419
  Apr 15 07:48:34.438: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0415 07:48:35.094199      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:36.095212      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/15/24 07:48:36.487
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 07:48:36.52
  E0415 07:48:37.096170      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:48:37.521: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 04/15/24 07:48:37.531
  STEP: create a namespace for the webhook @ 04/15/24 07:48:37.578
  STEP: create a configmap should be unconditionally rejected by the webhook @ 04/15/24 07:48:37.632
  Apr 15 07:48:37.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3757" for this suite. @ 04/15/24 07:48:37.889
  STEP: Destroying namespace "webhook-markers-7800" for this suite. @ 04/15/24 07:48:37.904
  STEP: Destroying namespace "fail-closed-namespace-2290" for this suite. @ 04/15/24 07:48:37.927
• [4.647 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 04/15/24 07:48:37.953
  Apr 15 07:48:37.953: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/15/24 07:48:37.957
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:48:38.015
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:48:38.022
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 04/15/24 07:48:38.03
  Apr 15 07:48:38.033: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  E0415 07:48:38.096180      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:39.097340      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:40.098256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:41.099629      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:42.099614      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:43.099910      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:44.100984      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:45.101123      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 04/15/24 07:48:45.879
  Apr 15 07:48:45.880: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  E0415 07:48:46.102395      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:47.102060      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:48:47.938: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  E0415 07:48:48.102873      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:49.103983      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:50.104278      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:51.105275      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:52.105864      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:53.109254      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:54.110902      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:55.112001      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:48:55.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1976" for this suite. @ 04/15/24 07:48:55.722
• [17.791 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 04/15/24 07:48:55.767
  Apr 15 07:48:55.767: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubelet-test @ 04/15/24 07:48:55.77
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:48:55.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:48:55.833
  E0415 07:48:56.112565      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:57.112811      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:58.112943      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:48:59.113469      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:48:59.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4872" for this suite. @ 04/15/24 07:48:59.912
• [4.171 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 04/15/24 07:48:59.938
  Apr 15 07:48:59.938: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename configmap @ 04/15/24 07:48:59.94
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:48:59.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:48:59.995
  STEP: Creating configMap with name configmap-test-volume-map-77778d97-70e1-4f01-afa9-fef25eb6e0f5 @ 04/15/24 07:49:00.008
  STEP: Creating a pod to test consume configMaps @ 04/15/24 07:49:00.021
  E0415 07:49:00.115127      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:01.114730      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:02.115529      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:03.115915      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 07:49:04.084
  Apr 15 07:49:04.091: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-configmaps-59f3430d-ad7c-4495-b52c-8647d93f1acb container agnhost-container: <nil>
  E0415 07:49:04.117008      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 04/15/24 07:49:04.161
  Apr 15 07:49:04.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6201" for this suite. @ 04/15/24 07:49:04.211
• [4.289 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 04/15/24 07:49:04.23
  Apr 15 07:49:04.230: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-runtime @ 04/15/24 07:49:04.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:49:04.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:49:04.279
  STEP: create the container @ 04/15/24 07:49:04.285
  W0415 07:49:04.302083      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/15/24 07:49:04.302
  E0415 07:49:05.117091      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:06.117226      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/15/24 07:49:06.331
  STEP: the container should be terminated @ 04/15/24 07:49:06.341
  STEP: the termination message should be set @ 04/15/24 07:49:06.341
  Apr 15 07:49:06.341: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 04/15/24 07:49:06.342
  Apr 15 07:49:06.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7980" for this suite. @ 04/15/24 07:49:06.39
• [2.175 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 04/15/24 07:49:06.405
  Apr 15 07:49:06.405: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename deployment @ 04/15/24 07:49:06.407
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:49:06.442
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:49:06.454
  Apr 15 07:49:06.494: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0415 07:49:07.117739      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:08.118354      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:09.118220      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:10.118616      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:11.118846      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:11.507: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/15/24 07:49:11.507
  Apr 15 07:49:11.507: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0415 07:49:12.119496      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:13.119206      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:13.518: INFO: Creating deployment "test-rollover-deployment"
  Apr 15 07:49:13.536: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0415 07:49:14.120144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:15.120381      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:15.554: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Apr 15 07:49:15.567: INFO: Ensure that both replica sets have 1 created replica
  Apr 15 07:49:15.582: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Apr 15 07:49:15.604: INFO: Updating deployment test-rollover-deployment
  Apr 15 07:49:15.605: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0415 07:49:16.120966      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:17.121095      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:17.628: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Apr 15 07:49:17.643: INFO: Make sure deployment "test-rollover-deployment" is complete
  Apr 15 07:49:17.660: INFO: all replica sets need to contain the pod-template-hash label
  Apr 15 07:49:17.660: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 17, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65cd7886c5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:49:18.122103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:19.122838      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:19.688: INFO: all replica sets need to contain the pod-template-hash label
  Apr 15 07:49:19.689: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 17, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65cd7886c5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:49:20.123843      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:21.124375      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:21.679: INFO: all replica sets need to contain the pod-template-hash label
  Apr 15 07:49:21.679: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 17, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65cd7886c5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:49:22.124820      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:23.124986      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:23.677: INFO: all replica sets need to contain the pod-template-hash label
  Apr 15 07:49:23.677: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 17, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65cd7886c5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:49:24.125450      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:25.126516      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:25.678: INFO: all replica sets need to contain the pod-template-hash label
  Apr 15 07:49:25.678: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 13, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 17, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65cd7886c5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:49:26.126614      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:27.126732      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:27.683: INFO: 
  Apr 15 07:49:27.685: INFO: Ensure that both old replica sets have no replicas
  Apr 15 07:49:27.719: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-9079  65d63c3e-c443-4c59-ab2d-dc8afa3e90f9 171602 2 2024-04-15 07:49:13 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-04-15 07:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 07:49:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.47 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044e13a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-04-15 07:49:13 +0000 UTC,LastTransitionTime:2024-04-15 07:49:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-65cd7886c5" has successfully progressed.,LastUpdateTime:2024-04-15 07:49:27 +0000 UTC,LastTransitionTime:2024-04-15 07:49:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 15 07:49:27.730: INFO: New ReplicaSet "test-rollover-deployment-65cd7886c5" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-65cd7886c5  deployment-9079  b9a3b5b4-dc94-43ff-ae2a-0be812f4d24d 171592 2 2024-04-15 07:49:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:65cd7886c5] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 65d63c3e-c443-4c59-ab2d-dc8afa3e90f9 0xc0044e1d07 0xc0044e1d08}] [] [{kube-controller-manager Update apps/v1 2024-04-15 07:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"65d63c3e-c443-4c59-ab2d-dc8afa3e90f9\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 07:49:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 65cd7886c5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:65cd7886c5] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.47 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044e1df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 07:49:27.731: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Apr 15 07:49:27.732: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9079  392d458d-9ebb-4563-8736-93742400cd6d 171601 2 2024-04-15 07:49:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 65d63c3e-c443-4c59-ab2d-dc8afa3e90f9 0xc0044e18a7 0xc0044e18a8}] [] [{e2e.test Update apps/v1 2024-04-15 07:49:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 07:49:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"65d63c3e-c443-4c59-ab2d-dc8afa3e90f9\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2024-04-15 07:49:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0044e19b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 07:49:27.733: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-9079  c416a224-dcad-4eb1-a9ae-e64e11d9eb17 171555 2 2024-04-15 07:49:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 65d63c3e-c443-4c59-ab2d-dc8afa3e90f9 0xc0044e1be7 0xc0044e1be8}] [] [{kube-controller-manager Update apps/v1 2024-04-15 07:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"65d63c3e-c443-4c59-ab2d-dc8afa3e90f9\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 07:49:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044e1c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 07:49:27.747: INFO: Pod "test-rollover-deployment-65cd7886c5-7t64v" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-65cd7886c5-7t64v test-rollover-deployment-65cd7886c5- deployment-9079  363342d7-31f5-453b-b7cb-cc497a899fe0 171572 0 2024-04-15 07:49:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:65cd7886c5] map[] [{apps/v1 ReplicaSet test-rollover-deployment-65cd7886c5 b9a3b5b4-dc94-43ff-ae2a-0be812f4d24d 0xc004446377 0xc004446378}] [] [{kube-controller-manager Update v1 2024-04-15 07:49:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9a3b5b4-dc94-43ff-ae2a-0be812f4d24d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:49:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.40\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-55nx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.47,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-55nx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:49:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:49:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:49:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.131,PodIP:10.233.66.40,StartTime:2024-04-15 07:49:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 07:49:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.47,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:c9997bf8d2e223d7d2a0078dcfb11a653e9b16cf09418829ec03e1d57ca9628a,ContainerID:cri-o://1b40b974631fd1502ac55d70bd45c5bdaf2a6e2d48ba2d5f963c34be7efa7ec4,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.40,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:49:27.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9079" for this suite. @ 04/15/24 07:49:27.764
• [21.372 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 04/15/24 07:49:27.785
  Apr 15 07:49:27.785: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename proxy @ 04/15/24 07:49:27.788
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:49:27.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:49:27.828
  STEP: starting an echo server on multiple ports @ 04/15/24 07:49:27.855
  STEP: creating replication controller proxy-service-9bjrs in namespace proxy-3978 @ 04/15/24 07:49:27.856
  I0415 07:49:27.879356      13 runners.go:194] Created replication controller with name: proxy-service-9bjrs, namespace: proxy-3978, replica count: 1
  E0415 07:49:28.127451      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0415 07:49:28.936870      13 runners.go:194] proxy-service-9bjrs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0415 07:49:29.127786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0415 07:49:29.937430      13 runners.go:194] proxy-service-9bjrs Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 15 07:49:29.946: INFO: setup took 2.112461695s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 04/15/24 07:49:29.946
  Apr 15 07:49:29.972: INFO: (0) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 25.45116ms)
  Apr 15 07:49:29.973: INFO: (0) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 25.05078ms)
  Apr 15 07:49:29.974: INFO: (0) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 27.709055ms)
  Apr 15 07:49:29.985: INFO: (0) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 36.728798ms)
  Apr 15 07:49:29.986: INFO: (0) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 39.333557ms)
  Apr 15 07:49:29.987: INFO: (0) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 39.200664ms)
  Apr 15 07:49:29.993: INFO: (0) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 45.347336ms)
  Apr 15 07:49:29.993: INFO: (0) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 44.415379ms)
  Apr 15 07:49:29.993: INFO: (0) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 45.171773ms)
  Apr 15 07:49:29.997: INFO: (0) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 48.425174ms)
  Apr 15 07:49:29.997: INFO: (0) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 49.571951ms)
  Apr 15 07:49:29.997: INFO: (0) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 50.482947ms)
  Apr 15 07:49:29.997: INFO: (0) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 50.126556ms)
  Apr 15 07:49:29.997: INFO: (0) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 48.722105ms)
  Apr 15 07:49:29.998: INFO: (0) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 50.497399ms)
  Apr 15 07:49:29.998: INFO: (0) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 51.393531ms)
  Apr 15 07:49:30.039: INFO: (1) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 38.800795ms)
  Apr 15 07:49:30.043: INFO: (1) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 44.516751ms)
  Apr 15 07:49:30.044: INFO: (1) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 41.707419ms)
  Apr 15 07:49:30.045: INFO: (1) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 44.436265ms)
  Apr 15 07:49:30.046: INFO: (1) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 45.286342ms)
  Apr 15 07:49:30.049: INFO: (1) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 50.203606ms)
  Apr 15 07:49:30.049: INFO: (1) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 46.70142ms)
  Apr 15 07:49:30.050: INFO: (1) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 50.063892ms)
  Apr 15 07:49:30.050: INFO: (1) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 46.902154ms)
  Apr 15 07:49:30.050: INFO: (1) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 47.541797ms)
  Apr 15 07:49:30.050: INFO: (1) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 47.442898ms)
  Apr 15 07:49:30.050: INFO: (1) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 49.543446ms)
  Apr 15 07:49:30.051: INFO: (1) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 49.020635ms)
  Apr 15 07:49:30.050: INFO: (1) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 51.235849ms)
  Apr 15 07:49:30.051: INFO: (1) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 49.88177ms)
  Apr 15 07:49:30.051: INFO: (1) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 50.808594ms)
  Apr 15 07:49:30.088: INFO: (2) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 24.478911ms)
  Apr 15 07:49:30.091: INFO: (2) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 33.326023ms)
  Apr 15 07:49:30.093: INFO: (2) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 36.516262ms)
  Apr 15 07:49:30.093: INFO: (2) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 35.751732ms)
  Apr 15 07:49:30.093: INFO: (2) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 34.954075ms)
  Apr 15 07:49:30.095: INFO: (2) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 40.527211ms)
  Apr 15 07:49:30.095: INFO: (2) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 29.608191ms)
  Apr 15 07:49:30.097: INFO: (2) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 31.254622ms)
  Apr 15 07:49:30.098: INFO: (2) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 33.016931ms)
  Apr 15 07:49:30.100: INFO: (2) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 34.771435ms)
  Apr 15 07:49:30.101: INFO: (2) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 35.719072ms)
  Apr 15 07:49:30.105: INFO: (2) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 39.727244ms)
  Apr 15 07:49:30.105: INFO: (2) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 39.861721ms)
  Apr 15 07:49:30.105: INFO: (2) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 39.724839ms)
  Apr 15 07:49:30.106: INFO: (2) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 50.127846ms)
  Apr 15 07:49:30.106: INFO: (2) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 40.87789ms)
  Apr 15 07:49:30.125: INFO: (3) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 18.439633ms)
  E0415 07:49:30.129010      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:30.129: INFO: (3) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 21.652588ms)
  Apr 15 07:49:30.129: INFO: (3) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 21.102315ms)
  Apr 15 07:49:30.131: INFO: (3) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 24.003446ms)
  Apr 15 07:49:30.130: INFO: (3) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 23.672121ms)
  Apr 15 07:49:30.131: INFO: (3) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 23.72707ms)
  Apr 15 07:49:30.132: INFO: (3) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 25.54823ms)
  Apr 15 07:49:30.132: INFO: (3) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 25.248095ms)
  Apr 15 07:49:30.134: INFO: (3) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 25.764375ms)
  Apr 15 07:49:30.136: INFO: (3) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 28.137371ms)
  Apr 15 07:49:30.136: INFO: (3) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 28.960759ms)
  Apr 15 07:49:30.136: INFO: (3) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 28.593054ms)
  Apr 15 07:49:30.136: INFO: (3) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 28.01105ms)
  Apr 15 07:49:30.137: INFO: (3) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 28.898084ms)
  Apr 15 07:49:30.139: INFO: (3) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 31.215512ms)
  Apr 15 07:49:30.140: INFO: (3) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 32.674451ms)
  Apr 15 07:49:30.155: INFO: (4) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 13.656377ms)
  Apr 15 07:49:30.156: INFO: (4) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 14.549685ms)
  Apr 15 07:49:30.159: INFO: (4) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 17.279005ms)
  Apr 15 07:49:30.163: INFO: (4) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 21.153046ms)
  Apr 15 07:49:30.166: INFO: (4) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 24.334726ms)
  Apr 15 07:49:30.168: INFO: (4) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 26.741908ms)
  Apr 15 07:49:30.169: INFO: (4) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 27.298779ms)
  Apr 15 07:49:30.169: INFO: (4) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 27.879596ms)
  Apr 15 07:49:30.169: INFO: (4) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 27.094824ms)
  Apr 15 07:49:30.169: INFO: (4) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 28.384987ms)
  Apr 15 07:49:30.169: INFO: (4) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 27.012794ms)
  Apr 15 07:49:30.169: INFO: (4) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 27.271305ms)
  Apr 15 07:49:30.171: INFO: (4) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 30.584228ms)
  Apr 15 07:49:30.172: INFO: (4) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 30.07943ms)
  Apr 15 07:49:30.172: INFO: (4) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 30.34665ms)
  Apr 15 07:49:30.172: INFO: (4) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 30.199456ms)
  Apr 15 07:49:30.187: INFO: (5) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 13.585193ms)
  Apr 15 07:49:30.188: INFO: (5) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 14.59245ms)
  Apr 15 07:49:30.188: INFO: (5) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 14.900881ms)
  Apr 15 07:49:30.190: INFO: (5) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 16.909149ms)
  Apr 15 07:49:30.190: INFO: (5) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 16.558355ms)
  Apr 15 07:49:30.191: INFO: (5) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 17.838675ms)
  Apr 15 07:49:30.191: INFO: (5) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 17.174197ms)
  Apr 15 07:49:30.191: INFO: (5) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 17.716436ms)
  Apr 15 07:49:30.198: INFO: (5) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 24.21406ms)
  Apr 15 07:49:30.200: INFO: (5) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 26.493354ms)
  Apr 15 07:49:30.198: INFO: (5) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 24.114623ms)
  Apr 15 07:49:30.198: INFO: (5) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 24.395095ms)
  Apr 15 07:49:30.198: INFO: (5) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 24.623861ms)
  Apr 15 07:49:30.198: INFO: (5) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 24.975875ms)
  Apr 15 07:49:30.198: INFO: (5) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 24.619514ms)
  Apr 15 07:49:30.202: INFO: (5) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 28.730739ms)
  Apr 15 07:49:30.229: INFO: (6) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 23.853958ms)
  Apr 15 07:49:30.231: INFO: (6) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 26.26084ms)
  Apr 15 07:49:30.232: INFO: (6) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 26.5512ms)
  Apr 15 07:49:30.232: INFO: (6) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 26.969367ms)
  Apr 15 07:49:30.232: INFO: (6) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 26.798537ms)
  Apr 15 07:49:30.233: INFO: (6) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 28.496877ms)
  Apr 15 07:49:30.233: INFO: (6) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 27.796609ms)
  Apr 15 07:49:30.233: INFO: (6) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 28.065901ms)
  Apr 15 07:49:30.234: INFO: (6) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 27.901347ms)
  Apr 15 07:49:30.234: INFO: (6) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 28.329487ms)
  Apr 15 07:49:30.234: INFO: (6) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 28.611856ms)
  Apr 15 07:49:30.239: INFO: (6) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 32.83016ms)
  Apr 15 07:49:30.239: INFO: (6) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 33.61074ms)
  Apr 15 07:49:30.240: INFO: (6) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 34.19785ms)
  Apr 15 07:49:30.241: INFO: (6) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 34.670438ms)
  Apr 15 07:49:30.242: INFO: (6) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 36.347652ms)
  Apr 15 07:49:30.258: INFO: (7) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 15.323682ms)
  Apr 15 07:49:30.261: INFO: (7) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 18.361781ms)
  Apr 15 07:49:30.261: INFO: (7) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 18.033574ms)
  Apr 15 07:49:30.261: INFO: (7) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 17.873309ms)
  Apr 15 07:49:30.261: INFO: (7) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 18.173139ms)
  Apr 15 07:49:30.265: INFO: (7) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 22.422025ms)
  Apr 15 07:49:30.272: INFO: (7) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 27.778415ms)
  Apr 15 07:49:30.274: INFO: (7) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 31.067183ms)
  Apr 15 07:49:30.277: INFO: (7) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 33.586426ms)
  Apr 15 07:49:30.282: INFO: (7) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 38.316673ms)
  Apr 15 07:49:30.282: INFO: (7) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 37.624936ms)
  Apr 15 07:49:30.283: INFO: (7) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 38.868078ms)
  Apr 15 07:49:30.284: INFO: (7) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 40.697883ms)
  Apr 15 07:49:30.284: INFO: (7) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 40.625541ms)
  Apr 15 07:49:30.285: INFO: (7) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 41.694754ms)
  Apr 15 07:49:30.285: INFO: (7) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 41.09129ms)
  Apr 15 07:49:30.310: INFO: (8) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 20.443871ms)
  Apr 15 07:49:30.317: INFO: (8) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 26.711567ms)
  Apr 15 07:49:30.317: INFO: (8) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 26.614003ms)
  Apr 15 07:49:30.317: INFO: (8) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 27.342284ms)
  Apr 15 07:49:30.318: INFO: (8) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 25.272084ms)
  Apr 15 07:49:30.318: INFO: (8) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 28.324296ms)
  Apr 15 07:49:30.319: INFO: (8) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 26.111746ms)
  Apr 15 07:49:30.319: INFO: (8) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 28.224238ms)
  Apr 15 07:49:30.319: INFO: (8) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 28.385776ms)
  Apr 15 07:49:30.319: INFO: (8) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 29.357254ms)
  Apr 15 07:49:30.319: INFO: (8) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 30.532063ms)
  Apr 15 07:49:30.323: INFO: (8) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 33.818247ms)
  Apr 15 07:49:30.323: INFO: (8) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 32.927482ms)
  Apr 15 07:49:30.323: INFO: (8) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 33.455346ms)
  Apr 15 07:49:30.324: INFO: (8) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 31.425984ms)
  Apr 15 07:49:30.327: INFO: (8) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 34.312227ms)
  Apr 15 07:49:30.347: INFO: (9) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 18.983878ms)
  Apr 15 07:49:30.355: INFO: (9) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 24.485688ms)
  Apr 15 07:49:30.355: INFO: (9) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 26.01543ms)
  Apr 15 07:49:30.355: INFO: (9) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 27.046278ms)
  Apr 15 07:49:30.355: INFO: (9) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 25.753162ms)
  Apr 15 07:49:30.361: INFO: (9) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 33.472787ms)
  Apr 15 07:49:30.361: INFO: (9) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 32.341203ms)
  Apr 15 07:49:30.361: INFO: (9) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 30.738403ms)
  Apr 15 07:49:30.361: INFO: (9) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 29.649207ms)
  Apr 15 07:49:30.361: INFO: (9) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 31.250583ms)
  Apr 15 07:49:30.361: INFO: (9) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 32.21583ms)
  Apr 15 07:49:30.365: INFO: (9) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 36.843004ms)
  Apr 15 07:49:30.365: INFO: (9) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 34.15107ms)
  Apr 15 07:49:30.369: INFO: (9) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 40.070748ms)
  Apr 15 07:49:30.374: INFO: (9) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 43.074169ms)
  Apr 15 07:49:30.374: INFO: (9) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 44.388781ms)
  Apr 15 07:49:30.386: INFO: (10) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 10.958809ms)
  Apr 15 07:49:30.389: INFO: (10) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 12.346014ms)
  Apr 15 07:49:30.395: INFO: (10) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 18.056926ms)
  Apr 15 07:49:30.399: INFO: (10) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 21.499366ms)
  Apr 15 07:49:30.400: INFO: (10) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 24.400943ms)
  Apr 15 07:49:30.401: INFO: (10) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 23.682177ms)
  Apr 15 07:49:30.401: INFO: (10) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 24.685113ms)
  Apr 15 07:49:30.402: INFO: (10) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 25.704532ms)
  Apr 15 07:49:30.408: INFO: (10) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 30.843914ms)
  Apr 15 07:49:30.409: INFO: (10) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 31.317678ms)
  Apr 15 07:49:30.409: INFO: (10) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 31.284094ms)
  Apr 15 07:49:30.409: INFO: (10) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 31.280514ms)
  Apr 15 07:49:30.409: INFO: (10) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 31.223842ms)
  Apr 15 07:49:30.409: INFO: (10) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 31.127245ms)
  Apr 15 07:49:30.409: INFO: (10) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 31.563339ms)
  Apr 15 07:49:30.409: INFO: (10) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 31.097862ms)
  Apr 15 07:49:30.423: INFO: (11) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 13.15595ms)
  Apr 15 07:49:30.424: INFO: (11) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 14.37721ms)
  Apr 15 07:49:30.424: INFO: (11) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 13.691197ms)
  Apr 15 07:49:30.427: INFO: (11) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 14.7781ms)
  Apr 15 07:49:30.428: INFO: (11) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 14.940157ms)
  Apr 15 07:49:30.430: INFO: (11) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 19.060557ms)
  Apr 15 07:49:30.430: INFO: (11) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 19.228054ms)
  Apr 15 07:49:30.430: INFO: (11) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 17.138795ms)
  Apr 15 07:49:30.431: INFO: (11) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 19.622258ms)
  Apr 15 07:49:30.433: INFO: (11) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 22.469439ms)
  Apr 15 07:49:30.436: INFO: (11) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 24.206066ms)
  Apr 15 07:49:30.438: INFO: (11) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 25.373838ms)
  Apr 15 07:49:30.440: INFO: (11) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 28.397728ms)
  Apr 15 07:49:30.440: INFO: (11) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 27.030601ms)
  Apr 15 07:49:30.440: INFO: (11) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 28.731517ms)
  Apr 15 07:49:30.440: INFO: (11) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 28.878707ms)
  Apr 15 07:49:30.457: INFO: (12) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 15.675984ms)
  Apr 15 07:49:30.457: INFO: (12) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 16.120326ms)
  Apr 15 07:49:30.458: INFO: (12) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 17.478028ms)
  Apr 15 07:49:30.459: INFO: (12) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 17.934021ms)
  Apr 15 07:49:30.462: INFO: (12) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 20.68972ms)
  Apr 15 07:49:30.462: INFO: (12) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 20.345406ms)
  Apr 15 07:49:30.464: INFO: (12) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 22.266586ms)
  Apr 15 07:49:30.466: INFO: (12) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 23.62154ms)
  Apr 15 07:49:30.466: INFO: (12) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 23.714273ms)
  Apr 15 07:49:30.473: INFO: (12) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 30.727286ms)
  Apr 15 07:49:30.473: INFO: (12) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 32.143354ms)
  Apr 15 07:49:30.474: INFO: (12) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 32.222636ms)
  Apr 15 07:49:30.474: INFO: (12) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 32.280815ms)
  Apr 15 07:49:30.484: INFO: (12) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 41.769815ms)
  Apr 15 07:49:30.484: INFO: (12) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 42.011846ms)
  Apr 15 07:49:30.487: INFO: (12) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 45.316784ms)
  Apr 15 07:49:30.501: INFO: (13) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 13.717497ms)
  Apr 15 07:49:30.505: INFO: (13) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 15.968001ms)
  Apr 15 07:49:30.507: INFO: (13) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 16.803177ms)
  Apr 15 07:49:30.507: INFO: (13) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 17.026887ms)
  Apr 15 07:49:30.509: INFO: (13) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 19.1142ms)
  Apr 15 07:49:30.509: INFO: (13) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 21.469965ms)
  Apr 15 07:49:30.511: INFO: (13) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 23.072101ms)
  Apr 15 07:49:30.512: INFO: (13) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 22.765464ms)
  Apr 15 07:49:30.515: INFO: (13) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 26.635414ms)
  Apr 15 07:49:30.519: INFO: (13) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 30.585215ms)
  Apr 15 07:49:30.523: INFO: (13) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 34.774486ms)
  Apr 15 07:49:30.524: INFO: (13) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 35.751841ms)
  Apr 15 07:49:30.524: INFO: (13) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 36.022233ms)
  Apr 15 07:49:30.524: INFO: (13) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 35.011313ms)
  Apr 15 07:49:30.526: INFO: (13) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 35.902247ms)
  Apr 15 07:49:30.527: INFO: (13) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 37.858922ms)
  Apr 15 07:49:30.542: INFO: (14) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 14.957652ms)
  Apr 15 07:49:30.545: INFO: (14) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 18.440237ms)
  Apr 15 07:49:30.548: INFO: (14) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 20.217485ms)
  Apr 15 07:49:30.548: INFO: (14) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 21.762186ms)
  Apr 15 07:49:30.549: INFO: (14) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 21.176026ms)
  Apr 15 07:49:30.551: INFO: (14) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 23.245043ms)
  Apr 15 07:49:30.551: INFO: (14) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 23.511931ms)
  Apr 15 07:49:30.551: INFO: (14) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 24.300122ms)
  Apr 15 07:49:30.551: INFO: (14) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 23.853726ms)
  Apr 15 07:49:30.552: INFO: (14) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 24.487847ms)
  Apr 15 07:49:30.553: INFO: (14) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 25.83005ms)
  Apr 15 07:49:30.556: INFO: (14) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 28.120568ms)
  Apr 15 07:49:30.556: INFO: (14) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 28.380893ms)
  Apr 15 07:49:30.556: INFO: (14) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 28.381445ms)
  Apr 15 07:49:30.558: INFO: (14) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 29.798444ms)
  Apr 15 07:49:30.558: INFO: (14) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 30.132659ms)
  Apr 15 07:49:30.582: INFO: (15) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 23.890293ms)
  Apr 15 07:49:30.585: INFO: (15) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 25.211743ms)
  Apr 15 07:49:30.586: INFO: (15) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 27.395671ms)
  Apr 15 07:49:30.586: INFO: (15) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 26.193848ms)
  Apr 15 07:49:30.593: INFO: (15) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 32.255922ms)
  Apr 15 07:49:30.599: INFO: (15) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 38.58935ms)
  Apr 15 07:49:30.599: INFO: (15) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 39.297121ms)
  Apr 15 07:49:30.599: INFO: (15) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 39.924643ms)
  Apr 15 07:49:30.599: INFO: (15) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 38.857413ms)
  Apr 15 07:49:30.599: INFO: (15) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 39.733732ms)
  Apr 15 07:49:30.599: INFO: (15) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 39.582712ms)
  Apr 15 07:49:30.603: INFO: (15) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 44.328285ms)
  Apr 15 07:49:30.604: INFO: (15) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 44.490641ms)
  Apr 15 07:49:30.606: INFO: (15) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 47.543531ms)
  Apr 15 07:49:30.614: INFO: (15) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 53.961926ms)
  Apr 15 07:49:30.614: INFO: (15) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 54.305709ms)
  Apr 15 07:49:30.630: INFO: (16) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 14.690051ms)
  Apr 15 07:49:30.630: INFO: (16) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 14.212171ms)
  Apr 15 07:49:30.648: INFO: (16) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 30.155688ms)
  Apr 15 07:49:30.648: INFO: (16) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 32.031899ms)
  Apr 15 07:49:30.649: INFO: (16) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 32.861491ms)
  Apr 15 07:49:30.650: INFO: (16) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 34.851227ms)
  Apr 15 07:49:30.652: INFO: (16) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 33.900306ms)
  Apr 15 07:49:30.652: INFO: (16) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 33.436789ms)
  Apr 15 07:49:30.654: INFO: (16) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 35.983286ms)
  Apr 15 07:49:30.654: INFO: (16) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 37.376451ms)
  Apr 15 07:49:30.655: INFO: (16) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 37.002771ms)
  Apr 15 07:49:30.662: INFO: (16) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 44.78404ms)
  Apr 15 07:49:30.663: INFO: (16) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 44.274267ms)
  Apr 15 07:49:30.663: INFO: (16) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 44.798171ms)
  Apr 15 07:49:30.669: INFO: (16) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 52.228304ms)
  Apr 15 07:49:30.672: INFO: (16) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 54.305002ms)
  Apr 15 07:49:30.695: INFO: (17) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 21.800591ms)
  Apr 15 07:49:30.696: INFO: (17) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 22.465684ms)
  Apr 15 07:49:30.698: INFO: (17) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 23.637941ms)
  Apr 15 07:49:30.698: INFO: (17) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 24.145719ms)
  Apr 15 07:49:30.702: INFO: (17) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 29.646226ms)
  Apr 15 07:49:30.703: INFO: (17) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 28.907445ms)
  Apr 15 07:49:30.707: INFO: (17) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 33.024902ms)
  Apr 15 07:49:30.707: INFO: (17) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 32.809438ms)
  Apr 15 07:49:30.710: INFO: (17) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 36.662607ms)
  Apr 15 07:49:30.710: INFO: (17) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 36.712104ms)
  Apr 15 07:49:30.713: INFO: (17) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 39.402161ms)
  Apr 15 07:49:30.713: INFO: (17) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 38.3217ms)
  Apr 15 07:49:30.713: INFO: (17) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 40.034625ms)
  Apr 15 07:49:30.713: INFO: (17) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 39.056735ms)
  Apr 15 07:49:30.714: INFO: (17) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 41.472277ms)
  Apr 15 07:49:30.715: INFO: (17) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 41.585973ms)
  Apr 15 07:49:30.736: INFO: (18) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 20.592947ms)
  Apr 15 07:49:30.737: INFO: (18) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 21.546284ms)
  Apr 15 07:49:30.737: INFO: (18) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 22.02836ms)
  Apr 15 07:49:30.737: INFO: (18) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 21.768405ms)
  Apr 15 07:49:30.759: INFO: (18) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 41.606164ms)
  Apr 15 07:49:30.759: INFO: (18) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 41.510226ms)
  Apr 15 07:49:30.764: INFO: (18) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 47.653525ms)
  Apr 15 07:49:30.765: INFO: (18) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 47.699912ms)
  Apr 15 07:49:30.766: INFO: (18) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 49.796946ms)
  Apr 15 07:49:30.780: INFO: (18) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 62.799666ms)
  Apr 15 07:49:30.780: INFO: (18) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 63.146105ms)
  Apr 15 07:49:30.780: INFO: (18) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 63.763344ms)
  Apr 15 07:49:30.781: INFO: (18) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 64.479642ms)
  Apr 15 07:49:30.784: INFO: (18) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 67.773385ms)
  Apr 15 07:49:30.784: INFO: (18) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 67.003787ms)
  Apr 15 07:49:30.788: INFO: (18) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 71.665393ms)
  Apr 15 07:49:30.815: INFO: (19) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">... (200; 26.660167ms)
  Apr 15 07:49:30.817: INFO: (19) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:460/proxy/: tls baz (200; 28.514176ms)
  Apr 15 07:49:30.817: INFO: (19) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname2/proxy/: tls qux (200; 28.777482ms)
  Apr 15 07:49:30.818: INFO: (19) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 24.398835ms)
  Apr 15 07:49:30.818: INFO: (19) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:1080/proxy/rewriteme">test<... (200; 24.707256ms)
  Apr 15 07:49:30.821: INFO: (19) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname2/proxy/: bar (200; 32.782346ms)
  Apr 15 07:49:30.821: INFO: (19) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 33.052564ms)
  Apr 15 07:49:30.836: INFO: (19) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname1/proxy/: foo (200; 47.17248ms)
  Apr 15 07:49:30.848: INFO: (19) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv/proxy/rewriteme">test</a> (200; 58.616527ms)
  Apr 15 07:49:30.849: INFO: (19) /api/v1/namespaces/proxy-3978/pods/http:proxy-service-9bjrs-59tjv:162/proxy/: bar (200; 43.462052ms)
  Apr 15 07:49:30.850: INFO: (19) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/: <a href="/api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:443/proxy/tlsrewritem... (200; 44.936545ms)
  Apr 15 07:49:30.852: INFO: (19) /api/v1/namespaces/proxy-3978/pods/https:proxy-service-9bjrs-59tjv:462/proxy/: tls qux (200; 57.015364ms)
  Apr 15 07:49:30.852: INFO: (19) /api/v1/namespaces/proxy-3978/services/http:proxy-service-9bjrs:portname2/proxy/: bar (200; 56.758635ms)
  Apr 15 07:49:30.854: INFO: (19) /api/v1/namespaces/proxy-3978/services/proxy-service-9bjrs:portname1/proxy/: foo (200; 58.884402ms)
  Apr 15 07:49:30.854: INFO: (19) /api/v1/namespaces/proxy-3978/pods/proxy-service-9bjrs-59tjv:160/proxy/: foo (200; 48.969972ms)
  Apr 15 07:49:30.855: INFO: (19) /api/v1/namespaces/proxy-3978/services/https:proxy-service-9bjrs:tlsportname1/proxy/: tls baz (200; 59.880637ms)
  Apr 15 07:49:30.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-9bjrs in namespace proxy-3978, will wait for the garbage collector to delete the pods @ 04/15/24 07:49:30.878
  Apr 15 07:49:30.970: INFO: Deleting ReplicationController proxy-service-9bjrs took: 24.858445ms
  Apr 15 07:49:31.071: INFO: Terminating ReplicationController proxy-service-9bjrs pods took: 101.303815ms
  E0415 07:49:31.129981      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:32.129677      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:33.130500      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-3978" for this suite. @ 04/15/24 07:49:33.573
• [5.806 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 04/15/24 07:49:33.609
  Apr 15 07:49:33.610: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 07:49:33.614
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:49:33.645
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:49:33.65
  STEP: Creating Pod @ 04/15/24 07:49:33.658
  E0415 07:49:34.130744      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:35.131432      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 04/15/24 07:49:35.72
  Apr 15 07:49:35.720: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4288 PodName:pod-sharedvolume-a7364a26-b9a9-496d-aa2f-b7dae4e44129 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 07:49:35.720: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:49:35.722: INFO: ExecWithOptions: Clientset creation
  Apr 15 07:49:35.722: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-4288/pods/pod-sharedvolume-a7364a26-b9a9-496d-aa2f-b7dae4e44129/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Apr 15 07:49:35.848: INFO: Exec stderr: ""
  Apr 15 07:49:35.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4288" for this suite. @ 04/15/24 07:49:35.863
• [2.272 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 04/15/24 07:49:35.882
  Apr 15 07:49:35.882: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename pods @ 04/15/24 07:49:35.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:49:35.921
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:49:35.928
  E0415 07:49:36.131967      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:37.132277      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:38.133245      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:39.134544      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:40.135411      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:41.135555      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 07:49:42.069
  Apr 15 07:49:42.075: INFO: Trying to get logs from node zaigh3ewotoh-3 pod client-envvars-492890ce-2677-418d-a9c6-8c9d3135841d container env3cont: <nil>
  STEP: delete the pod @ 04/15/24 07:49:42.092
  Apr 15 07:49:42.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0415 07:49:42.136574      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "pods-4262" for this suite. @ 04/15/24 07:49:42.142
• [6.278 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 04/15/24 07:49:42.178
  Apr 15 07:49:42.178: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename namespaces @ 04/15/24 07:49:42.181
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:49:42.219
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:49:42.227
  STEP: Updating Namespace "namespaces-1118" @ 04/15/24 07:49:42.235
  Apr 15 07:49:42.261: INFO: Namespace "namespaces-1118" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"fe35d8fe-a901-4578-84fa-433ba542a568", "kubernetes.io/metadata.name":"namespaces-1118", "namespaces-1118":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Apr 15 07:49:42.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1118" for this suite. @ 04/15/24 07:49:42.273
• [0.111 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:96
  STEP: Creating a kubernetes client @ 04/15/24 07:49:42.292
  Apr 15 07:49:42.292: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename aggregator @ 04/15/24 07:49:42.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:49:42.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:49:42.339
  Apr 15 07:49:42.349: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Registering the sample API server. @ 04/15/24 07:49:42.352
  E0415 07:49:43.136932      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:43.455: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Apr 15 07:49:43.544: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  E0415 07:49:44.137505      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:45.137718      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:45.669: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:49:46.138216      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:47.138485      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:47.685: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:49:48.141013      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:49.140160      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:49.678: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:49:50.140532      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:51.141132      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:51.680: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:49:52.141624      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:53.142109      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:53.677: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:49:54.143374      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:55.143404      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:55.679: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:49:56.144090      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:57.144063      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:57.677: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:49:58.144647      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:49:59.145111      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:49:59.678: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:50:00.146494      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:01.147237      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:50:01.677: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:50:02.148173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:03.149212      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:50:03.677: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:50:04.149651      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:05.150850      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:50:05.682: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 49, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:50:06.151504      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:07.152044      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:50:07.831: INFO: Waited 134.610796ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 04/15/24 07:50:07.95
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 04/15/24 07:50:07.958
  STEP: List APIServices @ 04/15/24 07:50:07.97
  Apr 15 07:50:07.983: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 04/15/24 07:50:07.984
  Apr 15 07:50:08.009: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 04/15/24 07:50:08.01
  Apr 15 07:50:08.027: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2024, time.April, 15, 7, 50, 7, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 04/15/24 07:50:08.027
  Apr 15 07:50:08.036: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2024-04-15 07:50:07 +0000 UTC Passed all checks passed}
  Apr 15 07:50:08.036: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 15 07:50:08.036: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 04/15/24 07:50:08.039
  Apr 15 07:50:08.077: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1094318157" @ 04/15/24 07:50:08.077
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 04/15/24 07:50:08.095
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 04/15/24 07:50:08.11
  STEP: Patch APIService Status @ 04/15/24 07:50:08.12
  E0415 07:50:08.152562      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 04/15/24 07:50:08.153
  Apr 15 07:50:08.164: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2024-04-15 07:50:07 +0000 UTC Passed all checks passed}
  Apr 15 07:50:08.165: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 15 07:50:08.165: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Apr 15 07:50:08.165: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 04/15/24 07:50:08.165
  STEP: Confirm that the generated APIService has been deleted @ 04/15/24 07:50:08.176
  Apr 15 07:50:08.176: INFO: Requesting list of APIServices to confirm quantity
  Apr 15 07:50:08.186: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Apr 15 07:50:08.186: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Apr 15 07:50:08.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-3242" for this suite. @ 04/15/24 07:50:08.711
• [26.434 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 04/15/24 07:50:08.726
  Apr 15 07:50:08.726: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename webhook @ 04/15/24 07:50:08.73
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:50:08.773
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:50:08.778
  STEP: Setting up server cert @ 04/15/24 07:50:08.829
  E0415 07:50:09.153035      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 07:50:09.885
  STEP: Deploying the webhook pod @ 04/15/24 07:50:09.894
  STEP: Wait for the deployment to be ready @ 04/15/24 07:50:09.916
  Apr 15 07:50:09.933: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0415 07:50:10.153755      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:11.153894      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/15/24 07:50:11.955
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 07:50:11.975
  E0415 07:50:12.154084      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:50:12.975: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 04/15/24 07:50:12.985
  STEP: create a pod that should be updated by the webhook @ 04/15/24 07:50:13.055
  Apr 15 07:50:13.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0415 07:50:13.155707      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-2824" for this suite. @ 04/15/24 07:50:13.261
  STEP: Destroying namespace "webhook-markers-3494" for this suite. @ 04/15/24 07:50:13.274
• [4.564 seconds]
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 04/15/24 07:50:13.291
  Apr 15 07:50:13.291: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename dns @ 04/15/24 07:50:13.295
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:50:13.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:50:13.34
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 04/15/24 07:50:13.365
  Apr 15 07:50:13.438: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8220  d9bf2e7e-94b2-4157-b71f-63702c94fc80 171973 0 2024-04-15 07:50:13 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2024-04-15 07:50:13 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zctbd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.47,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zctbd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0415 07:50:14.154905      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:15.155516      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 04/15/24 07:50:15.477
  Apr 15 07:50:15.478: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8220 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 07:50:15.478: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:50:15.482: INFO: ExecWithOptions: Clientset creation
  Apr 15 07:50:15.482: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-8220/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 04/15/24 07:50:15.693
  Apr 15 07:50:15.694: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8220 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 07:50:15.694: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 07:50:15.696: INFO: ExecWithOptions: Clientset creation
  Apr 15 07:50:15.696: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-8220/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 15 07:50:15.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 07:50:15.861: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-8220" for this suite. @ 04/15/24 07:50:15.901
• [2.624 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 04/15/24 07:50:15.922
  Apr 15 07:50:15.922: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename sched-pred @ 04/15/24 07:50:15.933
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:50:15.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:50:15.979
  Apr 15 07:50:15.988: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 15 07:50:16.030: INFO: Waiting for terminating namespaces to be deleted...
  Apr 15 07:50:16.040: INFO: 
  Logging pods the apiserver thinks is on node zaigh3ewotoh-1 before test
  Apr 15 07:50:16.063: INFO: coredns-5d78c9869d-t6h7j from kube-system started at 2024-04-15 06:16:15 +0000 UTC (1 container statuses recorded)
  Apr 15 07:50:16.063: INFO: 	Container coredns ready: true, restart count 0
  Apr 15 07:50:16.063: INFO: kube-addon-manager-zaigh3ewotoh-1 from kube-system started at 2024-04-15 06:11:57 +0000 UTC (1 container statuses recorded)
  Apr 15 07:50:16.063: INFO: 	Container kube-addon-manager ready: true, restart count 4
  Apr 15 07:50:16.063: INFO: kube-apiserver-zaigh3ewotoh-1 from kube-system started at 2024-04-15 06:11:57 +0000 UTC (1 container statuses recorded)
  Apr 15 07:50:16.063: INFO: 	Container kube-apiserver ready: true, restart count 4
  Apr 15 07:50:16.063: INFO: kube-controller-manager-zaigh3ewotoh-1 from kube-system started at 2024-04-15 06:11:57 +0000 UTC (1 container statuses recorded)
  Apr 15 07:50:16.063: INFO: 	Container kube-controller-manager ready: true, restart count 4
  Apr 15 07:50:16.063: INFO: kube-flannel-ds-wwbkw from kube-system started at 2024-04-15 06:16:17 +0000 UTC (1 container statuses recorded)
  Apr 15 07:50:16.063: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 07:50:16.063: INFO: kube-proxy-v9dxk from kube-system started at 2024-04-15 06:16:16 +0000 UTC (1 container statuses recorded)
  Apr 15 07:50:16.063: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 07:50:16.063: INFO: kube-scheduler-zaigh3ewotoh-1 from kube-system started at 2024-04-15 06:11:57 +0000 UTC (1 container statuses recorded)
  Apr 15 07:50:16.063: INFO: 	Container kube-scheduler ready: true, restart count 4
  Apr 15 07:50:16.063: INFO: sonobuoy-systemd-logs-daemon-set-409cd1623c554ca6-k6p5l from sonobuoy started at 2024-04-15 06:16:23 +0000 UTC (2 container statuses recorded)
  Apr 15 07:50:16.063: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 07:50:16.063: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 15 07:50:16.063: INFO: 
  Logging pods the apiserver thinks is on node zaigh3ewotoh-2 before test
  Apr 15 07:50:16.100: INFO: coredns-5d78c9869d-lc7qb from kube-system started at 2024-04-15 07:36:36 +0000 UTC (1 container statuses recorded)
  Apr 15 07:50:16.100: INFO: 	Container coredns ready: true, restart count 0
  Apr 15 07:50:16.100: INFO: kube-addon-manager-zaigh3ewotoh-2 from kube-system started at 2024-04-15 06:07:13 +0000 UTC (1 container statuses recorded)
  Apr 15 07:50:16.100: INFO: 	Container kube-addon-manager ready: true, restart count 4
  Apr 15 07:50:16.100: INFO: kube-apiserver-zaigh3ewotoh-2 from kube-system started at 2024-04-15 06:07:13 +0000 UTC (1 container statuses recorded)
  Apr 15 07:50:16.100: INFO: 	Container kube-apiserver ready: true, restart count 4
  Apr 15 07:50:16.100: INFO: kube-controller-manager-zaigh3ewotoh-2 from kube-system started at 2024-04-15 06:07:13 +0000 UTC (1 container statuses recorded)
  Apr 15 07:50:16.100: INFO: 	Container kube-controller-manager ready: true, restart count 4
  Apr 15 07:50:16.100: INFO: kube-flannel-ds-gs792 from kube-system started at 2024-04-15 06:16:17 +0000 UTC (1 container statuses recorded)
  Apr 15 07:50:16.100: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 07:50:16.100: INFO: kube-proxy-sqtk9 from kube-system started at 2024-04-15 06:16:16 +0000 UTC (1 container statuses recorded)
  Apr 15 07:50:16.100: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 07:50:16.100: INFO: kube-scheduler-zaigh3ewotoh-2 from kube-system started at 2024-04-15 06:07:13 +0000 UTC (1 container statuses recorded)
  Apr 15 07:50:16.100: INFO: 	Container kube-scheduler ready: true, restart count 4
  Apr 15 07:50:16.100: INFO: sonobuoy-systemd-logs-daemon-set-409cd1623c554ca6-pp7qg from sonobuoy started at 2024-04-15 06:16:23 +0000 UTC (2 container statuses recorded)
  Apr 15 07:50:16.100: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 07:50:16.100: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 15 07:50:16.100: INFO: 
  Logging pods the apiserver thinks is on node zaigh3ewotoh-3 before test
  Apr 15 07:50:16.127: INFO: kube-flannel-ds-6lvnc from kube-system started at 2024-04-15 07:39:29 +0000 UTC (1 container statuses recorded)
  Apr 15 07:50:16.127: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 07:50:16.127: INFO: kube-proxy-mxvh9 from kube-system started at 2024-04-15 06:16:16 +0000 UTC (1 container statuses recorded)
  Apr 15 07:50:16.128: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 07:50:16.128: INFO: sonobuoy from sonobuoy started at 2024-04-15 06:16:22 +0000 UTC (1 container statuses recorded)
  Apr 15 07:50:16.128: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 15 07:50:16.128: INFO: sonobuoy-e2e-job-7bb47b0523524b58 from sonobuoy started at 2024-04-15 06:16:23 +0000 UTC (2 container statuses recorded)
  Apr 15 07:50:16.129: INFO: 	Container e2e ready: true, restart count 0
  Apr 15 07:50:16.129: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 07:50:16.129: INFO: sonobuoy-systemd-logs-daemon-set-409cd1623c554ca6-9fkvf from sonobuoy started at 2024-04-15 06:16:23 +0000 UTC (2 container statuses recorded)
  Apr 15 07:50:16.130: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 07:50:16.130: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 15 07:50:16.130: INFO: webhook-to-be-mutated from webhook-2824 started at 2024-04-15 07:50:13 +0000 UTC (1 container statuses recorded)
  Apr 15 07:50:16.131: INFO: 	Container example ready: false, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 04/15/24 07:50:16.131
  E0415 07:50:16.156243      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.17c664b0325b31da], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] @ 04/15/24 07:50:16.203
  E0415 07:50:17.156801      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:50:17.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-5866" for this suite. @ 04/15/24 07:50:17.206
• [1.299 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 04/15/24 07:50:17.229
  Apr 15 07:50:17.230: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename configmap @ 04/15/24 07:50:17.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:50:17.268
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:50:17.272
  Apr 15 07:50:17.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5822" for this suite. @ 04/15/24 07:50:17.35
• [0.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 04/15/24 07:50:17.364
  Apr 15 07:50:17.364: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename disruption @ 04/15/24 07:50:17.366
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:50:17.399
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:50:17.404
  STEP: Creating a kubernetes client @ 04/15/24 07:50:17.41
  Apr 15 07:50:17.410: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename disruption-2 @ 04/15/24 07:50:17.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:50:17.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:50:17.441
  STEP: Waiting for the pdb to be processed @ 04/15/24 07:50:17.457
  E0415 07:50:18.156613      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:19.157020      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 04/15/24 07:50:19.491
  E0415 07:50:20.157171      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:21.157593      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 04/15/24 07:50:21.524
  STEP: listing a collection of PDBs across all namespaces @ 04/15/24 07:50:21.541
  STEP: listing a collection of PDBs in namespace disruption-909 @ 04/15/24 07:50:21.551
  STEP: deleting a collection of PDBs @ 04/15/24 07:50:21.558
  STEP: Waiting for the PDB collection to be deleted @ 04/15/24 07:50:21.583
  Apr 15 07:50:21.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 07:50:21.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-83" for this suite. @ 04/15/24 07:50:21.608
  STEP: Destroying namespace "disruption-909" for this suite. @ 04/15/24 07:50:21.622
• [4.270 seconds]
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 04/15/24 07:50:21.635
  Apr 15 07:50:21.635: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename gc @ 04/15/24 07:50:21.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:50:21.668
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:50:21.673
  STEP: create the rc @ 04/15/24 07:50:21.687
  W0415 07:50:21.696318      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0415 07:50:22.157719      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:23.158411      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:24.160630      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:25.160987      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:26.162101      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:27.162307      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 04/15/24 07:50:27.882
  STEP: wait for the rc to be deleted @ 04/15/24 07:50:27.994
  E0415 07:50:28.163340      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:29.164141      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:50:29.724: INFO: 81 pods remaining
  Apr 15 07:50:29.725: INFO: 80 pods has nil DeletionTimestamp
  Apr 15 07:50:29.725: INFO: 
  E0415 07:50:30.164822      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:50:30.213: INFO: 72 pods remaining
  Apr 15 07:50:30.213: INFO: 68 pods has nil DeletionTimestamp
  Apr 15 07:50:30.213: INFO: 
  E0415 07:50:31.165266      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:50:31.207: INFO: 59 pods remaining
  Apr 15 07:50:31.207: INFO: 59 pods has nil DeletionTimestamp
  Apr 15 07:50:31.207: INFO: 
  Apr 15 07:50:32.098: INFO: 42 pods remaining
  Apr 15 07:50:32.098: INFO: 41 pods has nil DeletionTimestamp
  Apr 15 07:50:32.098: INFO: 
  E0415 07:50:32.165677      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:33.166244      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:50:33.341: INFO: 32 pods remaining
  Apr 15 07:50:33.341: INFO: 29 pods has nil DeletionTimestamp
  Apr 15 07:50:33.341: INFO: 
  E0415 07:50:34.167288      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:50:34.229: INFO: 19 pods remaining
  Apr 15 07:50:34.229: INFO: 17 pods has nil DeletionTimestamp
  Apr 15 07:50:34.229: INFO: 
  Apr 15 07:50:35.040: INFO: 0 pods remaining
  Apr 15 07:50:35.040: INFO: 0 pods has nil DeletionTimestamp
  Apr 15 07:50:35.040: INFO: 
  E0415 07:50:35.169093      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 04/15/24 07:50:36.018
  E0415 07:50:36.170113      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:50:36.546: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 15 07:50:36.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-614" for this suite. @ 04/15/24 07:50:36.571
• [14.959 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 04/15/24 07:50:36.612
  Apr 15 07:50:36.612: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 07:50:36.618
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:50:36.699
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:50:36.704
  STEP: Creating a pod to test emptydir volume type on node default medium @ 04/15/24 07:50:36.712
  E0415 07:50:37.170665      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:38.170716      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:39.171144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:40.172037      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:41.175849      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:42.180330      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:43.180157      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:44.181142      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:45.181155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:46.181808      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:47.183300      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:48.183809      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 07:50:49.132
  Apr 15 07:50:49.155: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-5d54067e-d4d5-4c4c-9a61-d5a0c140ec2d container test-container: <nil>
  E0415 07:50:49.184902      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 04/15/24 07:50:49.206
  Apr 15 07:50:49.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3306" for this suite. @ 04/15/24 07:50:49.359
• [12.776 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 04/15/24 07:50:49.394
  Apr 15 07:50:49.394: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/15/24 07:50:49.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:50:49.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:50:49.545
  STEP: create the container to handle the HTTPGet hook request. @ 04/15/24 07:50:49.589
  E0415 07:50:50.185347      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:51.185248      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:52.186306      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:53.186429      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:54.187077      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:55.187358      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:56.188096      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:57.189007      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/15/24 07:50:57.79
  E0415 07:50:58.189972      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:50:59.190595      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 04/15/24 07:50:59.893
  E0415 07:51:00.191378      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:01.192177      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 04/15/24 07:51:01.93
  Apr 15 07:51:01.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1542" for this suite. @ 04/15/24 07:51:01.973
• [12.592 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 04/15/24 07:51:01.996
  Apr 15 07:51:01.996: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 07:51:02.001
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:51:02.036
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:51:02.045
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 04/15/24 07:51:02.051
  E0415 07:51:02.193314      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:03.193650      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:04.194485      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:05.194959      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 07:51:06.123
  Apr 15 07:51:06.132: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-41d25590-969a-4bcb-bdb8-95d2b371b7d2 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 07:51:06.147
  Apr 15 07:51:06.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0415 07:51:06.199707      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "emptydir-7420" for this suite. @ 04/15/24 07:51:06.205
• [4.223 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 04/15/24 07:51:06.232
  Apr 15 07:51:06.232: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 07:51:06.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:51:06.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:51:06.28
  STEP: Creating the pod @ 04/15/24 07:51:06.302
  E0415 07:51:07.196209      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:08.196573      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:51:08.914: INFO: Successfully updated pod "annotationupdate2d2f576f-7a70-45d0-9215-c51921434d97"
  E0415 07:51:09.197002      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:10.197620      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:51:10.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2107" for this suite. @ 04/15/24 07:51:10.964
• [4.754 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 04/15/24 07:51:10.997
  Apr 15 07:51:10.997: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename deployment @ 04/15/24 07:51:11
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:51:11.038
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:51:11.053
  Apr 15 07:51:11.087: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0415 07:51:11.198248      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:12.198671      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:13.198814      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:14.199157      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:15.199223      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:51:16.103: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/15/24 07:51:16.103
  Apr 15 07:51:16.103: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 04/15/24 07:51:16.134
  E0415 07:51:16.200291      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:17.200909      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:18.201463      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:19.205091      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:51:20.196: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2467  95c3a124-961c-44ef-83ab-23938b52cc44 173553 1 2024-04-15 07:51:16 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2024-04-15 07:51:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 07:51:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.47 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0068fab68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-04-15 07:51:16 +0000 UTC,LastTransitionTime:2024-04-15 07:51:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7c495bfbdb" has successfully progressed.,LastUpdateTime:2024-04-15 07:51:18 +0000 UTC,LastTransitionTime:2024-04-15 07:51:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  E0415 07:51:20.206017      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:51:20.206: INFO: New ReplicaSet "test-cleanup-deployment-7c495bfbdb" of Deployment "test-cleanup-deployment":
  &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7c495bfbdb  deployment-2467  28af09ef-6d27-4c36-9f8a-d556169cc4d8 173543 1 2024-04-15 07:51:16 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7c495bfbdb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 95c3a124-961c-44ef-83ab-23938b52cc44 0xc00699c387 0xc00699c388}] [] [{kube-controller-manager Update apps/v1 2024-04-15 07:51:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"95c3a124-961c-44ef-83ab-23938b52cc44\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 07:51:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7c495bfbdb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7c495bfbdb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.47 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00699c438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 07:51:20.215: INFO: Pod "test-cleanup-deployment-7c495bfbdb-fjkqr" is available:
  &Pod{ObjectMeta:{test-cleanup-deployment-7c495bfbdb-fjkqr test-cleanup-deployment-7c495bfbdb- deployment-2467  030151d3-9ab5-4ddf-b4ff-273ead8ac285 173542 0 2024-04-15 07:51:16 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7c495bfbdb] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7c495bfbdb 28af09ef-6d27-4c36-9f8a-d556169cc4d8 0xc00699c7d7 0xc00699c7d8}] [] [{kube-controller-manager Update v1 2024-04-15 07:51:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28af09ef-6d27-4c36-9f8a-d556169cc4d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:51:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bfrd5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.47,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bfrd5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:51:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:51:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:51:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:51:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.131,PodIP:10.233.66.89,StartTime:2024-04-15 07:51:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 07:51:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.47,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:c9997bf8d2e223d7d2a0078dcfb11a653e9b16cf09418829ec03e1d57ca9628a,ContainerID:cri-o://906989eecc4d55621dd803fe8618eb74d37cb328bdddf0af2078749a9db05774,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.89,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:51:20.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2467" for this suite. @ 04/15/24 07:51:20.23
• [9.247 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:289
  STEP: Creating a kubernetes client @ 04/15/24 07:51:20.254
  Apr 15 07:51:20.254: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename field-validation @ 04/15/24 07:51:20.257
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:51:20.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:51:20.292
  Apr 15 07:51:20.299: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  E0415 07:51:21.206584      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:22.206809      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:23.207523      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:51:23.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-601" for this suite. @ 04/15/24 07:51:23.68
• [3.438 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 04/15/24 07:51:23.724
  Apr 15 07:51:23.724: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename configmap @ 04/15/24 07:51:23.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:51:23.757
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:51:23.77
  STEP: Creating configMap that has name configmap-test-emptyKey-7ebb0909-0ecb-4bac-a003-1bad4846b73f @ 04/15/24 07:51:23.781
  Apr 15 07:51:23.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4513" for this suite. @ 04/15/24 07:51:23.795
• [0.093 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 04/15/24 07:51:23.827
  Apr 15 07:51:23.827: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 07:51:23.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:51:23.855
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:51:23.86
  STEP: Creating configMap with name projected-configmap-test-volume-map-f9cecc22-ee56-4272-985e-7d1ca79244d6 @ 04/15/24 07:51:23.865
  STEP: Creating a pod to test consume configMaps @ 04/15/24 07:51:23.872
  E0415 07:51:24.208412      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:25.208900      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:26.209872      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:27.209855      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 07:51:27.919
  Apr 15 07:51:27.927: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-projected-configmaps-b137a3ad-b802-41b1-9dde-ee715b8fd5f2 container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 07:51:27.946
  Apr 15 07:51:27.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9800" for this suite. @ 04/15/24 07:51:27.999
• [4.191 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 04/15/24 07:51:28.027
  Apr 15 07:51:28.027: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename init-container @ 04/15/24 07:51:28.03
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:51:28.062
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:51:28.067
  STEP: creating the pod @ 04/15/24 07:51:28.073
  Apr 15 07:51:28.073: INFO: PodSpec: initContainers in spec.initContainers
  E0415 07:51:28.210551      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:29.211759      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:30.213785      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:31.212408      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:51:31.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-7815" for this suite. @ 04/15/24 07:51:31.4
• [3.390 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 04/15/24 07:51:31.432
  Apr 15 07:51:31.432: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubelet-test @ 04/15/24 07:51:31.438
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:51:31.47
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:51:31.477
  Apr 15 07:51:31.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6331" for this suite. @ 04/15/24 07:51:31.557
• [0.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 04/15/24 07:51:31.583
  Apr 15 07:51:31.583: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 07:51:31.586
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:51:31.622
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:51:31.627
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/15/24 07:51:31.635
  Apr 15 07:51:31.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-6406 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Apr 15 07:51:31.943: INFO: stderr: ""
  Apr 15 07:51:31.943: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 04/15/24 07:51:31.943
  Apr 15 07:51:31.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-6406 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  E0415 07:51:32.212978      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:51:32.218: INFO: stderr: ""
  Apr 15 07:51:32.219: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/15/24 07:51:32.219
  Apr 15 07:51:32.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-6406 delete pods e2e-test-httpd-pod'
  E0415 07:51:33.214364      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:34.215739      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:51:34.404: INFO: stderr: ""
  Apr 15 07:51:34.405: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 15 07:51:34.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6406" for this suite. @ 04/15/24 07:51:34.425
• [2.862 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 04/15/24 07:51:34.452
  Apr 15 07:51:34.453: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/15/24 07:51:34.461
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:51:34.493
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:51:34.499
  STEP: fetching the /apis discovery document @ 04/15/24 07:51:34.509
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 04/15/24 07:51:34.513
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 04/15/24 07:51:34.513
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 04/15/24 07:51:34.514
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 04/15/24 07:51:34.516
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 04/15/24 07:51:34.516
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 04/15/24 07:51:34.518
  Apr 15 07:51:34.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6608" for this suite. @ 04/15/24 07:51:34.535
• [0.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 04/15/24 07:51:34.559
  Apr 15 07:51:34.559: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 07:51:34.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:51:34.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:51:34.605
  STEP: Creating pod liveness-97c67a32-5582-42b3-8e7f-b7fb7fb3b377 in namespace container-probe-6672 @ 04/15/24 07:51:34.619
  E0415 07:51:35.214255      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:36.215128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:51:36.682: INFO: Started pod liveness-97c67a32-5582-42b3-8e7f-b7fb7fb3b377 in namespace container-probe-6672
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/15/24 07:51:36.682
  Apr 15 07:51:36.691: INFO: Initial restart count of pod liveness-97c67a32-5582-42b3-8e7f-b7fb7fb3b377 is 0
  E0415 07:51:37.215702      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:38.216180      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:39.216357      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:40.217214      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:41.217890      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:42.218182      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:43.219112      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:44.219334      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:45.219441      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:46.220104      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:47.221026      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:48.221658      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:49.221883      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:50.222568      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:51.222963      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:52.223236      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:53.223746      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:54.223675      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:55.224048      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:56.224422      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:51:56.822: INFO: Restart count of pod container-probe-6672/liveness-97c67a32-5582-42b3-8e7f-b7fb7fb3b377 is now 1 (20.130602953s elapsed)
  E0415 07:51:57.225159      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:58.225992      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:51:59.226478      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:00.227029      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:01.227207      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:02.227548      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:03.228342      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:04.229636      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:05.229769      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:06.230451      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:07.231562      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:08.231753      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:09.232023      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:10.232351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:11.233205      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:12.233355      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:13.234415      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:14.235636      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:15.235914      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:16.236921      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:52:16.921: INFO: Restart count of pod container-probe-6672/liveness-97c67a32-5582-42b3-8e7f-b7fb7fb3b377 is now 2 (40.229280619s elapsed)
  E0415 07:52:17.237115      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:18.238216      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:19.237818      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:20.238052      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:21.238589      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:22.239171      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:23.239418      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:24.240523      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:25.241249      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:26.242115      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:27.243040      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:28.243785      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:29.243883      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:30.244340      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:31.245318      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:32.245604      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:33.245667      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:34.245829      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:35.246606      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:36.247704      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:52:37.061: INFO: Restart count of pod container-probe-6672/liveness-97c67a32-5582-42b3-8e7f-b7fb7fb3b377 is now 3 (1m0.369619686s elapsed)
  E0415 07:52:37.248796      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:38.249299      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:39.249860      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:40.250505      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:41.251361      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:42.252428      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:43.252489      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:44.253170      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:45.253309      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:46.253439      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:47.254420      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:48.255334      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:49.256423      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:50.257287      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:51.257522      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:52.258623      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:53.258598      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:54.259123      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:55.260203      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:56.260597      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:52:57.162: INFO: Restart count of pod container-probe-6672/liveness-97c67a32-5582-42b3-8e7f-b7fb7fb3b377 is now 4 (1m20.470493163s elapsed)
  E0415 07:52:57.261673      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:58.262634      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:52:59.262834      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:00.263604      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:01.263960      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:02.264042      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:03.264422      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:04.264839      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:05.265649      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:06.266323      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:07.266507      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:08.267410      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:09.268357      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:10.268574      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:11.269659      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:12.270056      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:13.270967      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:14.270985      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:15.271694      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:16.271891      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:17.272679      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:18.273079      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:19.273169      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:20.273919      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:21.274896      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:22.276019      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:23.277138      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:24.280808      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:25.278890      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:26.278256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:27.278462      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:28.279597      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:29.280372      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:30.280811      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:31.280957      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:32.281310      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:33.282240      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:34.282557      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:35.282937      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:36.283435      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:37.283616      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:38.284115      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:39.284283      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:40.284509      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:41.284998      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:42.286240      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:43.286642      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:44.286835      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:45.286915      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:46.287368      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:47.287462      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:48.287804      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:49.289261      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:50.289236      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:51.290105      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:52.291630      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:53.291673      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:54.292055      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:55.292324      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:56.292868      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:57.293202      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:53:57.471: INFO: Restart count of pod container-probe-6672/liveness-97c67a32-5582-42b3-8e7f-b7fb7fb3b377 is now 5 (2m20.779577977s elapsed)
  Apr 15 07:53:57.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 07:53:57.486
  STEP: Destroying namespace "container-probe-6672" for this suite. @ 04/15/24 07:53:57.512
• [142.971 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 04/15/24 07:53:57.534
  Apr 15 07:53:57.534: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 07:53:57.538
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:53:57.571
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:53:57.577
  STEP: Creating configMap with name projected-configmap-test-volume-c2fd1c79-7c75-4a9c-93a2-0176c99eabe8 @ 04/15/24 07:53:57.582
  STEP: Creating a pod to test consume configMaps @ 04/15/24 07:53:57.592
  E0415 07:53:58.294287      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:53:59.294442      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:00.295887      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:01.295406      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 07:54:01.636
  Apr 15 07:54:01.643: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-projected-configmaps-07a28868-2754-472c-8c8f-a5197dc6d8ed container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 07:54:01.675
  Apr 15 07:54:01.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3115" for this suite. @ 04/15/24 07:54:01.716
• [4.195 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 04/15/24 07:54:01.733
  Apr 15 07:54:01.733: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename svcaccounts @ 04/15/24 07:54:01.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:54:01.764
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:54:01.769
  STEP: Creating ServiceAccount "e2e-sa-knhkp"  @ 04/15/24 07:54:01.776
  Apr 15 07:54:01.787: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-knhkp"  @ 04/15/24 07:54:01.787
  Apr 15 07:54:01.805: INFO: AutomountServiceAccountToken: true
  Apr 15 07:54:01.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8882" for this suite. @ 04/15/24 07:54:01.817
• [0.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 04/15/24 07:54:01.839
  Apr 15 07:54:01.839: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename services @ 04/15/24 07:54:01.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:54:01.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:54:01.884
  STEP: creating an Endpoint @ 04/15/24 07:54:01.897
  STEP: waiting for available Endpoint @ 04/15/24 07:54:01.909
  STEP: listing all Endpoints @ 04/15/24 07:54:01.913
  STEP: updating the Endpoint @ 04/15/24 07:54:01.921
  STEP: fetching the Endpoint @ 04/15/24 07:54:01.935
  STEP: patching the Endpoint @ 04/15/24 07:54:01.942
  STEP: fetching the Endpoint @ 04/15/24 07:54:01.966
  STEP: deleting the Endpoint by Collection @ 04/15/24 07:54:01.973
  STEP: waiting for Endpoint deletion @ 04/15/24 07:54:01.995
  STEP: fetching the Endpoint @ 04/15/24 07:54:01.999
  Apr 15 07:54:02.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9937" for this suite. @ 04/15/24 07:54:02.037
• [0.214 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 04/15/24 07:54:02.055
  Apr 15 07:54:02.055: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename runtimeclass @ 04/15/24 07:54:02.058
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:54:02.087
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:54:02.092
  STEP: Deleting RuntimeClass runtimeclass-4563-delete-me @ 04/15/24 07:54:02.109
  STEP: Waiting for the RuntimeClass to disappear @ 04/15/24 07:54:02.125
  Apr 15 07:54:02.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-4563" for this suite. @ 04/15/24 07:54:02.156
• [0.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 04/15/24 07:54:02.171
  Apr 15 07:54:02.171: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 07:54:02.174
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:54:02.198
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:54:02.204
  STEP: Creating configMap with name cm-test-opt-del-968f4149-abaa-455e-8301-74bee86cb4d1 @ 04/15/24 07:54:02.218
  STEP: Creating configMap with name cm-test-opt-upd-93c6f9a2-3c9e-4aaa-908a-12857dac19e8 @ 04/15/24 07:54:02.229
  STEP: Creating the pod @ 04/15/24 07:54:02.238
  E0415 07:54:02.296299      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:03.298404      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:04.299965      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-968f4149-abaa-455e-8301-74bee86cb4d1 @ 04/15/24 07:54:04.33
  STEP: Updating configmap cm-test-opt-upd-93c6f9a2-3c9e-4aaa-908a-12857dac19e8 @ 04/15/24 07:54:04.343
  STEP: Creating configMap with name cm-test-opt-create-e968ab8c-e512-46d0-84d3-1077ae532c32 @ 04/15/24 07:54:04.353
  STEP: waiting to observe update in volume @ 04/15/24 07:54:04.36
  E0415 07:54:05.300213      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:06.300764      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:54:06.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5525" for this suite. @ 04/15/24 07:54:06.432
• [4.275 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 04/15/24 07:54:06.448
  Apr 15 07:54:06.448: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 07:54:06.453
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:54:06.506
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:54:06.513
  STEP: validating cluster-info @ 04/15/24 07:54:06.52
  Apr 15 07:54:06.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-4167 cluster-info'
  Apr 15 07:54:06.758: INFO: stderr: ""
  Apr 15 07:54:06.758: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Apr 15 07:54:06.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4167" for this suite. @ 04/15/24 07:54:06.776
• [0.380 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 04/15/24 07:54:06.836
  Apr 15 07:54:06.837: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename secrets @ 04/15/24 07:54:06.84
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:54:06.873
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:54:06.88
  STEP: Creating secret with name secret-test-map-299b1d9c-eb29-4979-a4e1-3aba3b3a002c @ 04/15/24 07:54:06.89
  STEP: Creating a pod to test consume secrets @ 04/15/24 07:54:06.901
  E0415 07:54:07.301746      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:08.303617      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:09.303257      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:10.303548      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 07:54:10.97
  Apr 15 07:54:10.980: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-secrets-ebe4f594-c3a1-49fc-9da5-0e5106410d36 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 07:54:10.997
  Apr 15 07:54:11.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9791" for this suite. @ 04/15/24 07:54:11.046
• [4.227 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 04/15/24 07:54:11.077
  Apr 15 07:54:11.078: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 07:54:11.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:54:11.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:54:11.125
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 04/15/24 07:54:11.133
  E0415 07:54:11.304173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:12.305160      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:13.305975      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:14.306169      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 07:54:15.193
  Apr 15 07:54:15.202: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-5c7501af-fd1c-47f0-aecc-5ec6dc54f4b6 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 07:54:15.22
  Apr 15 07:54:15.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3017" for this suite. @ 04/15/24 07:54:15.266
• [4.204 seconds]
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 04/15/24 07:54:15.284
  Apr 15 07:54:15.284: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename svcaccounts @ 04/15/24 07:54:15.288
  E0415 07:54:15.306741      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:54:15.319
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:54:15.326
  Apr 15 07:54:15.364: INFO: created pod
  E0415 07:54:16.307681      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:17.308493      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:18.309301      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:19.309756      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 07:54:19.405
  E0415 07:54:20.310424      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:21.310681      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:22.311873      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:23.312205      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:24.312262      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:25.313146      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:26.314314      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:27.314432      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:28.314672      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:29.315160      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:30.315882      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:31.316032      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:32.316386      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:33.317970      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:34.318256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:35.318486      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:36.319639      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:37.319768      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:38.319961      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:39.320252      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:40.321103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:41.321283      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:42.322227      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:43.322313      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:44.323125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:45.323317      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:46.324362      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:47.325133      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:48.325861      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:49.326678      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:54:49.406: INFO: polling logs
  Apr 15 07:54:49.440: INFO: Pod logs: 
  I0415 07:54:16.239379       1 log.go:245] OK: Got token
  I0415 07:54:16.244229       1 log.go:245] validating with in-cluster discovery
  I0415 07:54:16.247805       1 log.go:245] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0415 07:54:16.247942       1 log.go:245] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-3437:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:(*jwt.NumericDate)(0xc00023d5f0), NotBefore:(*jwt.NumericDate)(0xc00023d6d8), IssuedAt:(*jwt.NumericDate)(0xc00023d600), ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3437", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e942f96b-a6bf-4730-ac25-986d43ad32fb"}}}
  I0415 07:54:16.297949       1 log.go:245] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0415 07:54:16.313254       1 log.go:245] OK: Validated signature on JWT
  I0415 07:54:16.313434       1 log.go:245] OK: Got valid claims from token!
  I0415 07:54:16.313482       1 log.go:245] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-3437:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:(*jwt.NumericDate)(0xc0000c0a40), NotBefore:(*jwt.NumericDate)(0xc0000c0a68), IssuedAt:(*jwt.NumericDate)(0xc0000c0a48), ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3437", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e942f96b-a6bf-4730-ac25-986d43ad32fb"}}}

  Apr 15 07:54:49.440: INFO: completed pod
  Apr 15 07:54:49.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3437" for this suite. @ 04/15/24 07:54:49.481
• [34.212 seconds]
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:638
  STEP: Creating a kubernetes client @ 04/15/24 07:54:49.499
  Apr 15 07:54:49.500: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename statefulset @ 04/15/24 07:54:49.503
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:54:49.537
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:54:49.543
  STEP: Creating service test in namespace statefulset-8710 @ 04/15/24 07:54:49.549
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 04/15/24 07:54:49.56
  STEP: Creating stateful set ss in namespace statefulset-8710 @ 04/15/24 07:54:49.569
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8710 @ 04/15/24 07:54:49.584
  Apr 15 07:54:49.594: INFO: Found 0 stateful pods, waiting for 1
  E0415 07:54:50.327592      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:51.327893      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:52.328949      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:53.329697      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:54.330401      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:55.330968      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:56.331342      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:57.331686      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:58.331751      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:54:59.332354      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:54:59.603: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 04/15/24 07:54:59.603
  Apr 15 07:54:59.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-8710 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 07:54:59.968: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 07:54:59.968: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 07:54:59.968: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 15 07:54:59.979: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0415 07:55:00.332482      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:01.332911      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:02.333033      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:03.333245      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:04.333476      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:05.333652      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:06.334471      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:07.335297      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:08.334943      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:09.335469      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:09.990: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 15 07:55:09.990: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 07:55:10.043: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999256s
  E0415 07:55:10.336461      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:11.055: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.979977196s
  E0415 07:55:11.339609      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:12.064: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.96749824s
  E0415 07:55:12.338988      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:13.079: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.958531145s
  E0415 07:55:13.339492      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:14.091: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.944439493s
  E0415 07:55:14.340834      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:15.105: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.931095157s
  E0415 07:55:15.340550      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:16.119: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.915738147s
  E0415 07:55:16.341022      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:17.131: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.902905125s
  E0415 07:55:17.341532      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:18.141: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.891656086s
  E0415 07:55:18.342604      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:19.150: INFO: Verifying statefulset ss doesn't scale past 1 for another 882.0095ms
  E0415 07:55:19.343261      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8710 @ 04/15/24 07:55:20.151
  Apr 15 07:55:20.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-8710 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0415 07:55:20.343872      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:20.604: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 15 07:55:20.604: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 15 07:55:20.604: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 15 07:55:20.616: INFO: Found 1 stateful pods, waiting for 3
  E0415 07:55:21.344407      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:22.344662      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:23.344983      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:24.345475      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:25.345701      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:26.346040      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:27.346009      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:28.346254      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:29.346385      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:30.348595      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:30.634: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 07:55:30.635: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 07:55:30.636: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 04/15/24 07:55:30.638
  STEP: Scale down will halt with unhealthy stateful pod @ 04/15/24 07:55:30.638
  Apr 15 07:55:30.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-8710 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 07:55:30.966: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 07:55:30.966: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 07:55:30.966: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 15 07:55:30.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-8710 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0415 07:55:31.347863      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:31.377: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 07:55:31.377: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 07:55:31.377: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 15 07:55:31.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-8710 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 07:55:31.775: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 07:55:31.775: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 07:55:31.776: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 15 07:55:31.776: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 07:55:31.787: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
  E0415 07:55:32.348080      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:33.348203      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:34.348490      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:35.348659      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:36.349579      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:37.350430      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:38.350621      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:39.350954      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:40.351092      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:41.351224      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:41.818: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 15 07:55:41.819: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Apr 15 07:55:41.819: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Apr 15 07:55:41.855: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999504s
  E0415 07:55:42.352343      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:42.866: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.985590293s
  E0415 07:55:43.353316      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:43.876: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973576617s
  E0415 07:55:44.354159      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:44.891: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.963462809s
  E0415 07:55:45.355273      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:45.904: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.949266207s
  E0415 07:55:46.355550      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:46.917: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.934728371s
  E0415 07:55:47.355596      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:47.930: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.922465292s
  E0415 07:55:48.356364      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:48.940: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.910981209s
  E0415 07:55:49.356512      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:49.951: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.899542078s
  E0415 07:55:50.357366      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:50.963: INFO: Verifying statefulset ss doesn't scale past 3 for another 888.661145ms
  E0415 07:55:51.358312      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8710 @ 04/15/24 07:55:51.963
  Apr 15 07:55:51.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-8710 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 15 07:55:52.307: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 15 07:55:52.307: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 15 07:55:52.307: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 15 07:55:52.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-8710 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0415 07:55:52.358702      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:55:52.639: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 15 07:55:52.639: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 15 07:55:52.639: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 15 07:55:52.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-8710 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 15 07:55:52.965: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 15 07:55:52.965: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 15 07:55:52.965: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 15 07:55:52.965: INFO: Scaling statefulset ss to 0
  E0415 07:55:53.358776      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:54.359949      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:55.361563      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:56.362425      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:57.363506      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:58.364417      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:55:59.365121      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:00.365183      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:01.365655      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:02.365824      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 04/15/24 07:56:03.032
  Apr 15 07:56:03.034: INFO: Deleting all statefulset in ns statefulset-8710
  Apr 15 07:56:03.041: INFO: Scaling statefulset ss to 0
  Apr 15 07:56:03.068: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 07:56:03.075: INFO: Deleting statefulset ss
  Apr 15 07:56:03.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8710" for this suite. @ 04/15/24 07:56:03.124
• [73.645 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 04/15/24 07:56:03.158
  Apr 15 07:56:03.158: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/15/24 07:56:03.165
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:56:03.205
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:56:03.212
  STEP: set up a multi version CRD @ 04/15/24 07:56:03.219
  Apr 15 07:56:03.221: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  E0415 07:56:03.366053      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:04.366858      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:05.367786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:06.369076      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:07.368540      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: rename a version @ 04/15/24 07:56:08.04
  STEP: check the new version name is served @ 04/15/24 07:56:08.071
  E0415 07:56:08.368624      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:09.368959      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the old version name is removed @ 04/15/24 07:56:09.974
  E0415 07:56:10.370726      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 04/15/24 07:56:10.985
  E0415 07:56:11.371287      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:12.371578      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:13.372265      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:14.373382      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:56:15.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4208" for this suite. @ 04/15/24 07:56:15.167
• [12.023 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 04/15/24 07:56:15.181
  Apr 15 07:56:15.181: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 07:56:15.183
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:56:15.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:56:15.231
  STEP: creating a replication controller @ 04/15/24 07:56:15.24
  Apr 15 07:56:15.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 create -f -'
  E0415 07:56:15.373740      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:56:16.085: INFO: stderr: ""
  Apr 15 07:56:16.086: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/15/24 07:56:16.087
  Apr 15 07:56:16.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 15 07:56:16.310: INFO: stderr: ""
  Apr 15 07:56:16.310: INFO: stdout: "update-demo-nautilus-hcgss update-demo-nautilus-zf2rr "
  Apr 15 07:56:16.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 get pods update-demo-nautilus-hcgss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0415 07:56:16.374446      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:56:16.506: INFO: stderr: ""
  Apr 15 07:56:16.506: INFO: stdout: ""
  Apr 15 07:56:16.506: INFO: update-demo-nautilus-hcgss is created but not running
  E0415 07:56:17.374728      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:18.375043      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:19.375227      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:20.375431      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:21.375640      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:56:21.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 15 07:56:21.702: INFO: stderr: ""
  Apr 15 07:56:21.702: INFO: stdout: "update-demo-nautilus-hcgss update-demo-nautilus-zf2rr "
  Apr 15 07:56:21.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 get pods update-demo-nautilus-hcgss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 15 07:56:21.858: INFO: stderr: ""
  Apr 15 07:56:21.858: INFO: stdout: "true"
  Apr 15 07:56:21.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 get pods update-demo-nautilus-hcgss -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 15 07:56:22.050: INFO: stderr: ""
  Apr 15 07:56:22.050: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 15 07:56:22.050: INFO: validating pod update-demo-nautilus-hcgss
  Apr 15 07:56:22.090: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 15 07:56:22.090: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 15 07:56:22.090: INFO: update-demo-nautilus-hcgss is verified up and running
  Apr 15 07:56:22.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 get pods update-demo-nautilus-zf2rr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 15 07:56:22.283: INFO: stderr: ""
  Apr 15 07:56:22.283: INFO: stdout: "true"
  Apr 15 07:56:22.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 get pods update-demo-nautilus-zf2rr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E0415 07:56:22.377011      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:56:22.475: INFO: stderr: ""
  Apr 15 07:56:22.475: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 15 07:56:22.475: INFO: validating pod update-demo-nautilus-zf2rr
  Apr 15 07:56:22.519: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 15 07:56:22.520: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 15 07:56:22.520: INFO: update-demo-nautilus-zf2rr is verified up and running
  STEP: scaling down the replication controller @ 04/15/24 07:56:22.52
  Apr 15 07:56:22.553: INFO: scanned /root for discovery docs: <nil>
  Apr 15 07:56:22.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0415 07:56:23.377129      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:56:23.834: INFO: stderr: ""
  Apr 15 07:56:23.834: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/15/24 07:56:23.834
  Apr 15 07:56:23.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 15 07:56:24.040: INFO: stderr: ""
  Apr 15 07:56:24.040: INFO: stdout: "update-demo-nautilus-zf2rr "
  Apr 15 07:56:24.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 get pods update-demo-nautilus-zf2rr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 15 07:56:24.213: INFO: stderr: ""
  Apr 15 07:56:24.213: INFO: stdout: "true"
  Apr 15 07:56:24.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 get pods update-demo-nautilus-zf2rr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 15 07:56:24.376: INFO: stderr: ""
  Apr 15 07:56:24.376: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 15 07:56:24.376: INFO: validating pod update-demo-nautilus-zf2rr
  E0415 07:56:24.377074      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:56:24.385: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 15 07:56:24.385: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 15 07:56:24.385: INFO: update-demo-nautilus-zf2rr is verified up and running
  STEP: scaling up the replication controller @ 04/15/24 07:56:24.385
  Apr 15 07:56:24.397: INFO: scanned /root for discovery docs: <nil>
  Apr 15 07:56:24.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0415 07:56:25.377379      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:56:25.674: INFO: stderr: ""
  Apr 15 07:56:25.674: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/15/24 07:56:25.674
  Apr 15 07:56:25.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 15 07:56:25.864: INFO: stderr: ""
  Apr 15 07:56:25.865: INFO: stdout: "update-demo-nautilus-ht8rl update-demo-nautilus-zf2rr "
  Apr 15 07:56:25.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 get pods update-demo-nautilus-ht8rl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 15 07:56:26.031: INFO: stderr: ""
  Apr 15 07:56:26.031: INFO: stdout: "true"
  Apr 15 07:56:26.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 get pods update-demo-nautilus-ht8rl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 15 07:56:26.187: INFO: stderr: ""
  Apr 15 07:56:26.188: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 15 07:56:26.188: INFO: validating pod update-demo-nautilus-ht8rl
  Apr 15 07:56:26.203: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 15 07:56:26.203: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 15 07:56:26.203: INFO: update-demo-nautilus-ht8rl is verified up and running
  Apr 15 07:56:26.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 get pods update-demo-nautilus-zf2rr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 15 07:56:26.357: INFO: stderr: ""
  Apr 15 07:56:26.357: INFO: stdout: "true"
  Apr 15 07:56:26.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 get pods update-demo-nautilus-zf2rr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E0415 07:56:26.377848      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:56:26.518: INFO: stderr: ""
  Apr 15 07:56:26.518: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 15 07:56:26.518: INFO: validating pod update-demo-nautilus-zf2rr
  Apr 15 07:56:26.528: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 15 07:56:26.528: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 15 07:56:26.528: INFO: update-demo-nautilus-zf2rr is verified up and running
  STEP: using delete to clean up resources @ 04/15/24 07:56:26.528
  Apr 15 07:56:26.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 delete --grace-period=0 --force -f -'
  Apr 15 07:56:26.678: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 15 07:56:26.678: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Apr 15 07:56:26.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 get rc,svc -l name=update-demo --no-headers'
  Apr 15 07:56:26.933: INFO: stderr: "No resources found in kubectl-3666 namespace.\n"
  Apr 15 07:56:26.933: INFO: stdout: ""
  Apr 15 07:56:26.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-3666 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 15 07:56:27.179: INFO: stderr: ""
  Apr 15 07:56:27.179: INFO: stdout: ""
  Apr 15 07:56:27.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3666" for this suite. @ 04/15/24 07:56:27.194
• [12.024 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 04/15/24 07:56:27.209
  Apr 15 07:56:27.209: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename podtemplate @ 04/15/24 07:56:27.212
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:56:27.253
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:56:27.26
  STEP: Create a pod template @ 04/15/24 07:56:27.266
  STEP: Replace a pod template @ 04/15/24 07:56:27.28
  Apr 15 07:56:27.300: INFO: Found updated podtemplate annotation: "true"

  Apr 15 07:56:27.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-643" for this suite. @ 04/15/24 07:56:27.31
• [0.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 04/15/24 07:56:27.333
  Apr 15 07:56:27.333: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename security-context-test @ 04/15/24 07:56:27.336
  E0415 07:56:27.378861      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:56:27.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:56:27.388
  E0415 07:56:28.379152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:29.379602      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:30.379980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:31.380624      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:56:31.505: INFO: Got logs for pod "busybox-privileged-false-e97baae0-7a7c-4262-be09-5bc5df0dfc03": "ip: RTNETLINK answers: Operation not permitted\n"
  Apr 15 07:56:31.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-2750" for this suite. @ 04/15/24 07:56:31.517
• [4.205 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 04/15/24 07:56:31.54
  Apr 15 07:56:31.541: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename var-expansion @ 04/15/24 07:56:31.543
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:56:31.579
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:56:31.587
  STEP: Creating a pod to test env composition @ 04/15/24 07:56:31.594
  E0415 07:56:32.382471      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:33.381786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:34.381930      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:35.381920      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 07:56:35.644
  Apr 15 07:56:35.652: INFO: Trying to get logs from node zaigh3ewotoh-3 pod var-expansion-20499a2f-56a5-454b-a62b-83b50239a1e7 container dapi-container: <nil>
  STEP: delete the pod @ 04/15/24 07:56:35.67
  Apr 15 07:56:35.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4595" for this suite. @ 04/15/24 07:56:35.719
• [4.193 seconds]
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 04/15/24 07:56:35.736
  Apr 15 07:56:35.736: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubelet-test @ 04/15/24 07:56:35.744
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:56:35.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:56:35.793
  E0415 07:56:36.383096      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:37.383264      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:56:37.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2516" for this suite. @ 04/15/24 07:56:37.898
• [2.177 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 04/15/24 07:56:37.916
  Apr 15 07:56:37.916: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename security-context-test @ 04/15/24 07:56:37.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:56:37.961
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:56:37.968
  E0415 07:56:38.384364      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:39.384369      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:56:40.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5947" for this suite. @ 04/15/24 07:56:40.031
• [2.135 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 04/15/24 07:56:40.059
  Apr 15 07:56:40.059: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename webhook @ 04/15/24 07:56:40.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:56:40.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:56:40.12
  STEP: Setting up server cert @ 04/15/24 07:56:40.183
  E0415 07:56:40.385271      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:41.385439      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 07:56:41.426
  STEP: Deploying the webhook pod @ 04/15/24 07:56:41.442
  STEP: Wait for the deployment to be ready @ 04/15/24 07:56:41.465
  Apr 15 07:56:41.482: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0415 07:56:42.386170      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:43.386625      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/15/24 07:56:43.512
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 07:56:43.535
  E0415 07:56:44.386614      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:56:44.536: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 04/15/24 07:56:44.673
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/15/24 07:56:44.74
  STEP: Deleting the collection of validation webhooks @ 04/15/24 07:56:44.791
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/15/24 07:56:44.925
  Apr 15 07:56:44.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5989" for this suite. @ 04/15/24 07:56:45.163
  STEP: Destroying namespace "webhook-markers-6737" for this suite. @ 04/15/24 07:56:45.184
• [5.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 04/15/24 07:56:45.211
  Apr 15 07:56:45.211: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename lease-test @ 04/15/24 07:56:45.215
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:56:45.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:56:45.265
  E0415 07:56:45.387433      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:56:45.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-4886" for this suite. @ 04/15/24 07:56:45.473
• [0.278 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 04/15/24 07:56:45.492
  Apr 15 07:56:45.492: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename cronjob @ 04/15/24 07:56:45.496
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:56:45.527
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:56:45.533
  STEP: Creating a ReplaceConcurrent cronjob @ 04/15/24 07:56:45.539
  STEP: Ensuring a job is scheduled @ 04/15/24 07:56:45.565
  E0415 07:56:46.388440      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:47.388383      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:48.388939      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:49.389147      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:50.390548      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:51.391419      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:52.391940      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:53.392241      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:54.392627      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:55.393293      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:56.394117      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:57.394269      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:58.394520      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:56:59.395023      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:00.395017      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:01.395196      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 04/15/24 07:57:01.572
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 04/15/24 07:57:01.579
  STEP: Ensuring the job is replaced with a new one @ 04/15/24 07:57:01.588
  E0415 07:57:02.396319      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:03.398984      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:04.397241      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:05.397448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:06.398235      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:07.398602      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:08.398842      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:09.398997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:10.399700      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:11.400110      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:12.400615      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:13.400671      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:14.400947      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:15.401624      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:16.402144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:17.402612      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:18.403285      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:19.403203      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:20.403934      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:21.403930      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:22.403958      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:23.404338      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:24.405072      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:25.405156      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:26.406292      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:27.406683      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:28.406848      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:29.407053      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:30.408428      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:31.409204      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:32.409323      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:33.410351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:34.410467      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:35.411492      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:36.412275      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:37.412784      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:38.412969      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:39.414165      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:40.414321      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:41.414779      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:42.415281      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:43.415623      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:44.416436      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:45.416482      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:46.417231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:47.418301      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:48.418646      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:49.419049      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:50.419405      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:51.419452      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:52.420366      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:53.420424      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:54.420743      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:55.421398      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:56.421689      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:57.421748      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:58.421914      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:57:59.424204      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:00.423705      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:01.423901      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 04/15/24 07:58:01.597
  Apr 15 07:58:01.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-980" for this suite. @ 04/15/24 07:58:01.629
• [76.155 seconds]
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 04/15/24 07:58:01.648
  Apr 15 07:58:01.648: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename svcaccounts @ 04/15/24 07:58:01.652
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:58:01.696
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:58:01.702
  STEP: creating a ServiceAccount @ 04/15/24 07:58:01.708
  STEP: watching for the ServiceAccount to be added @ 04/15/24 07:58:01.728
  STEP: patching the ServiceAccount @ 04/15/24 07:58:01.734
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 04/15/24 07:58:01.75
  STEP: deleting the ServiceAccount @ 04/15/24 07:58:01.759
  Apr 15 07:58:01.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1661" for this suite. @ 04/15/24 07:58:01.792
• [0.156 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 04/15/24 07:58:01.812
  Apr 15 07:58:01.812: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/15/24 07:58:01.815
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:58:01.849
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:58:01.853
  Apr 15 07:58:01.858: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  E0415 07:58:02.425166      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:03.426079      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:04.426553      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:05.427058      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:06.427600      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:07.427785      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:58:08.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-1717" for this suite. @ 04/15/24 07:58:08.27
• [6.470 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 04/15/24 07:58:08.29
  Apr 15 07:58:08.291: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename projected @ 04/15/24 07:58:08.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:58:08.327
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:58:08.332
  STEP: Creating configMap with name configmap-projected-all-test-volume-919b7566-d3aa-484e-9c4f-2702992b06a9 @ 04/15/24 07:58:08.337
  STEP: Creating secret with name secret-projected-all-test-volume-bc1adf19-122a-4da2-8f2e-4c30eb9225fa @ 04/15/24 07:58:08.346
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 04/15/24 07:58:08.356
  E0415 07:58:08.428382      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:09.429166      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 07:58:10.395
  Apr 15 07:58:10.402: INFO: Trying to get logs from node zaigh3ewotoh-3 pod projected-volume-8ecd1e92-9658-42c7-88af-b4cd6515cf09 container projected-all-volume-test: <nil>
  E0415 07:58:10.429525      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 04/15/24 07:58:10.432
  Apr 15 07:58:10.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3772" for this suite. @ 04/15/24 07:58:10.468
• [2.189 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 04/15/24 07:58:10.482
  Apr 15 07:58:10.482: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename deployment @ 04/15/24 07:58:10.484
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:58:10.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:58:10.534
  Apr 15 07:58:10.539: INFO: Creating deployment "webserver-deployment"
  Apr 15 07:58:10.550: INFO: Waiting for observed generation 1
  E0415 07:58:11.430383      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:12.430445      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:58:12.596: INFO: Waiting for all required pods to come up
  Apr 15 07:58:12.610: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 04/15/24 07:58:12.611
  E0415 07:58:13.430765      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:14.430950      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:58:14.649: INFO: Waiting for deployment "webserver-deployment" to complete
  Apr 15 07:58:14.673: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Apr 15 07:58:14.705: INFO: Updating deployment webserver-deployment
  Apr 15 07:58:14.705: INFO: Waiting for observed generation 2
  E0415 07:58:15.434849      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:16.431233      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:58:16.726: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Apr 15 07:58:16.733: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Apr 15 07:58:16.739: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Apr 15 07:58:16.754: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Apr 15 07:58:16.755: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Apr 15 07:58:16.760: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Apr 15 07:58:16.769: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Apr 15 07:58:16.769: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Apr 15 07:58:16.782: INFO: Updating deployment webserver-deployment
  Apr 15 07:58:16.782: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Apr 15 07:58:16.796: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Apr 15 07:58:16.803: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Apr 15 07:58:16.830: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-5255  79a1a343-b4f3-4e65-8949-5f574eba0f59 175540 3 2024-04-15 07:58:10 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2024-04-15 07:58:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2024-04-15 07:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0068508f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-04-15 07:58:13 +0000 UTC,LastTransitionTime:2024-04-15 07:58:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2024-04-15 07:58:15 +0000 UTC,LastTransitionTime:2024-04-15 07:58:10 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Apr 15 07:58:16.849: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-5255  c9f973af-c26e-41df-96d6-ae2dff296e88 175544 3 2024-04-15 07:58:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 79a1a343-b4f3-4e65-8949-5f574eba0f59 0xc0067e98d7 0xc0067e98d8}] [] [{kube-controller-manager Update apps/v1 2024-04-15 07:58:15 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2024-04-15 07:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79a1a343-b4f3-4e65-8949-5f574eba0f59\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067e9978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 07:58:16.849: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Apr 15 07:58:16.849: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-5255  c3c77234-7c6d-451f-9829-4239a3241d49 175541 3 2024-04-15 07:58:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 79a1a343-b4f3-4e65-8949-5f574eba0f59 0xc0067e97d7 0xc0067e97d8}] [] [{kube-controller-manager Update apps/v1 2024-04-15 07:58:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2024-04-15 07:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79a1a343-b4f3-4e65-8949-5f574eba0f59\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067e9878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 07:58:16.868: INFO: Pod "webserver-deployment-67bd4bf6dc-crm7z" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-crm7z webserver-deployment-67bd4bf6dc- deployment-5255  0b9b2ae8-81e0-4ae4-8ab4-2a8cf449983d 175545 0 2024-04-15 07:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c3c77234-7c6d-451f-9829-4239a3241d49 0xc0067e9e97 0xc0067e9e98}] [] [{kube-controller-manager Update v1 2024-04-15 07:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3c77234-7c6d-451f-9829-4239a3241d49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r4q59,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r4q59,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:58:16.869: INFO: Pod "webserver-deployment-67bd4bf6dc-d6npc" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-d6npc webserver-deployment-67bd4bf6dc- deployment-5255  cd5cb499-68c5-4a2c-b4a3-f663c95ed2b0 175457 0 2024-04-15 07:58:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c3c77234-7c6d-451f-9829-4239a3241d49 0xc004350020 0xc004350021}] [] [{kube-controller-manager Update v1 2024-04-15 07:58:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3c77234-7c6d-451f-9829-4239a3241d49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:58:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6pvs8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6pvs8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.74,PodIP:10.233.64.2,StartTime:2024-04-15 07:58:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 07:58:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://6947be7a529e664e526a034c8c6c649fdbeb19736118f2b978f5d5bdbc04eacc,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.2,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:58:16.871: INFO: Pod "webserver-deployment-67bd4bf6dc-hqwt5" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hqwt5 webserver-deployment-67bd4bf6dc- deployment-5255  6e0bde92-2841-4484-9e06-a6a7ae769624 175459 0 2024-04-15 07:58:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c3c77234-7c6d-451f-9829-4239a3241d49 0xc0043505e7 0xc0043505e8}] [] [{kube-controller-manager Update v1 2024-04-15 07:58:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3c77234-7c6d-451f-9829-4239a3241d49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:58:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.254\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7vxn6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7vxn6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.74,PodIP:10.233.64.254,StartTime:2024-04-15 07:58:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 07:58:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://dbc5eda07a96af01a6d42ae8d7d1cc3ae7d9d6e1a7896225229d91dead81bc1a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.254,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:58:16.871: INFO: Pod "webserver-deployment-67bd4bf6dc-jg5q6" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jg5q6 webserver-deployment-67bd4bf6dc- deployment-5255  2c56e2db-28a1-4668-a09a-1d3014db06ce 175439 0 2024-04-15 07:58:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c3c77234-7c6d-451f-9829-4239a3241d49 0xc004351027 0xc004351028}] [] [{kube-controller-manager Update v1 2024-04-15 07:58:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3c77234-7c6d-451f-9829-4239a3241d49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:58:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6lrbf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6lrbf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.27,PodIP:10.233.65.218,StartTime:2024-04-15 07:58:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 07:58:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://4a34ef68682e0ce053bc56e07da89c4fa6295baed41aac2c7817fd898c5968e5,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.218,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:58:16.872: INFO: Pod "webserver-deployment-67bd4bf6dc-l2jfd" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-l2jfd webserver-deployment-67bd4bf6dc- deployment-5255  269841dd-be7a-48ae-9430-ead1dd3578bd 175426 0 2024-04-15 07:58:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c3c77234-7c6d-451f-9829-4239a3241d49 0xc004351bb7 0xc004351bb8}] [] [{kube-controller-manager Update v1 2024-04-15 07:58:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3c77234-7c6d-451f-9829-4239a3241d49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:58:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f59gb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f59gb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.131,PodIP:10.233.66.112,StartTime:2024-04-15 07:58:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 07:58:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://2a518f11f2cf5744ecd1b5e6554cbaab8edb864950d8f3be01f5d7a9e5298dfb,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.112,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:58:16.872: INFO: Pod "webserver-deployment-67bd4bf6dc-qz7x9" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-qz7x9 webserver-deployment-67bd4bf6dc- deployment-5255  bfb75fd5-c96d-47f8-abac-bb523a32adb7 175456 0 2024-04-15 07:58:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c3c77234-7c6d-451f-9829-4239a3241d49 0xc004351db7 0xc004351db8}] [] [{kube-controller-manager Update v1 2024-04-15 07:58:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3c77234-7c6d-451f-9829-4239a3241d49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:58:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6xdjn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6xdjn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.74,PodIP:10.233.64.3,StartTime:2024-04-15 07:58:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 07:58:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://5f24986a618e62b2a6237e8df2e3031796489faf1647a5ace95486d0975fe44e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.3,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:58:16.873: INFO: Pod "webserver-deployment-67bd4bf6dc-t86pf" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-t86pf webserver-deployment-67bd4bf6dc- deployment-5255  eaf8c9fa-19d8-41ea-926e-aad2c6e66ac1 175441 0 2024-04-15 07:58:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c3c77234-7c6d-451f-9829-4239a3241d49 0xc004351fc7 0xc004351fc8}] [] [{kube-controller-manager Update v1 2024-04-15 07:58:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3c77234-7c6d-451f-9829-4239a3241d49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:58:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qxtrj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qxtrj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.27,PodIP:10.233.65.219,StartTime:2024-04-15 07:58:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 07:58:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://01baa429298dd540f50eec578ac166b2afde95d30c70e1963171d2fef84b5191,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.219,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:58:16.873: INFO: Pod "webserver-deployment-67bd4bf6dc-vn6vp" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vn6vp webserver-deployment-67bd4bf6dc- deployment-5255  4dc1fff1-2ba6-4e65-9c98-d30c315440d2 175444 0 2024-04-15 07:58:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c3c77234-7c6d-451f-9829-4239a3241d49 0xc0042c0bd7 0xc0042c0bd8}] [] [{kube-controller-manager Update v1 2024-04-15 07:58:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3c77234-7c6d-451f-9829-4239a3241d49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:58:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.220\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6rkwb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6rkwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.27,PodIP:10.233.65.220,StartTime:2024-04-15 07:58:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 07:58:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://417ca0e296a15a6ea31c486b953e66ae95d0789d09adbed01d711f5d3dbc3bda,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.220,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:58:16.874: INFO: Pod "webserver-deployment-67bd4bf6dc-vwpgf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vwpgf webserver-deployment-67bd4bf6dc- deployment-5255  5261b449-beb2-4f02-a70a-5432f9d66b6d 175551 0 2024-04-15 07:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c3c77234-7c6d-451f-9829-4239a3241d49 0xc0042c1057 0xc0042c1058}] [] [{kube-controller-manager Update v1 2024-04-15 07:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3c77234-7c6d-451f-9829-4239a3241d49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58889,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58889,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:58:16.874: INFO: Pod "webserver-deployment-67bd4bf6dc-x6h6p" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-x6h6p webserver-deployment-67bd4bf6dc- deployment-5255  7f45b3fc-b315-43b8-914b-8eb0b343799b 175424 0 2024-04-15 07:58:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c3c77234-7c6d-451f-9829-4239a3241d49 0xc0042c14b0 0xc0042c14b1}] [] [{kube-controller-manager Update v1 2024-04-15 07:58:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3c77234-7c6d-451f-9829-4239a3241d49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:58:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b2mqj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b2mqj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.131,PodIP:10.233.66.111,StartTime:2024-04-15 07:58:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 07:58:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://8e93c678c22ddf4a8f6b1a306e4bfbbb9d3a1f2ce0684ed47d004da05807e7d3,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.111,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:58:16.874: INFO: Pod "webserver-deployment-67bd4bf6dc-xvpnf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xvpnf webserver-deployment-67bd4bf6dc- deployment-5255  c25dfbca-2c06-4ffa-be8a-1836e0f5b382 175549 0 2024-04-15 07:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc c3c77234-7c6d-451f-9829-4239a3241d49 0xc0042c1ad7 0xc0042c1ad8}] [] [{kube-controller-manager Update v1 2024-04-15 07:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3c77234-7c6d-451f-9829-4239a3241d49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-75fln,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-75fln,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:58:16.875: INFO: Pod "webserver-deployment-7b75d79cf5-846c2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-846c2 webserver-deployment-7b75d79cf5- deployment-5255  07d7dcb4-048c-40b0-afd7-2dd39d32fbcf 175509 0 2024-04-15 07:58:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 c9f973af-c26e-41df-96d6-ae2dff296e88 0xc0042c1ef0 0xc0042c1ef1}] [] [{kube-controller-manager Update v1 2024-04-15 07:58:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9f973af-c26e-41df-96d6-ae2dff296e88\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:58:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sfbrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sfbrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.131,PodIP:,StartTime:2024-04-15 07:58:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:58:16.875: INFO: Pod "webserver-deployment-7b75d79cf5-8gkd2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-8gkd2 webserver-deployment-7b75d79cf5- deployment-5255  9c3b7a25-587c-4202-9aad-69a0696638c8 175494 0 2024-04-15 07:58:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 c9f973af-c26e-41df-96d6-ae2dff296e88 0xc004128157 0xc004128158}] [] [{kube-controller-manager Update v1 2024-04-15 07:58:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9f973af-c26e-41df-96d6-ae2dff296e88\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:58:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fk4j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fk4j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.74,PodIP:,StartTime:2024-04-15 07:58:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:58:16.876: INFO: Pod "webserver-deployment-7b75d79cf5-fwjzg" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-fwjzg webserver-deployment-7b75d79cf5- deployment-5255  5aad6aa1-bfce-430d-acb1-adf40008cd53 175512 0 2024-04-15 07:58:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 c9f973af-c26e-41df-96d6-ae2dff296e88 0xc004128347 0xc004128348}] [] [{kube-controller-manager Update v1 2024-04-15 07:58:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9f973af-c26e-41df-96d6-ae2dff296e88\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:58:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8jzj8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8jzj8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.74,PodIP:,StartTime:2024-04-15 07:58:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:58:16.876: INFO: Pod "webserver-deployment-7b75d79cf5-lqhqv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lqhqv webserver-deployment-7b75d79cf5- deployment-5255  2e2d6c6c-5261-4a36-ad90-8a51a6017ac2 175490 0 2024-04-15 07:58:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 c9f973af-c26e-41df-96d6-ae2dff296e88 0xc004128537 0xc004128538}] [] [{kube-controller-manager Update v1 2024-04-15 07:58:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9f973af-c26e-41df-96d6-ae2dff296e88\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:58:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rv5t6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rv5t6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.131,PodIP:,StartTime:2024-04-15 07:58:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:58:16.877: INFO: Pod "webserver-deployment-7b75d79cf5-sb74d" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-sb74d webserver-deployment-7b75d79cf5- deployment-5255  9fe1d373-3b40-4fa2-85c5-108f51d28863 175547 0 2024-04-15 07:58:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 c9f973af-c26e-41df-96d6-ae2dff296e88 0xc004128727 0xc004128728}] [] [{kube-controller-manager Update v1 2024-04-15 07:58:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9f973af-c26e-41df-96d6-ae2dff296e88\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xvzvd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xvzvd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:58:16.877: INFO: Pod "webserver-deployment-7b75d79cf5-zsp5s" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-zsp5s webserver-deployment-7b75d79cf5- deployment-5255  9e9cc442-c25e-41f8-a4ae-b8858b08e295 175493 0 2024-04-15 07:58:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 c9f973af-c26e-41df-96d6-ae2dff296e88 0xc004128880 0xc004128881}] [] [{kube-controller-manager Update v1 2024-04-15 07:58:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9f973af-c26e-41df-96d6-ae2dff296e88\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 07:58:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qmr9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qmr9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:zaigh3ewotoh-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 07:58:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.27,PodIP:,StartTime:2024-04-15 07:58:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 07:58:16.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5255" for this suite. @ 04/15/24 07:58:16.898
• [6.433 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 04/15/24 07:58:16.918
  Apr 15 07:58:16.918: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 07:58:16.921
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:58:17.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:58:17.166
  STEP: Creating pod busybox-e78821e3-f9e3-4c21-a54b-7411c30a7146 in namespace container-probe-3347 @ 04/15/24 07:58:17.171
  E0415 07:58:17.432294      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:18.432813      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:58:19.204: INFO: Started pod busybox-e78821e3-f9e3-4c21-a54b-7411c30a7146 in namespace container-probe-3347
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/15/24 07:58:19.204
  Apr 15 07:58:19.210: INFO: Initial restart count of pod busybox-e78821e3-f9e3-4c21-a54b-7411c30a7146 is 0
  E0415 07:58:19.433821      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:20.434026      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:21.435128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:22.435284      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:23.436349      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:24.437449      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:25.437867      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:26.438149      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:27.438346      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:28.438739      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:29.439373      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:30.439750      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:31.440077      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:32.440992      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:33.442007      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:34.442491      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:35.442688      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:36.443773      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:37.444670      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:38.444851      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:39.445673      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:40.445901      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:41.446520      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:42.446697      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:43.447258      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:44.448188      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:45.448435      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:46.449453      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:47.449829      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:48.449984      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:49.450221      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:50.450626      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:51.451290      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:52.452316      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:53.453103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:54.453182      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:55.453303      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:56.454323      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:57.454761      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:58.454896      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:58:59.455910      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:00.456504      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:01.457044      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:02.458232      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:03.459089      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:04.459320      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:05.460065      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:06.460278      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:07.460949      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:08.461603      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:09.461597      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:59:09.464: INFO: Restart count of pod container-probe-3347/busybox-e78821e3-f9e3-4c21-a54b-7411c30a7146 is now 1 (50.254292241s elapsed)
  Apr 15 07:59:09.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 07:59:09.475
  STEP: Destroying namespace "container-probe-3347" for this suite. @ 04/15/24 07:59:09.509
• [52.609 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 04/15/24 07:59:09.531
  Apr 15 07:59:09.531: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename services @ 04/15/24 07:59:09.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:59:09.571
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:59:09.582
  STEP: creating a Service @ 04/15/24 07:59:09.599
  STEP: watching for the Service to be added @ 04/15/24 07:59:09.632
  Apr 15 07:59:09.640: INFO: Found Service test-service-jzjfb in namespace services-9188 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Apr 15 07:59:09.640: INFO: Service test-service-jzjfb created
  STEP: Getting /status @ 04/15/24 07:59:09.64
  Apr 15 07:59:09.650: INFO: Service test-service-jzjfb has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 04/15/24 07:59:09.651
  STEP: watching for the Service to be patched @ 04/15/24 07:59:09.67
  Apr 15 07:59:09.674: INFO: observed Service test-service-jzjfb in namespace services-9188 with annotations: map[] & LoadBalancer: {[]}
  Apr 15 07:59:09.674: INFO: Found Service test-service-jzjfb in namespace services-9188 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Apr 15 07:59:09.674: INFO: Service test-service-jzjfb has service status patched
  STEP: updating the ServiceStatus @ 04/15/24 07:59:09.674
  Apr 15 07:59:09.709: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 04/15/24 07:59:09.709
  Apr 15 07:59:09.712: INFO: Observed Service test-service-jzjfb in namespace services-9188 with annotations: map[] & Conditions: {[]}
  Apr 15 07:59:09.713: INFO: Observed event: &Service{ObjectMeta:{test-service-jzjfb  services-9188  4a3047fe-1173-499e-b0e6-9d5931909754 175949 0 2024-04-15 07:59:09 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2024-04-15 07:59:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2024-04-15 07:59:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.43.41,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.43.41],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Apr 15 07:59:09.713: INFO: Found Service test-service-jzjfb in namespace services-9188 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 15 07:59:09.713: INFO: Service test-service-jzjfb has service status updated
  STEP: patching the service @ 04/15/24 07:59:09.713
  STEP: watching for the Service to be patched @ 04/15/24 07:59:09.744
  Apr 15 07:59:09.747: INFO: observed Service test-service-jzjfb in namespace services-9188 with labels: map[test-service-static:true]
  Apr 15 07:59:09.748: INFO: observed Service test-service-jzjfb in namespace services-9188 with labels: map[test-service-static:true]
  Apr 15 07:59:09.748: INFO: observed Service test-service-jzjfb in namespace services-9188 with labels: map[test-service-static:true]
  Apr 15 07:59:09.749: INFO: Found Service test-service-jzjfb in namespace services-9188 with labels: map[test-service:patched test-service-static:true]
  Apr 15 07:59:09.749: INFO: Service test-service-jzjfb patched
  STEP: deleting the service @ 04/15/24 07:59:09.749
  STEP: watching for the Service to be deleted @ 04/15/24 07:59:09.778
  Apr 15 07:59:09.781: INFO: Observed event: ADDED
  Apr 15 07:59:09.782: INFO: Observed event: MODIFIED
  Apr 15 07:59:09.782: INFO: Observed event: MODIFIED
  Apr 15 07:59:09.783: INFO: Observed event: MODIFIED
  Apr 15 07:59:09.783: INFO: Found Service test-service-jzjfb in namespace services-9188 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Apr 15 07:59:09.783: INFO: Service test-service-jzjfb deleted
  Apr 15 07:59:09.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9188" for this suite. @ 04/15/24 07:59:09.793
• [0.274 seconds]
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 04/15/24 07:59:09.806
  Apr 15 07:59:09.806: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 07:59:09.81
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:59:09.838
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:59:09.844
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 07:59:09.849
  E0415 07:59:10.462663      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:11.463641      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:12.463963      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:13.464158      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 07:59:13.904
  Apr 15 07:59:13.916: INFO: Trying to get logs from node zaigh3ewotoh-3 pod downwardapi-volume-1ed95fd8-0606-406b-8285-22dee78542ae container client-container: <nil>
  STEP: delete the pod @ 04/15/24 07:59:13.934
  Apr 15 07:59:13.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2315" for this suite. @ 04/15/24 07:59:13.981
• [4.188 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:1028
  STEP: Creating a kubernetes client @ 04/15/24 07:59:13.996
  Apr 15 07:59:13.996: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename statefulset @ 04/15/24 07:59:14.003
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:59:14.077
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:59:14.083
  STEP: Creating service test in namespace statefulset-6937 @ 04/15/24 07:59:14.089
  STEP: Creating statefulset ss in namespace statefulset-6937 @ 04/15/24 07:59:14.113
  Apr 15 07:59:14.145: INFO: Found 0 stateful pods, waiting for 1
  E0415 07:59:14.465425      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:15.465119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:16.466157      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:17.467140      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:18.467227      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:19.470564      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:20.470816      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:21.470786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:22.471322      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:23.471452      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:59:24.157: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 04/15/24 07:59:24.175
  STEP: Getting /status @ 04/15/24 07:59:24.204
  Apr 15 07:59:24.219: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 04/15/24 07:59:24.219
  Apr 15 07:59:24.252: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 04/15/24 07:59:24.253
  Apr 15 07:59:24.258: INFO: Observed &StatefulSet event: ADDED
  Apr 15 07:59:24.258: INFO: Found Statefulset ss in namespace statefulset-6937 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 15 07:59:24.258: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 04/15/24 07:59:24.259
  Apr 15 07:59:24.259: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 15 07:59:24.272: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 04/15/24 07:59:24.272
  Apr 15 07:59:24.277: INFO: Observed &StatefulSet event: ADDED
  Apr 15 07:59:24.278: INFO: Deleting all statefulset in ns statefulset-6937
  Apr 15 07:59:24.286: INFO: Scaling statefulset ss to 0
  E0415 07:59:24.472511      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:25.473800      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:26.473668      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:27.473905      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:28.474174      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:29.475299      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:30.475329      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:31.475559      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:32.475680      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:33.476076      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:59:34.325: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 07:59:34.338: INFO: Deleting statefulset ss
  Apr 15 07:59:34.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6937" for this suite. @ 04/15/24 07:59:34.385
• [20.405 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 04/15/24 07:59:34.407
  Apr 15 07:59:34.407: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename webhook @ 04/15/24 07:59:34.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:59:34.45
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:59:34.459
  E0415 07:59:34.476513      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 04/15/24 07:59:34.524
  E0415 07:59:35.477254      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 07:59:35.692
  STEP: Deploying the webhook pod @ 04/15/24 07:59:35.711
  STEP: Wait for the deployment to be ready @ 04/15/24 07:59:35.746
  Apr 15 07:59:35.778: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0415 07:59:36.478596      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:37.478856      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:59:37.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6d58c8c59c\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:59:38.479025      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:39.479241      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:59:39.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6d58c8c59c\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:59:40.479404      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:41.479800      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:59:41.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6d58c8c59c\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:59:42.479875      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:43.480231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:59:43.812: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6d58c8c59c\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:59:44.481289      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:45.481875      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:59:45.810: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 7, 59, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6d58c8c59c\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 07:59:46.482217      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:47.483035      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/15/24 07:59:47.812
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 07:59:47.858
  E0415 07:59:48.483777      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 07:59:48.859: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 04/15/24 07:59:48.869
  STEP: create a pod that should be denied by the webhook @ 04/15/24 07:59:48.909
  STEP: create a pod that causes the webhook to hang @ 04/15/24 07:59:48.943
  E0415 07:59:49.484142      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:50.484357      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:51.485242      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:52.486212      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:53.486250      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:54.486854      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:55.487814      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:56.487884      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:57.488241      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 07:59:58.488760      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 04/15/24 07:59:58.96
  STEP: create a configmap that should be admitted by the webhook @ 04/15/24 07:59:58.992
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 04/15/24 07:59:59.025
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 04/15/24 07:59:59.053
  STEP: create a namespace that bypass the webhook @ 04/15/24 07:59:59.08
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 04/15/24 07:59:59.129
  Apr 15 07:59:59.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7389" for this suite. @ 04/15/24 07:59:59.301
  STEP: Destroying namespace "webhook-markers-9902" for this suite. @ 04/15/24 07:59:59.317
  STEP: Destroying namespace "exempted-namespace-9823" for this suite. @ 04/15/24 07:59:59.361
• [24.976 seconds]
------------------------------
SS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 04/15/24 07:59:59.384
  Apr 15 07:59:59.384: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename configmap @ 04/15/24 07:59:59.387
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:59:59.42
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:59:59.426
  STEP: creating a ConfigMap @ 04/15/24 07:59:59.432
  STEP: fetching the ConfigMap @ 04/15/24 07:59:59.445
  STEP: patching the ConfigMap @ 04/15/24 07:59:59.453
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 04/15/24 07:59:59.476
  E0415 07:59:59.489883      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the ConfigMap by collection with a label selector @ 04/15/24 07:59:59.49
  STEP: listing all ConfigMaps in test namespace @ 04/15/24 07:59:59.507
  Apr 15 07:59:59.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4568" for this suite. @ 04/15/24 07:59:59.529
• [0.161 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 04/15/24 07:59:59.546
  Apr 15 07:59:59.546: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename crd-webhook @ 04/15/24 07:59:59.553
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 07:59:59.59
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 07:59:59.596
  STEP: Setting up server cert @ 04/15/24 07:59:59.603
  E0415 08:00:00.489549      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 04/15/24 08:00:01.057
  STEP: Deploying the custom resource conversion webhook pod @ 04/15/24 08:00:01.069
  STEP: Wait for the deployment to be ready @ 04/15/24 08:00:01.092
  Apr 15 08:00:01.115: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0415 08:00:01.490872      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:02.490947      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/15/24 08:00:03.137
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 08:00:03.161
  E0415 08:00:03.491112      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 08:00:04.163: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Apr 15 08:00:04.176: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  E0415 08:00:04.492324      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:05.493331      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:06.494445      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 04/15/24 08:00:07.112
  STEP: Create a v2 custom resource @ 04/15/24 08:00:07.161
  STEP: List CRs in v1 @ 04/15/24 08:00:07.461
  STEP: List CRs in v2 @ 04/15/24 08:00:07.473
  Apr 15 08:00:07.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0415 08:00:07.496259      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-9451" for this suite. @ 04/15/24 08:00:08.141
• [8.615 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 04/15/24 08:00:08.163
  Apr 15 08:00:08.163: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename tables @ 04/15/24 08:00:08.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:00:08.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:00:08.218
  Apr 15 08:00:08.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-8975" for this suite. @ 04/15/24 08:00:08.237
• [0.086 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 04/15/24 08:00:08.26
  Apr 15 08:00:08.260: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 08:00:08.264
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:00:08.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:00:08.298
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 04/15/24 08:00:08.304
  E0415 08:00:08.497302      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:09.498244      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:10.498867      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:11.500211      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 08:00:12.358
  Apr 15 08:00:12.367: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-ecd7ab4c-3cec-4526-9454-ebc738886375 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 08:00:12.384
  Apr 15 08:00:12.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9049" for this suite. @ 04/15/24 08:00:12.44
• [4.196 seconds]
------------------------------
S
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 04/15/24 08:00:12.456
  Apr 15 08:00:12.456: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename services @ 04/15/24 08:00:12.46
  E0415 08:00:12.501529      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:00:12.541
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:00:12.547
  STEP: fetching services @ 04/15/24 08:00:12.553
  Apr 15 08:00:12.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8783" for this suite. @ 04/15/24 08:00:12.578
• [0.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 04/15/24 08:00:12.618
  Apr 15 08:00:12.618: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename init-container @ 04/15/24 08:00:12.622
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:00:12.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:00:12.67
  STEP: creating the pod @ 04/15/24 08:00:12.678
  Apr 15 08:00:12.679: INFO: PodSpec: initContainers in spec.initContainers
  E0415 08:00:13.501311      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:14.501582      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:15.501868      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:16.502693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:17.502852      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:18.503070      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:19.503429      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:20.503511      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:21.504822      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:22.505661      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:23.506099      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:24.506310      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:25.506789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:26.507576      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:27.509508      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:28.509250      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:29.509880      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:30.510227      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:31.510444      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:32.511278      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:33.511230      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:34.511398      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:35.512104      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:36.512998      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:37.513338      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:38.513754      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:39.514135      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:40.514499      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:41.515373      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:42.515878      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:43.516289      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:44.517064      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:45.517780      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:46.518314      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:47.518335      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:48.519051      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:49.519487      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:50.519907      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:51.520642      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:52.521326      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:53.521289      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:54.521757      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:55.521781      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:56.521938      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 08:00:57.138: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-65bf94b7-786d-49ff-8c85-b0f47ee4bbe5", GenerateName:"", Namespace:"init-container-7770", SelfLink:"", UID:"88ba53c7-e27a-4200-b5ce-c1b2946affc9", ResourceVersion:"176468", Generation:0, CreationTimestamp:time.Date(2024, time.April, 15, 8, 0, 12, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"679016200"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.April, 15, 8, 0, 12, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0066ca480), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.April, 15, 8, 0, 57, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0066ca4b0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-pz58l", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00416de40), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pz58l", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pz58l", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-pz58l", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00471d240), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"zaigh3ewotoh-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004b682a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00471d2d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00471d2f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00471d2f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00471d2fc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000c054d0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.April, 15, 8, 0, 12, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.April, 15, 8, 0, 12, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.April, 15, 8, 0, 12, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.April, 15, 8, 0, 12, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.131", PodIP:"10.233.66.123", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.66.123"}}, StartTime:time.Date(2024, time.April, 15, 8, 0, 12, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004b68540)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004b685b0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://88d50e64c11e6370a6de4eebf72c9166493f131a659becef488fe0469670c266", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00416dec0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00416dea0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00471d374), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Apr 15 08:00:57.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-7770" for this suite. @ 04/15/24 08:00:57.152
• [44.549 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 04/15/24 08:00:57.17
  Apr 15 08:00:57.170: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename configmap @ 04/15/24 08:00:57.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:00:57.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:00:57.217
  STEP: Creating configMap with name configmap-test-volume-map-b2072507-092e-4039-bf88-92a753cb1883 @ 04/15/24 08:00:57.223
  STEP: Creating a pod to test consume configMaps @ 04/15/24 08:00:57.232
  E0415 08:00:57.522877      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:00:58.523513      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 08:00:59.268
  Apr 15 08:00:59.280: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-configmaps-e9305cf6-c179-454b-a3f8-5fc9a4e21302 container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 08:00:59.294
  Apr 15 08:00:59.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7482" for this suite. @ 04/15/24 08:00:59.349
• [2.194 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 04/15/24 08:00:59.373
  Apr 15 08:00:59.373: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename pod-network-test @ 04/15/24 08:00:59.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:00:59.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:00:59.427
  STEP: Performing setup for networking test in namespace pod-network-test-4757 @ 04/15/24 08:00:59.432
  STEP: creating a selector @ 04/15/24 08:00:59.432
  STEP: Creating the service pods in kubernetes @ 04/15/24 08:00:59.432
  Apr 15 08:00:59.432: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0415 08:00:59.524070      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:00.524953      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:01.529018      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:02.530142      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:03.530373      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:04.530523      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:05.531330      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:06.532010      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:07.532095      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:08.532194      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:09.532431      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:10.533083      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:11.534319      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:12.534888      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:13.535034      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:14.535224      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:15.535331      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:16.535500      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:17.536807      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:18.536784      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:19.537108      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:20.537232      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:21.537584      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/15/24 08:01:21.738
  E0415 08:01:22.537963      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:23.538460      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 08:01:23.777: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 15 08:01:23.778: INFO: Breadth first check of 10.233.64.6 on host 192.168.121.74...
  Apr 15 08:01:23.785: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.126:9080/dial?request=hostname&protocol=http&host=10.233.64.6&port=8083&tries=1'] Namespace:pod-network-test-4757 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 08:01:23.785: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 08:01:23.788: INFO: ExecWithOptions: Clientset creation
  Apr 15 08:01:23.789: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4757/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.126%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.6%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 15 08:01:24.045: INFO: Waiting for responses: map[]
  Apr 15 08:01:24.046: INFO: reached 10.233.64.6 after 0/1 tries
  Apr 15 08:01:24.046: INFO: Breadth first check of 10.233.65.222 on host 192.168.121.27...
  Apr 15 08:01:24.059: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.126:9080/dial?request=hostname&protocol=http&host=10.233.65.222&port=8083&tries=1'] Namespace:pod-network-test-4757 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 08:01:24.060: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 08:01:24.065: INFO: ExecWithOptions: Clientset creation
  Apr 15 08:01:24.065: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4757/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.126%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.65.222%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 15 08:01:24.199: INFO: Waiting for responses: map[]
  Apr 15 08:01:24.199: INFO: reached 10.233.65.222 after 0/1 tries
  Apr 15 08:01:24.199: INFO: Breadth first check of 10.233.66.125 on host 192.168.121.131...
  Apr 15 08:01:24.207: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.126:9080/dial?request=hostname&protocol=http&host=10.233.66.125&port=8083&tries=1'] Namespace:pod-network-test-4757 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 08:01:24.207: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  Apr 15 08:01:24.210: INFO: ExecWithOptions: Clientset creation
  Apr 15 08:01:24.210: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4757/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.126%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.125%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 15 08:01:24.331: INFO: Waiting for responses: map[]
  Apr 15 08:01:24.332: INFO: reached 10.233.66.125 after 0/1 tries
  Apr 15 08:01:24.332: INFO: Going to retry 0 out of 3 pods....
  Apr 15 08:01:24.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4757" for this suite. @ 04/15/24 08:01:24.349
• [24.991 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 04/15/24 08:01:24.366
  Apr 15 08:01:24.366: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 08:01:24.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:01:24.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:01:24.425
  STEP: Counting existing ResourceQuota @ 04/15/24 08:01:24.435
  E0415 08:01:24.539248      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:25.539257      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:26.539772      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:27.540940      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:28.541573      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/15/24 08:01:29.444
  STEP: Ensuring resource quota status is calculated @ 04/15/24 08:01:29.455
  E0415 08:01:29.542252      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:30.542758      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 04/15/24 08:01:31.464
  STEP: Ensuring ResourceQuota status captures the pod usage @ 04/15/24 08:01:31.497
  E0415 08:01:31.543562      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:32.544201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 04/15/24 08:01:33.508
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 04/15/24 08:01:33.514
  STEP: Ensuring a pod cannot update its resource requirements @ 04/15/24 08:01:33.521
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 04/15/24 08:01:33.533
  E0415 08:01:33.545149      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:34.545271      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/15/24 08:01:35.542
  E0415 08:01:35.547065      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota status released the pod usage @ 04/15/24 08:01:35.57
  E0415 08:01:36.546718      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:37.546835      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 08:01:37.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3186" for this suite. @ 04/15/24 08:01:37.596
• [13.246 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 04/15/24 08:01:37.618
  Apr 15 08:01:37.619: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 08:01:37.623
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:01:37.664
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:01:37.675
  STEP: Creating a ResourceQuota @ 04/15/24 08:01:37.683
  STEP: Getting a ResourceQuota @ 04/15/24 08:01:37.696
  STEP: Updating a ResourceQuota @ 04/15/24 08:01:37.714
  STEP: Verifying a ResourceQuota was modified @ 04/15/24 08:01:37.731
  STEP: Deleting a ResourceQuota @ 04/15/24 08:01:37.744
  STEP: Verifying the deleted ResourceQuota @ 04/15/24 08:01:37.759
  Apr 15 08:01:37.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2575" for this suite. @ 04/15/24 08:01:37.783
• [0.181 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 04/15/24 08:01:37.805
  Apr 15 08:01:37.805: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename sysctl @ 04/15/24 08:01:37.809
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:01:37.843
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:01:37.85
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 04/15/24 08:01:37.858
  STEP: Watching for error events or started pod @ 04/15/24 08:01:37.881
  E0415 08:01:38.547233      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:39.548037      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 04/15/24 08:01:39.896
  E0415 08:01:40.547980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:41.548764      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 04/15/24 08:01:41.921
  STEP: Getting logs from the pod @ 04/15/24 08:01:41.921
  STEP: Checking that the sysctl is actually updated @ 04/15/24 08:01:41.942
  Apr 15 08:01:41.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-7" for this suite. @ 04/15/24 08:01:41.956
• [4.164 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 04/15/24 08:01:41.972
  Apr 15 08:01:41.973: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/15/24 08:01:41.976
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:01:42.005
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:01:42.01
  STEP: create the container to handle the HTTPGet hook request. @ 04/15/24 08:01:42.031
  E0415 08:01:42.549105      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:43.549585      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 04/15/24 08:01:44.077
  E0415 08:01:44.550173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:45.550907      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 04/15/24 08:01:46.137
  E0415 08:01:46.551980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:47.552002      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 04/15/24 08:01:48.177
  Apr 15 08:01:48.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-2037" for this suite. @ 04/15/24 08:01:48.204
• [6.247 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 04/15/24 08:01:48.223
  Apr 15 08:01:48.223: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename services @ 04/15/24 08:01:48.226
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:01:48.261
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:01:48.267
  STEP: creating service in namespace services-6938 @ 04/15/24 08:01:48.276
  STEP: creating service affinity-nodeport in namespace services-6938 @ 04/15/24 08:01:48.276
  STEP: creating replication controller affinity-nodeport in namespace services-6938 @ 04/15/24 08:01:48.326
  I0415 08:01:48.350087      13 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-6938, replica count: 3
  E0415 08:01:48.553054      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:49.554158      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:50.554112      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0415 08:01:51.402311      13 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 15 08:01:51.433: INFO: Creating new exec pod
  E0415 08:01:51.554462      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:52.554658      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:53.555038      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 08:01:54.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-6938 exec execpod-affinitykpm8b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  E0415 08:01:54.556148      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 08:01:54.865: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Apr 15 08:01:54.865: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 08:01:54.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-6938 exec execpod-affinitykpm8b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.26.105 80'
  Apr 15 08:01:55.134: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.26.105 80\nConnection to 10.233.26.105 80 port [tcp/http] succeeded!\n"
  Apr 15 08:01:55.134: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 08:01:55.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-6938 exec execpod-affinitykpm8b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.131 30665'
  Apr 15 08:01:55.491: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.131 30665\nConnection to 192.168.121.131 30665 port [tcp/*] succeeded!\n"
  Apr 15 08:01:55.491: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 08:01:55.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-6938 exec execpod-affinitykpm8b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.74 30665'
  E0415 08:01:55.556855      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 08:01:55.766: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.74 30665\nConnection to 192.168.121.74 30665 port [tcp/*] succeeded!\n"
  Apr 15 08:01:55.766: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 08:01:55.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=services-6938 exec execpod-affinitykpm8b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.74:30665/ ; done'
  Apr 15 08:01:56.331: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:30665/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:30665/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:30665/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:30665/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:30665/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:30665/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:30665/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:30665/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:30665/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:30665/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:30665/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:30665/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:30665/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:30665/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:30665/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.74:30665/\n"
  Apr 15 08:01:56.331: INFO: stdout: "\naffinity-nodeport-7h68t\naffinity-nodeport-7h68t\naffinity-nodeport-7h68t\naffinity-nodeport-7h68t\naffinity-nodeport-7h68t\naffinity-nodeport-7h68t\naffinity-nodeport-7h68t\naffinity-nodeport-7h68t\naffinity-nodeport-7h68t\naffinity-nodeport-7h68t\naffinity-nodeport-7h68t\naffinity-nodeport-7h68t\naffinity-nodeport-7h68t\naffinity-nodeport-7h68t\naffinity-nodeport-7h68t\naffinity-nodeport-7h68t"
  Apr 15 08:01:56.331: INFO: Received response from host: affinity-nodeport-7h68t
  Apr 15 08:01:56.331: INFO: Received response from host: affinity-nodeport-7h68t
  Apr 15 08:01:56.331: INFO: Received response from host: affinity-nodeport-7h68t
  Apr 15 08:01:56.332: INFO: Received response from host: affinity-nodeport-7h68t
  Apr 15 08:01:56.332: INFO: Received response from host: affinity-nodeport-7h68t
  Apr 15 08:01:56.332: INFO: Received response from host: affinity-nodeport-7h68t
  Apr 15 08:01:56.332: INFO: Received response from host: affinity-nodeport-7h68t
  Apr 15 08:01:56.332: INFO: Received response from host: affinity-nodeport-7h68t
  Apr 15 08:01:56.333: INFO: Received response from host: affinity-nodeport-7h68t
  Apr 15 08:01:56.333: INFO: Received response from host: affinity-nodeport-7h68t
  Apr 15 08:01:56.333: INFO: Received response from host: affinity-nodeport-7h68t
  Apr 15 08:01:56.333: INFO: Received response from host: affinity-nodeport-7h68t
  Apr 15 08:01:56.333: INFO: Received response from host: affinity-nodeport-7h68t
  Apr 15 08:01:56.333: INFO: Received response from host: affinity-nodeport-7h68t
  Apr 15 08:01:56.333: INFO: Received response from host: affinity-nodeport-7h68t
  Apr 15 08:01:56.333: INFO: Received response from host: affinity-nodeport-7h68t
  Apr 15 08:01:56.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 08:01:56.349: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-6938, will wait for the garbage collector to delete the pods @ 04/15/24 08:01:56.375
  Apr 15 08:01:56.450: INFO: Deleting ReplicationController affinity-nodeport took: 12.892621ms
  Apr 15 08:01:56.551: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.008568ms
  E0415 08:01:56.557347      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:57.557644      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:01:58.558280      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-6938" for this suite. @ 04/15/24 08:01:59.035
• [10.832 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:318
  STEP: Creating a kubernetes client @ 04/15/24 08:01:59.065
  Apr 15 08:01:59.065: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename statefulset @ 04/15/24 08:01:59.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:01:59.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:01:59.11
  STEP: Creating service test in namespace statefulset-5247 @ 04/15/24 08:01:59.117
  STEP: Creating a new StatefulSet @ 04/15/24 08:01:59.127
  Apr 15 08:01:59.162: INFO: Found 0 stateful pods, waiting for 3
  E0415 08:01:59.559287      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:00.559883      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:01.561090      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:02.561122      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:03.561300      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:04.561869      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:05.561993      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:06.562627      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:07.562947      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:08.563325      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 08:02:09.174: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 08:02:09.174: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 08:02:09.174: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 08:02:09.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-5247 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 08:02:09.496: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 08:02:09.496: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 08:02:09.496: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0415 08:02:09.563524      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:10.563605      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:11.564604      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:12.565005      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:13.566323      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:14.567360      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:15.568201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:16.568750      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:17.569081      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:18.570289      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 04/15/24 08:02:19.542
  E0415 08:02:19.571167      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 08:02:19.584: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 04/15/24 08:02:19.585
  E0415 08:02:20.571746      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:21.571901      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:22.572576      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:23.573000      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:24.573072      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:25.573921      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:26.574775      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:27.575041      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:28.575193      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:29.575415      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 04/15/24 08:02:29.63
  Apr 15 08:02:29.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-5247 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 15 08:02:30.014: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 15 08:02:30.014: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 15 08:02:30.014: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0415 08:02:30.575982      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:31.576272      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:32.576577      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:33.577105      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:34.580379      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:35.579339      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:36.579894      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:37.580184      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:38.580900      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:39.581504      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 04/15/24 08:02:40.075
  Apr 15 08:02:40.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-5247 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 08:02:40.468: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 08:02:40.468: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 08:02:40.468: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0415 08:02:40.582985      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:41.582805      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:42.582928      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:43.583701      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:44.584278      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:45.584586      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:46.585430      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:47.585791      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:48.586146      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:49.586282      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 08:02:50.555: INFO: Updating stateful set ss2
  E0415 08:02:50.587354      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:51.588098      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:52.588990      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:53.589053      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:54.589456      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:55.589551      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:56.590335      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:57.590787      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:58.591198      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:02:59.591763      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:00.592606      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 04/15/24 08:03:00.604
  Apr 15 08:03:00.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=statefulset-5247 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 15 08:03:01.033: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 15 08:03:01.033: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 15 08:03:01.033: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0415 08:03:01.593022      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:02.593233      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:03.594260      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:04.595203      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:05.595328      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:06.596327      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:07.596871      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:08.597743      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:09.598159      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:10.598559      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 08:03:11.121: INFO: Deleting all statefulset in ns statefulset-5247
  Apr 15 08:03:11.133: INFO: Scaling statefulset ss2 to 0
  E0415 08:03:11.599462      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:12.599618      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:13.600033      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:14.600264      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:15.600605      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:16.600854      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:17.601676      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:18.602195      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:19.602515      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:20.602527      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 08:03:21.192: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 08:03:21.199: INFO: Deleting statefulset ss2
  Apr 15 08:03:21.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5247" for this suite. @ 04/15/24 08:03:21.261
• [82.212 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 04/15/24 08:03:21.28
  Apr 15 08:03:21.280: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename watch @ 04/15/24 08:03:21.287
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:03:21.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:03:21.345
  STEP: getting a starting resourceVersion @ 04/15/24 08:03:21.36
  STEP: starting a background goroutine to produce watch events @ 04/15/24 08:03:21.37
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 04/15/24 08:03:21.37
  E0415 08:03:21.603371      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:22.603928      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:23.605081      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 08:03:24.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2807" for this suite. @ 04/15/24 08:03:24.147
• [2.923 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 04/15/24 08:03:24.206
  Apr 15 08:03:24.206: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename conformance-tests @ 04/15/24 08:03:24.209
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:03:24.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:03:24.246
  STEP: Getting node addresses @ 04/15/24 08:03:24.255
  Apr 15 08:03:24.256: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Apr 15 08:03:24.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-8908" for this suite. @ 04/15/24 08:03:24.286
• [0.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 04/15/24 08:03:24.315
  Apr 15 08:03:24.315: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename runtimeclass @ 04/15/24 08:03:24.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:03:24.351
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:03:24.358
  STEP: getting /apis @ 04/15/24 08:03:24.365
  STEP: getting /apis/node.k8s.io @ 04/15/24 08:03:24.374
  STEP: getting /apis/node.k8s.io/v1 @ 04/15/24 08:03:24.377
  STEP: creating @ 04/15/24 08:03:24.381
  STEP: watching @ 04/15/24 08:03:24.418
  Apr 15 08:03:24.418: INFO: starting watch
  STEP: getting @ 04/15/24 08:03:24.432
  STEP: listing @ 04/15/24 08:03:24.439
  STEP: patching @ 04/15/24 08:03:24.446
  STEP: updating @ 04/15/24 08:03:24.46
  Apr 15 08:03:24.494: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 04/15/24 08:03:24.495
  STEP: deleting a collection @ 04/15/24 08:03:24.518
  Apr 15 08:03:24.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9498" for this suite. @ 04/15/24 08:03:24.604
  E0415 08:03:24.605003      13 retrywatcher.go:130] "Watch failed" err="context canceled"
• [0.305 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 04/15/24 08:03:24.624
  Apr 15 08:03:24.624: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename var-expansion @ 04/15/24 08:03:24.627
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:03:24.657
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:03:24.663
  STEP: Creating a pod to test substitution in container's command @ 04/15/24 08:03:24.673
  E0415 08:03:25.606254      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:26.606529      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:27.606834      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:28.607231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 08:03:28.721
  Apr 15 08:03:28.727: INFO: Trying to get logs from node zaigh3ewotoh-3 pod var-expansion-0bb6837b-cbb8-4ff0-986c-f52468a5f7db container dapi-container: <nil>
  STEP: delete the pod @ 04/15/24 08:03:28.761
  Apr 15 08:03:28.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5321" for this suite. @ 04/15/24 08:03:28.813
• [4.206 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 04/15/24 08:03:28.834
  Apr 15 08:03:28.834: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 08:03:28.838
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:03:28.88
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:03:28.887
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/15/24 08:03:28.893
  Apr 15 08:03:28.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-4628 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Apr 15 08:03:29.106: INFO: stderr: ""
  Apr 15 08:03:29.106: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 04/15/24 08:03:29.106
  E0415 08:03:29.607064      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:30.608241      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:31.608256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:32.609023      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:33.610090      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 04/15/24 08:03:34.157
  Apr 15 08:03:34.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-4628 get pod e2e-test-httpd-pod -o json'
  Apr 15 08:03:34.372: INFO: stderr: ""
  Apr 15 08:03:34.372: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2024-04-15T08:03:29Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4628\",\n        \"resourceVersion\": \"177653\",\n        \"uid\": \"559f4806-9156-4324-9792-53f008da1601\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-84nqq\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"zaigh3ewotoh-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-84nqq\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-04-15T08:03:29Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-04-15T08:03:29Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-04-15T08:03:29Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-04-15T08:03:29Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://6543594f8539d3e1dd4e18e10df44e50a781e5094ffac50386e238bdeacfb9f9\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2024-04-15T08:03:29Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.131\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.66.136\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.66.136\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2024-04-15T08:03:29Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 04/15/24 08:03:34.373
  Apr 15 08:03:34.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-4628 replace -f -'
  E0415 08:03:34.610512      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 08:03:35.197: INFO: stderr: ""
  Apr 15 08:03:35.197: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 04/15/24 08:03:35.197
  Apr 15 08:03:35.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-634376780 --namespace=kubectl-4628 delete pods e2e-test-httpd-pod'
  E0415 08:03:35.611454      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:36.612150      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 08:03:37.075: INFO: stderr: ""
  Apr 15 08:03:37.075: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 15 08:03:37.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4628" for this suite. @ 04/15/24 08:03:37.087
• [8.275 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 04/15/24 08:03:37.121
  Apr 15 08:03:37.121: INFO: >>> kubeConfig: /tmp/kubeconfig-634376780
  STEP: Building a namespace api object, basename subpath @ 04/15/24 08:03:37.124
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:03:37.158
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:03:37.164
  STEP: Setting up data @ 04/15/24 08:03:37.17
  STEP: Creating pod pod-subpath-test-projected-s9dx @ 04/15/24 08:03:37.195
  STEP: Creating a pod to test atomic-volume-subpath @ 04/15/24 08:03:37.195
  E0415 08:03:37.612597      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:38.613577      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:39.613722      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:40.614577      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:41.614728      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:42.615471      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:43.616332      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:44.616160      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:45.616538      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:46.618135      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:47.617640      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:48.617681      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:49.617982      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:50.618238      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:51.621133      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:52.621370      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:53.621438      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:54.622074      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:55.623087      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:56.623648      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:57.624140      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 08:03:58.624986      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 08:03:59.358
  Apr 15 08:03:59.371: INFO: Trying to get logs from node zaigh3ewotoh-3 pod pod-subpath-test-projected-s9dx container test-container-subpath-projected-s9dx: <nil>
  STEP: delete the pod @ 04/15/24 08:03:59.401
  STEP: Deleting pod pod-subpath-test-projected-s9dx @ 04/15/24 08:03:59.436
  Apr 15 08:03:59.437: INFO: Deleting pod "pod-subpath-test-projected-s9dx" in namespace "subpath-4541"
  Apr 15 08:03:59.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4541" for this suite. @ 04/15/24 08:03:59.458
• [22.355 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Apr 15 08:03:59.520: INFO: Running AfterSuite actions on node 1
  Apr 15 08:03:59.520: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
  E0415 08:03:59.625445      13 retrywatcher.go:130] "Watch failed" err="context canceled"
[ReportAfterSuite] PASSED [0.162 seconds]
------------------------------

Ran 378 of 7209 Specs in 6452.359 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6831 Skipped
PASS

Ginkgo ran 1 suite in 1h47m34.691653885s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

