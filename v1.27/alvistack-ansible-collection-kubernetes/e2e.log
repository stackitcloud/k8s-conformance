  I1216 16:13:32.804961      13 e2e.go:117] Starting e2e run "e3ca193f-0902-4f7c-aa5c-6ee32ebbee92" on Ginkgo node 1
  Dec 16 16:13:32.879: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1702743212 - will randomize all specs

Will run 378 of 7209 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Dec 16 16:13:33.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 16:13:33.255: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Dec 16 16:13:33.349: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Dec 16 16:13:33.364: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  Dec 16 16:13:33.364: INFO: e2e test version: v1.27.8
  Dec 16 16:13:33.367: INFO: kube-apiserver version: v1.27.8
  Dec 16 16:13:33.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 16:13:33.384: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.136 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 12/16/23 16:13:33.931
  Dec 16 16:13:33.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename deployment @ 12/16/23 16:13:33.933
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:13:33.984
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:13:33.992
  STEP: creating a Deployment @ 12/16/23 16:13:34.017
  Dec 16 16:13:34.017: INFO: Creating simple deployment test-deployment-77qpq
  Dec 16 16:13:34.053: INFO: new replicaset for deployment "test-deployment-77qpq" is yet to be created
  Dec 16 16:13:36.077: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-77qpq-5994cf9475\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:13:38.086: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-77qpq-5994cf9475\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:13:40.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-77qpq-5994cf9475\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:13:42.093: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-77qpq-5994cf9475\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:13:44.091: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-77qpq-5994cf9475\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:13:46.090: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-77qpq-5994cf9475\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:13:48.087: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-77qpq-5994cf9475\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:13:50.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-77qpq-5994cf9475\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:13:52.096: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-77qpq-5994cf9475\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:13:54.106: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-77qpq-5994cf9475\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:13:56.086: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-77qpq-5994cf9475\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:13:58.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-77qpq-5994cf9475\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Getting /status @ 12/16/23 16:14:00.098
  Dec 16 16:14:00.111: INFO: Deployment test-deployment-77qpq has Conditions: [{Available True 2023-12-16 16:13:59 +0000 UTC 2023-12-16 16:13:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-12-16 16:13:59 +0000 UTC 2023-12-16 16:13:34 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-77qpq-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 12/16/23 16:14:00.111
  Dec 16 16:14:00.142: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 13, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 13, 34, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-77qpq-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 12/16/23 16:14:00.142
  Dec 16 16:14:00.153: INFO: Observed &Deployment event: ADDED
  Dec 16 16:14:00.153: INFO: Observed Deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-16 16:13:34 +0000 UTC 2023-12-16 16:13:34 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-77qpq-5994cf9475"}
  Dec 16 16:14:00.153: INFO: Observed &Deployment event: MODIFIED
  Dec 16 16:14:00.153: INFO: Observed Deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-16 16:13:34 +0000 UTC 2023-12-16 16:13:34 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-77qpq-5994cf9475"}
  Dec 16 16:14:00.153: INFO: Observed Deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-12-16 16:13:34 +0000 UTC 2023-12-16 16:13:34 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Dec 16 16:14:00.154: INFO: Observed &Deployment event: MODIFIED
  Dec 16 16:14:00.154: INFO: Observed Deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-12-16 16:13:34 +0000 UTC 2023-12-16 16:13:34 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Dec 16 16:14:00.154: INFO: Observed Deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-16 16:13:34 +0000 UTC 2023-12-16 16:13:34 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-77qpq-5994cf9475" is progressing.}
  Dec 16 16:14:00.155: INFO: Observed &Deployment event: MODIFIED
  Dec 16 16:14:00.155: INFO: Observed Deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-12-16 16:13:59 +0000 UTC 2023-12-16 16:13:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Dec 16 16:14:00.155: INFO: Observed Deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-16 16:13:59 +0000 UTC 2023-12-16 16:13:34 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-77qpq-5994cf9475" has successfully progressed.}
  Dec 16 16:14:00.156: INFO: Observed &Deployment event: MODIFIED
  Dec 16 16:14:00.156: INFO: Observed Deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-12-16 16:13:59 +0000 UTC 2023-12-16 16:13:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Dec 16 16:14:00.156: INFO: Observed Deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-16 16:13:59 +0000 UTC 2023-12-16 16:13:34 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-77qpq-5994cf9475" has successfully progressed.}
  Dec 16 16:14:00.156: INFO: Found Deployment test-deployment-77qpq in namespace deployment-7964 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Dec 16 16:14:00.156: INFO: Deployment test-deployment-77qpq has an updated status
  STEP: patching the Statefulset Status @ 12/16/23 16:14:00.156
  Dec 16 16:14:00.157: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Dec 16 16:14:00.189: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 12/16/23 16:14:00.189
  Dec 16 16:14:00.194: INFO: Observed &Deployment event: ADDED
  Dec 16 16:14:00.194: INFO: Observed deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-16 16:13:34 +0000 UTC 2023-12-16 16:13:34 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-77qpq-5994cf9475"}
  Dec 16 16:14:00.195: INFO: Observed &Deployment event: MODIFIED
  Dec 16 16:14:00.195: INFO: Observed deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-16 16:13:34 +0000 UTC 2023-12-16 16:13:34 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-77qpq-5994cf9475"}
  Dec 16 16:14:00.196: INFO: Observed deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-12-16 16:13:34 +0000 UTC 2023-12-16 16:13:34 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Dec 16 16:14:00.196: INFO: Observed &Deployment event: MODIFIED
  Dec 16 16:14:00.197: INFO: Observed deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-12-16 16:13:34 +0000 UTC 2023-12-16 16:13:34 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Dec 16 16:14:00.197: INFO: Observed deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-16 16:13:34 +0000 UTC 2023-12-16 16:13:34 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-77qpq-5994cf9475" is progressing.}
  Dec 16 16:14:00.198: INFO: Observed &Deployment event: MODIFIED
  Dec 16 16:14:00.198: INFO: Observed deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-12-16 16:13:59 +0000 UTC 2023-12-16 16:13:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Dec 16 16:14:00.199: INFO: Observed deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-16 16:13:59 +0000 UTC 2023-12-16 16:13:34 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-77qpq-5994cf9475" has successfully progressed.}
  Dec 16 16:14:00.200: INFO: Observed &Deployment event: MODIFIED
  Dec 16 16:14:00.201: INFO: Observed deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-12-16 16:13:59 +0000 UTC 2023-12-16 16:13:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Dec 16 16:14:00.201: INFO: Observed deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-16 16:13:59 +0000 UTC 2023-12-16 16:13:34 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-77qpq-5994cf9475" has successfully progressed.}
  Dec 16 16:14:00.201: INFO: Observed deployment test-deployment-77qpq in namespace deployment-7964 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Dec 16 16:14:00.203: INFO: Observed &Deployment event: MODIFIED
  Dec 16 16:14:00.203: INFO: Found deployment test-deployment-77qpq in namespace deployment-7964 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Dec 16 16:14:00.204: INFO: Deployment test-deployment-77qpq has a patched status
  Dec 16 16:14:00.218: INFO: Deployment "test-deployment-77qpq":
  &Deployment{ObjectMeta:{test-deployment-77qpq  deployment-7964  560c457d-9839-408d-9e3e-c136ec291dd6 2117 1 2023-12-16 16:13:34 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-12-16 16:13:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-12-16 16:14:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-12-16 16:14:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001caf158 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-77qpq-5994cf9475",LastUpdateTime:2023-12-16 16:14:00 +0000 UTC,LastTransitionTime:2023-12-16 16:14:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Dec 16 16:14:00.227: INFO: New ReplicaSet "test-deployment-77qpq-5994cf9475" of Deployment "test-deployment-77qpq":
  &ReplicaSet{ObjectMeta:{test-deployment-77qpq-5994cf9475  deployment-7964  6bced07d-ed79-43e2-872b-67d40117fd2c 2112 1 2023-12-16 16:13:34 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-77qpq 560c457d-9839-408d-9e3e-c136ec291dd6 0xc001bf95c7 0xc001bf95c8}] [] [{kube-controller-manager Update apps/v1 2023-12-16 16:13:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"560c457d-9839-408d-9e3e-c136ec291dd6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-16 16:13:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001bf9678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Dec 16 16:14:00.239: INFO: Pod "test-deployment-77qpq-5994cf9475-vj8j2" is available:
  &Pod{ObjectMeta:{test-deployment-77qpq-5994cf9475-vj8j2 test-deployment-77qpq-5994cf9475- deployment-7964  d8ae3a05-62a9-4457-86a8-39d1bbe4042c 2111 0 2023-12-16 16:13:34 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-77qpq-5994cf9475 6bced07d-ed79-43e2-872b-67d40117fd2c 0xc001bf9a27 0xc001bf9a28}] [] [{kube-controller-manager Update v1 2023-12-16 16:13:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6bced07d-ed79-43e2-872b-67d40117fd2c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 16:13:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rhts5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rhts5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 16:13:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 16:13:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 16:13:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 16:13:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.112,PodIP:10.233.66.5,StartTime:2023-12-16 16:13:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-16 16:13:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://f4505e95768be2422ce86e3c13af6cac2caf751e56a4a52c33d942c73e3c145a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.5,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 16:14:00.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7964" for this suite. @ 12/16/23 16:14:00.25
• [26.333 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 12/16/23 16:14:00.273
  Dec 16 16:14:00.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename pods @ 12/16/23 16:14:00.277
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:14:00.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:14:00.31
  Dec 16 16:14:00.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: creating the pod @ 12/16/23 16:14:00.316
  STEP: submitting the pod to kubernetes @ 12/16/23 16:14:00.317
  Dec 16 16:14:04.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-585" for this suite. @ 12/16/23 16:14:04.465
• [4.205 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 12/16/23 16:14:04.489
  Dec 16 16:14:04.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename runtimeclass @ 12/16/23 16:14:04.492
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:14:04.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:14:04.532
  STEP: getting /apis @ 12/16/23 16:14:04.547
  STEP: getting /apis/node.k8s.io @ 12/16/23 16:14:04.559
  STEP: getting /apis/node.k8s.io/v1 @ 12/16/23 16:14:04.562
  STEP: creating @ 12/16/23 16:14:04.57
  STEP: watching @ 12/16/23 16:14:04.74
  Dec 16 16:14:04.741: INFO: starting watch
  STEP: getting @ 12/16/23 16:14:04.76
  STEP: listing @ 12/16/23 16:14:04.767
  STEP: patching @ 12/16/23 16:14:04.774
  STEP: updating @ 12/16/23 16:14:04.793
  Dec 16 16:14:04.807: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 12/16/23 16:14:04.808
  STEP: deleting a collection @ 12/16/23 16:14:04.839
  Dec 16 16:14:04.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6361" for this suite. @ 12/16/23 16:14:04.906
• [0.437 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 12/16/23 16:14:04.929
  Dec 16 16:14:04.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename downward-api @ 12/16/23 16:14:04.932
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:14:04.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:14:04.988
  STEP: Creating a pod to test downward api env vars @ 12/16/23 16:14:04.997
  STEP: Saw pod success @ 12/16/23 16:14:09.046
  Dec 16 16:14:09.054: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downward-api-337cba39-c710-4e97-ad2b-4a96cdcd3790 container dapi-container: <nil>
  STEP: delete the pod @ 12/16/23 16:14:09.073
  Dec 16 16:14:09.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9039" for this suite. @ 12/16/23 16:14:09.119
• [4.207 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 12/16/23 16:14:09.137
  Dec 16 16:14:09.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 16:14:09.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:14:09.174
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:14:09.181
  STEP: Creating projection with secret that has name projected-secret-test-map-9653286a-6453-4915-a152-7daff9958d2c @ 12/16/23 16:14:09.189
  STEP: Creating a pod to test consume secrets @ 12/16/23 16:14:09.204
  STEP: Saw pod success @ 12/16/23 16:14:29.348
  Dec 16 16:14:29.354: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-projected-secrets-78ec125c-1515-4461-b0ca-722b627b7424 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 12/16/23 16:14:29.373
  Dec 16 16:14:29.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6032" for this suite. @ 12/16/23 16:14:29.425
• [20.302 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 12/16/23 16:14:29.443
  Dec 16 16:14:29.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename svcaccounts @ 12/16/23 16:14:29.448
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:14:29.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:14:29.493
  STEP: reading a file in the container @ 12/16/23 16:14:31.568
  Dec 16 16:14:31.569: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3154 pod-service-account-98276d71-ec72-4f81-bd60-a2f607d0d5b9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 12/16/23 16:14:32.078
  Dec 16 16:14:32.079: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3154 pod-service-account-98276d71-ec72-4f81-bd60-a2f607d0d5b9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 12/16/23 16:14:32.359
  Dec 16 16:14:32.359: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3154 pod-service-account-98276d71-ec72-4f81-bd60-a2f607d0d5b9 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Dec 16 16:14:32.659: INFO: Got root ca configmap in namespace "svcaccounts-3154"
  Dec 16 16:14:32.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3154" for this suite. @ 12/16/23 16:14:32.693
• [3.306 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 12/16/23 16:14:32.772
  Dec 16 16:14:32.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename dns @ 12/16/23 16:14:32.778
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:14:32.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:14:32.819
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 12/16/23 16:14:32.825
  Dec 16 16:14:32.840: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1114  a5337f62-50c6-4751-96d4-ba1ad38456a2 2288 0 2023-12-16 16:14:32 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-12-16 16:14:32 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-577t9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-577t9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 12/16/23 16:14:34.869
  Dec 16 16:14:34.869: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1114 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 16:14:34.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 16:14:34.872: INFO: ExecWithOptions: Clientset creation
  Dec 16 16:14:34.872: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-1114/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 12/16/23 16:14:35.045
  Dec 16 16:14:35.046: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1114 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 16:14:35.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 16:14:35.049: INFO: ExecWithOptions: Clientset creation
  Dec 16 16:14:35.050: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-1114/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Dec 16 16:14:35.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 16 16:14:35.233: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-1114" for this suite. @ 12/16/23 16:14:35.261
• [2.506 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 12/16/23 16:14:35.283
  Dec 16 16:14:35.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename emptydir @ 12/16/23 16:14:35.285
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:14:35.319
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:14:35.328
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 12/16/23 16:14:35.336
  STEP: Saw pod success @ 12/16/23 16:14:39.389
  Dec 16 16:14:39.400: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-c3398465-ee00-412f-8093-5ce9f3efa018 container test-container: <nil>
  STEP: delete the pod @ 12/16/23 16:14:39.421
  Dec 16 16:14:39.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9251" for this suite. @ 12/16/23 16:14:39.46
• [4.188 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 12/16/23 16:14:39.485
  Dec 16 16:14:39.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename services @ 12/16/23 16:14:39.488
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:14:39.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:14:39.531
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-3352 @ 12/16/23 16:14:39.538
  STEP: changing the ExternalName service to type=NodePort @ 12/16/23 16:14:39.552
  STEP: creating replication controller externalname-service in namespace services-3352 @ 12/16/23 16:14:39.593
  I1216 16:14:39.633045      13 runners.go:194] Created replication controller with name: externalname-service, namespace: services-3352, replica count: 2
  I1216 16:14:42.685627      13 runners.go:194] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I1216 16:14:45.689270      13 runners.go:194] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I1216 16:14:48.690344      13 runners.go:194] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I1216 16:14:51.691080      13 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Dec 16 16:14:51.691: INFO: Creating new exec pod
  Dec 16 16:14:54.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-3352 exec execpod9szj6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Dec 16 16:14:55.106: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Dec 16 16:14:55.106: INFO: stdout: "externalname-service-5mgxb"
  Dec 16 16:14:55.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-3352 exec execpod9szj6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.49.212 80'
  Dec 16 16:14:55.397: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.49.212 80\nConnection to 10.233.49.212 80 port [tcp/http] succeeded!\n"
  Dec 16 16:14:55.397: INFO: stdout: "externalname-service-5mgxb"
  Dec 16 16:14:55.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-3352 exec execpod9szj6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.112 31253'
  Dec 16 16:14:55.724: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.112 31253\nConnection to 192.168.121.112 31253 port [tcp/*] succeeded!\n"
  Dec 16 16:14:55.724: INFO: stdout: "externalname-service-phhwm"
  Dec 16 16:14:55.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-3352 exec execpod9szj6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.49 31253'
  Dec 16 16:14:55.997: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.49 31253\nConnection to 192.168.121.49 31253 port [tcp/*] succeeded!\n"
  Dec 16 16:14:55.997: INFO: stdout: "externalname-service-5mgxb"
  Dec 16 16:14:55.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 16 16:14:56.006: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-3352" for this suite. @ 12/16/23 16:14:56.056
• [16.589 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 12/16/23 16:14:56.076
  Dec 16 16:14:56.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename taint-multiple-pods @ 12/16/23 16:14:56.082
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:14:56.115
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:14:56.12
  Dec 16 16:14:56.129: INFO: Waiting up to 1m0s for all nodes to be ready
  Dec 16 16:15:56.178: INFO: Waiting for terminating namespaces to be deleted...
  Dec 16 16:15:56.187: INFO: Starting informer...
  STEP: Starting pods... @ 12/16/23 16:15:56.188
  Dec 16 16:15:56.468: INFO: Pod1 is running on phoh7xai9ouk-3. Tainting Node
  Dec 16 16:15:58.729: INFO: Pod2 is running on phoh7xai9ouk-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 12/16/23 16:15:58.73
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 12/16/23 16:15:58.756
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 12/16/23 16:15:58.77
  Dec 16 16:16:04.823: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  Dec 16 16:16:24.895: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Dec 16 16:16:24.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 12/16/23 16:16:24.958
  STEP: Destroying namespace "taint-multiple-pods-1198" for this suite. @ 12/16/23 16:16:24.969
• [88.910 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 12/16/23 16:16:24.987
  Dec 16 16:16:24.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 16:16:24.99
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:16:25.086
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:16:25.094
  STEP: Creating secret with name projected-secret-test-0ea1fdc7-71ef-498c-bb1a-9de1649efdc2 @ 12/16/23 16:16:25.108
  STEP: Creating a pod to test consume secrets @ 12/16/23 16:16:25.122
  STEP: Saw pod success @ 12/16/23 16:16:29.176
  Dec 16 16:16:29.185: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-projected-secrets-0ca26b7e-d701-41d5-aa43-98c352d7858c container secret-volume-test: <nil>
  STEP: delete the pod @ 12/16/23 16:16:29.227
  Dec 16 16:16:29.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1443" for this suite. @ 12/16/23 16:16:29.284
• [4.323 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 12/16/23 16:16:29.312
  Dec 16 16:16:29.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename downward-api @ 12/16/23 16:16:29.317
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:16:29.364
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:16:29.373
  STEP: Creating a pod to test downward API volume plugin @ 12/16/23 16:16:29.382
  STEP: Saw pod success @ 12/16/23 16:16:33.437
  Dec 16 16:16:33.452: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downwardapi-volume-df65d22f-bff1-4a61-8a7b-74f7dd6eb97f container client-container: <nil>
  STEP: delete the pod @ 12/16/23 16:16:33.475
  Dec 16 16:16:33.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1938" for this suite. @ 12/16/23 16:16:33.527
• [4.229 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 12/16/23 16:16:33.542
  Dec 16 16:16:33.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename emptydir @ 12/16/23 16:16:33.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:16:33.581
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:16:33.59
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 12/16/23 16:16:33.602
  STEP: Saw pod success @ 12/16/23 16:16:37.671
  Dec 16 16:16:37.680: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-baaacfa4-a872-420c-98c3-09b6d075ee14 container test-container: <nil>
  STEP: delete the pod @ 12/16/23 16:16:37.696
  Dec 16 16:16:37.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8241" for this suite. @ 12/16/23 16:16:37.748
• [4.222 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 12/16/23 16:16:37.77
  Dec 16 16:16:37.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename pods @ 12/16/23 16:16:37.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:16:37.829
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:16:37.836
  STEP: creating the pod @ 12/16/23 16:16:37.844
  STEP: submitting the pod to kubernetes @ 12/16/23 16:16:37.845
  W1216 16:16:37.874960      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: verifying the pod is in kubernetes @ 12/16/23 16:16:39.924
  STEP: updating the pod @ 12/16/23 16:16:39.929
  Dec 16 16:16:40.457: INFO: Successfully updated pod "pod-update-activedeadlineseconds-30bccaf3-ddea-4b4a-8ee8-fc1d4b9bf49a"
  Dec 16 16:16:44.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1538" for this suite. @ 12/16/23 16:16:44.493
• [6.733 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 12/16/23 16:16:44.506
  Dec 16 16:16:44.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename watch @ 12/16/23 16:16:44.509
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:16:44.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:16:44.548
  STEP: creating a new configmap @ 12/16/23 16:16:44.569
  STEP: modifying the configmap once @ 12/16/23 16:16:44.59
  STEP: modifying the configmap a second time @ 12/16/23 16:16:44.665
  STEP: deleting the configmap @ 12/16/23 16:16:44.715
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 12/16/23 16:16:44.725
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 12/16/23 16:16:44.729
  Dec 16 16:16:44.730: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3429  09b3fbd0-08f5-4f52-9289-c440d949df7d 2890 0 2023-12-16 16:16:44 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-12-16 16:16:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 16 16:16:44.731: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3429  09b3fbd0-08f5-4f52-9289-c440d949df7d 2891 0 2023-12-16 16:16:44 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-12-16 16:16:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 16 16:16:44.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-3429" for this suite. @ 12/16/23 16:16:44.74
• [0.244 seconds]
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 12/16/23 16:16:44.751
  Dec 16 16:16:44.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename crd-publish-openapi @ 12/16/23 16:16:44.754
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:16:44.834
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:16:44.85
  STEP: set up a multi version CRD @ 12/16/23 16:16:44.858
  Dec 16 16:16:44.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: mark a version not serverd @ 12/16/23 16:16:49.796
  STEP: check the unserved version gets removed @ 12/16/23 16:16:49.843
  STEP: check the other version is not changed @ 12/16/23 16:16:52.074
  Dec 16 16:16:55.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2083" for this suite. @ 12/16/23 16:16:55.66
• [10.924 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 12/16/23 16:16:55.677
  Dec 16 16:16:55.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubectl @ 12/16/23 16:16:55.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:16:55.709
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:16:55.715
  STEP: starting the proxy server @ 12/16/23 16:16:55.72
  Dec 16 16:16:55.721: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-4956 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 12/16/23 16:16:55.86
  Dec 16 16:16:55.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4956" for this suite. @ 12/16/23 16:16:55.889
• [0.226 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 12/16/23 16:16:55.904
  Dec 16 16:16:55.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename emptydir @ 12/16/23 16:16:55.906
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:16:55.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:16:55.94
  STEP: Creating a pod to test emptydir volume type on node default medium @ 12/16/23 16:16:55.946
  STEP: Saw pod success @ 12/16/23 16:17:00.001
  Dec 16 16:17:00.010: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-d24a734a-3230-4833-a1dd-e20f937f0ee6 container test-container: <nil>
  STEP: delete the pod @ 12/16/23 16:17:00.025
  Dec 16 16:17:00.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1920" for this suite. @ 12/16/23 16:17:00.058
• [4.165 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 12/16/23 16:17:00.071
  Dec 16 16:17:00.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename sched-pred @ 12/16/23 16:17:00.074
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:17:00.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:17:00.117
  Dec 16 16:17:00.124: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Dec 16 16:17:00.139: INFO: Waiting for terminating namespaces to be deleted...
  Dec 16 16:17:00.146: INFO: 
  Logging pods the apiserver thinks is on node phoh7xai9ouk-1 before test
  Dec 16 16:17:00.158: INFO: kube-flannel-ds-5sbtb from kube-flannel started at 2023-12-16 16:10:58 +0000 UTC (1 container statuses recorded)
  Dec 16 16:17:00.158: INFO: 	Container kube-flannel ready: true, restart count 1
  Dec 16 16:17:00.158: INFO: coredns-5d78c9869d-scnc5 from kube-system started at 2023-12-16 16:15:59 +0000 UTC (1 container statuses recorded)
  Dec 16 16:17:00.158: INFO: 	Container coredns ready: true, restart count 0
  Dec 16 16:17:00.158: INFO: kube-addon-manager-phoh7xai9ouk-1 from kube-system started at 2023-12-16 16:09:46 +0000 UTC (1 container statuses recorded)
  Dec 16 16:17:00.158: INFO: 	Container kube-addon-manager ready: true, restart count 1
  Dec 16 16:17:00.158: INFO: kube-apiserver-phoh7xai9ouk-1 from kube-system started at 2023-12-16 16:09:46 +0000 UTC (1 container statuses recorded)
  Dec 16 16:17:00.158: INFO: 	Container kube-apiserver ready: true, restart count 1
  Dec 16 16:17:00.158: INFO: kube-controller-manager-phoh7xai9ouk-1 from kube-system started at 2023-12-16 16:09:46 +0000 UTC (1 container statuses recorded)
  Dec 16 16:17:00.158: INFO: 	Container kube-controller-manager ready: true, restart count 1
  Dec 16 16:17:00.158: INFO: kube-proxy-7c5h8 from kube-system started at 2023-12-16 16:10:59 +0000 UTC (1 container statuses recorded)
  Dec 16 16:17:00.158: INFO: 	Container kube-proxy ready: true, restart count 0
  Dec 16 16:17:00.158: INFO: kube-scheduler-phoh7xai9ouk-1 from kube-system started at 2023-12-16 16:09:46 +0000 UTC (1 container statuses recorded)
  Dec 16 16:17:00.158: INFO: 	Container kube-scheduler ready: true, restart count 1
  Dec 16 16:17:00.158: INFO: sonobuoy-systemd-logs-daemon-set-0d7ed218a9fd4126-l6bt5 from sonobuoy started at 2023-12-16 16:12:30 +0000 UTC (2 container statuses recorded)
  Dec 16 16:17:00.158: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 16 16:17:00.158: INFO: 	Container systemd-logs ready: true, restart count 0
  Dec 16 16:17:00.158: INFO: 
  Logging pods the apiserver thinks is on node phoh7xai9ouk-2 before test
  Dec 16 16:17:00.174: INFO: kube-flannel-ds-p5tmh from kube-flannel started at 2023-12-16 16:11:26 +0000 UTC (1 container statuses recorded)
  Dec 16 16:17:00.174: INFO: 	Container kube-flannel ready: true, restart count 0
  Dec 16 16:17:00.174: INFO: coredns-5d78c9869d-848fd from kube-system started at 2023-12-16 16:11:25 +0000 UTC (1 container statuses recorded)
  Dec 16 16:17:00.174: INFO: 	Container coredns ready: true, restart count 0
  Dec 16 16:17:00.174: INFO: kube-addon-manager-phoh7xai9ouk-2 from kube-system started at 2023-12-16 16:09:29 +0000 UTC (1 container statuses recorded)
  Dec 16 16:17:00.174: INFO: 	Container kube-addon-manager ready: true, restart count 1
  Dec 16 16:17:00.174: INFO: kube-apiserver-phoh7xai9ouk-2 from kube-system started at 2023-12-16 16:09:29 +0000 UTC (1 container statuses recorded)
  Dec 16 16:17:00.174: INFO: 	Container kube-apiserver ready: true, restart count 1
  Dec 16 16:17:00.174: INFO: kube-controller-manager-phoh7xai9ouk-2 from kube-system started at 2023-12-16 16:09:29 +0000 UTC (1 container statuses recorded)
  Dec 16 16:17:00.174: INFO: 	Container kube-controller-manager ready: true, restart count 1
  Dec 16 16:17:00.174: INFO: kube-proxy-dzncl from kube-system started at 2023-12-16 16:11:26 +0000 UTC (1 container statuses recorded)
  Dec 16 16:17:00.174: INFO: 	Container kube-proxy ready: true, restart count 0
  Dec 16 16:17:00.174: INFO: kube-scheduler-phoh7xai9ouk-2 from kube-system started at 2023-12-16 16:09:29 +0000 UTC (1 container statuses recorded)
  Dec 16 16:17:00.174: INFO: 	Container kube-scheduler ready: true, restart count 1
  Dec 16 16:17:00.174: INFO: sonobuoy-systemd-logs-daemon-set-0d7ed218a9fd4126-2zwqp from sonobuoy started at 2023-12-16 16:12:30 +0000 UTC (2 container statuses recorded)
  Dec 16 16:17:00.174: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 16 16:17:00.174: INFO: 	Container systemd-logs ready: true, restart count 0
  Dec 16 16:17:00.174: INFO: 
  Logging pods the apiserver thinks is on node phoh7xai9ouk-3 before test
  Dec 16 16:17:00.185: INFO: kube-flannel-ds-xqwk5 from kube-flannel started at 2023-12-16 16:16:25 +0000 UTC (1 container statuses recorded)
  Dec 16 16:17:00.186: INFO: 	Container kube-flannel ready: true, restart count 0
  Dec 16 16:17:00.186: INFO: kube-proxy-w4mds from kube-system started at 2023-12-16 16:11:29 +0000 UTC (1 container statuses recorded)
  Dec 16 16:17:00.186: INFO: 	Container kube-proxy ready: true, restart count 0
  Dec 16 16:17:00.186: INFO: sonobuoy from sonobuoy started at 2023-12-16 16:12:19 +0000 UTC (1 container statuses recorded)
  Dec 16 16:17:00.186: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Dec 16 16:17:00.186: INFO: sonobuoy-e2e-job-8b3ba51fd9314720 from sonobuoy started at 2023-12-16 16:12:30 +0000 UTC (2 container statuses recorded)
  Dec 16 16:17:00.186: INFO: 	Container e2e ready: true, restart count 0
  Dec 16 16:17:00.186: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 16 16:17:00.186: INFO: sonobuoy-systemd-logs-daemon-set-0d7ed218a9fd4126-zq8f6 from sonobuoy started at 2023-12-16 16:12:30 +0000 UTC (2 container statuses recorded)
  Dec 16 16:17:00.186: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 16 16:17:00.186: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 12/16/23 16:17:00.186
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.17a15c1e8240c1ec], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] @ 12/16/23 16:17:00.242
  Dec 16 16:17:01.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-2805" for this suite. @ 12/16/23 16:17:01.298
• [1.256 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 12/16/23 16:17:01.331
  Dec 16 16:17:01.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename webhook @ 12/16/23 16:17:01.334
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:17:01.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:17:01.432
  STEP: Setting up server cert @ 12/16/23 16:17:01.543
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/16/23 16:17:02.416
  STEP: Deploying the webhook pod @ 12/16/23 16:17:02.433
  STEP: Wait for the deployment to be ready @ 12/16/23 16:17:02.451
  Dec 16 16:17:02.470: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 12/16/23 16:17:04.491
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 16:17:04.513
  Dec 16 16:17:05.514: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 12/16/23 16:17:05.525
  STEP: create a pod that should be denied by the webhook @ 12/16/23 16:17:05.578
  STEP: create a pod that causes the webhook to hang @ 12/16/23 16:17:05.605
  STEP: create a configmap that should be denied by the webhook @ 12/16/23 16:17:15.619
  STEP: create a configmap that should be admitted by the webhook @ 12/16/23 16:17:15.676
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 12/16/23 16:17:15.704
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 12/16/23 16:17:15.726
  STEP: create a namespace that bypass the webhook @ 12/16/23 16:17:15.743
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 12/16/23 16:17:15.781
  Dec 16 16:17:15.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8658" for this suite. @ 12/16/23 16:17:15.977
  STEP: Destroying namespace "webhook-markers-4488" for this suite. @ 12/16/23 16:17:16
  STEP: Destroying namespace "exempted-namespace-5322" for this suite. @ 12/16/23 16:17:16.02
• [14.707 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 12/16/23 16:17:16.04
  Dec 16 16:17:16.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename pod-network-test @ 12/16/23 16:17:16.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:17:16.079
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:17:16.091
  STEP: Performing setup for networking test in namespace pod-network-test-9039 @ 12/16/23 16:17:16.101
  STEP: creating a selector @ 12/16/23 16:17:16.101
  STEP: Creating the service pods in kubernetes @ 12/16/23 16:17:16.101
  Dec 16 16:17:16.101: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 12/16/23 16:17:58.577
  Dec 16 16:18:00.640: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Dec 16 16:18:00.641: INFO: Breadth first check of 10.233.64.4 on host 192.168.121.172...
  Dec 16 16:18:00.693: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.23:9080/dial?request=hostname&protocol=udp&host=10.233.64.4&port=8081&tries=1'] Namespace:pod-network-test-9039 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 16:18:00.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 16:18:00.698: INFO: ExecWithOptions: Clientset creation
  Dec 16 16:18:00.698: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9039/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.4%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Dec 16 16:18:00.860: INFO: Waiting for responses: map[]
  Dec 16 16:18:00.861: INFO: reached 10.233.64.4 after 0/1 tries
  Dec 16 16:18:00.861: INFO: Breadth first check of 10.233.65.3 on host 192.168.121.49...
  Dec 16 16:18:00.867: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.23:9080/dial?request=hostname&protocol=udp&host=10.233.65.3&port=8081&tries=1'] Namespace:pod-network-test-9039 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 16:18:00.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 16:18:00.869: INFO: ExecWithOptions: Clientset creation
  Dec 16 16:18:00.869: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9039/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.65.3%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Dec 16 16:18:00.974: INFO: Waiting for responses: map[]
  Dec 16 16:18:00.974: INFO: reached 10.233.65.3 after 0/1 tries
  Dec 16 16:18:00.975: INFO: Breadth first check of 10.233.66.22 on host 192.168.121.112...
  Dec 16 16:18:00.982: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.23:9080/dial?request=hostname&protocol=udp&host=10.233.66.22&port=8081&tries=1'] Namespace:pod-network-test-9039 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 16:18:00.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 16:18:00.983: INFO: ExecWithOptions: Clientset creation
  Dec 16 16:18:00.983: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9039/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.23%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.22%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Dec 16 16:18:01.097: INFO: Waiting for responses: map[]
  Dec 16 16:18:01.097: INFO: reached 10.233.66.22 after 0/1 tries
  Dec 16 16:18:01.097: INFO: Going to retry 0 out of 3 pods....
  Dec 16 16:18:01.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-9039" for this suite. @ 12/16/23 16:18:01.107
• [45.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 12/16/23 16:18:01.125
  Dec 16 16:18:01.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename endpointslice @ 12/16/23 16:18:01.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:18:01.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:18:01.162
  STEP: getting /apis @ 12/16/23 16:18:01.169
  STEP: getting /apis/discovery.k8s.io @ 12/16/23 16:18:01.179
  STEP: getting /apis/discovery.k8s.iov1 @ 12/16/23 16:18:01.182
  STEP: creating @ 12/16/23 16:18:01.184
  STEP: getting @ 12/16/23 16:18:01.217
  STEP: listing @ 12/16/23 16:18:01.224
  STEP: watching @ 12/16/23 16:18:01.231
  Dec 16 16:18:01.232: INFO: starting watch
  STEP: cluster-wide listing @ 12/16/23 16:18:01.234
  STEP: cluster-wide watching @ 12/16/23 16:18:01.242
  Dec 16 16:18:01.242: INFO: starting watch
  STEP: patching @ 12/16/23 16:18:01.25
  STEP: updating @ 12/16/23 16:18:01.264
  Dec 16 16:18:01.284: INFO: waiting for watch events with expected annotations
  Dec 16 16:18:01.285: INFO: saw patched and updated annotations
  STEP: deleting @ 12/16/23 16:18:01.287
  STEP: deleting a collection @ 12/16/23 16:18:01.327
  Dec 16 16:18:01.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8857" for this suite. @ 12/16/23 16:18:01.364
• [0.254 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 12/16/23 16:18:01.38
  Dec 16 16:18:01.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename gc @ 12/16/23 16:18:01.382
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:18:01.42
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:18:01.426
  STEP: create the deployment @ 12/16/23 16:18:01.431
  W1216 16:18:01.442954      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 12/16/23 16:18:01.443
  STEP: delete the deployment @ 12/16/23 16:18:01.959
  STEP: wait for all rs to be garbage collected @ 12/16/23 16:18:01.974
  STEP: expected 0 rs, got 1 rs @ 12/16/23 16:18:02.011
  STEP: expected 0 pods, got 2 pods @ 12/16/23 16:18:02.027
  STEP: Gathering metrics @ 12/16/23 16:18:02.566
  Dec 16 16:18:02.754: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Dec 16 16:18:02.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2327" for this suite. @ 12/16/23 16:18:02.763
• [1.396 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:443
  STEP: Creating a kubernetes client @ 12/16/23 16:18:02.778
  Dec 16 16:18:02.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename daemonsets @ 12/16/23 16:18:02.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:18:02.831
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:18:02.838
  Dec 16 16:18:02.910: INFO: Create a RollingUpdate DaemonSet
  Dec 16 16:18:02.926: INFO: Check that daemon pods launch on every node of the cluster
  Dec 16 16:18:02.963: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 16:18:02.963: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:03.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 16:18:03.982: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:04.987: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 16:18:04.988: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:05.987: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 16 16:18:05.987: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:06.989: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 16 16:18:06.989: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:07.991: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 16 16:18:07.992: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:08.983: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 16 16:18:08.984: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:09.984: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 16 16:18:09.984: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:10.995: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 16 16:18:10.997: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:11.978: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 16 16:18:11.979: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:13.004: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 16 16:18:13.004: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:13.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 16 16:18:13.982: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:15.015: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 16 16:18:15.016: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:15.987: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 16 16:18:15.987: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:16.995: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 16 16:18:16.995: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:17.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 16 16:18:17.986: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:18.983: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 16 16:18:18.984: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:19.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 16 16:18:19.982: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:20.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Dec 16 16:18:20.982: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Dec 16 16:18:20.982: INFO: Update the DaemonSet to trigger a rollout
  Dec 16 16:18:21.003: INFO: Updating DaemonSet daemon-set
  Dec 16 16:18:24.048: INFO: Roll back the DaemonSet before rollout is complete
  Dec 16 16:18:24.064: INFO: Updating DaemonSet daemon-set
  Dec 16 16:18:24.064: INFO: Make sure DaemonSet rollback is complete
  Dec 16 16:18:24.070: INFO: Wrong image for pod: daemon-set-482bg. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Dec 16 16:18:24.071: INFO: Pod daemon-set-482bg is not available
  Dec 16 16:18:30.105: INFO: Pod daemon-set-bwcsn is not available
  STEP: Deleting DaemonSet "daemon-set" @ 12/16/23 16:18:30.138
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-490, will wait for the garbage collector to delete the pods @ 12/16/23 16:18:30.138
  Dec 16 16:18:30.216: INFO: Deleting DaemonSet.extensions daemon-set took: 14.667096ms
  Dec 16 16:18:30.416: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.444726ms
  Dec 16 16:18:33.124: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 16:18:33.124: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Dec 16 16:18:33.130: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"3472"},"items":null}

  Dec 16 16:18:33.137: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"3472"},"items":null}

  Dec 16 16:18:33.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-490" for this suite. @ 12/16/23 16:18:33.174
• [30.414 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 12/16/23 16:18:33.203
  Dec 16 16:18:33.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename configmap @ 12/16/23 16:18:33.207
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:18:33.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:18:33.247
  STEP: Creating configMap with name configmap-test-upd-dabc9c3b-8e43-47f6-bb17-580a21186f55 @ 12/16/23 16:18:33.26
  STEP: Creating the pod @ 12/16/23 16:18:33.271
  STEP: Waiting for pod with text data @ 12/16/23 16:18:35.305
  STEP: Waiting for pod with binary data @ 12/16/23 16:18:35.334
  Dec 16 16:18:35.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7341" for this suite. @ 12/16/23 16:18:35.354
• [2.163 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:305
  STEP: Creating a kubernetes client @ 12/16/23 16:18:35.37
  Dec 16 16:18:35.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename daemonsets @ 12/16/23 16:18:35.373
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:18:35.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:18:35.405
  STEP: Creating a simple DaemonSet "daemon-set" @ 12/16/23 16:18:35.447
  STEP: Check that daemon pods launch on every node of the cluster. @ 12/16/23 16:18:35.459
  Dec 16 16:18:35.483: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 16:18:35.483: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:36.497: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 16:18:36.498: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:37.501: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 16 16:18:37.502: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:38.518: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Dec 16 16:18:38.518: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 12/16/23 16:18:38.529
  Dec 16 16:18:38.582: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Dec 16 16:18:38.582: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 12/16/23 16:18:38.582
  STEP: Deleting DaemonSet "daemon-set" @ 12/16/23 16:18:38.614
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6061, will wait for the garbage collector to delete the pods @ 12/16/23 16:18:38.614
  Dec 16 16:18:38.694: INFO: Deleting DaemonSet.extensions daemon-set took: 20.930041ms
  Dec 16 16:18:38.795: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.719649ms
  Dec 16 16:18:41.302: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 16:18:41.302: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Dec 16 16:18:41.331: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"3625"},"items":null}

  Dec 16 16:18:41.343: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"3625"},"items":null}

  Dec 16 16:18:41.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6061" for this suite. @ 12/16/23 16:18:41.393
• [6.049 seconds]
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 12/16/23 16:18:41.419
  Dec 16 16:18:41.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename downward-api @ 12/16/23 16:18:41.421
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:18:41.449
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:18:41.456
  STEP: Creating a pod to test downward API volume plugin @ 12/16/23 16:18:41.46
  STEP: Saw pod success @ 12/16/23 16:18:45.514
  Dec 16 16:18:45.524: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downwardapi-volume-603acd55-5147-4d24-b56e-c4236e91a983 container client-container: <nil>
  STEP: delete the pod @ 12/16/23 16:18:45.543
  Dec 16 16:18:45.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1738" for this suite. @ 12/16/23 16:18:45.615
• [4.217 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:836
  STEP: Creating a kubernetes client @ 12/16/23 16:18:45.639
  Dec 16 16:18:45.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename daemonsets @ 12/16/23 16:18:45.643
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:18:45.681
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:18:45.687
  STEP: Creating simple DaemonSet "daemon-set" @ 12/16/23 16:18:45.741
  STEP: Check that daemon pods launch on every node of the cluster. @ 12/16/23 16:18:45.756
  Dec 16 16:18:45.789: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 16:18:45.789: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:46.814: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 16:18:46.814: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:18:47.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Dec 16 16:18:47.809: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 12/16/23 16:18:47.815
  STEP: DeleteCollection of the DaemonSets @ 12/16/23 16:18:47.822
  STEP: Verify that ReplicaSets have been deleted @ 12/16/23 16:18:47.84
  Dec 16 16:18:47.862: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"3722"},"items":null}

  Dec 16 16:18:47.903: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"3722"},"items":[{"metadata":{"name":"daemon-set-62frl","generateName":"daemon-set-","namespace":"daemonsets-3507","uid":"b495be98-a941-4e78-9c39-8bb710ce2eae","resourceVersion":"3720","creationTimestamp":"2023-12-16T16:18:45Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"00214084-f3d8-41b3-bf6e-d22330821cdf","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-12-16T16:18:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00214084-f3d8-41b3-bf6e-d22330821cdf\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-12-16T16:18:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9jbf6","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9jbf6","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"phoh7xai9ouk-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["phoh7xai9ouk-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-16T16:18:45Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-16T16:18:47Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-16T16:18:47Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-16T16:18:45Z"}],"hostIP":"192.168.121.112","podIP":"10.233.66.29","podIPs":[{"ip":"10.233.66.29"}],"startTime":"2023-12-16T16:18:45Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-12-16T16:18:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://7113da496eb27de450231af05094cdfb0483aa40fd60f8a761dc1b2640a29b80","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-jvs6s","generateName":"daemon-set-","namespace":"daemonsets-3507","uid":"70d6cc67-0586-40a1-b8fb-ca9fb54a8eb0","resourceVersion":"3713","creationTimestamp":"2023-12-16T16:18:45Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"00214084-f3d8-41b3-bf6e-d22330821cdf","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-12-16T16:18:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00214084-f3d8-41b3-bf6e-d22330821cdf\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-12-16T16:18:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-nfzrt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-nfzrt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"phoh7xai9ouk-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["phoh7xai9ouk-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-16T16:18:45Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-16T16:18:47Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-16T16:18:47Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-16T16:18:45Z"}],"hostIP":"192.168.121.172","podIP":"10.233.64.10","podIPs":[{"ip":"10.233.64.10"}],"startTime":"2023-12-16T16:18:45Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-12-16T16:18:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://7d44a94e2a2208ec97248b85388570959ee8892d8d50b9ba75a983d4af6a4547","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-m29sx","generateName":"daemon-set-","namespace":"daemonsets-3507","uid":"c07406e9-78f9-4bfb-9080-765d0f27cad4","resourceVersion":"3716","creationTimestamp":"2023-12-16T16:18:45Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"00214084-f3d8-41b3-bf6e-d22330821cdf","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-12-16T16:18:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"00214084-f3d8-41b3-bf6e-d22330821cdf\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-12-16T16:18:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-lj2nj","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-lj2nj","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"phoh7xai9ouk-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["phoh7xai9ouk-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-16T16:18:45Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-16T16:18:47Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-16T16:18:47Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-16T16:18:45Z"}],"hostIP":"192.168.121.49","podIP":"10.233.65.7","podIPs":[{"ip":"10.233.65.7"}],"startTime":"2023-12-16T16:18:45Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-12-16T16:18:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://b79795544ff13b3a3f860828444bc5151c5cca3c38a9342df9409b64a594482e","started":true}],"qosClass":"BestEffort"}}]}

  Dec 16 16:18:47.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3507" for this suite. @ 12/16/23 16:18:47.949
• [2.325 seconds]
------------------------------
SS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 12/16/23 16:18:47.963
  Dec 16 16:18:47.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename events @ 12/16/23 16:18:47.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:18:48.002
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:18:48.007
  STEP: creating a test event @ 12/16/23 16:18:48.012
  STEP: listing all events in all namespaces @ 12/16/23 16:18:48.02
  STEP: patching the test event @ 12/16/23 16:18:48.043
  STEP: fetching the test event @ 12/16/23 16:18:48.059
  STEP: updating the test event @ 12/16/23 16:18:48.064
  STEP: getting the test event @ 12/16/23 16:18:48.084
  STEP: deleting the test event @ 12/16/23 16:18:48.094
  STEP: listing all events in all namespaces @ 12/16/23 16:18:48.139
  Dec 16 16:18:48.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-1481" for this suite. @ 12/16/23 16:18:48.341
• [0.396 seconds]
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:959
  STEP: Creating a kubernetes client @ 12/16/23 16:18:48.36
  Dec 16 16:18:48.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename statefulset @ 12/16/23 16:18:48.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:18:48.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:18:48.469
  STEP: Creating service test in namespace statefulset-1442 @ 12/16/23 16:18:48.484
  Dec 16 16:18:48.600: INFO: Found 0 stateful pods, waiting for 1
  Dec 16 16:18:58.610: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 12/16/23 16:18:58.621
  W1216 16:18:58.639647      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Dec 16 16:18:58.655: INFO: Found 1 stateful pods, waiting for 2
  Dec 16 16:19:08.667: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Dec 16 16:19:08.667: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 12/16/23 16:19:08.681
  STEP: Delete all of the StatefulSets @ 12/16/23 16:19:08.687
  STEP: Verify that StatefulSets have been deleted @ 12/16/23 16:19:08.71
  Dec 16 16:19:08.724: INFO: Deleting all statefulset in ns statefulset-1442
  Dec 16 16:19:08.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1442" for this suite. @ 12/16/23 16:19:08.898
• [20.550 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 12/16/23 16:19:08.913
  Dec 16 16:19:08.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename var-expansion @ 12/16/23 16:19:08.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:19:08.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:19:08.97
  STEP: creating the pod with failed condition @ 12/16/23 16:19:08.975
  STEP: updating the pod @ 12/16/23 16:21:09.003
  Dec 16 16:21:09.532: INFO: Successfully updated pod "var-expansion-8474c10c-3f56-454a-96d6-f1adfad48d94"
  STEP: waiting for pod running @ 12/16/23 16:21:09.532
  STEP: deleting the pod gracefully @ 12/16/23 16:21:11.552
  Dec 16 16:21:11.552: INFO: Deleting pod "var-expansion-8474c10c-3f56-454a-96d6-f1adfad48d94" in namespace "var-expansion-416"
  Dec 16 16:21:11.571: INFO: Wait up to 5m0s for pod "var-expansion-8474c10c-3f56-454a-96d6-f1adfad48d94" to be fully deleted
  Dec 16 16:21:43.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-416" for this suite. @ 12/16/23 16:21:43.766
• [154.872 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 12/16/23 16:21:43.79
  Dec 16 16:21:43.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename var-expansion @ 12/16/23 16:21:43.794
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:21:43.839
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:21:43.845
  Dec 16 16:21:45.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 16 16:21:45.919: INFO: Deleting pod "var-expansion-f8fe8cde-649c-4f51-853d-bffb3a923f98" in namespace "var-expansion-7122"
  Dec 16 16:21:45.939: INFO: Wait up to 5m0s for pod "var-expansion-f8fe8cde-649c-4f51-853d-bffb3a923f98" to be fully deleted
  STEP: Destroying namespace "var-expansion-7122" for this suite. @ 12/16/23 16:21:47.956
• [4.185 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 12/16/23 16:21:47.976
  Dec 16 16:21:47.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename lease-test @ 12/16/23 16:21:47.978
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:21:48.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:21:48.018
  Dec 16 16:21:48.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-6155" for this suite. @ 12/16/23 16:21:48.197
• [0.231 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 12/16/23 16:21:48.211
  Dec 16 16:21:48.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename emptydir @ 12/16/23 16:21:48.214
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:21:48.248
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:21:48.255
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 12/16/23 16:21:48.263
  STEP: Saw pod success @ 12/16/23 16:21:52.32
  Dec 16 16:21:52.330: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-21a742e7-ddbf-4e08-b730-1ad7b9985d27 container test-container: <nil>
  STEP: delete the pod @ 12/16/23 16:21:52.386
  Dec 16 16:21:52.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9480" for this suite. @ 12/16/23 16:21:52.44
• [4.246 seconds]
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 12/16/23 16:21:52.456
  Dec 16 16:21:52.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename replication-controller @ 12/16/23 16:21:52.46
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:21:52.494
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:21:52.503
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 12/16/23 16:21:52.51
  STEP: When a replication controller with a matching selector is created @ 12/16/23 16:21:54.556
  STEP: Then the orphan pod is adopted @ 12/16/23 16:21:54.571
  Dec 16 16:21:55.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1032" for this suite. @ 12/16/23 16:21:55.602
• [3.160 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 12/16/23 16:21:55.63
  Dec 16 16:21:55.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename configmap @ 12/16/23 16:21:55.633
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:21:55.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:21:55.671
  STEP: Creating configMap with name configmap-test-volume-map-a3c99939-82e1-49e6-a296-c6b75cb2e94d @ 12/16/23 16:21:55.681
  STEP: Creating a pod to test consume configMaps @ 12/16/23 16:21:55.693
  STEP: Saw pod success @ 12/16/23 16:21:59.745
  Dec 16 16:21:59.754: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-configmaps-32085fda-115d-405b-8fb2-b1218299c3e8 container agnhost-container: <nil>
  STEP: delete the pod @ 12/16/23 16:21:59.78
  Dec 16 16:21:59.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2424" for this suite. @ 12/16/23 16:21:59.825
• [4.208 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 12/16/23 16:21:59.843
  Dec 16 16:21:59.843: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename namespaces @ 12/16/23 16:21:59.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:21:59.879
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:21:59.884
  STEP: Creating a test namespace @ 12/16/23 16:21:59.889
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:21:59.94
  STEP: Creating a pod in the namespace @ 12/16/23 16:21:59.949
  STEP: Waiting for the pod to have running status @ 12/16/23 16:21:59.969
  STEP: Deleting the namespace @ 12/16/23 16:22:01.989
  STEP: Waiting for the namespace to be removed. @ 12/16/23 16:22:02.01
  STEP: Recreating the namespace @ 12/16/23 16:22:14.026
  STEP: Verifying there are no pods in the namespace @ 12/16/23 16:22:14.085
  Dec 16 16:22:14.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9038" for this suite. @ 12/16/23 16:22:14.112
  STEP: Destroying namespace "nsdeletetest-7239" for this suite. @ 12/16/23 16:22:14.13
  Dec 16 16:22:14.138: INFO: Namespace nsdeletetest-7239 was already deleted
  STEP: Destroying namespace "nsdeletetest-4553" for this suite. @ 12/16/23 16:22:14.139
• [14.312 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 12/16/23 16:22:14.183
  Dec 16 16:22:14.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename svcaccounts @ 12/16/23 16:22:14.189
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:22:14.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:22:14.251
  Dec 16 16:22:14.295: INFO: Got root ca configmap in namespace "svcaccounts-5504"
  Dec 16 16:22:14.311: INFO: Deleted root ca configmap in namespace "svcaccounts-5504"
  STEP: waiting for a new root ca configmap created @ 12/16/23 16:22:14.813
  Dec 16 16:22:14.825: INFO: Recreated root ca configmap in namespace "svcaccounts-5504"
  Dec 16 16:22:14.838: INFO: Updated root ca configmap in namespace "svcaccounts-5504"
  STEP: waiting for the root ca configmap reconciled @ 12/16/23 16:22:15.339
  Dec 16 16:22:15.351: INFO: Reconciled root ca configmap in namespace "svcaccounts-5504"
  Dec 16 16:22:15.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5504" for this suite. @ 12/16/23 16:22:15.364
• [1.203 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 12/16/23 16:22:15.39
  Dec 16 16:22:15.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubectl @ 12/16/23 16:22:15.395
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:22:15.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:22:15.443
  STEP: create deployment with httpd image @ 12/16/23 16:22:15.453
  Dec 16 16:22:15.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9376 create -f -'
  Dec 16 16:22:17.071: INFO: stderr: ""
  Dec 16 16:22:17.071: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 12/16/23 16:22:17.071
  Dec 16 16:22:17.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9376 diff -f -'
  Dec 16 16:22:17.880: INFO: rc: 1
  Dec 16 16:22:17.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9376 delete -f -'
  Dec 16 16:22:18.120: INFO: stderr: ""
  Dec 16 16:22:18.120: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Dec 16 16:22:18.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9376" for this suite. @ 12/16/23 16:22:18.136
• [2.763 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:96
  STEP: Creating a kubernetes client @ 12/16/23 16:22:18.165
  Dec 16 16:22:18.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename aggregator @ 12/16/23 16:22:18.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:22:18.219
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:22:18.233
  Dec 16 16:22:18.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Registering the sample API server. @ 12/16/23 16:22:18.239
  Dec 16 16:22:19.236: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Dec 16 16:22:19.324: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  Dec 16 16:22:21.432: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:23.443: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:25.451: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:27.456: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:29.443: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:31.447: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:33.440: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:35.443: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:37.441: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:39.440: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:41.441: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:43.448: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:45.443: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:47.444: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:49.441: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:51.446: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:53.440: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:55.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:57.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:22:59.447: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:23:01.470: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:23:03.441: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:23:05.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:23:07.441: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:23:09.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:23:11.446: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:23:13.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:23:15.443: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:23:17.446: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:23:19.441: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:23:21.446: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 16, 22, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 16 16:23:23.606: INFO: Waited 141.986318ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 12/16/23 16:23:23.753
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 12/16/23 16:23:23.761
  STEP: List APIServices @ 12/16/23 16:23:23.778
  Dec 16 16:23:23.806: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 12/16/23 16:23:23.806
  Dec 16 16:23:23.845: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 12/16/23 16:23:23.845
  Dec 16 16:23:23.881: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.December, 16, 16, 23, 23, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 12/16/23 16:23:23.881
  Dec 16 16:23:23.891: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-12-16 16:23:23 +0000 UTC Passed all checks passed}
  Dec 16 16:23:23.891: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Dec 16 16:23:23.891: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 12/16/23 16:23:23.891
  Dec 16 16:23:23.923: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-766074489" @ 12/16/23 16:23:23.923
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 12/16/23 16:23:23.958
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 12/16/23 16:23:23.981
  STEP: Patch APIService Status @ 12/16/23 16:23:23.994
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 12/16/23 16:23:24.021
  Dec 16 16:23:24.033: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-12-16 16:23:23 +0000 UTC Passed all checks passed}
  Dec 16 16:23:24.034: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Dec 16 16:23:24.034: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Dec 16 16:23:24.034: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 12/16/23 16:23:24.035
  STEP: Confirm that the generated APIService has been deleted @ 12/16/23 16:23:24.049
  Dec 16 16:23:24.049: INFO: Requesting list of APIServices to confirm quantity
  Dec 16 16:23:24.060: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Dec 16 16:23:24.060: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Dec 16 16:23:24.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-5446" for this suite. @ 12/16/23 16:23:24.43
• [66.282 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 12/16/23 16:23:24.452
  Dec 16 16:23:24.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 16:23:24.459
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:23:24.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:23:24.517
  STEP: Creating configMap with name projected-configmap-test-volume-map-a23b4843-64fc-448d-a569-1b374bbb139c @ 12/16/23 16:23:24.523
  STEP: Creating a pod to test consume configMaps @ 12/16/23 16:23:24.534
  STEP: Saw pod success @ 12/16/23 16:23:28.593
  Dec 16 16:23:28.602: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-projected-configmaps-a78dfcad-54d5-4172-9cc3-1cac84f9851f container agnhost-container: <nil>
  STEP: delete the pod @ 12/16/23 16:23:28.684
  Dec 16 16:23:28.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3998" for this suite. @ 12/16/23 16:23:28.755
• [4.327 seconds]
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 12/16/23 16:23:28.782
  Dec 16 16:23:28.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename pods @ 12/16/23 16:23:28.79
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:23:28.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:23:28.841
  STEP: Create a pod @ 12/16/23 16:23:28.852
  STEP: patching /status @ 12/16/23 16:23:30.898
  Dec 16 16:23:30.917: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Dec 16 16:23:30.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7477" for this suite. @ 12/16/23 16:23:30.929
• [2.163 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 12/16/23 16:23:30.946
  Dec 16 16:23:30.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename runtimeclass @ 12/16/23 16:23:30.949
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:23:30.986
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:23:30.992
  Dec 16 16:23:33.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2323" for this suite. @ 12/16/23 16:23:33.1
• [2.180 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 12/16/23 16:23:33.129
  Dec 16 16:23:33.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename events @ 12/16/23 16:23:33.132
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:23:33.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:23:33.186
  STEP: Create set of events @ 12/16/23 16:23:33.191
  STEP: get a list of Events with a label in the current namespace @ 12/16/23 16:23:33.231
  STEP: delete a list of events @ 12/16/23 16:23:33.243
  Dec 16 16:23:33.243: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 12/16/23 16:23:33.305
  Dec 16 16:23:33.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-1024" for this suite. @ 12/16/23 16:23:33.325
• [0.216 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 12/16/23 16:23:33.346
  Dec 16 16:23:33.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename gc @ 12/16/23 16:23:33.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:23:33.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:23:33.391
  Dec 16 16:23:33.530: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"7bd5ec54-2ec6-45e6-8820-22bfb3b3df8a", Controller:(*bool)(0xc0012e83f6), BlockOwnerDeletion:(*bool)(0xc0012e83f7)}}
  Dec 16 16:23:33.552: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"e5b9e974-2bdd-4d9f-8f1d-e5b99e0c498e", Controller:(*bool)(0xc0030408ce), BlockOwnerDeletion:(*bool)(0xc0030408cf)}}
  Dec 16 16:23:33.577: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"5a53902d-7023-46c9-888b-c581cfd5508c", Controller:(*bool)(0xc0012e8696), BlockOwnerDeletion:(*bool)(0xc0012e8697)}}
  Dec 16 16:23:38.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1969" for this suite. @ 12/16/23 16:23:38.618
• [5.291 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 12/16/23 16:23:38.641
  Dec 16 16:23:38.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename security-context @ 12/16/23 16:23:38.644
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:23:38.71
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:23:38.723
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 12/16/23 16:23:38.738
  STEP: Saw pod success @ 12/16/23 16:23:42.806
  Dec 16 16:23:42.815: INFO: Trying to get logs from node phoh7xai9ouk-3 pod security-context-2d40ada6-7622-43a5-ab3a-0ca4b8b5587e container test-container: <nil>
  STEP: delete the pod @ 12/16/23 16:23:42.831
  Dec 16 16:23:42.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-9683" for this suite. @ 12/16/23 16:23:42.882
• [4.272 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 12/16/23 16:23:42.924
  Dec 16 16:23:42.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-probe @ 12/16/23 16:23:42.928
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:23:42.98
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:23:42.987
  STEP: Creating pod test-webserver-d6522603-6237-478b-8463-ebd1e5afa051 in namespace container-probe-5544 @ 12/16/23 16:23:42.998
  Dec 16 16:23:45.042: INFO: Started pod test-webserver-d6522603-6237-478b-8463-ebd1e5afa051 in namespace container-probe-5544
  STEP: checking the pod's current state and verifying that restartCount is present @ 12/16/23 16:23:45.042
  Dec 16 16:23:45.049: INFO: Initial restart count of pod test-webserver-d6522603-6237-478b-8463-ebd1e5afa051 is 0
  Dec 16 16:27:46.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/16/23 16:27:46.428
  STEP: Destroying namespace "container-probe-5544" for this suite. @ 12/16/23 16:27:46.472
• [243.568 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 12/16/23 16:27:46.498
  Dec 16 16:27:46.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-probe @ 12/16/23 16:27:46.504
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:27:46.534
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:27:46.539
  Dec 16 16:28:08.772: INFO: Container started at 2023-12-16 16:27:47 +0000 UTC, pod became ready at 2023-12-16 16:28:06 +0000 UTC
  Dec 16 16:28:08.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-5984" for this suite. @ 12/16/23 16:28:08.793
• [22.316 seconds]
------------------------------
SS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 12/16/23 16:28:08.815
  Dec 16 16:28:08.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename cronjob @ 12/16/23 16:28:08.818
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:28:08.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:28:08.863
  STEP: Creating a ReplaceConcurrent cronjob @ 12/16/23 16:28:08.868
  STEP: Ensuring a job is scheduled @ 12/16/23 16:28:08.882
  STEP: Ensuring exactly one is scheduled @ 12/16/23 16:29:00.892
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 12/16/23 16:29:00.9
  STEP: Ensuring the job is replaced with a new one @ 12/16/23 16:29:00.906
  STEP: Removing cronjob @ 12/16/23 16:30:00.921
  Dec 16 16:30:00.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5186" for this suite. @ 12/16/23 16:30:00.96
• [112.168 seconds]
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 12/16/23 16:30:00.983
  Dec 16 16:30:00.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 16:30:00.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:30:01.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:30:01.118
  STEP: Creating configMap with name projected-configmap-test-volume-map-9d196f66-0bd2-4cb1-a461-9e4f6b50fe34 @ 12/16/23 16:30:01.126
  STEP: Creating a pod to test consume configMaps @ 12/16/23 16:30:01.139
  STEP: Saw pod success @ 12/16/23 16:30:05.202
  Dec 16 16:30:05.209: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-projected-configmaps-751d2f05-63f7-4c80-9d07-5f8097d04143 container agnhost-container: <nil>
  STEP: delete the pod @ 12/16/23 16:30:05.246
  Dec 16 16:30:05.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6341" for this suite. @ 12/16/23 16:30:05.285
• [4.315 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 12/16/23 16:30:05.3
  Dec 16 16:30:05.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename replicaset @ 12/16/23 16:30:05.303
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:30:05.329
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:30:05.336
  STEP: Create a ReplicaSet @ 12/16/23 16:30:05.346
  STEP: Verify that the required pods have come up @ 12/16/23 16:30:05.364
  Dec 16 16:30:05.375: INFO: Pod name sample-pod: Found 1 pods out of 3
  Dec 16 16:30:10.392: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 12/16/23 16:30:10.394
  Dec 16 16:30:10.406: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 12/16/23 16:30:10.407
  STEP: DeleteCollection of the ReplicaSets @ 12/16/23 16:30:10.42
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 12/16/23 16:30:10.448
  Dec 16 16:30:10.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-827" for this suite. @ 12/16/23 16:30:10.467
• [5.278 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 12/16/23 16:30:10.593
  Dec 16 16:30:10.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename crd-publish-openapi @ 12/16/23 16:30:10.597
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:30:10.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:30:10.681
  Dec 16 16:30:10.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 12/16/23 16:30:12.73
  Dec 16 16:30:12.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-1600 --namespace=crd-publish-openapi-1600 create -f -'
  Dec 16 16:30:14.283: INFO: stderr: ""
  Dec 16 16:30:14.284: INFO: stdout: "e2e-test-crd-publish-openapi-5201-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Dec 16 16:30:14.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-1600 --namespace=crd-publish-openapi-1600 delete e2e-test-crd-publish-openapi-5201-crds test-cr'
  Dec 16 16:30:14.604: INFO: stderr: ""
  Dec 16 16:30:14.604: INFO: stdout: "e2e-test-crd-publish-openapi-5201-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Dec 16 16:30:14.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-1600 --namespace=crd-publish-openapi-1600 apply -f -'
  Dec 16 16:30:15.895: INFO: stderr: ""
  Dec 16 16:30:15.895: INFO: stdout: "e2e-test-crd-publish-openapi-5201-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Dec 16 16:30:15.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-1600 --namespace=crd-publish-openapi-1600 delete e2e-test-crd-publish-openapi-5201-crds test-cr'
  Dec 16 16:30:16.098: INFO: stderr: ""
  Dec 16 16:30:16.099: INFO: stdout: "e2e-test-crd-publish-openapi-5201-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 12/16/23 16:30:16.099
  Dec 16 16:30:16.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-1600 explain e2e-test-crd-publish-openapi-5201-crds'
  Dec 16 16:30:16.593: INFO: stderr: ""
  Dec 16 16:30:16.594: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-5201-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  Dec 16 16:30:19.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1600" for this suite. @ 12/16/23 16:30:19.262
• [8.682 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 12/16/23 16:30:19.29
  Dec 16 16:30:19.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubectl @ 12/16/23 16:30:19.295
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:30:19.329
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:30:19.336
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 12/16/23 16:30:19.342
  Dec 16 16:30:19.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-5497 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Dec 16 16:30:19.514: INFO: stderr: ""
  Dec 16 16:30:19.514: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 12/16/23 16:30:19.514
  STEP: verifying the pod e2e-test-httpd-pod was created @ 12/16/23 16:30:24.565
  Dec 16 16:30:24.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-5497 get pod e2e-test-httpd-pod -o json'
  Dec 16 16:30:24.842: INFO: stderr: ""
  Dec 16 16:30:24.842: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-12-16T16:30:19Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5497\",\n        \"resourceVersion\": \"5824\",\n        \"uid\": \"e9bf7c80-063b-4778-a33d-443280675fe2\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-4l5vd\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"phoh7xai9ouk-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-4l5vd\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-12-16T16:30:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-12-16T16:30:20Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-12-16T16:30:20Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-12-16T16:30:19Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://e0d9a11bfcc4640dc35a7d718acaae83df96f8aa248077e69ee942efb853e18e\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-12-16T16:30:20Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.112\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.66.52\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.66.52\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-12-16T16:30:19Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 12/16/23 16:30:24.843
  Dec 16 16:30:24.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-5497 replace -f -'
  Dec 16 16:30:26.210: INFO: stderr: ""
  Dec 16 16:30:26.210: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 12/16/23 16:30:26.21
  Dec 16 16:30:26.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-5497 delete pods e2e-test-httpd-pod'
  Dec 16 16:30:27.797: INFO: stderr: ""
  Dec 16 16:30:27.797: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Dec 16 16:30:27.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5497" for this suite. @ 12/16/23 16:30:27.81
• [8.534 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 12/16/23 16:30:27.826
  Dec 16 16:30:27.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename pods @ 12/16/23 16:30:27.83
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:30:27.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:30:27.872
  STEP: Saw pod success @ 12/16/23 16:30:34.003
  Dec 16 16:30:34.009: INFO: Trying to get logs from node phoh7xai9ouk-3 pod client-envvars-1d25b2fd-4bbe-4371-a4e4-8c723a689fdc container env3cont: <nil>
  STEP: delete the pod @ 12/16/23 16:30:34.048
  Dec 16 16:30:34.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3408" for this suite. @ 12/16/23 16:30:34.084
• [6.271 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 12/16/23 16:30:34.098
  Dec 16 16:30:34.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename init-container @ 12/16/23 16:30:34.1
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:30:34.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:30:34.142
  STEP: creating the pod @ 12/16/23 16:30:34.149
  Dec 16 16:30:34.149: INFO: PodSpec: initContainers in spec.initContainers
  Dec 16 16:30:39.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-8146" for this suite. @ 12/16/23 16:30:39.02
• [4.939 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 12/16/23 16:30:39.04
  Dec 16 16:30:39.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubectl @ 12/16/23 16:30:39.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:30:39.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:30:39.092
  STEP: creating Agnhost RC @ 12/16/23 16:30:39.102
  Dec 16 16:30:39.102: INFO: namespace kubectl-3555
  Dec 16 16:30:39.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-3555 create -f -'
  Dec 16 16:30:39.856: INFO: stderr: ""
  Dec 16 16:30:39.856: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 12/16/23 16:30:39.856
  Dec 16 16:30:40.874: INFO: Selector matched 1 pods for map[app:agnhost]
  Dec 16 16:30:40.874: INFO: Found 0 / 1
  Dec 16 16:30:41.864: INFO: Selector matched 1 pods for map[app:agnhost]
  Dec 16 16:30:41.864: INFO: Found 1 / 1
  Dec 16 16:30:41.864: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Dec 16 16:30:41.874: INFO: Selector matched 1 pods for map[app:agnhost]
  Dec 16 16:30:41.874: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Dec 16 16:30:41.874: INFO: wait on agnhost-primary startup in kubectl-3555 
  Dec 16 16:30:41.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-3555 logs agnhost-primary-4sr66 agnhost-primary'
  Dec 16 16:30:42.097: INFO: stderr: ""
  Dec 16 16:30:42.098: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 12/16/23 16:30:42.098
  Dec 16 16:30:42.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-3555 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Dec 16 16:30:42.290: INFO: stderr: ""
  Dec 16 16:30:42.290: INFO: stdout: "service/rm2 exposed\n"
  Dec 16 16:30:42.311: INFO: Service rm2 in namespace kubectl-3555 found.
  STEP: exposing service @ 12/16/23 16:30:44.357
  Dec 16 16:30:44.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-3555 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Dec 16 16:30:44.761: INFO: stderr: ""
  Dec 16 16:30:44.761: INFO: stdout: "service/rm3 exposed\n"
  Dec 16 16:30:44.794: INFO: Service rm3 in namespace kubectl-3555 found.
  Dec 16 16:30:46.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3555" for this suite. @ 12/16/23 16:30:46.818
• [7.794 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 12/16/23 16:30:46.838
  Dec 16 16:30:46.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename cronjob @ 12/16/23 16:30:46.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:30:46.874
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:30:46.89
  STEP: Creating a cronjob @ 12/16/23 16:30:46.906
  STEP: Ensuring more than one job is running at a time @ 12/16/23 16:30:46.938
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 12/16/23 16:32:00.948
  STEP: Removing cronjob @ 12/16/23 16:32:00.957
  Dec 16 16:32:00.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-6246" for this suite. @ 12/16/23 16:32:00.984
• [74.166 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 12/16/23 16:32:01.008
  Dec 16 16:32:01.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename crd-publish-openapi @ 12/16/23 16:32:01.015
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:32:01.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:32:01.095
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 12/16/23 16:32:01.107
  Dec 16 16:32:01.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 16:32:03.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 16:32:10.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3716" for this suite. @ 12/16/23 16:32:10.757
• [9.765 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 12/16/23 16:32:10.782
  Dec 16 16:32:10.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-probe @ 12/16/23 16:32:10.785
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:32:10.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:32:10.831
  STEP: Creating pod test-grpc-51afa200-335f-4086-a6d4-663dc19cccd1 in namespace container-probe-8500 @ 12/16/23 16:32:10.836
  Dec 16 16:32:12.872: INFO: Started pod test-grpc-51afa200-335f-4086-a6d4-663dc19cccd1 in namespace container-probe-8500
  STEP: checking the pod's current state and verifying that restartCount is present @ 12/16/23 16:32:12.873
  Dec 16 16:32:12.880: INFO: Initial restart count of pod test-grpc-51afa200-335f-4086-a6d4-663dc19cccd1 is 0
  Dec 16 16:36:14.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/16/23 16:36:14.188
  STEP: Destroying namespace "container-probe-8500" for this suite. @ 12/16/23 16:36:14.223
• [243.515 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 12/16/23 16:36:14.302
  Dec 16 16:36:14.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename replication-controller @ 12/16/23 16:36:14.308
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:36:14.374
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:36:14.386
  STEP: Creating replication controller my-hostname-basic-c478a7f1-ee29-4ca9-a0cf-f46e154e356d @ 12/16/23 16:36:14.395
  Dec 16 16:36:14.424: INFO: Pod name my-hostname-basic-c478a7f1-ee29-4ca9-a0cf-f46e154e356d: Found 0 pods out of 1
  Dec 16 16:36:19.432: INFO: Pod name my-hostname-basic-c478a7f1-ee29-4ca9-a0cf-f46e154e356d: Found 1 pods out of 1
  Dec 16 16:36:19.432: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c478a7f1-ee29-4ca9-a0cf-f46e154e356d" are running
  Dec 16 16:36:19.440: INFO: Pod "my-hostname-basic-c478a7f1-ee29-4ca9-a0cf-f46e154e356d-l8gwl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-16 16:36:14 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-16 16:36:16 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-16 16:36:16 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-16 16:36:14 +0000 UTC Reason: Message:}])
  Dec 16 16:36:19.440: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 12/16/23 16:36:19.441
  Dec 16 16:36:19.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-5914" for this suite. @ 12/16/23 16:36:19.485
• [5.200 seconds]
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 12/16/23 16:36:19.503
  Dec 16 16:36:19.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 16:36:19.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:36:19.544
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:36:19.55
  STEP: Creating a pod to test downward API volume plugin @ 12/16/23 16:36:19.554
  STEP: Saw pod success @ 12/16/23 16:36:23.629
  Dec 16 16:36:23.635: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downwardapi-volume-4fa8eda8-956d-41c4-9ed9-2d9096eec679 container client-container: <nil>
  STEP: delete the pod @ 12/16/23 16:36:23.688
  Dec 16 16:36:23.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3156" for this suite. @ 12/16/23 16:36:23.727
• [4.236 seconds]
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 12/16/23 16:36:23.74
  Dec 16 16:36:23.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename runtimeclass @ 12/16/23 16:36:23.743
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:36:23.768
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:36:23.773
  Dec 16 16:36:25.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2227" for this suite. @ 12/16/23 16:36:25.859
• [2.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 12/16/23 16:36:25.883
  Dec 16 16:36:25.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename pods @ 12/16/23 16:36:25.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:36:25.926
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:36:25.932
  STEP: creating pod @ 12/16/23 16:36:25.94
  Dec 16 16:36:27.995: INFO: Pod pod-hostip-e4f5c7b5-c245-4a41-a0ec-77d0692a5da4 has hostIP: 192.168.121.112
  Dec 16 16:36:27.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-611" for this suite. @ 12/16/23 16:36:28.005
• [2.137 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 12/16/23 16:36:28.029
  Dec 16 16:36:28.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename var-expansion @ 12/16/23 16:36:28.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:36:28.062
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:36:28.069
  STEP: Creating a pod to test env composition @ 12/16/23 16:36:28.074
  STEP: Saw pod success @ 12/16/23 16:36:32.123
  Dec 16 16:36:32.133: INFO: Trying to get logs from node phoh7xai9ouk-3 pod var-expansion-0cd21b66-4a0a-479d-b048-fa566f9208eb container dapi-container: <nil>
  STEP: delete the pod @ 12/16/23 16:36:32.152
  Dec 16 16:36:32.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4139" for this suite. @ 12/16/23 16:36:32.195
• [4.182 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 12/16/23 16:36:32.217
  Dec 16 16:36:32.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename crd-publish-openapi @ 12/16/23 16:36:32.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:36:32.246
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:36:32.252
  STEP: set up a multi version CRD @ 12/16/23 16:36:32.26
  Dec 16 16:36:32.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: rename a version @ 12/16/23 16:36:37.203
  STEP: check the new version name is served @ 12/16/23 16:36:37.244
  STEP: check the old version name is removed @ 12/16/23 16:36:38.642
  STEP: check the other version is not changed @ 12/16/23 16:36:39.586
  Dec 16 16:36:43.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-773" for this suite. @ 12/16/23 16:36:43.728
• [11.526 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 12/16/23 16:36:43.745
  Dec 16 16:36:43.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename sched-preemption @ 12/16/23 16:36:43.748
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:36:43.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:36:43.786
  Dec 16 16:36:43.821: INFO: Waiting up to 1m0s for all nodes to be ready
  Dec 16 16:37:43.870: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 12/16/23 16:37:43.879
  Dec 16 16:37:43.943: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Dec 16 16:37:43.960: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Dec 16 16:37:43.998: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Dec 16 16:37:44.011: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Dec 16 16:37:44.106: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Dec 16 16:37:44.135: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 12/16/23 16:37:44.135
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 12/16/23 16:37:46.202
  Dec 16 16:37:50.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-1961" for this suite. @ 12/16/23 16:37:50.456
• [66.724 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:385
  STEP: Creating a kubernetes client @ 12/16/23 16:37:50.487
  Dec 16 16:37:50.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename daemonsets @ 12/16/23 16:37:50.492
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:37:50.527
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:37:50.532
  Dec 16 16:37:50.569: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 12/16/23 16:37:50.581
  Dec 16 16:37:50.597: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 16:37:50.599: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:37:51.622: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 16 16:37:51.622: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  Dec 16 16:37:52.689: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Dec 16 16:37:52.689: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 12/16/23 16:37:52.719
  STEP: Check that daemon pods images are updated. @ 12/16/23 16:37:52.76
  Dec 16 16:37:52.768: INFO: Wrong image for pod: daemon-set-5jv5m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Dec 16 16:37:52.768: INFO: Wrong image for pod: daemon-set-cx9dh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Dec 16 16:37:52.768: INFO: Wrong image for pod: daemon-set-m4m4b. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Dec 16 16:37:53.790: INFO: Wrong image for pod: daemon-set-5jv5m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Dec 16 16:37:53.791: INFO: Wrong image for pod: daemon-set-cx9dh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Dec 16 16:37:53.791: INFO: Pod daemon-set-zxk6v is not available
  Dec 16 16:37:54.789: INFO: Wrong image for pod: daemon-set-5jv5m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Dec 16 16:37:54.790: INFO: Wrong image for pod: daemon-set-cx9dh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Dec 16 16:37:54.791: INFO: Pod daemon-set-zxk6v is not available
  Dec 16 16:37:55.798: INFO: Pod daemon-set-bgxg5 is not available
  Dec 16 16:37:55.798: INFO: Wrong image for pod: daemon-set-cx9dh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Dec 16 16:37:56.787: INFO: Pod daemon-set-bgxg5 is not available
  Dec 16 16:37:56.787: INFO: Wrong image for pod: daemon-set-cx9dh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Dec 16 16:37:57.790: INFO: Pod daemon-set-d7fgx is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 12/16/23 16:37:57.798
  Dec 16 16:37:57.822: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 16 16:37:57.822: INFO: Node phoh7xai9ouk-3 is running 0 daemon pod, expected 1
  Dec 16 16:37:58.840: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Dec 16 16:37:58.840: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 12/16/23 16:37:58.875
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1608, will wait for the garbage collector to delete the pods @ 12/16/23 16:37:58.875
  Dec 16 16:37:58.947: INFO: Deleting DaemonSet.extensions daemon-set took: 16.043884ms
  Dec 16 16:37:59.048: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.167622ms
  Dec 16 16:38:00.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 16:38:00.761: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Dec 16 16:38:00.768: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7283"},"items":null}

  Dec 16 16:38:00.774: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7283"},"items":null}

  Dec 16 16:38:00.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1608" for this suite. @ 12/16/23 16:38:00.81
• [10.337 seconds]
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 12/16/23 16:38:00.825
  Dec 16 16:38:00.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename services @ 12/16/23 16:38:00.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:38:00.857
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:38:00.863
  STEP: fetching services @ 12/16/23 16:38:00.867
  Dec 16 16:38:00.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3037" for this suite. @ 12/16/23 16:38:00.882
• [0.070 seconds]
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 12/16/23 16:38:00.895
  Dec 16 16:38:00.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename downward-api @ 12/16/23 16:38:00.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:38:00.929
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:38:00.934
  STEP: Creating a pod to test downward api env vars @ 12/16/23 16:38:00.94
  STEP: Saw pod success @ 12/16/23 16:38:02.983
  Dec 16 16:38:02.991: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downward-api-62ce7f29-e218-4e60-9cb5-1a38d5cd7054 container dapi-container: <nil>
  STEP: delete the pod @ 12/16/23 16:38:03.038
  Dec 16 16:38:03.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6824" for this suite. @ 12/16/23 16:38:03.076
• [2.194 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 12/16/23 16:38:03.105
  Dec 16 16:38:03.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 16:38:03.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:38:03.152
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:38:03.157
  STEP: Creating the pod @ 12/16/23 16:38:03.164
  Dec 16 16:38:05.767: INFO: Successfully updated pod "annotationupdate132aa0dd-1928-4474-aa69-52f7b489a200"
  Dec 16 16:38:07.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3568" for this suite. @ 12/16/23 16:38:07.826
• [4.751 seconds]
------------------------------
SSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 12/16/23 16:38:07.858
  Dec 16 16:38:07.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename secrets @ 12/16/23 16:38:07.863
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:38:07.895
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:38:07.903
  STEP: creating secret secrets-7856/secret-test-a9119b8b-ccd0-46b3-b6dc-b46b4caf2046 @ 12/16/23 16:38:07.913
  STEP: Creating a pod to test consume secrets @ 12/16/23 16:38:07.924
  STEP: Saw pod success @ 12/16/23 16:38:11.972
  Dec 16 16:38:11.980: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-configmaps-569ad897-c43a-4ac7-aaaa-95241d43c55b container env-test: <nil>
  STEP: delete the pod @ 12/16/23 16:38:11.995
  Dec 16 16:38:12.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7856" for this suite. @ 12/16/23 16:38:12.037
• [4.208 seconds]
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 12/16/23 16:38:12.069
  Dec 16 16:38:12.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubelet-test @ 12/16/23 16:38:12.072
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:38:12.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:38:12.117
  Dec 16 16:38:16.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8241" for this suite. @ 12/16/23 16:38:16.173
• [4.129 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 12/16/23 16:38:16.202
  Dec 16 16:38:16.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename replicaset @ 12/16/23 16:38:16.205
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:38:16.244
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:38:16.25
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 12/16/23 16:38:16.255
  STEP: When a replicaset with a matching selector is created @ 12/16/23 16:38:18.303
  STEP: Then the orphan pod is adopted @ 12/16/23 16:38:18.315
  STEP: When the matched label of one of its pods change @ 12/16/23 16:38:19.334
  Dec 16 16:38:19.344: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 12/16/23 16:38:19.381
  Dec 16 16:38:20.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8553" for this suite. @ 12/16/23 16:38:20.407
• [4.218 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 12/16/23 16:38:20.433
  Dec 16 16:38:20.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename replication-controller @ 12/16/23 16:38:20.435
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:38:20.474
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:38:20.479
  STEP: Given a ReplicationController is created @ 12/16/23 16:38:20.488
  STEP: When the matched label of one of its pods change @ 12/16/23 16:38:20.498
  Dec 16 16:38:20.505: INFO: Pod name pod-release: Found 0 pods out of 1
  Dec 16 16:38:25.528: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 12/16/23 16:38:25.564
  Dec 16 16:38:25.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9155" for this suite. @ 12/16/23 16:38:25.593
• [5.191 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 12/16/23 16:38:25.67
  Dec 16 16:38:25.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 16:38:25.685
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:38:25.743
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:38:25.748
  STEP: Creating a pod to test downward API volume plugin @ 12/16/23 16:38:25.758
  STEP: Saw pod success @ 12/16/23 16:38:29.814
  Dec 16 16:38:29.823: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downwardapi-volume-9b8a3b14-1d09-4aa1-a1b5-f0daef8d5235 container client-container: <nil>
  STEP: delete the pod @ 12/16/23 16:38:29.838
  Dec 16 16:38:29.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5609" for this suite. @ 12/16/23 16:38:29.893
• [4.249 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 12/16/23 16:38:29.922
  Dec 16 16:38:29.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename resourcequota @ 12/16/23 16:38:29.925
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:38:29.952
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:38:29.957
  STEP: Counting existing ResourceQuota @ 12/16/23 16:38:29.962
  STEP: Creating a ResourceQuota @ 12/16/23 16:38:34.971
  STEP: Ensuring resource quota status is calculated @ 12/16/23 16:38:34.983
  STEP: Creating a Pod that fits quota @ 12/16/23 16:38:36.995
  STEP: Ensuring ResourceQuota status captures the pod usage @ 12/16/23 16:38:37.044
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 12/16/23 16:38:39.053
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 12/16/23 16:38:39.057
  STEP: Ensuring a pod cannot update its resource requirements @ 12/16/23 16:38:39.062
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 12/16/23 16:38:39.069
  STEP: Deleting the pod @ 12/16/23 16:38:41.082
  STEP: Ensuring resource quota status released the pod usage @ 12/16/23 16:38:41.105
  Dec 16 16:38:43.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7800" for this suite. @ 12/16/23 16:38:43.133
• [13.228 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 12/16/23 16:38:43.153
  Dec 16 16:38:43.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename endpointslicemirroring @ 12/16/23 16:38:43.156
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:38:43.209
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:38:43.218
  STEP: mirroring a new custom Endpoint @ 12/16/23 16:38:43.255
  Dec 16 16:38:43.289: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 12/16/23 16:38:45.301
  Dec 16 16:38:45.317: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  STEP: mirroring deletion of a custom Endpoint @ 12/16/23 16:38:47.332
  Dec 16 16:38:47.355: INFO: Waiting for 0 EndpointSlices to exist, got 1
  Dec 16 16:38:49.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-2127" for this suite. @ 12/16/23 16:38:49.38
• [6.245 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 12/16/23 16:38:49.4
  Dec 16 16:38:49.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename taint-single-pod @ 12/16/23 16:38:49.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:38:49.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:38:49.474
  Dec 16 16:38:49.481: INFO: Waiting up to 1m0s for all nodes to be ready
  Dec 16 16:39:49.532: INFO: Waiting for terminating namespaces to be deleted...
  Dec 16 16:39:49.542: INFO: Starting informer...
  STEP: Starting pod... @ 12/16/23 16:39:49.544
  Dec 16 16:39:49.774: INFO: Pod is running on phoh7xai9ouk-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 12/16/23 16:39:49.775
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 12/16/23 16:39:49.803
  STEP: Waiting short time to make sure Pod is queued for deletion @ 12/16/23 16:39:49.818
  Dec 16 16:39:49.819: INFO: Pod wasn't evicted. Proceeding
  Dec 16 16:39:49.820: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 12/16/23 16:39:49.864
  STEP: Waiting some time to make sure that toleration time passed. @ 12/16/23 16:39:49.875
  Dec 16 16:41:04.875: INFO: Pod wasn't evicted. Test successful
  Dec 16 16:41:04.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-3334" for this suite. @ 12/16/23 16:41:04.894
• [135.509 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:790
  STEP: Creating a kubernetes client @ 12/16/23 16:41:04.916
  Dec 16 16:41:04.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename statefulset @ 12/16/23 16:41:04.92
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:41:04.977
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:41:04.981
  STEP: Creating service test in namespace statefulset-5649 @ 12/16/23 16:41:04.991
  STEP: Looking for a node to schedule stateful set and pod @ 12/16/23 16:41:05.003
  STEP: Creating pod with conflicting port in namespace statefulset-5649 @ 12/16/23 16:41:05.028
  STEP: Waiting until pod test-pod will start running in namespace statefulset-5649 @ 12/16/23 16:41:05.058
  STEP: Creating statefulset with conflicting port in namespace statefulset-5649 @ 12/16/23 16:41:07.078
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5649 @ 12/16/23 16:41:07.09
  Dec 16 16:41:07.123: INFO: Observed stateful pod in namespace: statefulset-5649, name: ss-0, uid: 904ed098-c3fc-45bd-a96c-4b84ee546440, status phase: Pending. Waiting for statefulset controller to delete.
  Dec 16 16:41:07.161: INFO: Observed stateful pod in namespace: statefulset-5649, name: ss-0, uid: 904ed098-c3fc-45bd-a96c-4b84ee546440, status phase: Failed. Waiting for statefulset controller to delete.
  Dec 16 16:41:07.179: INFO: Observed stateful pod in namespace: statefulset-5649, name: ss-0, uid: 904ed098-c3fc-45bd-a96c-4b84ee546440, status phase: Failed. Waiting for statefulset controller to delete.
  Dec 16 16:41:07.196: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5649
  STEP: Removing pod with conflicting port in namespace statefulset-5649 @ 12/16/23 16:41:07.197
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5649 and will be in running state @ 12/16/23 16:41:07.26
  Dec 16 16:41:09.290: INFO: Deleting all statefulset in ns statefulset-5649
  Dec 16 16:41:09.295: INFO: Scaling statefulset ss to 0
  Dec 16 16:41:19.335: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 16 16:41:19.342: INFO: Deleting statefulset ss
  Dec 16 16:41:19.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5649" for this suite. @ 12/16/23 16:41:19.386
• [14.481 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 12/16/23 16:41:19.402
  Dec 16 16:41:19.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename replication-controller @ 12/16/23 16:41:19.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:41:19.449
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:41:19.459
  STEP: creating a ReplicationController @ 12/16/23 16:41:19.48
  STEP: waiting for RC to be added @ 12/16/23 16:41:19.497
  STEP: waiting for available Replicas @ 12/16/23 16:41:19.499
  STEP: patching ReplicationController @ 12/16/23 16:41:20.623
  STEP: waiting for RC to be modified @ 12/16/23 16:41:20.643
  STEP: patching ReplicationController status @ 12/16/23 16:41:20.643
  STEP: waiting for RC to be modified @ 12/16/23 16:41:20.699
  STEP: waiting for available Replicas @ 12/16/23 16:41:20.699
  STEP: fetching ReplicationController status @ 12/16/23 16:41:20.713
  STEP: patching ReplicationController scale @ 12/16/23 16:41:20.73
  STEP: waiting for RC to be modified @ 12/16/23 16:41:20.745
  STEP: waiting for ReplicationController's scale to be the max amount @ 12/16/23 16:41:20.746
  STEP: fetching ReplicationController; ensuring that it's patched @ 12/16/23 16:41:24.712
  STEP: updating ReplicationController status @ 12/16/23 16:41:24.719
  STEP: waiting for RC to be modified @ 12/16/23 16:41:24.739
  STEP: listing all ReplicationControllers @ 12/16/23 16:41:24.74
  STEP: checking that ReplicationController has expected values @ 12/16/23 16:41:24.748
  STEP: deleting ReplicationControllers by collection @ 12/16/23 16:41:24.748
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 12/16/23 16:41:24.772
  Dec 16 16:41:24.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E1216 16:41:24.850524      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-5923" for this suite. @ 12/16/23 16:41:24.859
• [5.476 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 12/16/23 16:41:24.884
  Dec 16 16:41:24.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubectl @ 12/16/23 16:41:24.887
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:41:24.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:41:24.927
  STEP: creating a replication controller @ 12/16/23 16:41:24.932
  Dec 16 16:41:24.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9950 create -f -'
  Dec 16 16:41:25.794: INFO: stderr: ""
  Dec 16 16:41:25.794: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 12/16/23 16:41:25.794
  Dec 16 16:41:25.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9950 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E1216 16:41:25.853446      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:41:26.041: INFO: stderr: ""
  Dec 16 16:41:26.041: INFO: stdout: "update-demo-nautilus-8w6nj update-demo-nautilus-97d5r "
  Dec 16 16:41:26.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9950 get pods update-demo-nautilus-8w6nj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Dec 16 16:41:26.198: INFO: stderr: ""
  Dec 16 16:41:26.198: INFO: stdout: ""
  Dec 16 16:41:26.198: INFO: update-demo-nautilus-8w6nj is created but not running
  E1216 16:41:26.853218      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:27.853918      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:28.854637      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:29.855269      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:30.855955      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:41:31.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9950 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Dec 16 16:41:31.434: INFO: stderr: ""
  Dec 16 16:41:31.434: INFO: stdout: "update-demo-nautilus-8w6nj update-demo-nautilus-97d5r "
  Dec 16 16:41:31.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9950 get pods update-demo-nautilus-8w6nj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Dec 16 16:41:31.607: INFO: stderr: ""
  Dec 16 16:41:31.607: INFO: stdout: ""
  Dec 16 16:41:31.607: INFO: update-demo-nautilus-8w6nj is created but not running
  E1216 16:41:31.856897      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:32.857542      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:33.857665      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:34.858146      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:35.875563      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:41:36.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9950 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Dec 16 16:41:36.812: INFO: stderr: ""
  Dec 16 16:41:36.812: INFO: stdout: "update-demo-nautilus-8w6nj update-demo-nautilus-97d5r "
  Dec 16 16:41:36.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9950 get pods update-demo-nautilus-8w6nj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E1216 16:41:36.859345      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:41:36.986: INFO: stderr: ""
  Dec 16 16:41:36.986: INFO: stdout: ""
  Dec 16 16:41:36.986: INFO: update-demo-nautilus-8w6nj is created but not running
  E1216 16:41:37.860434      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:38.861044      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:39.861640      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:40.862278      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:41.862228      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:41:41.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9950 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Dec 16 16:41:42.167: INFO: stderr: ""
  Dec 16 16:41:42.167: INFO: stdout: "update-demo-nautilus-8w6nj update-demo-nautilus-97d5r "
  Dec 16 16:41:42.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9950 get pods update-demo-nautilus-8w6nj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Dec 16 16:41:42.363: INFO: stderr: ""
  Dec 16 16:41:42.363: INFO: stdout: "true"
  Dec 16 16:41:42.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9950 get pods update-demo-nautilus-8w6nj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Dec 16 16:41:42.564: INFO: stderr: ""
  Dec 16 16:41:42.565: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Dec 16 16:41:42.565: INFO: validating pod update-demo-nautilus-8w6nj
  Dec 16 16:41:42.584: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Dec 16 16:41:42.584: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Dec 16 16:41:42.584: INFO: update-demo-nautilus-8w6nj is verified up and running
  Dec 16 16:41:42.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9950 get pods update-demo-nautilus-97d5r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Dec 16 16:41:42.754: INFO: stderr: ""
  Dec 16 16:41:42.754: INFO: stdout: "true"
  Dec 16 16:41:42.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9950 get pods update-demo-nautilus-97d5r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E1216 16:41:42.863517      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:41:42.910: INFO: stderr: ""
  Dec 16 16:41:42.910: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Dec 16 16:41:42.910: INFO: validating pod update-demo-nautilus-97d5r
  Dec 16 16:41:42.941: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Dec 16 16:41:42.941: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Dec 16 16:41:42.941: INFO: update-demo-nautilus-97d5r is verified up and running
  STEP: using delete to clean up resources @ 12/16/23 16:41:42.941
  Dec 16 16:41:42.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9950 delete --grace-period=0 --force -f -'
  Dec 16 16:41:43.123: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Dec 16 16:41:43.123: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Dec 16 16:41:43.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9950 get rc,svc -l name=update-demo --no-headers'
  Dec 16 16:41:43.308: INFO: stderr: "No resources found in kubectl-9950 namespace.\n"
  Dec 16 16:41:43.308: INFO: stdout: ""
  Dec 16 16:41:43.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9950 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Dec 16 16:41:43.476: INFO: stderr: ""
  Dec 16 16:41:43.476: INFO: stdout: ""
  Dec 16 16:41:43.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9950" for this suite. @ 12/16/23 16:41:43.486
• [18.622 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 12/16/23 16:41:43.507
  Dec 16 16:41:43.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename downward-api @ 12/16/23 16:41:43.51
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:41:43.55
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:41:43.557
  STEP: Creating a pod to test downward api env vars @ 12/16/23 16:41:43.566
  E1216 16:41:43.863647      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:44.864009      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:45.863962      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:46.864177      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 16:41:47.622
  Dec 16 16:41:47.631: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downward-api-827d6d68-9680-4498-8b2b-5ccfd86a0921 container dapi-container: <nil>
  STEP: delete the pod @ 12/16/23 16:41:47.668
  Dec 16 16:41:47.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6356" for this suite. @ 12/16/23 16:41:47.703
• [4.209 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 12/16/23 16:41:47.724
  Dec 16 16:41:47.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 16:41:47.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:41:47.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:41:47.765
  STEP: Creating a pod to test downward API volume plugin @ 12/16/23 16:41:47.769
  E1216 16:41:47.864274      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:48.864706      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:49.865087      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:50.865132      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 16:41:51.822
  Dec 16 16:41:51.829: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downwardapi-volume-7d0da963-a61e-453d-a42e-bba04544c5a3 container client-container: <nil>
  STEP: delete the pod @ 12/16/23 16:41:51.855
  E1216 16:41:51.865695      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:41:51.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5554" for this suite. @ 12/16/23 16:41:51.9
• [4.192 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 12/16/23 16:41:51.917
  Dec 16 16:41:51.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename emptydir @ 12/16/23 16:41:51.919
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:41:51.956
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:41:51.961
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 12/16/23 16:41:51.967
  E1216 16:41:52.866032      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:53.866420      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:54.866378      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:55.866652      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 16:41:56.014
  Dec 16 16:41:56.021: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-f699a5f4-f93b-4250-b4ba-fee4b4d2d33d container test-container: <nil>
  STEP: delete the pod @ 12/16/23 16:41:56.036
  Dec 16 16:41:56.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4119" for this suite. @ 12/16/23 16:41:56.068
• [4.162 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 12/16/23 16:41:56.082
  Dec 16 16:41:56.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename emptydir @ 12/16/23 16:41:56.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:41:56.114
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:41:56.119
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 12/16/23 16:41:56.125
  E1216 16:41:56.867395      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:57.868473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:58.868681      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:41:59.869626      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 16:42:00.176
  Dec 16 16:42:00.184: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-b348ea91-4024-4624-83c3-c3a6b733ee2a container test-container: <nil>
  STEP: delete the pod @ 12/16/23 16:42:00.202
  Dec 16 16:42:00.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-571" for this suite. @ 12/16/23 16:42:00.244
• [4.175 seconds]
------------------------------
SS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 12/16/23 16:42:00.257
  Dec 16 16:42:00.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename downward-api @ 12/16/23 16:42:00.26
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:42:00.294
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:42:00.304
  STEP: Creating a pod to test downward api env vars @ 12/16/23 16:42:00.31
  E1216 16:42:00.869423      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:01.874724      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:02.872789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:03.873298      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 16:42:04.361
  Dec 16 16:42:04.367: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downward-api-75ea317a-53fc-4001-9eb1-c55a2fa6fa2c container dapi-container: <nil>
  STEP: delete the pod @ 12/16/23 16:42:04.389
  Dec 16 16:42:04.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6465" for this suite. @ 12/16/23 16:42:04.434
• [4.192 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 12/16/23 16:42:04.455
  Dec 16 16:42:04.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename csiinlinevolumes @ 12/16/23 16:42:04.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:42:04.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:42:04.503
  STEP: creating @ 12/16/23 16:42:04.514
  STEP: getting @ 12/16/23 16:42:04.551
  STEP: listing @ 12/16/23 16:42:04.562
  STEP: deleting @ 12/16/23 16:42:04.569
  Dec 16 16:42:04.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-5772" for this suite. @ 12/16/23 16:42:04.675
• [0.236 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 12/16/23 16:42:04.692
  Dec 16 16:42:04.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename secrets @ 12/16/23 16:42:04.696
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:42:04.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:42:04.741
  STEP: Creating secret with name secret-test-861477ec-06ac-4e77-aaea-1d229b199464 @ 12/16/23 16:42:04.747
  STEP: Creating a pod to test consume secrets @ 12/16/23 16:42:04.759
  E1216 16:42:04.874067      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:05.874834      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:06.874857      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:07.875743      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 16:42:08.807
  Dec 16 16:42:08.813: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-secrets-6ab7fe2d-6d3d-438a-8adb-851e84785f96 container secret-volume-test: <nil>
  STEP: delete the pod @ 12/16/23 16:42:08.829
  Dec 16 16:42:08.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E1216 16:42:08.876384      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "secrets-2251" for this suite. @ 12/16/23 16:42:08.877
• [4.202 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 12/16/23 16:42:08.896
  Dec 16 16:42:08.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename services @ 12/16/23 16:42:08.899
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:42:08.934
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:42:08.945
  STEP: creating service in namespace services-4937 @ 12/16/23 16:42:08.952
  STEP: creating service affinity-clusterip-transition in namespace services-4937 @ 12/16/23 16:42:08.953
  STEP: creating replication controller affinity-clusterip-transition in namespace services-4937 @ 12/16/23 16:42:08.979
  I1216 16:42:08.992550      13 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-4937, replica count: 3
  E1216 16:42:09.877095      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:10.877796      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:11.878168      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 16:42:12.045968      13 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Dec 16 16:42:12.059: INFO: Creating new exec pod
  E1216 16:42:12.878548      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:13.878753      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:14.879005      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:42:15.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-4937 exec execpod-affinityxwrmx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Dec 16 16:42:15.441: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Dec 16 16:42:15.441: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 16 16:42:15.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-4937 exec execpod-affinityxwrmx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.10.116 80'
  Dec 16 16:42:15.698: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.10.116 80\nConnection to 10.233.10.116 80 port [tcp/http] succeeded!\n"
  Dec 16 16:42:15.698: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 16 16:42:15.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-4937 exec execpod-affinityxwrmx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.10.116:80/ ; done'
  E1216 16:42:15.879581      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:42:16.250: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n"
  Dec 16 16:42:16.250: INFO: stdout: "\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-ft7sv\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-8k7ml\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-8k7ml\naffinity-clusterip-transition-8k7ml\naffinity-clusterip-transition-8k7ml\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-8k7ml\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-ft7sv\naffinity-clusterip-transition-bhbl2"
  Dec 16 16:42:16.250: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.250: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.250: INFO: Received response from host: affinity-clusterip-transition-ft7sv
  Dec 16 16:42:16.250: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.250: INFO: Received response from host: affinity-clusterip-transition-8k7ml
  Dec 16 16:42:16.250: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.250: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.250: INFO: Received response from host: affinity-clusterip-transition-8k7ml
  Dec 16 16:42:16.250: INFO: Received response from host: affinity-clusterip-transition-8k7ml
  Dec 16 16:42:16.250: INFO: Received response from host: affinity-clusterip-transition-8k7ml
  Dec 16 16:42:16.250: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.250: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.250: INFO: Received response from host: affinity-clusterip-transition-8k7ml
  Dec 16 16:42:16.250: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.250: INFO: Received response from host: affinity-clusterip-transition-ft7sv
  Dec 16 16:42:16.250: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-4937 exec execpod-affinityxwrmx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.10.116:80/ ; done'
  Dec 16 16:42:16.850: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.10.116:80/\n"
  Dec 16 16:42:16.850: INFO: stdout: "\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-bhbl2\naffinity-clusterip-transition-bhbl2"
  Dec 16 16:42:16.850: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.850: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.850: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.850: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.850: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.850: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.850: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.850: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.850: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.850: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.850: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.850: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.850: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.850: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.850: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.850: INFO: Received response from host: affinity-clusterip-transition-bhbl2
  Dec 16 16:42:16.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 16 16:42:16.860: INFO: Cleaning up the exec pod
  E1216 16:42:16.880496      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4937, will wait for the garbage collector to delete the pods @ 12/16/23 16:42:16.888
  Dec 16 16:42:16.969: INFO: Deleting ReplicationController affinity-clusterip-transition took: 13.014362ms
  Dec 16 16:42:17.069: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.71176ms
  E1216 16:42:17.880350      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:18.880678      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-4937" for this suite. @ 12/16/23 16:42:19.502
• [10.627 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 12/16/23 16:42:19.527
  Dec 16 16:42:19.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename secrets @ 12/16/23 16:42:19.529
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:42:19.559
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:42:19.564
  STEP: Creating projection with secret that has name secret-emptykey-test-0df19f00-38ee-47b2-9cf6-377f43362550 @ 12/16/23 16:42:19.571
  Dec 16 16:42:19.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3439" for this suite. @ 12/16/23 16:42:19.586
• [0.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 12/16/23 16:42:19.616
  Dec 16 16:42:19.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename sched-preemption @ 12/16/23 16:42:19.619
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:42:19.651
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:42:19.657
  Dec 16 16:42:19.690: INFO: Waiting up to 1m0s for all nodes to be ready
  E1216 16:42:19.880806      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:20.881125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:21.882285      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:22.882801      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:23.883581      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:24.883791      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:25.885057      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:26.886452      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:27.886702      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:28.886855      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:29.888125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:30.888898      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:31.889478      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:32.889842      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:33.890398      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:34.891224      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:35.892113      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:36.892628      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:37.893450      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:38.893982      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:39.894732      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:40.895374      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:41.895668      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:42.896077      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:43.896275      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:44.896546      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:45.897110      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:46.897424      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:47.898467      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:48.899113      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:49.900316      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:50.900505      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:51.901255      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:52.902074      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:53.902053      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:54.902772      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:55.903496      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:56.904346      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:57.904473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:58.905063      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:42:59.905591      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:00.906073      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:01.906241      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:02.906967      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:03.907277      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:04.908039      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:05.907903      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:06.908243      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:07.908512      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:08.908649      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:09.909530      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:10.909790      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:11.910555      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:12.911622      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:13.912642      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:14.913107      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:15.913213      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:16.914159      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:17.915048      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:18.915677      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:43:19.746: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 12/16/23 16:43:19.756
  Dec 16 16:43:19.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename sched-preemption-path @ 12/16/23 16:43:19.761
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:43:19.799
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:43:19.807
  STEP: Finding an available node @ 12/16/23 16:43:19.815
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 12/16/23 16:43:19.815
  E1216 16:43:19.915895      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:20.916726      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 12/16/23 16:43:21.866
  Dec 16 16:43:21.886: INFO: found a healthy node: phoh7xai9ouk-3
  E1216 16:43:21.921985      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:22.922300      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:23.922647      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:24.922814      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:25.923500      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:26.923878      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:27.924475      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:43:28.062: INFO: pods created so far: [1 1 1]
  Dec 16 16:43:28.063: INFO: length of pods created so far: 3
  E1216 16:43:28.925608      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:29.925850      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:43:30.085: INFO: pods created so far: [2 2 1]
  E1216 16:43:30.926747      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:31.926913      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:32.927770      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:33.928068      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:34.928379      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:35.929132      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:36.929396      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:43:37.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 16 16:43:37.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-6891" for this suite. @ 12/16/23 16:43:37.297
  STEP: Destroying namespace "sched-preemption-5722" for this suite. @ 12/16/23 16:43:37.311
• [77.706 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 12/16/23 16:43:37.331
  Dec 16 16:43:37.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename controllerrevisions @ 12/16/23 16:43:37.334
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:43:37.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:43:37.377
  STEP: Creating DaemonSet "e2e-hv7gm-daemon-set" @ 12/16/23 16:43:37.42
  STEP: Check that daemon pods launch on every node of the cluster. @ 12/16/23 16:43:37.431
  Dec 16 16:43:37.448: INFO: Number of nodes with available pods controlled by daemonset e2e-hv7gm-daemon-set: 0
  Dec 16 16:43:37.448: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  E1216 16:43:37.930790      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:43:38.469: INFO: Number of nodes with available pods controlled by daemonset e2e-hv7gm-daemon-set: 1
  Dec 16 16:43:38.470: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  E1216 16:43:38.931341      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:43:39.467: INFO: Number of nodes with available pods controlled by daemonset e2e-hv7gm-daemon-set: 3
  Dec 16 16:43:39.467: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-hv7gm-daemon-set
  STEP: Confirm DaemonSet "e2e-hv7gm-daemon-set" successfully created with "daemonset-name=e2e-hv7gm-daemon-set" label @ 12/16/23 16:43:39.474
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-hv7gm-daemon-set" @ 12/16/23 16:43:39.488
  Dec 16 16:43:39.497: INFO: Located ControllerRevision: "e2e-hv7gm-daemon-set-765c4957cd"
  STEP: Patching ControllerRevision "e2e-hv7gm-daemon-set-765c4957cd" @ 12/16/23 16:43:39.503
  Dec 16 16:43:39.517: INFO: e2e-hv7gm-daemon-set-765c4957cd has been patched
  STEP: Create a new ControllerRevision @ 12/16/23 16:43:39.517
  Dec 16 16:43:39.529: INFO: Created ControllerRevision: e2e-hv7gm-daemon-set-78977b7f64
  STEP: Confirm that there are two ControllerRevisions @ 12/16/23 16:43:39.529
  Dec 16 16:43:39.530: INFO: Requesting list of ControllerRevisions to confirm quantity
  Dec 16 16:43:39.538: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-hv7gm-daemon-set-765c4957cd" @ 12/16/23 16:43:39.538
  STEP: Confirm that there is only one ControllerRevision @ 12/16/23 16:43:39.551
  Dec 16 16:43:39.551: INFO: Requesting list of ControllerRevisions to confirm quantity
  Dec 16 16:43:39.557: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-hv7gm-daemon-set-78977b7f64" @ 12/16/23 16:43:39.563
  Dec 16 16:43:39.577: INFO: e2e-hv7gm-daemon-set-78977b7f64 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 12/16/23 16:43:39.578
  W1216 16:43:39.591272      13 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 12/16/23 16:43:39.591
  Dec 16 16:43:39.592: INFO: Requesting list of ControllerRevisions to confirm quantity
  E1216 16:43:39.931405      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:43:40.600: INFO: Requesting list of ControllerRevisions to confirm quantity
  Dec 16 16:43:40.637: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-hv7gm-daemon-set-78977b7f64=updated" @ 12/16/23 16:43:40.637
  STEP: Confirm that there is only one ControllerRevision @ 12/16/23 16:43:40.695
  Dec 16 16:43:40.695: INFO: Requesting list of ControllerRevisions to confirm quantity
  Dec 16 16:43:40.702: INFO: Found 1 ControllerRevisions
  Dec 16 16:43:40.711: INFO: ControllerRevision "e2e-hv7gm-daemon-set-6c7d4b97f7" has revision 3
  STEP: Deleting DaemonSet "e2e-hv7gm-daemon-set" @ 12/16/23 16:43:40.719
  STEP: deleting DaemonSet.extensions e2e-hv7gm-daemon-set in namespace controllerrevisions-5385, will wait for the garbage collector to delete the pods @ 12/16/23 16:43:40.72
  Dec 16 16:43:40.796: INFO: Deleting DaemonSet.extensions e2e-hv7gm-daemon-set took: 16.987245ms
  Dec 16 16:43:40.897: INFO: Terminating DaemonSet.extensions e2e-hv7gm-daemon-set pods took: 101.520378ms
  E1216 16:43:40.931684      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:41.932668      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:43:42.303: INFO: Number of nodes with available pods controlled by daemonset e2e-hv7gm-daemon-set: 0
  Dec 16 16:43:42.303: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-hv7gm-daemon-set
  Dec 16 16:43:42.307: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8918"},"items":null}

  Dec 16 16:43:42.313: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8918"},"items":null}

  Dec 16 16:43:42.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-5385" for this suite. @ 12/16/23 16:43:42.361
• [5.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 12/16/23 16:43:42.383
  Dec 16 16:43:42.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename resourcequota @ 12/16/23 16:43:42.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:43:42.437
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:43:42.445
  STEP: Counting existing ResourceQuota @ 12/16/23 16:43:42.453
  E1216 16:43:42.933712      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:43.934370      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:44.935961      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:45.936008      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:46.936392      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 12/16/23 16:43:47.461
  STEP: Ensuring resource quota status is calculated @ 12/16/23 16:43:47.472
  E1216 16:43:47.936615      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:48.937339      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 12/16/23 16:43:49.481
  STEP: Ensuring resource quota status captures replicaset creation @ 12/16/23 16:43:49.501
  E1216 16:43:49.938397      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:50.938586      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 12/16/23 16:43:51.51
  STEP: Ensuring resource quota status released usage @ 12/16/23 16:43:51.527
  E1216 16:43:51.939386      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:52.939784      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:43:53.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1351" for this suite. @ 12/16/23 16:43:53.548
• [11.178 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 12/16/23 16:43:53.564
  Dec 16 16:43:53.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename subpath @ 12/16/23 16:43:53.567
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:43:53.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:43:53.606
  STEP: Setting up data @ 12/16/23 16:43:53.611
  STEP: Creating pod pod-subpath-test-projected-ph62 @ 12/16/23 16:43:53.629
  STEP: Creating a pod to test atomic-volume-subpath @ 12/16/23 16:43:53.629
  E1216 16:43:53.942070      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:54.941008      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:55.941262      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:56.941868      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:57.942473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:58.943330      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:43:59.943667      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:00.944395      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:01.944946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:02.945477      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:03.947970      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:04.947348      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:05.947493      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:06.948677      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:07.948355      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:08.949301      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:09.949551      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:10.949811      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:11.949955      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:12.950025      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:13.950974      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:14.951787      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 16:44:15.756
  Dec 16 16:44:15.766: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-subpath-test-projected-ph62 container test-container-subpath-projected-ph62: <nil>
  STEP: delete the pod @ 12/16/23 16:44:15.813
  STEP: Deleting pod pod-subpath-test-projected-ph62 @ 12/16/23 16:44:15.852
  Dec 16 16:44:15.853: INFO: Deleting pod "pod-subpath-test-projected-ph62" in namespace "subpath-5050"
  Dec 16 16:44:15.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5050" for this suite. @ 12/16/23 16:44:15.871
• [22.323 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 12/16/23 16:44:15.892
  Dec 16 16:44:15.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename emptydir-wrapper @ 12/16/23 16:44:15.895
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:44:15.949
  E1216 16:44:15.951817      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:44:15.956
  E1216 16:44:16.952512      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:17.953032      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:44:18.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 12/16/23 16:44:18.066
  STEP: Cleaning up the configmap @ 12/16/23 16:44:18.077
  STEP: Cleaning up the pod @ 12/16/23 16:44:18.086
  STEP: Destroying namespace "emptydir-wrapper-4660" for this suite. @ 12/16/23 16:44:18.105
• [2.224 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 12/16/23 16:44:18.118
  Dec 16 16:44:18.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 16:44:18.12
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:44:18.158
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:44:18.165
  STEP: Creating a pod to test downward API volume plugin @ 12/16/23 16:44:18.17
  E1216 16:44:18.953354      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:19.954265      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:20.954629      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:21.955498      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 16:44:22.228
  Dec 16 16:44:22.235: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downwardapi-volume-c89e48a5-5c36-43aa-8514-8758a8cae603 container client-container: <nil>
  STEP: delete the pod @ 12/16/23 16:44:22.25
  Dec 16 16:44:22.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2132" for this suite. @ 12/16/23 16:44:22.315
• [4.210 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 12/16/23 16:44:22.333
  Dec 16 16:44:22.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename webhook @ 12/16/23 16:44:22.335
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:44:22.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:44:22.377
  STEP: Setting up server cert @ 12/16/23 16:44:22.43
  E1216 16:44:22.955502      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:23.955958      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/16/23 16:44:24.171
  STEP: Deploying the webhook pod @ 12/16/23 16:44:24.186
  STEP: Wait for the deployment to be ready @ 12/16/23 16:44:24.213
  Dec 16 16:44:24.231: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E1216 16:44:24.956023      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:25.958037      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/16/23 16:44:26.259
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 16:44:26.283
  E1216 16:44:26.957971      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:44:27.284: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 12/16/23 16:44:27.291
  STEP: create a pod @ 12/16/23 16:44:27.338
  E1216 16:44:27.958674      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:28.959278      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 12/16/23 16:44:29.38
  Dec 16 16:44:29.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=webhook-6615 attach --namespace=webhook-6615 to-be-attached-pod -i -c=container1'
  Dec 16 16:44:29.668: INFO: rc: 1
  Dec 16 16:44:29.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6615" for this suite. @ 12/16/23 16:44:29.792
  STEP: Destroying namespace "webhook-markers-2650" for this suite. @ 12/16/23 16:44:29.806
• [7.501 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:329
  STEP: Creating a kubernetes client @ 12/16/23 16:44:29.835
  Dec 16 16:44:29.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename statefulset @ 12/16/23 16:44:29.838
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:44:29.9
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:44:29.912
  STEP: Creating service test in namespace statefulset-9482 @ 12/16/23 16:44:29.92
  STEP: Creating a new StatefulSet @ 12/16/23 16:44:29.932
  E1216 16:44:29.959684      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:44:29.963: INFO: Found 0 stateful pods, waiting for 3
  E1216 16:44:30.959974      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:31.960106      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:32.961143      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:33.961813      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:34.962013      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:35.962318      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:36.962343      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:37.962455      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:38.962600      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:39.962852      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:44:39.975: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Dec 16 16:44:39.975: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Dec 16 16:44:39.975: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 12/16/23 16:44:39.998
  Dec 16 16:44:40.030: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 12/16/23 16:44:40.031
  E1216 16:44:40.964060      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:41.965011      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:42.965656      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:43.965826      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:44.966148      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:45.966325      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:46.966978      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:47.967149      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:48.967579      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:49.968026      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not applying an update when the partition is greater than the number of replicas @ 12/16/23 16:44:50.067
  STEP: Performing a canary update @ 12/16/23 16:44:50.067
  Dec 16 16:44:50.097: INFO: Updating stateful set ss2
  Dec 16 16:44:50.111: INFO: Waiting for Pod statefulset-9482/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E1216 16:44:50.968299      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:51.969970      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:52.970185      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:53.970247      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:54.971246      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:55.971971      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:56.972276      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:57.972902      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:58.973566      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:44:59.974151      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Restoring Pods to the correct revision when they are deleted @ 12/16/23 16:45:00.133
  Dec 16 16:45:00.221: INFO: Found 1 stateful pods, waiting for 3
  E1216 16:45:00.977300      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:01.978447      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:02.977488      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:03.978164      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:04.979260      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:05.980216      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:06.980551      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:07.981387      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:08.983080      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:09.983596      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:45:10.245: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Dec 16 16:45:10.245: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Dec 16 16:45:10.245: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 12/16/23 16:45:10.277
  Dec 16 16:45:10.313: INFO: Updating stateful set ss2
  Dec 16 16:45:10.362: INFO: Waiting for Pod statefulset-9482/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E1216 16:45:10.984609      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:11.984842      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:12.986051      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:13.986661      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:14.986341      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:15.986549      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:16.986747      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:17.987757      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:18.987951      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:19.988266      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:45:20.415: INFO: Updating stateful set ss2
  Dec 16 16:45:20.430: INFO: Waiting for StatefulSet statefulset-9482/ss2 to complete update
  Dec 16 16:45:20.430: INFO: Waiting for Pod statefulset-9482/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E1216 16:45:20.988589      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:21.988948      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:22.989972      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:23.990730      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:24.991093      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:25.991569      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:26.992518      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:27.993151      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:28.993698      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:29.994233      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:45:30.456: INFO: Waiting for StatefulSet statefulset-9482/ss2 to complete update
  E1216 16:45:30.995399      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:31.995902      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:32.996998      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:33.997179      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:34.998170      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:35.998621      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:36.999054      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:37.999912      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:39.000388      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:40.000894      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:45:40.448: INFO: Deleting all statefulset in ns statefulset-9482
  Dec 16 16:45:40.454: INFO: Scaling statefulset ss2 to 0
  E1216 16:45:41.001959      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:42.002735      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:43.003881      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:44.004488      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:45.005065      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:46.005193      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:47.005332      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:48.005475      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:49.006284      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:50.007606      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:45:50.493: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 16 16:45:50.500: INFO: Deleting statefulset ss2
  Dec 16 16:45:50.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9482" for this suite. @ 12/16/23 16:45:50.53
• [80.707 seconds]
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 12/16/23 16:45:50.543
  Dec 16 16:45:50.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename runtimeclass @ 12/16/23 16:45:50.547
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:45:50.609
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:45:50.616
  STEP: Deleting RuntimeClass runtimeclass-8180-delete-me @ 12/16/23 16:45:50.637
  STEP: Waiting for the RuntimeClass to disappear @ 12/16/23 16:45:50.658
  Dec 16 16:45:50.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8180" for this suite. @ 12/16/23 16:45:50.688
• [0.160 seconds]
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:899
  STEP: Creating a kubernetes client @ 12/16/23 16:45:50.703
  Dec 16 16:45:50.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename statefulset @ 12/16/23 16:45:50.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:45:50.734
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:45:50.739
  STEP: Creating service test in namespace statefulset-4168 @ 12/16/23 16:45:50.746
  STEP: Creating statefulset ss in namespace statefulset-4168 @ 12/16/23 16:45:50.755
  Dec 16 16:45:50.775: INFO: Found 0 stateful pods, waiting for 1
  E1216 16:45:51.008866      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:52.009090      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:53.009797      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:54.010672      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:55.011042      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:56.011495      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:57.011858      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:58.012544      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:45:59.013086      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:00.013628      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:46:00.788: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 12/16/23 16:46:00.802
  STEP: updating a scale subresource @ 12/16/23 16:46:00.809
  STEP: verifying the statefulset Spec.Replicas was modified @ 12/16/23 16:46:00.828
  STEP: Patch a scale subresource @ 12/16/23 16:46:00.834
  STEP: verifying the statefulset Spec.Replicas was modified @ 12/16/23 16:46:00.847
  Dec 16 16:46:00.856: INFO: Deleting all statefulset in ns statefulset-4168
  Dec 16 16:46:00.864: INFO: Scaling statefulset ss to 0
  E1216 16:46:01.015001      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:02.015639      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:03.015988      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:04.016272      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:05.016637      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:06.016976      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:07.017884      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:08.018520      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:09.019307      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:10.020231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:11.021242      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:12.020998      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:13.021123      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:14.021318      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:15.021738      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:16.022201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:17.022587      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:18.023595      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:19.024034      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:20.024175      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:21.024993      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:22.026638      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:23.026725      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:24.026880      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:25.027091      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:26.027305      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:27.027535      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:28.028688      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:29.029016      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:30.029211      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:31.030245      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:32.030493      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:33.031538      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:34.031927      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:35.032285      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:36.032810      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:37.033128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:38.033946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:39.034328      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:40.034505      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:46:40.933: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 16 16:46:40.950: INFO: Deleting statefulset ss
  Dec 16 16:46:40.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4168" for this suite. @ 12/16/23 16:46:40.999
• [50.311 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 12/16/23 16:46:41.019
  Dec 16 16:46:41.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubectl @ 12/16/23 16:46:41.023
  E1216 16:46:41.034996      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:46:41.063
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:46:41.069
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 12/16/23 16:46:41.077
  Dec 16 16:46:41.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9803 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Dec 16 16:46:41.284: INFO: stderr: ""
  Dec 16 16:46:41.284: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 12/16/23 16:46:41.284
  Dec 16 16:46:41.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9803 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Dec 16 16:46:41.522: INFO: stderr: ""
  Dec 16 16:46:41.522: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 12/16/23 16:46:41.522
  Dec 16 16:46:41.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9803 delete pods e2e-test-httpd-pod'
  E1216 16:46:42.036036      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:43.036568      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:46:43.265: INFO: stderr: ""
  Dec 16 16:46:43.265: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Dec 16 16:46:43.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9803" for this suite. @ 12/16/23 16:46:43.285
• [2.280 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 12/16/23 16:46:43.302
  Dec 16 16:46:43.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename services @ 12/16/23 16:46:43.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:46:43.34
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:46:43.347
  Dec 16 16:46:43.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6818" for this suite. @ 12/16/23 16:46:43.37
• [0.079 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 12/16/23 16:46:43.385
  Dec 16 16:46:43.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename resourcequota @ 12/16/23 16:46:43.388
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:46:43.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:46:43.421
  E1216 16:46:44.037687      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:45.038599      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:46.038924      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:47.039617      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:48.040325      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:49.041146      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:50.041068      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:51.041229      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:52.041818      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:53.042151      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:54.042299      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:55.042979      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:56.043082      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:57.043323      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:58.043520      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:46:59.044082      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:00.044645      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 12/16/23 16:47:00.437
  E1216 16:47:01.044670      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:02.044675      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:03.044845      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:04.045821      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:05.046660      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 12/16/23 16:47:05.446
  STEP: Ensuring resource quota status is calculated @ 12/16/23 16:47:05.455
  E1216 16:47:06.046879      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:07.047319      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 12/16/23 16:47:07.465
  STEP: Ensuring resource quota status captures configMap creation @ 12/16/23 16:47:07.489
  E1216 16:47:08.048383      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:09.048720      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 12/16/23 16:47:09.499
  STEP: Ensuring resource quota status released usage @ 12/16/23 16:47:09.52
  E1216 16:47:10.049539      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:11.049899      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:47:11.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3419" for this suite. @ 12/16/23 16:47:11.541
• [28.168 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 12/16/23 16:47:11.555
  Dec 16 16:47:11.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-probe @ 12/16/23 16:47:11.559
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:47:11.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:47:11.603
  STEP: Creating pod test-grpc-42226569-3780-492e-88d2-1ccbff89e917 in namespace container-probe-3997 @ 12/16/23 16:47:11.609
  E1216 16:47:12.050113      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:13.050794      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:47:13.644: INFO: Started pod test-grpc-42226569-3780-492e-88d2-1ccbff89e917 in namespace container-probe-3997
  STEP: checking the pod's current state and verifying that restartCount is present @ 12/16/23 16:47:13.644
  Dec 16 16:47:13.649: INFO: Initial restart count of pod test-grpc-42226569-3780-492e-88d2-1ccbff89e917 is 0
  E1216 16:47:14.051011      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:15.051810      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:16.053455      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:17.053997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:18.054780      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:19.055875      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:20.055464      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:21.056202      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:22.057710      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:23.058315      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:24.059309      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:25.059629      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:26.059758      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:27.060148      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:28.061274      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:29.062201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:30.063094      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:31.063115      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:32.063328      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:33.063731      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:34.064040      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:35.064402      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:36.065203      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:37.065348      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:38.066159      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:39.066545      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:40.067607      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:41.068038      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:42.068532      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:43.068866      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:44.069052      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:45.069343      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:46.070209      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:47.070985      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:48.071644      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:49.071782      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:50.072578      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:51.073257      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:52.074392      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:53.074696      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:54.075726      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:55.076289      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:56.077082      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:57.077905      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:58.078180      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:47:59.078615      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:00.078990      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:01.079690      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:02.080394      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:03.080551      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:04.081524      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:05.082688      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:06.082787      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:07.083170      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:08.084139      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:09.084443      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:10.084614      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:11.085654      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:12.085580      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:13.086823      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:14.087079      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:15.087240      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:16.088416      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:17.089054      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:18.089166      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:19.089671      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:20.090229      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:21.090103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:22.090609      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:23.091983      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:24.092304      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:25.092657      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:26.093089      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:27.093322      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:48:28.016: INFO: Restart count of pod container-probe-3997/test-grpc-42226569-3780-492e-88d2-1ccbff89e917 is now 1 (1m14.366548107s elapsed)
  Dec 16 16:48:28.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/16/23 16:48:28.03
  STEP: Destroying namespace "container-probe-3997" for this suite. @ 12/16/23 16:48:28.058
• [76.518 seconds]
------------------------------
SSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 12/16/23 16:48:28.076
  Dec 16 16:48:28.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename hostport @ 12/16/23 16:48:28.081
  E1216 16:48:28.094500      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:48:28.115
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:48:28.122
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 12/16/23 16:48:28.138
  E1216 16:48:29.095694      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:30.095386      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.49 on the node which pod1 resides and expect scheduled @ 12/16/23 16:48:30.177
  E1216 16:48:31.095785      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:32.096083      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.49 but use UDP protocol on the node which pod2 resides @ 12/16/23 16:48:32.207
  E1216 16:48:33.097727      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:34.098058      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:35.098112      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:36.098299      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 12/16/23 16:48:36.321
  Dec 16 16:48:36.321: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.49 http://127.0.0.1:54323/hostname] Namespace:hostport-6579 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 16:48:36.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 16:48:36.324: INFO: ExecWithOptions: Clientset creation
  Dec 16 16:48:36.324: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-6579/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.49+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.49, port: 54323 @ 12/16/23 16:48:36.51
  Dec 16 16:48:36.511: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.49:54323/hostname] Namespace:hostport-6579 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 16:48:36.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 16:48:36.512: INFO: ExecWithOptions: Clientset creation
  Dec 16 16:48:36.513: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-6579/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.49%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.49, port: 54323 UDP @ 12/16/23 16:48:36.682
  Dec 16 16:48:36.682: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.121.49 54323] Namespace:hostport-6579 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 16:48:36.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 16:48:36.685: INFO: ExecWithOptions: Clientset creation
  Dec 16 16:48:36.686: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-6579/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.121.49+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E1216 16:48:37.098342      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:38.099322      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:39.101513      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:40.100135      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:41.100618      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:48:41.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-6579" for this suite. @ 12/16/23 16:48:41.827
• [13.771 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 12/16/23 16:48:41.85
  Dec 16 16:48:41.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename resourcequota @ 12/16/23 16:48:41.855
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:48:41.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:48:41.897
  STEP: Counting existing ResourceQuota @ 12/16/23 16:48:41.905
  E1216 16:48:42.101066      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:43.101961      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:44.102340      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:45.102527      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:46.102963      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 12/16/23 16:48:46.919
  STEP: Ensuring resource quota status is calculated @ 12/16/23 16:48:46.931
  E1216 16:48:47.103935      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:48.104571      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:48:48.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7382" for this suite. @ 12/16/23 16:48:48.962
• [7.140 seconds]
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 12/16/23 16:48:48.99
  Dec 16 16:48:48.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename replicaset @ 12/16/23 16:48:48.993
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:48:49.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:48:49.04
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 12/16/23 16:48:49.046
  Dec 16 16:48:49.065: INFO: Pod name sample-pod: Found 0 pods out of 1
  E1216 16:48:49.105152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:50.105539      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:51.105779      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:52.106342      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:53.106980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:48:54.079: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 12/16/23 16:48:54.079
  STEP: getting scale subresource @ 12/16/23 16:48:54.08
  STEP: updating a scale subresource @ 12/16/23 16:48:54.09
  STEP: verifying the replicaset Spec.Replicas was modified @ 12/16/23 16:48:54.106
  E1216 16:48:54.106895      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patch a scale subresource @ 12/16/23 16:48:54.117
  Dec 16 16:48:54.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-574" for this suite. @ 12/16/23 16:48:54.156
• [5.199 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 12/16/23 16:48:54.203
  Dec 16 16:48:54.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename dns @ 12/16/23 16:48:54.205
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:48:54.25
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:48:54.26
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7049.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7049.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 12/16/23 16:48:54.266
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7049.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7049.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 12/16/23 16:48:54.267
  STEP: creating a pod to probe /etc/hosts @ 12/16/23 16:48:54.267
  STEP: submitting the pod to kubernetes @ 12/16/23 16:48:54.267
  E1216 16:48:55.107824      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:56.108229      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:57.108232      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:58.109071      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:48:59.109768      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:00.111865      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:01.113944      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:02.111000      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:03.111567      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:04.111781      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:05.112516      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:06.112990      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:07.113164      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:08.114264      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:09.114470      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:10.114580      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:11.115742      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:12.116236      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:13.116989      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:14.117244      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:15.118946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:16.119563      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:17.120382      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:18.120727      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:19.121226      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:20.121839      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 12/16/23 16:49:20.464
  STEP: looking for the results for each expected name from probers @ 12/16/23 16:49:20.473
  Dec 16 16:49:20.510: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-7049/dns-test-eb07f797-cd61-4744-90b6-389c453a2b51: the server could not find the requested resource (get pods dns-test-eb07f797-cd61-4744-90b6-389c453a2b51)
  Dec 16 16:49:20.510: INFO: Lookups using dns-7049/dns-test-eb07f797-cd61-4744-90b6-389c453a2b51 failed for: [jessie_hosts@dns-querier-1]

  E1216 16:49:21.122270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:22.122332      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:23.122710      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:24.122914      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:25.123127      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:49:25.551: INFO: DNS probes using dns-7049/dns-test-eb07f797-cd61-4744-90b6-389c453a2b51 succeeded

  Dec 16 16:49:25.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/16/23 16:49:25.565
  STEP: Destroying namespace "dns-7049" for this suite. @ 12/16/23 16:49:25.599
• [31.422 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 12/16/23 16:49:25.626
  Dec 16 16:49:25.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename job @ 12/16/23 16:49:25.634
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:49:25.669
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:49:25.673
  STEP: Creating a job @ 12/16/23 16:49:25.68
  STEP: Ensure pods equal to parallelism count is attached to the job @ 12/16/23 16:49:25.694
  E1216 16:49:26.123402      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:27.123810      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 12/16/23 16:49:27.704
  STEP: updating /status @ 12/16/23 16:49:27.724
  STEP: get /status @ 12/16/23 16:49:27.785
  Dec 16 16:49:27.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8321" for this suite. @ 12/16/23 16:49:27.805
• [2.193 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 12/16/23 16:49:27.821
  Dec 16 16:49:27.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename secrets @ 12/16/23 16:49:27.824
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:49:27.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:49:27.861
  STEP: Creating secret with name secret-test-e6a993b3-b310-4488-8446-d52db584eaf7 @ 12/16/23 16:49:27.868
  STEP: Creating a pod to test consume secrets @ 12/16/23 16:49:27.882
  E1216 16:49:28.124701      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:29.124715      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:30.125748      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:31.126595      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 16:49:31.94
  Dec 16 16:49:31.946: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-secrets-86fa422f-96a3-4d29-b371-d62cbd442db8 container secret-volume-test: <nil>
  STEP: delete the pod @ 12/16/23 16:49:31.984
  Dec 16 16:49:32.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8808" for this suite. @ 12/16/23 16:49:32.028
• [4.221 seconds]
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 12/16/23 16:49:32.045
  Dec 16 16:49:32.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename limitrange @ 12/16/23 16:49:32.049
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:49:32.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:49:32.085
  STEP: Creating LimitRange "e2e-limitrange-tqpmq" in namespace "limitrange-3914" @ 12/16/23 16:49:32.091
  STEP: Creating another limitRange in another namespace @ 12/16/23 16:49:32.106
  E1216 16:49:32.126975      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:49:32.138: INFO: Namespace "e2e-limitrange-tqpmq-7919" created
  Dec 16 16:49:32.139: INFO: Creating LimitRange "e2e-limitrange-tqpmq" in namespace "e2e-limitrange-tqpmq-7919"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-tqpmq" @ 12/16/23 16:49:32.152
  Dec 16 16:49:32.159: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-tqpmq" in "limitrange-3914" namespace @ 12/16/23 16:49:32.159
  Dec 16 16:49:32.177: INFO: LimitRange "e2e-limitrange-tqpmq" has been patched
  STEP: Delete LimitRange "e2e-limitrange-tqpmq" by Collection with labelSelector: "e2e-limitrange-tqpmq=patched" @ 12/16/23 16:49:32.177
  STEP: Confirm that the limitRange "e2e-limitrange-tqpmq" has been deleted @ 12/16/23 16:49:32.192
  Dec 16 16:49:32.192: INFO: Requesting list of LimitRange to confirm quantity
  Dec 16 16:49:32.199: INFO: Found 0 LimitRange with label "e2e-limitrange-tqpmq=patched"
  Dec 16 16:49:32.200: INFO: LimitRange "e2e-limitrange-tqpmq" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-tqpmq" @ 12/16/23 16:49:32.2
  Dec 16 16:49:32.208: INFO: Found 1 limitRange
  Dec 16 16:49:32.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-3914" for this suite. @ 12/16/23 16:49:32.218
  STEP: Destroying namespace "e2e-limitrange-tqpmq-7919" for this suite. @ 12/16/23 16:49:32.23
• [0.202 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 12/16/23 16:49:32.248
  Dec 16 16:49:32.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 16:49:32.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:49:32.281
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:49:32.287
  STEP: Creating configMap with name projected-configmap-test-volume-09521d0b-5ed4-42f1-b3a3-5c27d0a7ad9d @ 12/16/23 16:49:32.295
  STEP: Creating a pod to test consume configMaps @ 12/16/23 16:49:32.303
  E1216 16:49:33.127960      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:34.129159      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:35.128879      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:36.129352      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 16:49:36.36
  Dec 16 16:49:36.368: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-projected-configmaps-2524cf4a-fab8-4e04-821f-1ef3c46f507e container agnhost-container: <nil>
  STEP: delete the pod @ 12/16/23 16:49:36.383
  Dec 16 16:49:36.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6227" for this suite. @ 12/16/23 16:49:36.431
• [4.196 seconds]
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 12/16/23 16:49:36.446
  Dec 16 16:49:36.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename disruption @ 12/16/23 16:49:36.449
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:49:36.484
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:49:36.489
  STEP: Creating a kubernetes client @ 12/16/23 16:49:36.496
  Dec 16 16:49:36.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename disruption-2 @ 12/16/23 16:49:36.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:49:36.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:49:36.535
  STEP: Waiting for the pdb to be processed @ 12/16/23 16:49:36.552
  E1216 16:49:37.129425      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:38.130354      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 12/16/23 16:49:38.602
  E1216 16:49:39.131258      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:40.132009      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 12/16/23 16:49:40.645
  E1216 16:49:41.133144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:42.133673      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 12/16/23 16:49:42.664
  STEP: listing a collection of PDBs in namespace disruption-6476 @ 12/16/23 16:49:42.677
  STEP: deleting a collection of PDBs @ 12/16/23 16:49:42.686
  STEP: Waiting for the PDB collection to be deleted @ 12/16/23 16:49:42.723
  Dec 16 16:49:42.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 16 16:49:42.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-9207" for this suite. @ 12/16/23 16:49:42.761
  STEP: Destroying namespace "disruption-6476" for this suite. @ 12/16/23 16:49:42.774
• [6.343 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 12/16/23 16:49:42.791
  Dec 16 16:49:42.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename job @ 12/16/23 16:49:42.794
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:49:42.83
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:49:42.837
  STEP: Creating a job @ 12/16/23 16:49:42.842
  STEP: Ensuring active pods == parallelism @ 12/16/23 16:49:42.856
  E1216 16:49:43.133449      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:44.134119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 12/16/23 16:49:44.868
  E1216 16:49:45.134993      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:49:45.406: INFO: Successfully updated pod "adopt-release-kpfpp"
  STEP: Checking that the Job readopts the Pod @ 12/16/23 16:49:45.406
  E1216 16:49:46.135978      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:47.137084      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 12/16/23 16:49:47.427
  Dec 16 16:49:47.956: INFO: Successfully updated pod "adopt-release-kpfpp"
  STEP: Checking that the Job releases the Pod @ 12/16/23 16:49:47.956
  E1216 16:49:48.137243      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:49.137706      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:49:49.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9390" for this suite. @ 12/16/23 16:49:49.991
• [7.225 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 12/16/23 16:49:50.023
  Dec 16 16:49:50.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename pod-network-test @ 12/16/23 16:49:50.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:49:50.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:49:50.086
  STEP: Performing setup for networking test in namespace pod-network-test-7105 @ 12/16/23 16:49:50.091
  STEP: creating a selector @ 12/16/23 16:49:50.092
  STEP: Creating the service pods in kubernetes @ 12/16/23 16:49:50.092
  Dec 16 16:49:50.092: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E1216 16:49:50.137865      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:51.138244      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:52.138460      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:53.138472      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:54.139449      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:55.140114      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:56.140326      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:57.140567      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:58.140589      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:49:59.141286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:00.142182      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:01.142882      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:02.143539      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 12/16/23 16:50:02.288
  E1216 16:50:03.144956      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:04.145135      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:50:04.364: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Dec 16 16:50:04.364: INFO: Going to poll 10.233.64.25 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Dec 16 16:50:04.373: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.25 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7105 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 16:50:04.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 16:50:04.376: INFO: ExecWithOptions: Clientset creation
  Dec 16 16:50:04.376: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7105/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.25+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E1216 16:50:05.145721      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:50:05.562: INFO: Found all 1 expected endpoints: [netserver-0]
  Dec 16 16:50:05.563: INFO: Going to poll 10.233.65.23 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Dec 16 16:50:05.574: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.23 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7105 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 16:50:05.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 16:50:05.575: INFO: ExecWithOptions: Clientset creation
  Dec 16 16:50:05.576: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7105/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.65.23+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E1216 16:50:06.145999      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:50:06.759: INFO: Found all 1 expected endpoints: [netserver-1]
  Dec 16 16:50:06.759: INFO: Going to poll 10.233.66.113 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Dec 16 16:50:06.767: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.113 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7105 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 16:50:06.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 16:50:06.768: INFO: ExecWithOptions: Clientset creation
  Dec 16 16:50:06.769: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7105/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.113+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E1216 16:50:07.147266      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:50:07.908: INFO: Found all 1 expected endpoints: [netserver-2]
  Dec 16 16:50:07.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-7105" for this suite. @ 12/16/23 16:50:07.922
• [17.912 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 12/16/23 16:50:07.955
  Dec 16 16:50:07.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename deployment @ 12/16/23 16:50:07.957
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:50:07.993
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:50:07.998
  Dec 16 16:50:08.003: INFO: Creating deployment "test-recreate-deployment"
  Dec 16 16:50:08.013: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Dec 16 16:50:08.030: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
  E1216 16:50:08.147843      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:09.148349      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:50:10.048: INFO: Waiting deployment "test-recreate-deployment" to complete
  Dec 16 16:50:10.055: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Dec 16 16:50:10.079: INFO: Updating deployment test-recreate-deployment
  Dec 16 16:50:10.079: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  E1216 16:50:10.148786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:50:10.322: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-8247  14ee32b9-ba75-4e39-b2ab-f9b8feb57a90 10806 2 2023-12-16 16:50:08 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-12-16 16:50:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-16 16:50:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004803228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-12-16 16:50:10 +0000 UTC,LastTransitionTime:2023-12-16 16:50:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-12-16 16:50:10 +0000 UTC,LastTransitionTime:2023-12-16 16:50:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Dec 16 16:50:10.335: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-8247  7bce094f-751c-40fa-9784-f8dc9a6242b2 10805 1 2023-12-16 16:50:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 14ee32b9-ba75-4e39-b2ab-f9b8feb57a90 0xc0048035e7 0xc0048035e8}] [] [{kube-controller-manager Update apps/v1 2023-12-16 16:50:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14ee32b9-ba75-4e39-b2ab-f9b8feb57a90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-16 16:50:10 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004803688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Dec 16 16:50:10.335: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Dec 16 16:50:10.335: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-8247  2c9e2578-425f-41e2-a056-97f8cf47329b 10793 2 2023-12-16 16:50:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 14ee32b9-ba75-4e39-b2ab-f9b8feb57a90 0xc0048036f7 0xc0048036f8}] [] [{kube-controller-manager Update apps/v1 2023-12-16 16:50:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14ee32b9-ba75-4e39-b2ab-f9b8feb57a90\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-16 16:50:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048037a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Dec 16 16:50:10.343: INFO: Pod "test-recreate-deployment-54757ffd6c-rhkt6" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-rhkt6 test-recreate-deployment-54757ffd6c- deployment-8247  0c5bd178-98a0-4960-b673-8390022ca00d 10801 0 2023-12-16 16:50:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 7bce094f-751c-40fa-9784-f8dc9a6242b2 0xc001bf9dd7 0xc001bf9dd8}] [] [{kube-controller-manager Update v1 2023-12-16 16:50:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7bce094f-751c-40fa-9784-f8dc9a6242b2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 16:50:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6l5zn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6l5zn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 16:50:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 16:50:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 16:50:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 16:50:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.112,PodIP:,StartTime:2023-12-16 16:50:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 16:50:10.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8247" for this suite. @ 12/16/23 16:50:10.357
• [2.424 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 12/16/23 16:50:10.383
  Dec 16 16:50:10.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename pods @ 12/16/23 16:50:10.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:50:10.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:50:10.427
  STEP: creating the pod @ 12/16/23 16:50:10.434
  STEP: setting up watch @ 12/16/23 16:50:10.435
  STEP: submitting the pod to kubernetes @ 12/16/23 16:50:10.545
  STEP: verifying the pod is in kubernetes @ 12/16/23 16:50:10.564
  STEP: verifying pod creation was observed @ 12/16/23 16:50:10.58
  E1216 16:50:11.148941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:12.149465      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 12/16/23 16:50:12.67
  STEP: verifying pod deletion was observed @ 12/16/23 16:50:12.7
  E1216 16:50:13.150220      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:14.151509      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:15.151722      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:50:15.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8511" for this suite. @ 12/16/23 16:50:15.331
• [4.969 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 12/16/23 16:50:15.355
  Dec 16 16:50:15.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename watch @ 12/16/23 16:50:15.36
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:50:15.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:50:15.418
  STEP: getting a starting resourceVersion @ 12/16/23 16:50:15.425
  STEP: starting a background goroutine to produce watch events @ 12/16/23 16:50:15.433
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 12/16/23 16:50:15.434
  E1216 16:50:16.151932      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:17.152424      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:18.153164      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:50:18.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5579" for this suite. @ 12/16/23 16:50:18.224
• [2.923 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 12/16/23 16:50:18.281
  Dec 16 16:50:18.281: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename resourcequota @ 12/16/23 16:50:18.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:50:18.319
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:50:18.325
  STEP: Creating resourceQuota "e2e-rq-status-4p2s4" @ 12/16/23 16:50:18.337
  Dec 16 16:50:18.353: INFO: Resource quota "e2e-rq-status-4p2s4" reports spec: hard cpu limit of 500m
  Dec 16 16:50:18.353: INFO: Resource quota "e2e-rq-status-4p2s4" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-4p2s4" /status @ 12/16/23 16:50:18.353
  STEP: Confirm /status for "e2e-rq-status-4p2s4" resourceQuota via watch @ 12/16/23 16:50:18.369
  Dec 16 16:50:18.371: INFO: observed resourceQuota "e2e-rq-status-4p2s4" in namespace "resourcequota-5961" with hard status: v1.ResourceList(nil)
  Dec 16 16:50:18.371: INFO: Found resourceQuota "e2e-rq-status-4p2s4" in namespace "resourcequota-5961" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Dec 16 16:50:18.372: INFO: ResourceQuota "e2e-rq-status-4p2s4" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 12/16/23 16:50:18.377
  Dec 16 16:50:18.386: INFO: Resource quota "e2e-rq-status-4p2s4" reports spec: hard cpu limit of 1
  Dec 16 16:50:18.386: INFO: Resource quota "e2e-rq-status-4p2s4" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-4p2s4" /status @ 12/16/23 16:50:18.386
  STEP: Confirm /status for "e2e-rq-status-4p2s4" resourceQuota via watch @ 12/16/23 16:50:18.396
  Dec 16 16:50:18.401: INFO: observed resourceQuota "e2e-rq-status-4p2s4" in namespace "resourcequota-5961" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Dec 16 16:50:18.401: INFO: Found resourceQuota "e2e-rq-status-4p2s4" in namespace "resourcequota-5961" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Dec 16 16:50:18.402: INFO: ResourceQuota "e2e-rq-status-4p2s4" /status was patched
  STEP: Get "e2e-rq-status-4p2s4" /status @ 12/16/23 16:50:18.402
  Dec 16 16:50:18.409: INFO: Resourcequota "e2e-rq-status-4p2s4" reports status: hard cpu of 1
  Dec 16 16:50:18.409: INFO: Resourcequota "e2e-rq-status-4p2s4" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-4p2s4" /status before checking Spec is unchanged @ 12/16/23 16:50:18.414
  Dec 16 16:50:18.430: INFO: Resourcequota "e2e-rq-status-4p2s4" reports status: hard cpu of 2
  Dec 16 16:50:18.430: INFO: Resourcequota "e2e-rq-status-4p2s4" reports status: hard memory of 2Gi
  Dec 16 16:50:18.432: INFO: observed resourceQuota "e2e-rq-status-4p2s4" in namespace "resourcequota-5961" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Dec 16 16:50:18.432: INFO: Found resourceQuota "e2e-rq-status-4p2s4" in namespace "resourcequota-5961" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E1216 16:50:19.154417      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:20.154823      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:21.155134      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:22.155231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:23.155523      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:24.156071      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:25.155963      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:26.156491      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:27.157332      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:28.158348      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:29.158522      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:30.158964      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:31.159827      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:32.160246      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:33.160978      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:34.161912      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:35.163931      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:36.163220      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:37.164188      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:38.164903      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:39.165128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:40.165770      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:41.165619      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:42.165785      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:43.166385      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:44.166550      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:45.167256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:46.167795      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:47.168267      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:48.168973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:49.168859      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:50.169865      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:51.170013      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:52.170774      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:53.171100      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:54.172003      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:55.172430      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:56.173654      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:57.174212      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:50:58.174716      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:50:58.447: INFO: ResourceQuota "e2e-rq-status-4p2s4" Spec was unchanged and /status reset
  Dec 16 16:50:58.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5961" for this suite. @ 12/16/23 16:50:58.461
• [40.196 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 12/16/23 16:50:58.489
  Dec 16 16:50:58.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename downward-api @ 12/16/23 16:50:58.492
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:50:58.532
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:50:58.539
  STEP: Creating a pod to test downward API volume plugin @ 12/16/23 16:50:58.548
  E1216 16:50:59.175592      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:00.175894      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:01.176220      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:02.178436      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 16:51:02.614
  Dec 16 16:51:02.622: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downwardapi-volume-b8436192-11a0-4ca0-bbe8-bf2249493138 container client-container: <nil>
  STEP: delete the pod @ 12/16/23 16:51:02.635
  Dec 16 16:51:02.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7537" for this suite. @ 12/16/23 16:51:02.675
• [4.206 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 12/16/23 16:51:02.702
  Dec 16 16:51:02.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubectl @ 12/16/23 16:51:02.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:51:02.758
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:51:02.767
  STEP: creating Agnhost RC @ 12/16/23 16:51:02.777
  Dec 16 16:51:02.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1189 create -f -'
  E1216 16:51:03.176617      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:51:03.507: INFO: stderr: ""
  Dec 16 16:51:03.507: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 12/16/23 16:51:03.507
  E1216 16:51:04.177638      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:51:04.520: INFO: Selector matched 1 pods for map[app:agnhost]
  Dec 16 16:51:04.520: INFO: Found 0 / 1
  E1216 16:51:05.178261      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:51:05.516: INFO: Selector matched 1 pods for map[app:agnhost]
  Dec 16 16:51:05.517: INFO: Found 1 / 1
  Dec 16 16:51:05.517: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 12/16/23 16:51:05.517
  Dec 16 16:51:05.525: INFO: Selector matched 1 pods for map[app:agnhost]
  Dec 16 16:51:05.525: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Dec 16 16:51:05.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1189 patch pod agnhost-primary-8bmmc -p {"metadata":{"annotations":{"x":"y"}}}'
  Dec 16 16:51:05.704: INFO: stderr: ""
  Dec 16 16:51:05.704: INFO: stdout: "pod/agnhost-primary-8bmmc patched\n"
  STEP: checking annotations @ 12/16/23 16:51:05.704
  Dec 16 16:51:05.709: INFO: Selector matched 1 pods for map[app:agnhost]
  Dec 16 16:51:05.710: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Dec 16 16:51:05.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1189" for this suite. @ 12/16/23 16:51:05.718
• [3.028 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 12/16/23 16:51:05.732
  Dec 16 16:51:05.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename endpointslice @ 12/16/23 16:51:05.734
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:51:05.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:51:05.772
  Dec 16 16:51:05.798: INFO: Endpoints addresses: [192.168.121.172 192.168.121.49] , ports: [6443]
  Dec 16 16:51:05.798: INFO: EndpointSlices addresses: [192.168.121.172 192.168.121.49] , ports: [6443]
  Dec 16 16:51:05.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5447" for this suite. @ 12/16/23 16:51:05.81
• [0.091 seconds]
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 12/16/23 16:51:05.825
  Dec 16 16:51:05.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename var-expansion @ 12/16/23 16:51:05.828
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:51:05.858
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:51:05.864
  E1216 16:51:06.179220      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:07.179160      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:51:07.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 16 16:51:07.921: INFO: Deleting pod "var-expansion-76793000-4e8a-4fe6-b06c-ab338fc99568" in namespace "var-expansion-9546"
  Dec 16 16:51:07.943: INFO: Wait up to 5m0s for pod "var-expansion-76793000-4e8a-4fe6-b06c-ab338fc99568" to be fully deleted
  E1216 16:51:08.179905      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:09.180980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-9546" for this suite. @ 12/16/23 16:51:09.962
• [4.155 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 12/16/23 16:51:09.981
  Dec 16 16:51:09.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 16:51:09.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:51:10.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:51:10.027
  STEP: Creating a pod to test downward API volume plugin @ 12/16/23 16:51:10.033
  E1216 16:51:10.181612      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:11.182743      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:12.182824      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:13.182980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 16:51:14.091
  Dec 16 16:51:14.098: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downwardapi-volume-e37e57ea-2bf8-493f-8b1a-dda1acd3bd3c container client-container: <nil>
  STEP: delete the pod @ 12/16/23 16:51:14.117
  Dec 16 16:51:14.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1373" for this suite. @ 12/16/23 16:51:14.157
• [4.190 seconds]
------------------------------
SS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 12/16/23 16:51:14.172
  Dec 16 16:51:14.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename cronjob @ 12/16/23 16:51:14.176
  E1216 16:51:14.183504      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:51:14.203
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:51:14.209
  STEP: Creating a suspended cronjob @ 12/16/23 16:51:14.215
  STEP: Ensuring no jobs are scheduled @ 12/16/23 16:51:14.227
  E1216 16:51:15.185050      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:16.185497      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:17.186489      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:18.187555      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:19.187003      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:20.187172      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:21.187367      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:22.188249      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:23.189187      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:24.189927      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:25.190699      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:26.190949      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:27.191114      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:28.191728      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:29.192011      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:30.192200      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:31.192631      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:32.193043      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:33.193644      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:34.193858      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:35.194150      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:36.194309      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:37.194648      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:38.195713      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:39.195831      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:40.196017      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:41.196453      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:42.196885      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:43.197102      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:44.197861      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:45.198310      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:46.198883      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:47.199506      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:48.199951      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:49.200249      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:50.200722      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:51.200637      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:52.201004      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:53.201898      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:54.202104      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:55.202262      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:56.202528      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:57.202720      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:58.203467      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:51:59.204020      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:00.204376      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:01.204677      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:02.205711      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:03.206571      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:04.207136      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:05.207448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:06.207543      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:07.207968      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:08.208901      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:09.209601      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:10.209483      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:11.209616      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:12.210947      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:13.211707      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:14.212297      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:15.212534      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:16.213198      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:17.214156      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:18.214555      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:19.214801      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:20.215582      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:21.216933      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:22.217825      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:23.217996      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:24.218191      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:25.218363      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:26.219041      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:27.219698      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:28.219657      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:29.219889      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:30.220053      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:31.220156      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:32.220695      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:33.221465      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:34.222651      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:35.222348      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:36.222702      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:37.223256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:38.223925      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:39.224117      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:40.224434      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:41.224630      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:42.224909      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:43.224992      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:44.225863      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:45.226257      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:46.227111      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:47.227327      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:48.227503      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:49.227539      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:50.227739      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:51.228799      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:52.228703      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:53.229046      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:54.231155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:55.231274      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:56.231427      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:57.232416      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:58.232888      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:52:59.233445      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:00.234254      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:01.234238      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:02.272119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:03.254190      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:04.254192      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:05.255390      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:06.255669      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:07.255754      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:08.256474      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:09.256873      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:10.257382      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:11.257518      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:12.258176      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:13.260866      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:14.260966      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:15.261310      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:16.261735      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:17.262566      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:18.262719      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:19.263899      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:20.264410      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:21.265944      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:22.265963      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:23.266360      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:24.266365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:25.266748      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:26.267654      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:27.267411      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:28.268501      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:29.269258      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:30.269357      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:31.270157      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:32.271129      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:33.272125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:34.273094      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:35.273011      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:36.273062      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:37.273467      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:38.273352      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:39.273927      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:40.274068      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:41.274683      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:42.274777      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:43.274802      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:44.275459      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:45.276261      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:46.276678      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:47.277062      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:48.277764      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:49.278548      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:50.279693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:51.279722      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:52.280921      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:53.281342      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:54.282010      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:55.282445      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:56.282989      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:57.283676      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:58.284503      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:53:59.284801      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:00.284946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:01.285369      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:02.285434      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:03.286757      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:04.286449      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:05.287460      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:06.288916      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:07.289173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:08.289941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:09.292082      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:10.290280      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:11.290431      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:12.291467      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:13.292575      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:14.292152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:15.292654      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:16.293607      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:17.294541      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:18.295389      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:19.296272      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:20.297306      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:21.298337      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:22.299388      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:23.299633      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:24.299950      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:25.300940      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:26.301266      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:27.302177      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:28.303025      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:29.303342      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:30.304008      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:31.304254      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:32.304696      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:33.304968      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:34.305760      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:35.306106      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:36.307786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:37.308829      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:38.309262      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:39.309057      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:40.309608      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:41.311557      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:42.310932      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:43.311038      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:44.312103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:45.312666      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:46.312816      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:47.313271      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:48.314288      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:49.315293      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:50.315909      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:51.315870      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:52.316262      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:53.316132      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:54.316966      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:55.317596      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:56.318577      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:57.318778      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:58.318726      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:54:59.319605      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:00.320555      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:01.321038      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:02.321092      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:03.321477      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:04.321230      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:05.322049      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:06.323075      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:07.323645      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:08.324067      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:09.324869      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:10.325730      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:11.326638      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:12.327353      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:13.327652      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:14.328265      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:15.328799      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:16.329507      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:17.329373      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:18.329927      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:19.332662      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:20.331022      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:21.331835      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:22.332135      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:23.332452      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:24.333212      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:25.333501      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:26.334028      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:27.334255      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:28.334791      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:29.334890      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:30.335295      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:31.335825      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:32.336817      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:33.337061      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:34.337297      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:35.338131      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:36.339092      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:37.340311      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:38.340639      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:39.341092      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:40.341910      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:41.342373      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:42.343004      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:43.343351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:44.344107      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:45.344409      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:46.345146      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:47.345156      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:48.345187      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:49.345465      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:50.346053      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:51.346569      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:52.347099      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:53.348270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:54.349911      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:55.349696      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:56.350379      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:57.350388      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:58.351127      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:55:59.351458      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:00.352016      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:01.353152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:02.354248      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:03.354969      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:04.354927      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:05.355663      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:06.355970      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:07.356074      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:08.356826      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:09.357473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:10.358598      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:11.359305      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:12.359945      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:13.360683      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 12/16/23 16:56:14.239
  STEP: Removing cronjob @ 12/16/23 16:56:14.245
  Dec 16 16:56:14.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8731" for this suite. @ 12/16/23 16:56:14.284
• [300.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 12/16/23 16:56:14.306
  Dec 16 16:56:14.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 16:56:14.312
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:56:14.353
  E1216 16:56:14.360628      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:56:14.361
  STEP: Creating projection with secret that has name projected-secret-test-7c920693-7a5e-444e-80d8-4c07c4e1d684 @ 12/16/23 16:56:14.366
  STEP: Creating a pod to test consume secrets @ 12/16/23 16:56:14.373
  E1216 16:56:15.360969      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:16.361172      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:17.361247      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:18.361466      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 16:56:18.414
  Dec 16 16:56:18.420: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-projected-secrets-04966c86-95c8-4f2f-9257-a10638187597 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 12/16/23 16:56:18.456
  Dec 16 16:56:18.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6329" for this suite. @ 12/16/23 16:56:18.491
• [4.196 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 12/16/23 16:56:18.507
  Dec 16 16:56:18.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubelet-test @ 12/16/23 16:56:18.51
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:56:18.551
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:56:18.557
  STEP: Waiting for pod completion @ 12/16/23 16:56:18.576
  E1216 16:56:19.362303      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:20.362394      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:21.362550      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:22.362855      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:56:22.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-9480" for this suite. @ 12/16/23 16:56:22.628
• [4.135 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 12/16/23 16:56:22.646
  Dec 16 16:56:22.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename watch @ 12/16/23 16:56:22.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:56:22.686
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:56:22.691
  STEP: creating a watch on configmaps with label A @ 12/16/23 16:56:22.697
  STEP: creating a watch on configmaps with label B @ 12/16/23 16:56:22.701
  STEP: creating a watch on configmaps with label A or B @ 12/16/23 16:56:22.704
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 12/16/23 16:56:22.706
  Dec 16 16:56:22.715: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7359  b545cbf5-f161-443a-8295-e21f10a27dc6 11837 0 2023-12-16 16:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-12-16 16:56:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 16 16:56:22.716: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7359  b545cbf5-f161-443a-8295-e21f10a27dc6 11837 0 2023-12-16 16:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-12-16 16:56:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 12/16/23 16:56:22.716
  Dec 16 16:56:22.730: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7359  b545cbf5-f161-443a-8295-e21f10a27dc6 11838 0 2023-12-16 16:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-12-16 16:56:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 16 16:56:22.731: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7359  b545cbf5-f161-443a-8295-e21f10a27dc6 11838 0 2023-12-16 16:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-12-16 16:56:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 12/16/23 16:56:22.731
  Dec 16 16:56:22.748: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7359  b545cbf5-f161-443a-8295-e21f10a27dc6 11839 0 2023-12-16 16:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-12-16 16:56:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 16 16:56:22.749: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7359  b545cbf5-f161-443a-8295-e21f10a27dc6 11839 0 2023-12-16 16:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-12-16 16:56:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 12/16/23 16:56:22.749
  Dec 16 16:56:22.763: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7359  b545cbf5-f161-443a-8295-e21f10a27dc6 11840 0 2023-12-16 16:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-12-16 16:56:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 16 16:56:22.764: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7359  b545cbf5-f161-443a-8295-e21f10a27dc6 11840 0 2023-12-16 16:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-12-16 16:56:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 12/16/23 16:56:22.764
  Dec 16 16:56:22.773: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7359  a2bdea1c-8cb0-489d-abfd-fd16001833ac 11841 0 2023-12-16 16:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-12-16 16:56:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 16 16:56:22.773: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7359  a2bdea1c-8cb0-489d-abfd-fd16001833ac 11841 0 2023-12-16 16:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-12-16 16:56:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E1216 16:56:23.363027      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:24.363803      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:25.364109      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:26.365281      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:27.366375      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:28.366743      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:29.368905      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:30.368967      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:31.369118      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:32.369318      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 12/16/23 16:56:32.774
  Dec 16 16:56:32.794: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7359  a2bdea1c-8cb0-489d-abfd-fd16001833ac 11878 0 2023-12-16 16:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-12-16 16:56:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 16 16:56:32.795: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7359  a2bdea1c-8cb0-489d-abfd-fd16001833ac 11878 0 2023-12-16 16:56:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-12-16 16:56:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E1216 16:56:33.369585      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:34.369682      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:35.369869      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:36.370156      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:37.370476      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:38.370946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:39.371109      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:40.371678      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:41.372452      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:42.373009      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:56:42.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7359" for this suite. @ 12/16/23 16:56:42.808
• [20.176 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 12/16/23 16:56:42.827
  Dec 16 16:56:42.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubectl @ 12/16/23 16:56:42.831
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:56:42.865
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:56:42.871
  STEP: creating a replication controller @ 12/16/23 16:56:42.879
  Dec 16 16:56:42.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 create -f -'
  E1216 16:56:43.373074      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:56:43.635: INFO: stderr: ""
  Dec 16 16:56:43.635: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 12/16/23 16:56:43.636
  Dec 16 16:56:43.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Dec 16 16:56:43.832: INFO: stderr: ""
  Dec 16 16:56:43.832: INFO: stdout: "update-demo-nautilus-cgj8n update-demo-nautilus-n5nj6 "
  Dec 16 16:56:43.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 get pods update-demo-nautilus-cgj8n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Dec 16 16:56:44.002: INFO: stderr: ""
  Dec 16 16:56:44.002: INFO: stdout: ""
  Dec 16 16:56:44.002: INFO: update-demo-nautilus-cgj8n is created but not running
  E1216 16:56:44.373250      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:45.374104      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:46.374175      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:47.374320      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:48.374531      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:56:49.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Dec 16 16:56:49.176: INFO: stderr: ""
  Dec 16 16:56:49.176: INFO: stdout: "update-demo-nautilus-cgj8n update-demo-nautilus-n5nj6 "
  Dec 16 16:56:49.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 get pods update-demo-nautilus-cgj8n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Dec 16 16:56:49.327: INFO: stderr: ""
  Dec 16 16:56:49.327: INFO: stdout: "true"
  Dec 16 16:56:49.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 get pods update-demo-nautilus-cgj8n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E1216 16:56:49.374579      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:56:49.514: INFO: stderr: ""
  Dec 16 16:56:49.514: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Dec 16 16:56:49.514: INFO: validating pod update-demo-nautilus-cgj8n
  Dec 16 16:56:49.533: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Dec 16 16:56:49.533: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Dec 16 16:56:49.533: INFO: update-demo-nautilus-cgj8n is verified up and running
  Dec 16 16:56:49.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 get pods update-demo-nautilus-n5nj6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Dec 16 16:56:49.741: INFO: stderr: ""
  Dec 16 16:56:49.741: INFO: stdout: "true"
  Dec 16 16:56:49.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 get pods update-demo-nautilus-n5nj6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Dec 16 16:56:49.929: INFO: stderr: ""
  Dec 16 16:56:49.929: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Dec 16 16:56:49.929: INFO: validating pod update-demo-nautilus-n5nj6
  Dec 16 16:56:49.942: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Dec 16 16:56:49.942: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Dec 16 16:56:49.943: INFO: update-demo-nautilus-n5nj6 is verified up and running
  STEP: scaling down the replication controller @ 12/16/23 16:56:49.943
  Dec 16 16:56:49.958: INFO: scanned /root for discovery docs: <nil>
  Dec 16 16:56:49.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E1216 16:56:50.375619      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:56:51.166: INFO: stderr: ""
  Dec 16 16:56:51.166: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 12/16/23 16:56:51.166
  Dec 16 16:56:51.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Dec 16 16:56:51.321: INFO: stderr: ""
  Dec 16 16:56:51.321: INFO: stdout: "update-demo-nautilus-cgj8n "
  Dec 16 16:56:51.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 get pods update-demo-nautilus-cgj8n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E1216 16:56:51.376229      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:56:51.459: INFO: stderr: ""
  Dec 16 16:56:51.459: INFO: stdout: "true"
  Dec 16 16:56:51.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 get pods update-demo-nautilus-cgj8n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Dec 16 16:56:51.604: INFO: stderr: ""
  Dec 16 16:56:51.604: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Dec 16 16:56:51.604: INFO: validating pod update-demo-nautilus-cgj8n
  Dec 16 16:56:51.611: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Dec 16 16:56:51.611: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Dec 16 16:56:51.611: INFO: update-demo-nautilus-cgj8n is verified up and running
  STEP: scaling up the replication controller @ 12/16/23 16:56:51.611
  Dec 16 16:56:51.624: INFO: scanned /root for discovery docs: <nil>
  Dec 16 16:56:51.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E1216 16:56:52.377151      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:56:52.826: INFO: stderr: ""
  Dec 16 16:56:52.826: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 12/16/23 16:56:52.826
  Dec 16 16:56:52.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Dec 16 16:56:53.000: INFO: stderr: ""
  Dec 16 16:56:53.000: INFO: stdout: "update-demo-nautilus-cgj8n update-demo-nautilus-gm8rc "
  Dec 16 16:56:53.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 get pods update-demo-nautilus-cgj8n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Dec 16 16:56:53.159: INFO: stderr: ""
  Dec 16 16:56:53.159: INFO: stdout: "true"
  Dec 16 16:56:53.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 get pods update-demo-nautilus-cgj8n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Dec 16 16:56:53.313: INFO: stderr: ""
  Dec 16 16:56:53.313: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Dec 16 16:56:53.313: INFO: validating pod update-demo-nautilus-cgj8n
  Dec 16 16:56:53.322: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Dec 16 16:56:53.322: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Dec 16 16:56:53.322: INFO: update-demo-nautilus-cgj8n is verified up and running
  Dec 16 16:56:53.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 get pods update-demo-nautilus-gm8rc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E1216 16:56:53.377568      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:56:53.479: INFO: stderr: ""
  Dec 16 16:56:53.479: INFO: stdout: "true"
  Dec 16 16:56:53.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 get pods update-demo-nautilus-gm8rc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Dec 16 16:56:53.631: INFO: stderr: ""
  Dec 16 16:56:53.631: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Dec 16 16:56:53.631: INFO: validating pod update-demo-nautilus-gm8rc
  Dec 16 16:56:53.643: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Dec 16 16:56:53.643: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Dec 16 16:56:53.643: INFO: update-demo-nautilus-gm8rc is verified up and running
  STEP: using delete to clean up resources @ 12/16/23 16:56:53.643
  Dec 16 16:56:53.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 delete --grace-period=0 --force -f -'
  Dec 16 16:56:53.817: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Dec 16 16:56:53.817: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Dec 16 16:56:53.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 get rc,svc -l name=update-demo --no-headers'
  Dec 16 16:56:54.052: INFO: stderr: "No resources found in kubectl-1491 namespace.\n"
  Dec 16 16:56:54.052: INFO: stdout: ""
  Dec 16 16:56:54.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-1491 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Dec 16 16:56:54.359: INFO: stderr: ""
  Dec 16 16:56:54.359: INFO: stdout: ""
  Dec 16 16:56:54.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1491" for this suite. @ 12/16/23 16:56:54.371
  E1216 16:56:54.378066      13 retrywatcher.go:130] "Watch failed" err="context canceled"
• [11.557 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 12/16/23 16:56:54.389
  Dec 16 16:56:54.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-probe @ 12/16/23 16:56:54.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:56:54.439
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:56:54.448
  E1216 16:56:55.379006      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:56.379296      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:57.379813      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:58.380371      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:56:59.381455      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:00.382201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:01.383684      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:02.383963      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:03.383703      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:04.383705      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:05.384042      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:06.384521      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:07.385046      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:08.385822      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:09.386018      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:10.387359      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:11.387682      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:12.389961      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:13.390950      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:14.391543      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:15.392558      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:16.392785      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:17.393474      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:18.394329      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:19.395494      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:20.395485      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:21.396578      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:22.397294      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:23.398685      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:24.398624      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:25.398974      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:26.400147      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:27.402636      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:28.401365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:29.402312      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:30.402230      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:31.403481      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:32.403748      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:33.404525      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:34.404819      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:35.404934      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:36.405816      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:37.406187      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:38.407512      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:39.407467      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:40.407839      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:41.409130      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:42.409276      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:43.410344      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:44.411387      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:45.411249      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:46.411451      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:47.411847      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:48.412437      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:49.413521      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:50.413874      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:51.414733      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:52.415783      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:53.416615      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:54.416798      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:57:54.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-4382" for this suite. @ 12/16/23 16:57:54.504
• [60.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 12/16/23 16:57:54.525
  Dec 16 16:57:54.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename replicaset @ 12/16/23 16:57:54.529
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:57:54.569
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:57:54.575
  Dec 16 16:57:54.582: INFO: Creating ReplicaSet my-hostname-basic-597a4d48-99d8-48e0-9f89-8d5116b76df8
  Dec 16 16:57:54.601: INFO: Pod name my-hostname-basic-597a4d48-99d8-48e0-9f89-8d5116b76df8: Found 0 pods out of 1
  E1216 16:57:55.416910      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:56.417307      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:57.417555      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:58.418226      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:57:59.417890      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:57:59.611: INFO: Pod name my-hostname-basic-597a4d48-99d8-48e0-9f89-8d5116b76df8: Found 1 pods out of 1
  Dec 16 16:57:59.611: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-597a4d48-99d8-48e0-9f89-8d5116b76df8" is running
  Dec 16 16:57:59.621: INFO: Pod "my-hostname-basic-597a4d48-99d8-48e0-9f89-8d5116b76df8-4cvpn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-16 16:57:54 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-16 16:57:56 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-16 16:57:56 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-16 16:57:54 +0000 UTC Reason: Message:}])
  Dec 16 16:57:59.621: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 12/16/23 16:57:59.622
  Dec 16 16:57:59.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-351" for this suite. @ 12/16/23 16:57:59.662
• [5.152 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 12/16/23 16:57:59.684
  Dec 16 16:57:59.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename webhook @ 12/16/23 16:57:59.687
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:57:59.725
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:57:59.733
  STEP: Setting up server cert @ 12/16/23 16:57:59.791
  E1216 16:58:00.418339      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/16/23 16:58:00.921
  STEP: Deploying the webhook pod @ 12/16/23 16:58:00.941
  STEP: Wait for the deployment to be ready @ 12/16/23 16:58:00.968
  Dec 16 16:58:00.983: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E1216 16:58:01.418098      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:02.420020      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/16/23 16:58:03.006
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 16:58:03.034
  E1216 16:58:03.419124      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:58:04.035: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 12/16/23 16:58:04.043
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 12/16/23 16:58:04.09
  STEP: Creating a configMap that should not be mutated @ 12/16/23 16:58:04.103
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 12/16/23 16:58:04.121
  STEP: Creating a configMap that should be mutated @ 12/16/23 16:58:04.134
  Dec 16 16:58:04.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8957" for this suite. @ 12/16/23 16:58:04.294
  STEP: Destroying namespace "webhook-markers-5605" for this suite. @ 12/16/23 16:58:04.31
• [4.637 seconds]
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 12/16/23 16:58:04.329
  Dec 16 16:58:04.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename init-container @ 12/16/23 16:58:04.337
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:58:04.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:58:04.376
  STEP: creating the pod @ 12/16/23 16:58:04.381
  Dec 16 16:58:04.381: INFO: PodSpec: initContainers in spec.initContainers
  E1216 16:58:04.421181      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:05.421946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:06.422731      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:07.422964      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:58:08.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-2910" for this suite. @ 12/16/23 16:58:08.143
• [3.832 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:205
  STEP: Creating a kubernetes client @ 12/16/23 16:58:08.169
  Dec 16 16:58:08.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename daemonsets @ 12/16/23 16:58:08.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:58:08.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:58:08.218
  Dec 16 16:58:08.274: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 12/16/23 16:58:08.286
  Dec 16 16:58:08.294: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 16:58:08.294: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 12/16/23 16:58:08.294
  Dec 16 16:58:08.343: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 16:58:08.344: INFO: Node phoh7xai9ouk-3 is running 0 daemon pod, expected 1
  E1216 16:58:08.429600      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:58:09.353: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 16 16:58:09.353: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 12/16/23 16:58:09.36
  Dec 16 16:58:09.407: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 16 16:58:09.408: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E1216 16:58:09.430186      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:58:10.416: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 16:58:10.416: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 12/16/23 16:58:10.416
  E1216 16:58:10.430614      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:58:10.433: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 16:58:10.433: INFO: Node phoh7xai9ouk-3 is running 0 daemon pod, expected 1
  E1216 16:58:11.430976      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:58:11.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 16:58:11.441: INFO: Node phoh7xai9ouk-3 is running 0 daemon pod, expected 1
  E1216 16:58:12.431288      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:58:12.442: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 16 16:58:12.443: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 12/16/23 16:58:12.466
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6839, will wait for the garbage collector to delete the pods @ 12/16/23 16:58:12.466
  Dec 16 16:58:12.538: INFO: Deleting DaemonSet.extensions daemon-set took: 14.729013ms
  Dec 16 16:58:12.641: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.181715ms
  E1216 16:58:13.432285      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:14.432392      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:58:15.354: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 16:58:15.354: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Dec 16 16:58:15.360: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12362"},"items":null}

  Dec 16 16:58:15.367: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12362"},"items":null}

  Dec 16 16:58:15.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E1216 16:58:15.433046      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "daemonsets-6839" for this suite. @ 12/16/23 16:58:15.434
• [7.279 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 12/16/23 16:58:15.451
  Dec 16 16:58:15.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename namespaces @ 12/16/23 16:58:15.455
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:58:15.49
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:58:15.5
  STEP: Creating namespace "e2e-ns-mz72z" @ 12/16/23 16:58:15.513
  Dec 16 16:58:15.551: INFO: Namespace "e2e-ns-mz72z-6976" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-mz72z-6976" @ 12/16/23 16:58:15.551
  Dec 16 16:58:15.572: INFO: Namespace "e2e-ns-mz72z-6976" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-mz72z-6976" @ 12/16/23 16:58:15.578
  Dec 16 16:58:15.605: INFO: Namespace "e2e-ns-mz72z-6976" has []v1.FinalizerName{"kubernetes"}
  Dec 16 16:58:15.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5503" for this suite. @ 12/16/23 16:58:15.621
  STEP: Destroying namespace "e2e-ns-mz72z-6976" for this suite. @ 12/16/23 16:58:15.638
• [0.203 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 12/16/23 16:58:15.668
  Dec 16 16:58:15.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubectl @ 12/16/23 16:58:15.672
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:58:15.706
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:58:15.711
  STEP: validating cluster-info @ 12/16/23 16:58:15.718
  Dec 16 16:58:15.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-2775 cluster-info'
  Dec 16 16:58:15.950: INFO: stderr: ""
  Dec 16 16:58:15.950: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Dec 16 16:58:15.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2775" for this suite. @ 12/16/23 16:58:15.958
• [0.300 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 12/16/23 16:58:15.971
  Dec 16 16:58:15.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename sched-preemption @ 12/16/23 16:58:15.974
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:58:16.015
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:58:16.021
  Dec 16 16:58:16.057: INFO: Waiting up to 1m0s for all nodes to be ready
  E1216 16:58:16.434180      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:17.435517      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:18.435980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:19.436103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:20.436845      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:21.437092      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:22.437869      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:23.438019      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:24.438787      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:25.439429      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:26.440362      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:27.440580      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:28.441515      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:29.441652      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:30.442125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:31.442316      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:32.442531      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:33.442832      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:34.443592      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:35.444102      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:36.444978      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:37.445701      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:38.446598      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:39.446451      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:40.447724      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:41.447897      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:42.448384      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:43.448488      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:44.449822      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:45.449856      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:46.450920      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:47.451820      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:48.452011      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:49.453295      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:50.453753      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:51.454514      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:52.454888      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:53.455261      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:54.456012      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:55.456363      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:56.457272      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:57.457560      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:58.458636      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:58:59.458801      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:00.458909      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:01.459427      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:02.460192      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:03.461338      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:04.461521      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:05.462207      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:06.462452      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:07.462955      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:08.463804      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:09.464036      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:10.464202      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:11.465085      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:12.465189      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:13.466190      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:14.467263      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:15.467224      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:59:16.114: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 12/16/23 16:59:16.122
  Dec 16 16:59:16.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename sched-preemption-path @ 12/16/23 16:59:16.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:59:16.177
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:59:16.183
  Dec 16 16:59:16.216: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Dec 16 16:59:16.225: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Dec 16 16:59:16.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 16 16:59:16.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-3524" for this suite. @ 12/16/23 16:59:16.425
  STEP: Destroying namespace "sched-preemption-6454" for this suite. @ 12/16/23 16:59:16.445
• [60.490 seconds]
------------------------------
SSSSSSSSSSSSS  E1216 16:59:16.467231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 12/16/23 16:59:16.484
  Dec 16 16:59:16.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename configmap @ 12/16/23 16:59:16.487
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:59:16.521
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:59:16.527
  STEP: Creating configMap configmap-7198/configmap-test-67cf69b6-e309-432d-b108-2ae9fa2a1c97 @ 12/16/23 16:59:16.534
  STEP: Creating a pod to test consume configMaps @ 12/16/23 16:59:16.543
  E1216 16:59:17.467766      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:18.469396      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 16:59:18.578
  Dec 16 16:59:18.582: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-configmaps-2c40697c-6baa-41a9-8064-5a9ea0acd0ea container env-test: <nil>
  STEP: delete the pod @ 12/16/23 16:59:18.621
  Dec 16 16:59:18.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7198" for this suite. @ 12/16/23 16:59:18.664
• [2.194 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 12/16/23 16:59:18.687
  Dec 16 16:59:18.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename webhook @ 12/16/23 16:59:18.69
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:59:18.722
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:59:18.727
  STEP: Setting up server cert @ 12/16/23 16:59:18.772
  E1216 16:59:19.469463      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/16/23 16:59:20.032
  STEP: Deploying the webhook pod @ 12/16/23 16:59:20.047
  STEP: Wait for the deployment to be ready @ 12/16/23 16:59:20.069
  Dec 16 16:59:20.080: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E1216 16:59:20.469827      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:21.470752      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/16/23 16:59:22.101
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 16:59:22.118
  E1216 16:59:22.471807      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:59:23.119: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 12/16/23 16:59:23.126
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 12/16/23 16:59:23.157
  STEP: Creating a dummy validating-webhook-configuration object @ 12/16/23 16:59:23.189
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 12/16/23 16:59:23.205
  STEP: Creating a dummy mutating-webhook-configuration object @ 12/16/23 16:59:23.217
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 12/16/23 16:59:23.233
  Dec 16 16:59:23.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9492" for this suite. @ 12/16/23 16:59:23.406
  STEP: Destroying namespace "webhook-markers-4110" for this suite. @ 12/16/23 16:59:23.426
• [4.751 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 12/16/23 16:59:23.447
  Dec 16 16:59:23.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename crd-publish-openapi @ 12/16/23 16:59:23.45
  E1216 16:59:23.472440      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:59:23.48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:59:23.484
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 12/16/23 16:59:23.487
  Dec 16 16:59:23.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 16:59:24.472906      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:59:25.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 16:59:25.473675      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:26.473715      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:27.474133      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:28.474814      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:29.476979      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:30.477060      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:31.478256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:32.478755      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:59:33.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1462" for this suite. @ 12/16/23 16:59:33.143
• [9.708 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 12/16/23 16:59:33.172
  Dec 16 16:59:33.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename configmap @ 12/16/23 16:59:33.178
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:59:33.209
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:59:33.213
  STEP: Creating configMap that has name configmap-test-emptyKey-bf7415db-7db4-431e-ba46-fc8a87a0e421 @ 12/16/23 16:59:33.217
  Dec 16 16:59:33.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8847" for this suite. @ 12/16/23 16:59:33.227
• [0.067 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 12/16/23 16:59:33.24
  Dec 16 16:59:33.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 12/16/23 16:59:33.244
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:59:33.27
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:59:33.274
  STEP: create the container to handle the HTTPGet hook request. @ 12/16/23 16:59:33.285
  E1216 16:59:33.478916      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:34.480172      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 12/16/23 16:59:35.348
  E1216 16:59:35.480117      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:36.480677      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 12/16/23 16:59:37.408
  E1216 16:59:37.481177      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:38.480998      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 12/16/23 16:59:39.441
  Dec 16 16:59:39.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E1216 16:59:39.482515      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "container-lifecycle-hook-7789" for this suite. @ 12/16/23 16:59:39.497
• [6.272 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 12/16/23 16:59:39.524
  Dec 16 16:59:39.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename webhook @ 12/16/23 16:59:39.532
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:59:39.563
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:59:39.569
  STEP: Setting up server cert @ 12/16/23 16:59:39.618
  E1216 16:59:40.483889      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/16/23 16:59:41.166
  STEP: Deploying the webhook pod @ 12/16/23 16:59:41.184
  STEP: Wait for the deployment to be ready @ 12/16/23 16:59:41.21
  Dec 16 16:59:41.225: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E1216 16:59:41.483480      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:42.484228      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/16/23 16:59:43.249
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 16:59:43.289
  E1216 16:59:43.485057      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:59:44.292: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 12/16/23 16:59:44.3
  STEP: Creating a custom resource definition that should be denied by the webhook @ 12/16/23 16:59:44.334
  Dec 16 16:59:44.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 16:59:44.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E1216 16:59:44.485892      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-3071" for this suite. @ 12/16/23 16:59:44.488
  STEP: Destroying namespace "webhook-markers-4819" for this suite. @ 12/16/23 16:59:44.508
• [5.002 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 12/16/23 16:59:44.527
  Dec 16 16:59:44.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename emptydir @ 12/16/23 16:59:44.531
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:59:44.57
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:59:44.576
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 12/16/23 16:59:44.582
  E1216 16:59:45.486154      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:46.486520      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:47.486608      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:48.487678      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 16:59:48.747
  Dec 16 16:59:48.754: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-a666ccfc-b121-4f10-a7c9-085616dc304c container test-container: <nil>
  STEP: delete the pod @ 12/16/23 16:59:48.793
  Dec 16 16:59:48.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3213" for this suite. @ 12/16/23 16:59:48.839
• [4.330 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:318
  STEP: Creating a kubernetes client @ 12/16/23 16:59:48.862
  Dec 16 16:59:48.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename statefulset @ 12/16/23 16:59:48.865
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 16:59:48.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 16:59:48.903
  STEP: Creating service test in namespace statefulset-2946 @ 12/16/23 16:59:48.913
  STEP: Creating a new StatefulSet @ 12/16/23 16:59:48.926
  Dec 16 16:59:48.959: INFO: Found 0 stateful pods, waiting for 3
  E1216 16:59:49.488959      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:50.489205      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:51.489444      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:52.490348      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:53.491318      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:54.492050      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:55.492598      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:56.492487      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:57.493313      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 16:59:58.493762      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 16:59:58.970: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Dec 16 16:59:58.970: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Dec 16 16:59:58.971: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Dec 16 16:59:58.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-2946 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Dec 16 16:59:59.279: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 16 16:59:59.279: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 16 16:59:59.279: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E1216 16:59:59.494293      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:00.495270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:01.496191      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:02.496426      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:03.497291      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:04.497477      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:05.497696      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:06.497857      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:07.498130      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:08.499620      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 12/16/23 17:00:09.32
  Dec 16 17:00:09.353: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 12/16/23 17:00:09.353
  E1216 17:00:09.498987      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:10.499632      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:11.499699      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:12.500836      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:13.501239      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:14.501454      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:15.501704      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:16.501738      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:17.501983      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:18.502121      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 12/16/23 17:00:19.393
  Dec 16 17:00:19.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-2946 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E1216 17:00:19.502667      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:00:19.744: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Dec 16 17:00:19.744: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Dec 16 17:00:19.744: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E1216 17:00:20.503009      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:21.503177      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:22.503893      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:23.504072      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:24.504985      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:25.505189      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:26.505946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:27.506782      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:28.506787      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:29.507666      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 12/16/23 17:00:29.804
  Dec 16 17:00:29.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-2946 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Dec 16 17:00:30.181: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 16 17:00:30.181: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 16 17:00:30.181: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E1216 17:00:30.507584      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:31.507746      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:32.508423      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:33.508373      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:34.508845      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:35.510127      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:36.509334      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:37.509752      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:38.509733      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:39.510085      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:00:40.241: INFO: Updating stateful set ss2
  E1216 17:00:40.511258      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:41.511456      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:42.511744      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:43.511843      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:44.512565      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:45.512848      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:46.512994      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:47.513129      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:48.513321      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:49.514035      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 12/16/23 17:00:50.276
  Dec 16 17:00:50.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-2946 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E1216 17:00:50.515162      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:00:50.664: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Dec 16 17:00:50.664: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Dec 16 17:00:50.664: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E1216 17:00:51.515486      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:52.515804      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:53.515867      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:54.516110      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:55.516348      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:56.516827      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:57.517262      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:58.518247      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:00:59.518700      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:00.519374      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:01:00.719: INFO: Deleting all statefulset in ns statefulset-2946
  Dec 16 17:01:00.729: INFO: Scaling statefulset ss2 to 0
  E1216 17:01:01.520456      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:02.522986      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:03.522678      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:04.522814      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:05.522865      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:06.523095      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:07.524127      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:08.524256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:09.525400      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:10.525869      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:01:10.780: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 16 17:01:10.788: INFO: Deleting statefulset ss2
  Dec 16 17:01:10.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2946" for this suite. @ 12/16/23 17:01:10.845
• [82.010 seconds]
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 12/16/23 17:01:10.872
  Dec 16 17:01:10.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename pod-network-test @ 12/16/23 17:01:10.877
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:01:10.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:01:10.961
  STEP: Performing setup for networking test in namespace pod-network-test-243 @ 12/16/23 17:01:10.967
  STEP: creating a selector @ 12/16/23 17:01:10.967
  STEP: Creating the service pods in kubernetes @ 12/16/23 17:01:10.967
  Dec 16 17:01:10.967: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E1216 17:01:11.526593      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:12.526803      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:13.527128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:14.527074      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:15.527919      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:16.528045      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:17.529094      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:18.529466      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:19.529870      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:20.530568      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:21.531011      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:22.531811      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 12/16/23 17:01:23.213
  E1216 17:01:23.531995      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:24.532903      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:01:25.245: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Dec 16 17:01:25.245: INFO: Breadth first check of 10.233.64.32 on host 192.168.121.172...
  Dec 16 17:01:25.253: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.137:9080/dial?request=hostname&protocol=http&host=10.233.64.32&port=8083&tries=1'] Namespace:pod-network-test-243 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:01:25.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:01:25.255: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:01:25.255: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-243/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.32%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Dec 16 17:01:25.465: INFO: Waiting for responses: map[]
  Dec 16 17:01:25.465: INFO: reached 10.233.64.32 after 0/1 tries
  Dec 16 17:01:25.465: INFO: Breadth first check of 10.233.65.29 on host 192.168.121.49...
  Dec 16 17:01:25.473: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.137:9080/dial?request=hostname&protocol=http&host=10.233.65.29&port=8083&tries=1'] Namespace:pod-network-test-243 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:01:25.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:01:25.475: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:01:25.475: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-243/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.65.29%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  E1216 17:01:25.533197      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:01:25.623: INFO: Waiting for responses: map[]
  Dec 16 17:01:25.624: INFO: reached 10.233.65.29 after 0/1 tries
  Dec 16 17:01:25.624: INFO: Breadth first check of 10.233.66.136 on host 192.168.121.112...
  Dec 16 17:01:25.635: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.137:9080/dial?request=hostname&protocol=http&host=10.233.66.136&port=8083&tries=1'] Namespace:pod-network-test-243 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:01:25.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:01:25.636: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:01:25.637: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-243/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.137%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.136%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Dec 16 17:01:25.751: INFO: Waiting for responses: map[]
  Dec 16 17:01:25.751: INFO: reached 10.233.66.136 after 0/1 tries
  Dec 16 17:01:25.751: INFO: Going to retry 0 out of 3 pods....
  Dec 16 17:01:25.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-243" for this suite. @ 12/16/23 17:01:25.762
• [14.902 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 12/16/23 17:01:25.78
  Dec 16 17:01:25.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename certificates @ 12/16/23 17:01:25.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:01:25.817
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:01:25.825
  E1216 17:01:26.533719      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:27.535427      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting /apis @ 12/16/23 17:01:27.918
  STEP: getting /apis/certificates.k8s.io @ 12/16/23 17:01:27.929
  STEP: getting /apis/certificates.k8s.io/v1 @ 12/16/23 17:01:27.931
  STEP: creating @ 12/16/23 17:01:27.933
  STEP: getting @ 12/16/23 17:01:27.977
  STEP: listing @ 12/16/23 17:01:27.983
  STEP: watching @ 12/16/23 17:01:27.99
  Dec 16 17:01:27.990: INFO: starting watch
  STEP: patching @ 12/16/23 17:01:27.992
  STEP: updating @ 12/16/23 17:01:28.005
  Dec 16 17:01:28.016: INFO: waiting for watch events with expected annotations
  Dec 16 17:01:28.016: INFO: saw patched and updated annotations
  STEP: getting /approval @ 12/16/23 17:01:28.017
  STEP: patching /approval @ 12/16/23 17:01:28.025
  STEP: updating /approval @ 12/16/23 17:01:28.037
  STEP: getting /status @ 12/16/23 17:01:28.051
  STEP: patching /status @ 12/16/23 17:01:28.059
  STEP: updating /status @ 12/16/23 17:01:28.074
  STEP: deleting @ 12/16/23 17:01:28.099
  STEP: deleting a collection @ 12/16/23 17:01:28.135
  Dec 16 17:01:28.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-3544" for this suite. @ 12/16/23 17:01:28.189
• [2.430 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 12/16/23 17:01:28.217
  Dec 16 17:01:28.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename secrets @ 12/16/23 17:01:28.221
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:01:28.25
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:01:28.259
  STEP: Creating secret with name secret-test-map-c2852efe-65b5-464b-9bd8-aadc76037120 @ 12/16/23 17:01:28.266
  STEP: Creating a pod to test consume secrets @ 12/16/23 17:01:28.279
  E1216 17:01:28.535546      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:29.536619      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:30.536681      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:31.537261      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:01:32.341
  Dec 16 17:01:32.347: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-secrets-126e93b3-e7c3-4678-9021-d1d4a0a15315 container secret-volume-test: <nil>
  STEP: delete the pod @ 12/16/23 17:01:32.382
  Dec 16 17:01:32.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2614" for this suite. @ 12/16/23 17:01:32.428
• [4.231 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 12/16/23 17:01:32.453
  Dec 16 17:01:32.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename resourcequota @ 12/16/23 17:01:32.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:01:32.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:01:32.506
  STEP: Creating a ResourceQuota @ 12/16/23 17:01:32.511
  STEP: Getting a ResourceQuota @ 12/16/23 17:01:32.521
  STEP: Updating a ResourceQuota @ 12/16/23 17:01:32.533
  E1216 17:01:32.538129      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying a ResourceQuota was modified @ 12/16/23 17:01:32.546
  STEP: Deleting a ResourceQuota @ 12/16/23 17:01:32.558
  STEP: Verifying the deleted ResourceQuota @ 12/16/23 17:01:32.568
  Dec 16 17:01:32.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-517" for this suite. @ 12/16/23 17:01:32.593
• [0.176 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 12/16/23 17:01:32.631
  Dec 16 17:01:32.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-probe @ 12/16/23 17:01:32.634
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:01:32.717
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:01:32.721
  STEP: Creating pod liveness-356e0deb-80ae-488b-9d43-ffa4db2cf8e8 in namespace container-probe-809 @ 12/16/23 17:01:32.726
  E1216 17:01:33.538632      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:34.538900      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:01:34.800: INFO: Started pod liveness-356e0deb-80ae-488b-9d43-ffa4db2cf8e8 in namespace container-probe-809
  STEP: checking the pod's current state and verifying that restartCount is present @ 12/16/23 17:01:34.8
  Dec 16 17:01:34.807: INFO: Initial restart count of pod liveness-356e0deb-80ae-488b-9d43-ffa4db2cf8e8 is 0
  E1216 17:01:35.539771      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:36.540357      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:37.540409      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:38.540898      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:39.541248      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:40.542172      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:41.542470      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:42.542842      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:43.543026      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:44.543159      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:45.544087      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:46.545011      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:47.545069      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:48.546174      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:49.546492      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:50.546686      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:51.547709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:52.547876      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:53.549002      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:54.549664      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:01:54.935: INFO: Restart count of pod container-probe-809/liveness-356e0deb-80ae-488b-9d43-ffa4db2cf8e8 is now 1 (20.128164791s elapsed)
  E1216 17:01:55.550337      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:56.550753      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:57.551073      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:58.552449      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:01:59.552248      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:00.552546      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:01.553163      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:02.554473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:03.553880      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:04.554023      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:05.554957      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:06.555045      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:07.556006      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:08.556778      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:09.557066      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:10.557773      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:11.557904      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:12.558089      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:13.558306      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:14.558870      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:02:15.028: INFO: Restart count of pod container-probe-809/liveness-356e0deb-80ae-488b-9d43-ffa4db2cf8e8 is now 2 (40.221247013s elapsed)
  E1216 17:02:15.559081      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:16.559650      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:17.559743      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:18.560958      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:19.561723      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:20.562566      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:21.562718      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:22.562856      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:23.563122      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:24.563263      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:25.563452      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:26.563717      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:27.564345      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:28.564547      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:29.564683      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:30.564871      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:31.564988      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:32.565139      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:33.565380      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:34.566043      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:02:35.134: INFO: Restart count of pod container-probe-809/liveness-356e0deb-80ae-488b-9d43-ffa4db2cf8e8 is now 3 (1m0.327046277s elapsed)
  E1216 17:02:35.566971      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:36.567903      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:37.569028      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:38.569077      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:39.569995      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:40.570773      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:41.571914      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:42.572610      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:43.573066      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:44.573715      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:45.575770      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:46.574337      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:47.574766      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:48.576028      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:49.575655      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:50.576097      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:51.576890      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:52.577635      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:53.578162      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:54.578424      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:02:55.229: INFO: Restart count of pod container-probe-809/liveness-356e0deb-80ae-488b-9d43-ffa4db2cf8e8 is now 4 (1m20.422378643s elapsed)
  E1216 17:02:55.578587      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:56.578861      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:57.579682      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:58.579865      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:02:59.580935      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:00.581821      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:01.582131      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:02.582220      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:03.582519      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:04.583328      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:05.583338      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:06.584459      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:07.585656      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:08.585817      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:09.585936      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:10.586122      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:11.586420      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:12.586733      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:13.587337      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:14.587758      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:15.588547      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:16.588866      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:17.589688      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:18.589832      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:19.591025      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:20.591063      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:21.591811      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:22.592785      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:23.593641      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:24.594048      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:25.594562      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:26.595064      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:27.595646      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:28.596822      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:29.597173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:30.597625      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:31.598078      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:32.598774      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:33.598683      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:34.602009      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:35.601973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:36.602071      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:37.602729      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:38.603824      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:39.604794      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:40.605030      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:41.606139      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:42.606764      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:43.606968      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:44.607148      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:45.607989      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:46.608268      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:47.608518      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:48.609585      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:49.610287      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:50.610473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:51.610687      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:52.610842      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:53.611817      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:54.612457      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:03:55.486: INFO: Restart count of pod container-probe-809/liveness-356e0deb-80ae-488b-9d43-ffa4db2cf8e8 is now 5 (2m20.67914205s elapsed)
  Dec 16 17:03:55.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/16/23 17:03:55.499
  STEP: Destroying namespace "container-probe-809" for this suite. @ 12/16/23 17:03:55.529
• [142.913 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 12/16/23 17:03:55.55
  Dec 16 17:03:55.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename resourcequota @ 12/16/23 17:03:55.555
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:03:55.588
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:03:55.593
  STEP: Counting existing ResourceQuota @ 12/16/23 17:03:55.598
  E1216 17:03:55.614949      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:56.613755      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:57.614756      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:58.615276      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:03:59.615997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:00.616784      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 12/16/23 17:04:00.629
  STEP: Ensuring resource quota status is calculated @ 12/16/23 17:04:00.679
  E1216 17:04:01.617203      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:02.618066      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 12/16/23 17:04:02.69
  STEP: Ensuring resource quota status captures replication controller creation @ 12/16/23 17:04:02.715
  E1216 17:04:03.618384      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:04.619278      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 12/16/23 17:04:04.723
  STEP: Ensuring resource quota status released usage @ 12/16/23 17:04:04.739
  E1216 17:04:05.619513      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:06.619782      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:04:06.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2505" for this suite. @ 12/16/23 17:04:06.757
• [11.220 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 12/16/23 17:04:06.775
  Dec 16 17:04:06.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename replication-controller @ 12/16/23 17:04:06.78
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:04:06.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:04:06.819
  STEP: Creating ReplicationController "e2e-rc-qj6rj" @ 12/16/23 17:04:06.824
  Dec 16 17:04:06.833: INFO: Get Replication Controller "e2e-rc-qj6rj" to confirm replicas
  E1216 17:04:07.619996      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:04:07.840: INFO: Get Replication Controller "e2e-rc-qj6rj" to confirm replicas
  Dec 16 17:04:07.848: INFO: Found 1 replicas for "e2e-rc-qj6rj" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-qj6rj" @ 12/16/23 17:04:07.848
  STEP: Updating a scale subresource @ 12/16/23 17:04:07.854
  STEP: Verifying replicas where modified for replication controller "e2e-rc-qj6rj" @ 12/16/23 17:04:07.863
  Dec 16 17:04:07.863: INFO: Get Replication Controller "e2e-rc-qj6rj" to confirm replicas
  E1216 17:04:08.620006      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:04:08.870: INFO: Get Replication Controller "e2e-rc-qj6rj" to confirm replicas
  Dec 16 17:04:08.877: INFO: Found 2 replicas for "e2e-rc-qj6rj" replication controller
  Dec 16 17:04:08.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7478" for this suite. @ 12/16/23 17:04:08.886
• [2.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 12/16/23 17:04:08.906
  Dec 16 17:04:08.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename var-expansion @ 12/16/23 17:04:08.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:04:08.941
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:04:08.947
  STEP: creating the pod @ 12/16/23 17:04:08.955
  STEP: waiting for pod running @ 12/16/23 17:04:08.975
  E1216 17:04:09.622799      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:10.621517      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 12/16/23 17:04:10.995
  Dec 16 17:04:11.001: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4427 PodName:var-expansion-cbe0d04e-c856-46b4-95f9-ae4ddde894a9 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:04:11.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:04:11.004: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:04:11.005: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-4427/pods/var-expansion-cbe0d04e-c856-46b4-95f9-ae4ddde894a9/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 12/16/23 17:04:11.125
  Dec 16 17:04:11.132: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4427 PodName:var-expansion-cbe0d04e-c856-46b4-95f9-ae4ddde894a9 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:04:11.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:04:11.136: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:04:11.136: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-4427/pods/var-expansion-cbe0d04e-c856-46b4-95f9-ae4ddde894a9/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 12/16/23 17:04:11.242
  E1216 17:04:11.622322      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:04:11.770: INFO: Successfully updated pod "var-expansion-cbe0d04e-c856-46b4-95f9-ae4ddde894a9"
  STEP: waiting for annotated pod running @ 12/16/23 17:04:11.77
  STEP: deleting the pod gracefully @ 12/16/23 17:04:11.797
  Dec 16 17:04:11.798: INFO: Deleting pod "var-expansion-cbe0d04e-c856-46b4-95f9-ae4ddde894a9" in namespace "var-expansion-4427"
  Dec 16 17:04:11.816: INFO: Wait up to 5m0s for pod "var-expansion-cbe0d04e-c856-46b4-95f9-ae4ddde894a9" to be fully deleted
  E1216 17:04:12.624021      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:13.623645      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:14.624550      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:15.625053      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:16.625747      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:17.626475      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:18.626589      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:19.627063      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:20.627422      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:21.628010      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:22.628514      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:23.628651      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:24.628823      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:25.629199      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:26.630332      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:27.630671      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:28.630926      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:29.631032      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:30.631198      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:31.631351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:32.632815      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:33.633299      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:34.633504      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:35.633710      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:36.633870      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:37.634057      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:38.634140      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:39.634425      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:40.645227      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:41.646187      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:42.648146      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:43.647280      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:04:43.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4427" for this suite. @ 12/16/23 17:04:43.977
• [35.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 12/16/23 17:04:44.005
  Dec 16 17:04:44.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename crd-publish-openapi @ 12/16/23 17:04:44.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:04:44.044
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:04:44.051
  Dec 16 17:04:44.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:04:44.648152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:45.662955      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 12/16/23 17:04:46.054
  Dec 16 17:04:46.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2798 --namespace=crd-publish-openapi-2798 create -f -'
  E1216 17:04:46.663691      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:04:47.564: INFO: stderr: ""
  Dec 16 17:04:47.564: INFO: stdout: "e2e-test-crd-publish-openapi-3736-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Dec 16 17:04:47.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2798 --namespace=crd-publish-openapi-2798 delete e2e-test-crd-publish-openapi-3736-crds test-cr'
  E1216 17:04:47.663853      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:04:47.770: INFO: stderr: ""
  Dec 16 17:04:47.770: INFO: stdout: "e2e-test-crd-publish-openapi-3736-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Dec 16 17:04:47.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2798 --namespace=crd-publish-openapi-2798 apply -f -'
  E1216 17:04:48.664856      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:04:49.290: INFO: stderr: ""
  Dec 16 17:04:49.290: INFO: stdout: "e2e-test-crd-publish-openapi-3736-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Dec 16 17:04:49.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2798 --namespace=crd-publish-openapi-2798 delete e2e-test-crd-publish-openapi-3736-crds test-cr'
  Dec 16 17:04:49.462: INFO: stderr: ""
  Dec 16 17:04:49.462: INFO: stdout: "e2e-test-crd-publish-openapi-3736-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 12/16/23 17:04:49.462
  Dec 16 17:04:49.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2798 explain e2e-test-crd-publish-openapi-3736-crds'
  E1216 17:04:49.669763      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:04:49.914: INFO: stderr: ""
  Dec 16 17:04:49.914: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-3736-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E1216 17:04:50.673186      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:51.674000      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:04:52.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2798" for this suite. @ 12/16/23 17:04:52.311
• [8.318 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:875
  STEP: Creating a kubernetes client @ 12/16/23 17:04:52.324
  Dec 16 17:04:52.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename daemonsets @ 12/16/23 17:04:52.327
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:04:52.354
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:04:52.361
  STEP: Creating simple DaemonSet "daemon-set" @ 12/16/23 17:04:52.42
  STEP: Check that daemon pods launch on every node of the cluster. @ 12/16/23 17:04:52.436
  Dec 16 17:04:52.455: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 17:04:52.455: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  E1216 17:04:52.674803      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:04:53.475: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 17:04:53.475: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  E1216 17:04:53.675880      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:04:54.474: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Dec 16 17:04:54.474: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 12/16/23 17:04:54.48
  Dec 16 17:04:54.490: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 12/16/23 17:04:54.491
  Dec 16 17:04:54.525: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 12/16/23 17:04:54.525
  Dec 16 17:04:54.531: INFO: Observed &DaemonSet event: ADDED
  Dec 16 17:04:54.531: INFO: Observed &DaemonSet event: MODIFIED
  Dec 16 17:04:54.531: INFO: Observed &DaemonSet event: MODIFIED
  Dec 16 17:04:54.531: INFO: Observed &DaemonSet event: MODIFIED
  Dec 16 17:04:54.532: INFO: Observed &DaemonSet event: MODIFIED
  Dec 16 17:04:54.532: INFO: Found daemon set daemon-set in namespace daemonsets-3264 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Dec 16 17:04:54.532: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 12/16/23 17:04:54.532
  STEP: watching for the daemon set status to be patched @ 12/16/23 17:04:54.549
  Dec 16 17:04:54.561: INFO: Observed &DaemonSet event: ADDED
  Dec 16 17:04:54.561: INFO: Observed &DaemonSet event: MODIFIED
  Dec 16 17:04:54.562: INFO: Observed &DaemonSet event: MODIFIED
  Dec 16 17:04:54.563: INFO: Observed &DaemonSet event: MODIFIED
  Dec 16 17:04:54.563: INFO: Observed &DaemonSet event: MODIFIED
  Dec 16 17:04:54.563: INFO: Observed daemon set daemon-set in namespace daemonsets-3264 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Dec 16 17:04:54.564: INFO: Observed &DaemonSet event: MODIFIED
  Dec 16 17:04:54.564: INFO: Found daemon set daemon-set in namespace daemonsets-3264 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Dec 16 17:04:54.564: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 12/16/23 17:04:54.574
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3264, will wait for the garbage collector to delete the pods @ 12/16/23 17:04:54.574
  Dec 16 17:04:54.651: INFO: Deleting DaemonSet.extensions daemon-set took: 15.256812ms
  E1216 17:04:54.676842      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:04:54.752: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.637786ms
  E1216 17:04:55.677650      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:04:56.059: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 17:04:56.060: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Dec 16 17:04:56.067: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14137"},"items":null}

  Dec 16 17:04:56.074: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14137"},"items":null}

  Dec 16 17:04:56.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3264" for this suite. @ 12/16/23 17:04:56.119
• [3.816 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 12/16/23 17:04:56.142
  Dec 16 17:04:56.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename emptydir @ 12/16/23 17:04:56.146
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:04:56.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:04:56.185
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 12/16/23 17:04:56.201
  E1216 17:04:56.678165      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:57.679791      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:58.679881      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:04:59.680897      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:05:00.244
  Dec 16 17:05:00.252: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-0f61a6fb-4da1-44a9-8a3a-75022dc19eae container test-container: <nil>
  STEP: delete the pod @ 12/16/23 17:05:00.284
  Dec 16 17:05:00.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9664" for this suite. @ 12/16/23 17:05:00.343
• [4.220 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 12/16/23 17:05:00.364
  Dec 16 17:05:00.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename webhook @ 12/16/23 17:05:00.368
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:05:00.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:05:00.418
  STEP: Setting up server cert @ 12/16/23 17:05:00.48
  E1216 17:05:00.681289      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/16/23 17:05:01.641
  STEP: Deploying the webhook pod @ 12/16/23 17:05:01.664
  E1216 17:05:01.681929      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Wait for the deployment to be ready @ 12/16/23 17:05:01.697
  Dec 16 17:05:01.721: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E1216 17:05:02.682377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:03.682607      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/16/23 17:05:03.746
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 17:05:03.767
  E1216 17:05:04.683206      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:05:04.767: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 12/16/23 17:05:04.775
  STEP: create a namespace for the webhook @ 12/16/23 17:05:04.811
  STEP: create a configmap should be unconditionally rejected by the webhook @ 12/16/23 17:05:04.841
  Dec 16 17:05:04.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-805" for this suite. @ 12/16/23 17:05:05.003
  STEP: Destroying namespace "webhook-markers-8760" for this suite. @ 12/16/23 17:05:05.02
  STEP: Destroying namespace "fail-closed-namespace-7856" for this suite. @ 12/16/23 17:05:05.031
• [4.682 seconds]
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 12/16/23 17:05:05.047
  Dec 16 17:05:05.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename init-container @ 12/16/23 17:05:05.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:05:05.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:05:05.089
  STEP: creating the pod @ 12/16/23 17:05:05.096
  Dec 16 17:05:05.096: INFO: PodSpec: initContainers in spec.initContainers
  E1216 17:05:05.684230      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:06.684496      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:07.684831      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:08.684943      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:09.685404      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:10.685675      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:11.686618      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:12.686794      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:13.687056      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:14.687217      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:15.687533      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:16.687972      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:17.688232      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:18.689031      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:19.689243      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:20.690283      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:21.691121      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:22.691794      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:23.692011      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:24.692316      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:25.693017      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:26.692807      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:27.692972      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:28.694000      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:29.694113      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:30.695450      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:31.695227      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:32.695759      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:33.695693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:34.695911      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:35.696127      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:36.697004      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:37.696933      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:38.696975      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:39.698094      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:40.698660      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:41.699283      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:42.700190      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:43.701137      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:44.702167      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:45.702663      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:46.703086      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:47.703434      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:48.704032      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:49.704571      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:50.705292      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:51.705327      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:05:52.138: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-652344fd-e3de-4f4f-86f1-0f9a8cb5ac91", GenerateName:"", Namespace:"init-container-2036", SelfLink:"", UID:"6d77a193-6ac7-4c1e-8cc3-ed2b19677359", ResourceVersion:"14382", Generation:0, CreationTimestamp:time.Date(2023, time.December, 16, 17, 5, 5, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"96557130"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.December, 16, 17, 5, 5, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000e3fa28), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.December, 16, 17, 5, 52, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000e3fa88), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-qkghb", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0051f06c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qkghb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qkghb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-qkghb", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0042e8148), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"phoh7xai9ouk-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000244930), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0042e81d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0042e81f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0042e81f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0042e81fc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000f1f680), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.December, 16, 17, 5, 5, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.December, 16, 17, 5, 5, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.December, 16, 17, 5, 5, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.December, 16, 17, 5, 5, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.112", PodIP:"10.233.66.145", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.66.145"}}, StartTime:time.Date(2023, time.December, 16, 17, 5, 5, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000244af0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000244bd0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://9813ab56b818b1ea9396e8d493ed4c82f825d81f8258b9b8edecbcc70d2bb873", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0051f0740), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0051f0720), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0042e8274), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Dec 16 17:05:52.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-2036" for this suite. @ 12/16/23 17:05:52.155
• [47.122 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 12/16/23 17:05:52.175
  Dec 16 17:05:52.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename deployment @ 12/16/23 17:05:52.18
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:05:52.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:05:52.222
  STEP: creating a Deployment @ 12/16/23 17:05:52.238
  STEP: waiting for Deployment to be created @ 12/16/23 17:05:52.249
  STEP: waiting for all Replicas to be Ready @ 12/16/23 17:05:52.252
  Dec 16 17:05:52.258: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Dec 16 17:05:52.258: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Dec 16 17:05:52.281: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Dec 16 17:05:52.281: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Dec 16 17:05:52.322: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Dec 16 17:05:52.322: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Dec 16 17:05:52.446: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Dec 16 17:05:52.446: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E1216 17:05:52.705494      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:05:53.169: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Dec 16 17:05:53.169: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  E1216 17:05:53.706482      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:05:53.810: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 12/16/23 17:05:53.81
  W1216 17:05:53.825197      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Dec 16 17:05:53.838: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 12/16/23 17:05:53.839
  Dec 16 17:05:53.845: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 0
  Dec 16 17:05:53.845: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 0
  Dec 16 17:05:53.845: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 0
  Dec 16 17:05:53.845: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 0
  Dec 16 17:05:53.845: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 0
  Dec 16 17:05:53.845: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 0
  Dec 16 17:05:53.846: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 0
  Dec 16 17:05:53.846: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 0
  Dec 16 17:05:53.846: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1
  Dec 16 17:05:53.846: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1
  Dec 16 17:05:53.846: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 2
  Dec 16 17:05:53.846: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 2
  Dec 16 17:05:53.847: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 2
  Dec 16 17:05:53.847: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 2
  Dec 16 17:05:53.884: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 2
  Dec 16 17:05:53.884: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 2
  Dec 16 17:05:53.935: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 2
  Dec 16 17:05:53.935: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 2
  Dec 16 17:05:54.001: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1
  Dec 16 17:05:54.001: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1
  Dec 16 17:05:54.034: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1
  Dec 16 17:05:54.034: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1
  E1216 17:05:54.707009      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:05:55.201: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 2
  Dec 16 17:05:55.201: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 2
  Dec 16 17:05:55.289: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1
  STEP: listing Deployments @ 12/16/23 17:05:55.289
  Dec 16 17:05:55.299: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 12/16/23 17:05:55.299
  Dec 16 17:05:55.324: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 12/16/23 17:05:55.324
  Dec 16 17:05:55.339: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Dec 16 17:05:55.382: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Dec 16 17:05:55.509: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Dec 16 17:05:55.563: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Dec 16 17:05:55.600: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E1216 17:05:55.708130      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:05:56.214: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Dec 16 17:05:56.257: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  Dec 16 17:05:56.316: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Dec 16 17:05:56.385: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E1216 17:05:56.708956      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:57.709421      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:05:57.865: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 12/16/23 17:05:57.922
  STEP: fetching the DeploymentStatus @ 12/16/23 17:05:57.94
  Dec 16 17:05:57.949: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1
  Dec 16 17:05:57.949: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1
  Dec 16 17:05:57.950: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1
  Dec 16 17:05:57.950: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1
  Dec 16 17:05:57.950: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 1
  Dec 16 17:05:57.950: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 2
  Dec 16 17:05:57.951: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 3
  Dec 16 17:05:57.951: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 2
  Dec 16 17:05:57.952: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 2
  Dec 16 17:05:57.953: INFO: observed Deployment test-deployment in namespace deployment-3823 with ReadyReplicas 3
  STEP: deleting the Deployment @ 12/16/23 17:05:57.956
  Dec 16 17:05:57.975: INFO: observed event type MODIFIED
  Dec 16 17:05:57.976: INFO: observed event type MODIFIED
  Dec 16 17:05:57.977: INFO: observed event type MODIFIED
  Dec 16 17:05:57.978: INFO: observed event type MODIFIED
  Dec 16 17:05:57.979: INFO: observed event type MODIFIED
  Dec 16 17:05:57.980: INFO: observed event type MODIFIED
  Dec 16 17:05:57.981: INFO: observed event type MODIFIED
  Dec 16 17:05:57.982: INFO: observed event type MODIFIED
  Dec 16 17:05:57.985: INFO: observed event type MODIFIED
  Dec 16 17:05:57.986: INFO: observed event type MODIFIED
  Dec 16 17:05:57.986: INFO: observed event type MODIFIED
  Dec 16 17:05:58.009: INFO: Log out all the ReplicaSets if there is no deployment created
  Dec 16 17:05:58.019: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-3823  e18c133e-605d-4ae9-a7b0-f1ddd8a4ef48 14461 3 2023-12-16 17:05:52 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment fed65112-7a2b-4f40-be48-a185eb2bfbf7 0xc003396597 0xc003396598}] [] [{kube-controller-manager Update apps/v1 2023-12-16 17:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fed65112-7a2b-4f40-be48-a185eb2bfbf7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-16 17:05:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003396620 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Dec 16 17:05:58.030: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-3823  be7295ce-5088-4d14-844e-80af1be32b8b 14551 4 2023-12-16 17:05:53 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment fed65112-7a2b-4f40-be48-a185eb2bfbf7 0xc003396687 0xc003396688}] [] [{kube-controller-manager Update apps/v1 2023-12-16 17:05:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fed65112-7a2b-4f40-be48-a185eb2bfbf7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-16 17:05:57 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003396710 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Dec 16 17:05:58.040: INFO: pod: "test-deployment-5b5dcbcd95-h8l24":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-h8l24 test-deployment-5b5dcbcd95- deployment-3823  f3df238c-f0d4-4d16-a580-2f1ac7a0cb4d 14546 0 2023-12-16 17:05:55 +0000 UTC 2023-12-16 17:05:58 +0000 UTC 0xc003396aa8 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 be7295ce-5088-4d14-844e-80af1be32b8b 0xc003396ad7 0xc003396ad8}] [] [{kube-controller-manager Update v1 2023-12-16 17:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"be7295ce-5088-4d14-844e-80af1be32b8b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:05:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d654w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d654w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:05:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:05:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:05:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:05:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.49,PodIP:10.233.65.32,StartTime:2023-12-16 17:05:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-16 17:05:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://b03af07bebfaa1c93b5c027b163784c6a0d245379713dfd98dba130081c6750d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.32,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Dec 16 17:05:58.041: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-3823  4db8d85e-c888-4b19-a94f-cfd90e5df3a6 14543 2 2023-12-16 17:05:55 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment fed65112-7a2b-4f40-be48-a185eb2bfbf7 0xc003396777 0xc003396778}] [] [{kube-controller-manager Update apps/v1 2023-12-16 17:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fed65112-7a2b-4f40-be48-a185eb2bfbf7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-16 17:05:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003396800 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Dec 16 17:05:58.060: INFO: pod: "test-deployment-6fc78d85c6-4rlls":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-4rlls test-deployment-6fc78d85c6- deployment-3823  6ed21215-4825-44fa-a782-c99c53ffc385 14557 0 2023-12-16 17:05:55 +0000 UTC 2023-12-16 17:05:59 +0000 UTC 0xc0033976c8 map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 4db8d85e-c888-4b19-a94f-cfd90e5df3a6 0xc0033976f7 0xc0033976f8}] [] [{kube-controller-manager Update v1 2023-12-16 17:05:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4db8d85e-c888-4b19-a94f-cfd90e5df3a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:05:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kb2lb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kb2lb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:05:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:05:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:05:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:05:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.112,PodIP:10.233.66.148,StartTime:2023-12-16 17:05:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-16 17:05:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://e272a7becf32d47915ea85bf8c613d3a9e8d4e7868cdf94a672967f745575c6a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.148,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Dec 16 17:05:58.061: INFO: pod: "test-deployment-6fc78d85c6-qqbqm":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-qqbqm test-deployment-6fc78d85c6- deployment-3823  ff7c5ebc-7df7-468b-9d72-81147e14fed2 14558 0 2023-12-16 17:05:56 +0000 UTC 2023-12-16 17:05:59 +0000 UTC 0xc0033978c0 map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 4db8d85e-c888-4b19-a94f-cfd90e5df3a6 0xc0033978f7 0xc0033978f8}] [] [{kube-controller-manager Update v1 2023-12-16 17:05:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4db8d85e-c888-4b19-a94f-cfd90e5df3a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:05:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tv8zr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tv8zr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:05:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:05:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:05:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:05:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.172,PodIP:10.233.64.35,StartTime:2023-12-16 17:05:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-16 17:05:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://2ddfdc874bf117818df477f082e6eabcc47980265e98ffb3bb2a5cd836797715,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.35,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Dec 16 17:05:58.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3823" for this suite. @ 12/16/23 17:05:58.076
• [5.917 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 12/16/23 17:05:58.093
  Dec 16 17:05:58.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename sched-pred @ 12/16/23 17:05:58.095
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:05:58.127
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:05:58.133
  Dec 16 17:05:58.137: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Dec 16 17:05:58.158: INFO: Waiting for terminating namespaces to be deleted...
  Dec 16 17:05:58.184: INFO: 
  Logging pods the apiserver thinks is on node phoh7xai9ouk-1 before test
  Dec 16 17:05:58.232: INFO: test-deployment-6fc78d85c6-qqbqm from deployment-3823 started at 2023-12-16 17:05:56 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.232: INFO: 	Container test-deployment ready: true, restart count 0
  Dec 16 17:05:58.233: INFO: kube-flannel-ds-5sbtb from kube-flannel started at 2023-12-16 16:10:58 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.233: INFO: 	Container kube-flannel ready: true, restart count 1
  Dec 16 17:05:58.233: INFO: coredns-5d78c9869d-scnc5 from kube-system started at 2023-12-16 16:15:59 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.233: INFO: 	Container coredns ready: true, restart count 0
  Dec 16 17:05:58.234: INFO: kube-addon-manager-phoh7xai9ouk-1 from kube-system started at 2023-12-16 16:09:46 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.234: INFO: 	Container kube-addon-manager ready: true, restart count 1
  Dec 16 17:05:58.234: INFO: kube-apiserver-phoh7xai9ouk-1 from kube-system started at 2023-12-16 16:09:46 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.234: INFO: 	Container kube-apiserver ready: true, restart count 1
  Dec 16 17:05:58.234: INFO: kube-controller-manager-phoh7xai9ouk-1 from kube-system started at 2023-12-16 16:09:46 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.235: INFO: 	Container kube-controller-manager ready: true, restart count 1
  Dec 16 17:05:58.235: INFO: kube-proxy-7c5h8 from kube-system started at 2023-12-16 16:10:59 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.235: INFO: 	Container kube-proxy ready: true, restart count 0
  Dec 16 17:05:58.235: INFO: kube-scheduler-phoh7xai9ouk-1 from kube-system started at 2023-12-16 16:09:46 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.236: INFO: 	Container kube-scheduler ready: true, restart count 1
  Dec 16 17:05:58.236: INFO: sonobuoy-systemd-logs-daemon-set-0d7ed218a9fd4126-l6bt5 from sonobuoy started at 2023-12-16 16:12:30 +0000 UTC (2 container statuses recorded)
  Dec 16 17:05:58.236: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 16 17:05:58.236: INFO: 	Container systemd-logs ready: true, restart count 0
  Dec 16 17:05:58.237: INFO: 
  Logging pods the apiserver thinks is on node phoh7xai9ouk-2 before test
  Dec 16 17:05:58.268: INFO: test-deployment-5b5dcbcd95-h8l24 from deployment-3823 started at 2023-12-16 17:05:55 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.269: INFO: 	Container test-deployment ready: false, restart count 0
  Dec 16 17:05:58.269: INFO: kube-flannel-ds-p5tmh from kube-flannel started at 2023-12-16 16:11:26 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.271: INFO: 	Container kube-flannel ready: true, restart count 0
  Dec 16 17:05:58.271: INFO: coredns-5d78c9869d-848fd from kube-system started at 2023-12-16 16:11:25 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.272: INFO: 	Container coredns ready: true, restart count 0
  Dec 16 17:05:58.272: INFO: kube-addon-manager-phoh7xai9ouk-2 from kube-system started at 2023-12-16 16:09:29 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.273: INFO: 	Container kube-addon-manager ready: true, restart count 1
  Dec 16 17:05:58.273: INFO: kube-apiserver-phoh7xai9ouk-2 from kube-system started at 2023-12-16 16:09:29 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.273: INFO: 	Container kube-apiserver ready: true, restart count 1
  Dec 16 17:05:58.273: INFO: kube-controller-manager-phoh7xai9ouk-2 from kube-system started at 2023-12-16 16:09:29 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.274: INFO: 	Container kube-controller-manager ready: true, restart count 1
  Dec 16 17:05:58.278: INFO: kube-proxy-dzncl from kube-system started at 2023-12-16 16:11:26 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.278: INFO: 	Container kube-proxy ready: true, restart count 0
  Dec 16 17:05:58.279: INFO: kube-scheduler-phoh7xai9ouk-2 from kube-system started at 2023-12-16 16:09:29 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.279: INFO: 	Container kube-scheduler ready: true, restart count 1
  Dec 16 17:05:58.280: INFO: sonobuoy-systemd-logs-daemon-set-0d7ed218a9fd4126-2zwqp from sonobuoy started at 2023-12-16 16:12:30 +0000 UTC (2 container statuses recorded)
  Dec 16 17:05:58.280: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 16 17:05:58.280: INFO: 	Container systemd-logs ready: true, restart count 0
  Dec 16 17:05:58.281: INFO: 
  Logging pods the apiserver thinks is on node phoh7xai9ouk-3 before test
  Dec 16 17:05:58.299: INFO: test-deployment-6fc78d85c6-4rlls from deployment-3823 started at 2023-12-16 17:05:55 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.301: INFO: 	Container test-deployment ready: true, restart count 0
  Dec 16 17:05:58.302: INFO: pod-init-652344fd-e3de-4f4f-86f1-0f9a8cb5ac91 from init-container-2036 started at 2023-12-16 17:05:05 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.305: INFO: 	Container run1 ready: false, restart count 0
  Dec 16 17:05:58.305: INFO: kube-flannel-ds-dk7xw from kube-flannel started at 2023-12-16 16:39:50 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.305: INFO: 	Container kube-flannel ready: true, restart count 0
  Dec 16 17:05:58.306: INFO: kube-proxy-w4mds from kube-system started at 2023-12-16 16:11:29 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.306: INFO: 	Container kube-proxy ready: true, restart count 0
  Dec 16 17:05:58.306: INFO: sonobuoy from sonobuoy started at 2023-12-16 16:12:19 +0000 UTC (1 container statuses recorded)
  Dec 16 17:05:58.307: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Dec 16 17:05:58.307: INFO: sonobuoy-e2e-job-8b3ba51fd9314720 from sonobuoy started at 2023-12-16 16:12:30 +0000 UTC (2 container statuses recorded)
  Dec 16 17:05:58.307: INFO: 	Container e2e ready: true, restart count 0
  Dec 16 17:05:58.307: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 16 17:05:58.307: INFO: sonobuoy-systemd-logs-daemon-set-0d7ed218a9fd4126-zq8f6 from sonobuoy started at 2023-12-16 16:12:30 +0000 UTC (2 container statuses recorded)
  Dec 16 17:05:58.307: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 16 17:05:58.307: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 12/16/23 17:05:58.308
  E1216 17:05:58.709554      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:05:59.710093      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 12/16/23 17:06:00.352
  STEP: Trying to apply a random label on the found node. @ 12/16/23 17:06:00.38
  STEP: verifying the node has the label kubernetes.io/e2e-0a41d686-8710-4c69-aeb2-9d6c42639fa3 42 @ 12/16/23 17:06:00.409
  STEP: Trying to relaunch the pod, now with labels. @ 12/16/23 17:06:00.415
  E1216 17:06:00.710536      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:01.710945      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-0a41d686-8710-4c69-aeb2-9d6c42639fa3 off the node phoh7xai9ouk-3 @ 12/16/23 17:06:02.469
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-0a41d686-8710-4c69-aeb2-9d6c42639fa3 @ 12/16/23 17:06:02.504
  Dec 16 17:06:02.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-3022" for this suite. @ 12/16/23 17:06:02.53
• [4.458 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 12/16/23 17:06:02.562
  Dec 16 17:06:02.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename configmap @ 12/16/23 17:06:02.57
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:06:02.611
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:06:02.617
  STEP: Creating configMap with name configmap-test-volume-5e16cf05-89f3-4cfe-985e-c7380c77d7ea @ 12/16/23 17:06:02.624
  STEP: Creating a pod to test consume configMaps @ 12/16/23 17:06:02.638
  E1216 17:06:02.712408      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:03.713480      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:04.713845      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:05.714249      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:06:06.689
  Dec 16 17:06:06.697: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-configmaps-2450ae41-7ff1-4c37-9b7b-3ed9ab65ad8f container agnhost-container: <nil>
  STEP: delete the pod @ 12/16/23 17:06:06.713
  E1216 17:06:06.714473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:06:06.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3583" for this suite. @ 12/16/23 17:06:06.756
• [4.209 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 12/16/23 17:06:06.773
  Dec 16 17:06:06.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename svcaccounts @ 12/16/23 17:06:06.775
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:06:06.81
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:06:06.816
  Dec 16 17:06:06.854: INFO: created pod
  E1216 17:06:07.714935      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:08.715204      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:09.715317      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:10.715685      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:06:10.89
  E1216 17:06:11.716218      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:12.716130      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:13.716345      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:14.716577      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:15.717188      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:16.717179      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:17.717335      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:18.717548      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:19.717765      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:20.717943      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:21.718212      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:22.718326      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:23.718508      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:24.718671      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:25.718914      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:26.719288      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:27.719669      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:28.721231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:29.721661      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:30.721996      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:31.722343      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:32.722675      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:33.722848      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:34.723227      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:35.723519      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:36.724203      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:37.724389      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:38.725358      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:39.725706      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:40.725934      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:06:40.890: INFO: polling logs
  Dec 16 17:06:40.914: INFO: Pod logs: 
  I1216 17:06:07.554710       1 log.go:198] OK: Got token
  I1216 17:06:07.555253       1 log.go:198] validating with in-cluster discovery
  I1216 17:06:07.557457       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I1216 17:06:07.557626       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1532:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1702746967, NotBefore:1702746367, IssuedAt:1702746367, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1532", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"474452ba-7c39-4b20-becb-425a8b3bf161"}}}
  I1216 17:06:07.587176       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I1216 17:06:07.599912       1 log.go:198] OK: Validated signature on JWT
  I1216 17:06:07.600274       1 log.go:198] OK: Got valid claims from token!
  I1216 17:06:07.600398       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1532:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1702746967, NotBefore:1702746367, IssuedAt:1702746367, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1532", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"474452ba-7c39-4b20-becb-425a8b3bf161"}}}

  Dec 16 17:06:40.916: INFO: completed pod
  Dec 16 17:06:40.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1532" for this suite. @ 12/16/23 17:06:40.942
• [34.188 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:289
  STEP: Creating a kubernetes client @ 12/16/23 17:06:40.971
  Dec 16 17:06:40.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename field-validation @ 12/16/23 17:06:40.974
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:06:41.016
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:06:41.024
  Dec 16 17:06:41.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:06:41.727010      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:42.726464      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:43.726644      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:06:44.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-661" for this suite. @ 12/16/23 17:06:44.385
• [3.427 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 12/16/23 17:06:44.4
  Dec 16 17:06:44.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename namespaces @ 12/16/23 17:06:44.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:06:44.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:06:44.438
  STEP: creating a Namespace @ 12/16/23 17:06:44.444
  STEP: patching the Namespace @ 12/16/23 17:06:44.472
  STEP: get the Namespace and ensuring it has the label @ 12/16/23 17:06:44.482
  Dec 16 17:06:44.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5321" for this suite. @ 12/16/23 17:06:44.499
  STEP: Destroying namespace "nspatchtest-6eedb073-56ac-4d77-b640-1bd1446ad8a4-400" for this suite. @ 12/16/23 17:06:44.508
• [0.124 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 12/16/23 17:06:44.527
  Dec 16 17:06:44.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename emptydir @ 12/16/23 17:06:44.529
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:06:44.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:06:44.573
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 12/16/23 17:06:44.58
  E1216 17:06:44.728198      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:45.729103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:46.729851      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:47.730353      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:06:48.693
  Dec 16 17:06:48.700: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-f74717e8-25c6-4f66-9480-11d89d78fb6f container test-container: <nil>
  STEP: delete the pod @ 12/16/23 17:06:48.713
  E1216 17:06:48.729990      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:06:48.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2783" for this suite. @ 12/16/23 17:06:48.761
• [4.251 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 12/16/23 17:06:48.781
  Dec 16 17:06:48.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-probe @ 12/16/23 17:06:48.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:06:48.814
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:06:48.82
  STEP: Creating pod liveness-07c76f5e-b0b2-479d-8200-3c4d06779413 in namespace container-probe-4117 @ 12/16/23 17:06:48.828
  E1216 17:06:49.730674      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:50.731114      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:06:50.863: INFO: Started pod liveness-07c76f5e-b0b2-479d-8200-3c4d06779413 in namespace container-probe-4117
  STEP: checking the pod's current state and verifying that restartCount is present @ 12/16/23 17:06:50.864
  Dec 16 17:06:50.875: INFO: Initial restart count of pod liveness-07c76f5e-b0b2-479d-8200-3c4d06779413 is 0
  E1216 17:06:51.731951      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:52.736335      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:53.736942      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:54.737169      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:55.738159      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:56.738330      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:57.738520      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:58.739336      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:06:59.739950      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:00.740148      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:01.741210      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:02.741358      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:03.741623      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:04.741746      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:05.741986      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:06.742153      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:07.742319      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:08.742445      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:09.742627      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:10.742930      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:07:10.964: INFO: Restart count of pod container-probe-4117/liveness-07c76f5e-b0b2-479d-8200-3c4d06779413 is now 1 (20.088727885s elapsed)
  Dec 16 17:07:10.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/16/23 17:07:10.978
  STEP: Destroying namespace "container-probe-4117" for this suite. @ 12/16/23 17:07:11.023
• [22.260 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 12/16/23 17:07:11.055
  Dec 16 17:07:11.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename services @ 12/16/23 17:07:11.058
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:07:11.095
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:07:11.1
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9520 @ 12/16/23 17:07:11.109
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 12/16/23 17:07:11.138
  STEP: creating service externalsvc in namespace services-9520 @ 12/16/23 17:07:11.138
  STEP: creating replication controller externalsvc in namespace services-9520 @ 12/16/23 17:07:11.18
  I1216 17:07:11.203442      13 runners.go:194] Created replication controller with name: externalsvc, namespace: services-9520, replica count: 2
  E1216 17:07:11.744018      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:12.744869      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:13.745040      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:07:14.255515      13 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 12/16/23 17:07:14.262
  Dec 16 17:07:14.289: INFO: Creating new exec pod
  E1216 17:07:14.745236      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:15.745303      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:07:16.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-9520 exec execpodsprkm -- /bin/sh -x -c nslookup clusterip-service.services-9520.svc.cluster.local'
  Dec 16 17:07:16.728: INFO: stderr: "+ nslookup clusterip-service.services-9520.svc.cluster.local\n"
  Dec 16 17:07:16.728: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-9520.svc.cluster.local\tcanonical name = externalsvc.services-9520.svc.cluster.local.\nName:\texternalsvc.services-9520.svc.cluster.local\nAddress: 10.233.53.97\n\n"
  Dec 16 17:07:16.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-9520, will wait for the garbage collector to delete the pods @ 12/16/23 17:07:16.739
  E1216 17:07:16.745263      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:07:16.820: INFO: Deleting ReplicationController externalsvc took: 13.665077ms
  Dec 16 17:07:16.922: INFO: Terminating ReplicationController externalsvc pods took: 101.627547ms
  E1216 17:07:17.746243      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:18.746881      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:07:18.768: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-9520" for this suite. @ 12/16/23 17:07:18.807
• [7.783 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 12/16/23 17:07:18.839
  Dec 16 17:07:18.839: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename dns @ 12/16/23 17:07:18.841
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:07:18.874
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:07:18.878
  STEP: Creating a test headless service @ 12/16/23 17:07:18.885
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3066 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3066;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3066 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3066;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3066.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3066.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3066.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3066.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3066.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3066.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3066.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3066.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3066.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3066.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3066.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3066.svc;check="$$(dig +notcp +noall +answer +search 177.3.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.3.177_udp@PTR;check="$$(dig +tcp +noall +answer +search 177.3.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.3.177_tcp@PTR;sleep 1; done
   @ 12/16/23 17:07:18.946
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3066 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3066;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3066 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3066;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3066.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3066.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3066.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3066.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3066.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3066.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3066.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3066.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3066.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3066.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3066.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3066.svc;check="$$(dig +notcp +noall +answer +search 177.3.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.3.177_udp@PTR;check="$$(dig +tcp +noall +answer +search 177.3.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.3.177_tcp@PTR;sleep 1; done
   @ 12/16/23 17:07:18.946
  STEP: creating a pod to probe DNS @ 12/16/23 17:07:18.946
  STEP: submitting the pod to kubernetes @ 12/16/23 17:07:18.946
  E1216 17:07:19.748340      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:20.748954      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 12/16/23 17:07:21.006
  STEP: looking for the results for each expected name from probers @ 12/16/23 17:07:21.013
  Dec 16 17:07:21.025: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:21.032: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:21.042: INFO: Unable to read wheezy_udp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:21.049: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:21.057: INFO: Unable to read wheezy_udp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:21.064: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:21.074: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:21.088: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:21.137: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:21.143: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:21.149: INFO: Unable to read jessie_udp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:21.157: INFO: Unable to read jessie_tcp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:21.165: INFO: Unable to read jessie_udp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:21.171: INFO: Unable to read jessie_tcp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:21.179: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:21.187: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:21.224: INFO: Lookups using dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3066 wheezy_tcp@dns-test-service.dns-3066 wheezy_udp@dns-test-service.dns-3066.svc wheezy_tcp@dns-test-service.dns-3066.svc wheezy_udp@_http._tcp.dns-test-service.dns-3066.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3066.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3066 jessie_tcp@dns-test-service.dns-3066 jessie_udp@dns-test-service.dns-3066.svc jessie_tcp@dns-test-service.dns-3066.svc jessie_udp@_http._tcp.dns-test-service.dns-3066.svc jessie_tcp@_http._tcp.dns-test-service.dns-3066.svc]

  E1216 17:07:21.749625      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:22.750179      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:23.750558      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:24.753555      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:25.753573      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:07:26.234: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:26.240: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:26.248: INFO: Unable to read wheezy_udp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:26.257: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:26.262: INFO: Unable to read wheezy_udp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:26.268: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:26.275: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:26.280: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:26.313: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:26.319: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:26.326: INFO: Unable to read jessie_udp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:26.332: INFO: Unable to read jessie_tcp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:26.338: INFO: Unable to read jessie_udp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:26.345: INFO: Unable to read jessie_tcp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:26.351: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:26.361: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:26.388: INFO: Lookups using dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3066 wheezy_tcp@dns-test-service.dns-3066 wheezy_udp@dns-test-service.dns-3066.svc wheezy_tcp@dns-test-service.dns-3066.svc wheezy_udp@_http._tcp.dns-test-service.dns-3066.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3066.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3066 jessie_tcp@dns-test-service.dns-3066 jessie_udp@dns-test-service.dns-3066.svc jessie_tcp@dns-test-service.dns-3066.svc jessie_udp@_http._tcp.dns-test-service.dns-3066.svc jessie_tcp@_http._tcp.dns-test-service.dns-3066.svc]

  E1216 17:07:26.754777      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:27.754750      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:28.755269      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:29.757081      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:30.756173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:07:31.237: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:31.245: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:31.252: INFO: Unable to read wheezy_udp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:31.261: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:31.267: INFO: Unable to read wheezy_udp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:31.278: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:31.287: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:31.294: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:31.336: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:31.342: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:31.352: INFO: Unable to read jessie_udp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:31.362: INFO: Unable to read jessie_tcp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:31.369: INFO: Unable to read jessie_udp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:31.377: INFO: Unable to read jessie_tcp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:31.384: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:31.389: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:31.419: INFO: Lookups using dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3066 wheezy_tcp@dns-test-service.dns-3066 wheezy_udp@dns-test-service.dns-3066.svc wheezy_tcp@dns-test-service.dns-3066.svc wheezy_udp@_http._tcp.dns-test-service.dns-3066.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3066.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3066 jessie_tcp@dns-test-service.dns-3066 jessie_udp@dns-test-service.dns-3066.svc jessie_tcp@dns-test-service.dns-3066.svc jessie_udp@_http._tcp.dns-test-service.dns-3066.svc jessie_tcp@_http._tcp.dns-test-service.dns-3066.svc]

  E1216 17:07:31.756541      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:32.757683      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:33.758348      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:34.758379      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:35.758486      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:07:36.241: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:36.250: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:36.261: INFO: Unable to read wheezy_udp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:36.271: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:36.290: INFO: Unable to read wheezy_udp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:36.300: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:36.308: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:36.317: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:36.363: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:36.371: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:36.381: INFO: Unable to read jessie_udp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:36.395: INFO: Unable to read jessie_tcp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:36.406: INFO: Unable to read jessie_udp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:36.418: INFO: Unable to read jessie_tcp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:36.427: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:36.438: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:36.481: INFO: Lookups using dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3066 wheezy_tcp@dns-test-service.dns-3066 wheezy_udp@dns-test-service.dns-3066.svc wheezy_tcp@dns-test-service.dns-3066.svc wheezy_udp@_http._tcp.dns-test-service.dns-3066.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3066.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3066 jessie_tcp@dns-test-service.dns-3066 jessie_udp@dns-test-service.dns-3066.svc jessie_tcp@dns-test-service.dns-3066.svc jessie_udp@_http._tcp.dns-test-service.dns-3066.svc jessie_tcp@_http._tcp.dns-test-service.dns-3066.svc]

  E1216 17:07:36.759751      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:37.760228      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:38.760223      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:39.760334      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:40.761038      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:07:41.234: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:41.242: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:41.249: INFO: Unable to read wheezy_udp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:41.255: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:41.262: INFO: Unable to read wheezy_udp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:41.268: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:41.276: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:41.282: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:41.320: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:41.328: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:41.337: INFO: Unable to read jessie_udp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:41.345: INFO: Unable to read jessie_tcp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:41.354: INFO: Unable to read jessie_udp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:41.366: INFO: Unable to read jessie_tcp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:41.377: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:41.384: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:41.420: INFO: Lookups using dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3066 wheezy_tcp@dns-test-service.dns-3066 wheezy_udp@dns-test-service.dns-3066.svc wheezy_tcp@dns-test-service.dns-3066.svc wheezy_udp@_http._tcp.dns-test-service.dns-3066.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3066.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3066 jessie_tcp@dns-test-service.dns-3066 jessie_udp@dns-test-service.dns-3066.svc jessie_tcp@dns-test-service.dns-3066.svc jessie_udp@_http._tcp.dns-test-service.dns-3066.svc jessie_tcp@_http._tcp.dns-test-service.dns-3066.svc]

  E1216 17:07:41.761420      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:42.762523      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:43.763022      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:44.763006      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:45.764683      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:07:46.233: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:46.241: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:46.256: INFO: Unable to read wheezy_udp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:46.263: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:46.270: INFO: Unable to read wheezy_udp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:46.277: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:46.283: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:46.295: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:46.330: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:46.338: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:46.345: INFO: Unable to read jessie_udp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:46.352: INFO: Unable to read jessie_tcp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:46.359: INFO: Unable to read jessie_udp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:46.367: INFO: Unable to read jessie_tcp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:46.372: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:46.379: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:46.407: INFO: Lookups using dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3066 wheezy_tcp@dns-test-service.dns-3066 wheezy_udp@dns-test-service.dns-3066.svc wheezy_tcp@dns-test-service.dns-3066.svc wheezy_udp@_http._tcp.dns-test-service.dns-3066.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3066.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3066 jessie_tcp@dns-test-service.dns-3066 jessie_udp@dns-test-service.dns-3066.svc jessie_tcp@dns-test-service.dns-3066.svc jessie_udp@_http._tcp.dns-test-service.dns-3066.svc jessie_tcp@_http._tcp.dns-test-service.dns-3066.svc]

  E1216 17:07:46.764959      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:47.764979      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:48.765173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:49.765373      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:50.765801      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:07:51.233: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:51.240: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:51.248: INFO: Unable to read wheezy_udp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:51.254: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:51.261: INFO: Unable to read wheezy_udp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:51.268: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:51.275: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:51.281: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:51.311: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:51.319: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:51.325: INFO: Unable to read jessie_udp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:51.332: INFO: Unable to read jessie_tcp@dns-test-service.dns-3066 from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:51.338: INFO: Unable to read jessie_udp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:51.344: INFO: Unable to read jessie_tcp@dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:51.351: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:51.357: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3066.svc from pod dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86: the server could not find the requested resource (get pods dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86)
  Dec 16 17:07:51.386: INFO: Lookups using dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3066 wheezy_tcp@dns-test-service.dns-3066 wheezy_udp@dns-test-service.dns-3066.svc wheezy_tcp@dns-test-service.dns-3066.svc wheezy_udp@_http._tcp.dns-test-service.dns-3066.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3066.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3066 jessie_tcp@dns-test-service.dns-3066 jessie_udp@dns-test-service.dns-3066.svc jessie_tcp@dns-test-service.dns-3066.svc jessie_udp@_http._tcp.dns-test-service.dns-3066.svc jessie_tcp@_http._tcp.dns-test-service.dns-3066.svc]

  E1216 17:07:51.766201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:52.767039      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:53.767981      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:54.768400      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:55.769113      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:07:56.431: INFO: DNS probes using dns-3066/dns-test-9f3f2e86-4079-4359-a6d4-cf583a681c86 succeeded

  Dec 16 17:07:56.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/16/23 17:07:56.447
  STEP: deleting the test service @ 12/16/23 17:07:56.485
  STEP: deleting the test headless service @ 12/16/23 17:07:56.66
  E1216 17:07:56.769930      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "dns-3066" for this suite. @ 12/16/23 17:07:56.833
• [38.015 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 12/16/23 17:07:56.863
  Dec 16 17:07:56.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename secrets @ 12/16/23 17:07:56.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:07:56.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:07:56.915
  Dec 16 17:07:57.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9597" for this suite. @ 12/16/23 17:07:57.041
• [0.192 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 12/16/23 17:07:57.06
  Dec 16 17:07:57.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename configmap @ 12/16/23 17:07:57.062
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:07:57.1
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:07:57.109
  STEP: Creating configMap configmap-586/configmap-test-caaf0394-91ce-4f67-a0f5-8f7a58a17ba1 @ 12/16/23 17:07:57.12
  STEP: Creating a pod to test consume configMaps @ 12/16/23 17:07:57.133
  E1216 17:07:57.770807      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:58.770857      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:07:59.770848      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:00.772229      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:08:01.191
  Dec 16 17:08:01.199: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-configmaps-c01c7d3d-edbe-4387-9002-5157e0329144 container env-test: <nil>
  STEP: delete the pod @ 12/16/23 17:08:01.22
  Dec 16 17:08:01.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-586" for this suite. @ 12/16/23 17:08:01.279
• [4.238 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 12/16/23 17:08:01.314
  Dec 16 17:08:01.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename cronjob @ 12/16/23 17:08:01.316
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:08:01.35
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:08:01.358
  STEP: Creating a ForbidConcurrent cronjob @ 12/16/23 17:08:01.364
  STEP: Ensuring a job is scheduled @ 12/16/23 17:08:01.374
  E1216 17:08:01.772699      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:02.773555      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:03.773908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:04.774652      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:05.775022      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:06.775588      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:07.776851      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:08.776988      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:09.777578      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:10.777928      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:11.778019      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:12.778366      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:13.779025      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:14.779448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:15.780234      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:16.780441      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:17.780642      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:18.780967      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:19.781863      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:20.782225      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:21.782810      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:22.783341      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:23.783651      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:24.783910      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:25.784210      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:26.785246      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:27.785906      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:28.785940      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:29.785929      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:30.786210      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:31.786262      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:32.786611      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:33.786688      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:34.786996      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:35.787375      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:36.787700      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:37.788674      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:38.788467      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:39.788962      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:40.789191      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:41.790567      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:42.790819      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:43.791518      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:44.791758      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:45.792282      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:46.794572      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:47.795619      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:48.795875      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:49.796473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:50.797164      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:51.798324      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:52.802259      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:53.802448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:54.802330      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:55.802827      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:56.802992      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:57.803418      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:58.804135      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:08:59.805086      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:00.805280      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 12/16/23 17:09:01.384
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 12/16/23 17:09:01.392
  STEP: Ensuring no more jobs are scheduled @ 12/16/23 17:09:01.399
  E1216 17:09:01.805926      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:02.806613      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:03.807140      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:04.807613      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:05.808071      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:06.807995      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:07.808032      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:08.808380      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:09.809241      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:10.810371      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:11.810458      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:12.811049      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:13.815210      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:14.815701      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:15.816615      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:16.816842      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:17.817190      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:18.817772      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:19.818383      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:20.818763      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:21.819043      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:22.821138      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:23.820541      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:24.821075      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:25.821262      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:26.821110      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:27.821488      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:28.822493      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:29.822887      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:30.824656      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:31.824222      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:32.825339      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:33.826019      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:34.826380      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:35.826515      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:36.827357      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:37.828071      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:38.828592      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:39.828563      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:40.829370      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:41.830260      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:42.830633      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:43.831206      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:44.831586      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:45.831910      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:46.832771      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:47.833943      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:48.833900      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:49.834295      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:50.834477      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:51.834974      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:52.835222      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:53.835911      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:54.836122      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:55.836710      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:56.836997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:57.837525      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:58.837739      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:09:59.837890      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:00.838736      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:01.839938      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:02.840234      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:03.841300      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:04.841170      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:05.842246      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:06.842696      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:07.843181      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:08.843746      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:09.844531      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:10.845079      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:11.845392      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:12.847059      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:13.847264      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:14.848005      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:15.848607      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:16.849361      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:17.849887      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:18.850265      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:19.851700      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:20.851506      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:21.851898      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:22.853101      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:23.853026      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:24.854027      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:25.855014      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:26.854844      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:27.855124      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:28.855720      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:29.856242      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:30.856690      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:31.857183      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:32.857141      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:33.858146      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:34.858234      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:35.858497      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:36.859392      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:37.860391      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:38.861036      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:39.861696      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:40.861962      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:41.862183      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:42.863200      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:43.863749      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:44.863752      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:45.864678      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:46.865117      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:47.865447      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:48.865647      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:49.866592      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:50.866865      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:51.867715      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:52.870915      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:53.869413      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:54.869396      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:55.870602      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:56.870847      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:57.871209      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:58.871626      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:10:59.872372      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:00.875111      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:01.874447      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:02.874796      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:03.875356      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:04.876186      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:05.875971      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:06.877061      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:07.879569      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:08.878280      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:09.879170      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:10.879421      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:11.880289      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:12.880792      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:13.883034      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:14.884883      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:15.884108      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:16.884377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:17.885110      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:18.885331      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:19.886404      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:20.887161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:21.887329      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:22.888002      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:23.889167      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:24.890067      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:25.890292      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:26.890421      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:27.891408      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:28.891651      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:29.892566      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:30.892992      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:31.893804      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:32.894423      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:33.894609      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:34.894742      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:35.895911      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:36.896620      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:37.897413      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:38.898078      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:39.898493      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:40.899386      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:41.899496      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:42.901019      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:43.901119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:44.909873      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:45.904297      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:46.904252      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:47.905402      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:48.906237      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:49.905942      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:50.906378      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:51.906852      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:52.907368      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:53.908209      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:54.908408      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:55.909619      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:56.909775      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:57.910546      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:58.911401      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:11:59.911580      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:00.912544      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:01.912941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:02.913971      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:03.914983      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:04.916021      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:05.916924      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:06.917630      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:07.918472      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:08.919377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:09.920450      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:10.921667      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:11.921330      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:12.922323      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:13.922772      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:14.922991      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:15.923297      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:16.923468      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:17.924044      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:18.924529      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:19.925285      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:20.926468      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:21.926628      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:22.927146      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:23.928310      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:24.928644      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:25.929490      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:26.930976      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:27.930492      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:28.931420      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:29.931377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:30.931509      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:31.931889      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:32.931939      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:33.933013      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:34.933929      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:35.934151      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:36.934721      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:37.935559      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:38.935918      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:39.937038      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:40.937145      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:41.938102      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:42.938821      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:43.939123      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:44.939374      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:45.939931      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:46.940358      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:47.941111      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:48.941418      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:49.942194      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:50.942399      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:51.942992      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:52.943905      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:53.944304      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:54.944340      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:55.944689      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:56.945023      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:57.945535      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:58.945693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:12:59.945986      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:00.946785      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:01.947927      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:02.948912      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:03.949605      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:04.949938      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:05.950236      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:06.953267      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:07.952922      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:08.958861      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:09.955847      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:10.957085      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:11.957693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:12.958225      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:13.959071      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:14.959498      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:15.960455      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:16.961635      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:17.962082      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:18.962222      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:19.962604      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:20.963653      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:21.963893      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:22.964964      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:23.965190      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:24.965404      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:25.965685      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:26.965799      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:27.966923      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:28.968254      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:29.968170      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:30.968500      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:31.969450      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:32.969638      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:33.969789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:34.970017      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:35.970636      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:36.971058      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:37.972124      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:38.972190      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:39.973661      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:40.972875      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:41.974040      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:42.975378      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:43.975337      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:44.977298      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:45.976782      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:46.977546      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:47.978805      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:48.979489      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:49.979070      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:50.979571      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:51.980084      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:52.980541      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:53.980607      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:54.981683      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:55.981733      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:56.981782      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:57.983112      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:58.983235      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:13:59.983518      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:00.984372      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 12/16/23 17:14:01.417
  Dec 16 17:14:01.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-6654" for this suite. @ 12/16/23 17:14:01.45
• [360.160 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 12/16/23 17:14:01.479
  Dec 16 17:14:01.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-runtime @ 12/16/23 17:14:01.484
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:14:01.541
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:14:01.55
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 12/16/23 17:14:01.596
  E1216 17:14:01.985252      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:02.985448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:03.985580      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:04.986599      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:05.987313      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:06.987568      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:07.988228      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:08.988479      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:09.989279      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:10.990076      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:11.991076      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:12.991297      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:13.991638      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:14.992799      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:15.993703      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:16.994360      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:17.996109      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:18.996184      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:19.997155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:20.998204      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 12/16/23 17:14:21.826
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 12/16/23 17:14:21.833
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 12/16/23 17:14:21.849
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 12/16/23 17:14:21.849
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 12/16/23 17:14:21.895
  E1216 17:14:22.000007      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:23.000013      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:24.000843      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 12/16/23 17:14:24.952
  E1216 17:14:25.000853      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 12/16/23 17:14:25.979
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 12/16/23 17:14:25.996
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 12/16/23 17:14:25.996
  E1216 17:14:26.001432      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 12/16/23 17:14:26.049
  E1216 17:14:27.001728      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 12/16/23 17:14:27.069
  E1216 17:14:28.001678      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:29.002149      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 12/16/23 17:14:29.099
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 12/16/23 17:14:29.121
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 12/16/23 17:14:29.121
  Dec 16 17:14:29.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9823" for this suite. @ 12/16/23 17:14:29.214
• [27.754 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 12/16/23 17:14:29.236
  Dec 16 17:14:29.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename deployment @ 12/16/23 17:14:29.24
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:14:29.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:14:29.304
  Dec 16 17:14:29.316: INFO: Creating simple deployment test-new-deployment
  Dec 16 17:14:29.379: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E1216 17:14:30.002125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:31.002569      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 12/16/23 17:14:31.407
  STEP: updating a scale subresource @ 12/16/23 17:14:31.413
  STEP: verifying the deployment Spec.Replicas was modified @ 12/16/23 17:14:31.424
  STEP: Patch a scale subresource @ 12/16/23 17:14:31.43
  Dec 16 17:14:31.479: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-6354  5fdc3a0d-6b38-472e-8120-1a7d33d7a84f 16031 3 2023-12-16 17:14:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-12-16 17:14:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-16 17:14:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b32698 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-12-16 17:14:30 +0000 UTC,LastTransitionTime:2023-12-16 17:14:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-12-16 17:14:30 +0000 UTC,LastTransitionTime:2023-12-16 17:14:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Dec 16 17:14:31.494: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-6354  b889992c-24de-45b2-8f6b-b7ac84a30aa8 16034 3 2023-12-16 17:14:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 5fdc3a0d-6b38-472e-8120-1a7d33d7a84f 0xc001b33737 0xc001b33738}] [] [{kube-controller-manager Update apps/v1 2023-12-16 17:14:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-12-16 17:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5fdc3a0d-6b38-472e-8120-1a7d33d7a84f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b33a08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Dec 16 17:14:31.543: INFO: Pod "test-new-deployment-67bd4bf6dc-8xtf8" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-8xtf8 test-new-deployment-67bd4bf6dc- deployment-6354  c0ba19bd-4abb-440a-b8d8-471a160afd37 16023 0 2023-12-16 17:14:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc b889992c-24de-45b2-8f6b-b7ac84a30aa8 0xc006b545d7 0xc006b545d8}] [] [{kube-controller-manager Update v1 2023-12-16 17:14:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b889992c-24de-45b2-8f6b-b7ac84a30aa8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:14:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6ncbl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6ncbl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:14:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:14:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:14:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:14:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.112,PodIP:10.233.66.163,StartTime:2023-12-16 17:14:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-16 17:14:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://4e1d38c1d13d8763a9c39b45cf73e995f02ac81555f06614cd7f43ccb1efa493,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.163,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:14:31.544: INFO: Pod "test-new-deployment-67bd4bf6dc-vqbml" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-vqbml test-new-deployment-67bd4bf6dc- deployment-6354  8b37702e-1e8c-466d-8ba1-16e465844472 16038 0 2023-12-16 17:14:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc b889992c-24de-45b2-8f6b-b7ac84a30aa8 0xc006b547c7 0xc006b547c8}] [] [{kube-controller-manager Update v1 2023-12-16 17:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b889992c-24de-45b2-8f6b-b7ac84a30aa8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c5fmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c5fmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:14:31.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6354" for this suite. @ 12/16/23 17:14:31.559
• [2.343 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 12/16/23 17:14:31.582
  Dec 16 17:14:31.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename secrets @ 12/16/23 17:14:31.584
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:14:31.62
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:14:31.625
  STEP: Creating secret with name secret-test-map-c1e3c8c6-a6f3-45b9-93e1-f3eb3a826645 @ 12/16/23 17:14:31.632
  STEP: Creating a pod to test consume secrets @ 12/16/23 17:14:31.661
  E1216 17:14:32.002747      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:33.003433      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:34.004386      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:35.004877      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:14:35.711
  Dec 16 17:14:35.717: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-secrets-560d9ba8-5066-41b6-a58c-c00f0c62cb89 container secret-volume-test: <nil>
  STEP: delete the pod @ 12/16/23 17:14:35.763
  Dec 16 17:14:35.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7060" for this suite. @ 12/16/23 17:14:35.811
• [4.240 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 12/16/23 17:14:35.838
  Dec 16 17:14:35.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename disruption @ 12/16/23 17:14:35.84
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:14:35.872
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:14:35.877
  STEP: Creating a pdb that targets all three pods in a test replica set @ 12/16/23 17:14:35.884
  STEP: Waiting for the pdb to be processed @ 12/16/23 17:14:35.894
  E1216 17:14:36.005458      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:37.005608      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: First trying to evict a pod which shouldn't be evictable @ 12/16/23 17:14:37.934
  STEP: Waiting for all pods to be running @ 12/16/23 17:14:37.934
  Dec 16 17:14:37.942: INFO: pods: 0 < 3
  E1216 17:14:38.005709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:39.006103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 12/16/23 17:14:39.954
  STEP: Updating the pdb to allow a pod to be evicted @ 12/16/23 17:14:39.982
  E1216 17:14:40.005989      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 12/16/23 17:14:40.006
  E1216 17:14:41.006366      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:42.006909      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 12/16/23 17:14:42.026
  STEP: Waiting for all pods to be running @ 12/16/23 17:14:42.026
  STEP: Waiting for the pdb to observed all healthy pods @ 12/16/23 17:14:42.035
  STEP: Patching the pdb to disallow a pod to be evicted @ 12/16/23 17:14:42.1
  STEP: Waiting for the pdb to be processed @ 12/16/23 17:14:42.151
  E1216 17:14:43.007754      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:44.008038      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 12/16/23 17:14:44.181
  STEP: locating a running pod @ 12/16/23 17:14:44.188
  STEP: Deleting the pdb to allow a pod to be evicted @ 12/16/23 17:14:44.207
  STEP: Waiting for the pdb to be deleted @ 12/16/23 17:14:44.22
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 12/16/23 17:14:44.227
  STEP: Waiting for all pods to be running @ 12/16/23 17:14:44.227
  Dec 16 17:14:44.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-7005" for this suite. @ 12/16/23 17:14:44.302
• [8.536 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:474
  STEP: Creating a kubernetes client @ 12/16/23 17:14:44.375
  Dec 16 17:14:44.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename field-validation @ 12/16/23 17:14:44.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:14:44.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:14:44.446
  Dec 16 17:14:44.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:14:45.009262      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:46.010213      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:47.010253      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  W1216 17:14:47.225934      13 warnings.go:70] unknown field "alpha"
  W1216 17:14:47.226070      13 warnings.go:70] unknown field "beta"
  W1216 17:14:47.226101      13 warnings.go:70] unknown field "delta"
  W1216 17:14:47.226122      13 warnings.go:70] unknown field "epsilon"
  W1216 17:14:47.226144      13 warnings.go:70] unknown field "gamma"
  Dec 16 17:14:47.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7722" for this suite. @ 12/16/23 17:14:47.84
• [3.487 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 12/16/23 17:14:47.868
  Dec 16 17:14:47.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename configmap @ 12/16/23 17:14:47.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:14:47.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:14:47.927
  STEP: Creating configMap with name configmap-test-upd-3b38c766-8618-42b1-a2eb-dbe5c2fb4d05 @ 12/16/23 17:14:47.946
  STEP: Creating the pod @ 12/16/23 17:14:47.96
  E1216 17:14:48.010598      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:49.012159      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:50.011751      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap configmap-test-upd-3b38c766-8618-42b1-a2eb-dbe5c2fb4d05 @ 12/16/23 17:14:50.085
  STEP: waiting to observe update in volume @ 12/16/23 17:14:50.104
  E1216 17:14:51.011866      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:52.012480      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:53.013071      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:54.015560      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:55.014158      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:56.014290      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:57.014467      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:58.015587      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:14:59.016107      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:00.016843      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:01.017117      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:02.017490      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:03.017990      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:04.018762      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:05.019243      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:06.020059      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:07.020060      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:08.020796      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:09.021031      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:10.021562      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:11.021762      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:12.021897      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:13.022122      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:14.022868      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:15.023039      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:16.023755      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:17.024526      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:18.025790      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:19.026655      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:20.026406      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:21.026423      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:22.026666      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:23.026896      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:24.027163      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:25.027808      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:26.027860      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:27.027937      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:28.028575      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:29.029282      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:30.029376      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:31.029844      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:32.030503      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:33.031047      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:34.031337      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:35.032269      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:36.033020      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:37.034254      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:38.034629      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:39.035119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:40.035136      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:41.035408      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:42.035726      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:43.036217      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:44.036105      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:45.038350      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:46.038253      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:47.039327      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:48.039580      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:49.039791      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:50.040299      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:51.041173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:52.041952      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:53.042600      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:54.042706      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:55.044410      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:56.043858      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:57.044955      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:58.045755      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:15:59.046412      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:00.046893      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:01.047255      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:02.047992      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:03.048423      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:04.049155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:05.050123      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:06.050809      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:07.051069      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:08.051250      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:09.052020      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:10.051958      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:11.053540      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:12.053404      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:13.053970      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:14.054242      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:15.054705      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:16.054965      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:17.055201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:16:17.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4061" for this suite. @ 12/16/23 17:16:17.112
• [89.265 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 12/16/23 17:16:17.15
  Dec 16 17:16:17.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename sched-pred @ 12/16/23 17:16:17.154
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:16:17.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:16:17.209
  Dec 16 17:16:17.217: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Dec 16 17:16:17.263: INFO: Waiting for terminating namespaces to be deleted...
  Dec 16 17:16:17.277: INFO: 
  Logging pods the apiserver thinks is on node phoh7xai9ouk-1 before test
  Dec 16 17:16:17.304: INFO: kube-flannel-ds-5sbtb from kube-flannel started at 2023-12-16 16:10:58 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.304: INFO: 	Container kube-flannel ready: true, restart count 1
  Dec 16 17:16:17.304: INFO: coredns-5d78c9869d-scnc5 from kube-system started at 2023-12-16 16:15:59 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.304: INFO: 	Container coredns ready: true, restart count 0
  Dec 16 17:16:17.304: INFO: kube-addon-manager-phoh7xai9ouk-1 from kube-system started at 2023-12-16 16:09:46 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.304: INFO: 	Container kube-addon-manager ready: true, restart count 1
  Dec 16 17:16:17.304: INFO: kube-apiserver-phoh7xai9ouk-1 from kube-system started at 2023-12-16 16:09:46 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.304: INFO: 	Container kube-apiserver ready: true, restart count 1
  Dec 16 17:16:17.304: INFO: kube-controller-manager-phoh7xai9ouk-1 from kube-system started at 2023-12-16 16:09:46 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.304: INFO: 	Container kube-controller-manager ready: true, restart count 1
  Dec 16 17:16:17.304: INFO: kube-proxy-7c5h8 from kube-system started at 2023-12-16 16:10:59 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.304: INFO: 	Container kube-proxy ready: true, restart count 0
  Dec 16 17:16:17.304: INFO: kube-scheduler-phoh7xai9ouk-1 from kube-system started at 2023-12-16 16:09:46 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.304: INFO: 	Container kube-scheduler ready: true, restart count 1
  Dec 16 17:16:17.304: INFO: sonobuoy-systemd-logs-daemon-set-0d7ed218a9fd4126-l6bt5 from sonobuoy started at 2023-12-16 16:12:30 +0000 UTC (2 container statuses recorded)
  Dec 16 17:16:17.304: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 16 17:16:17.304: INFO: 	Container systemd-logs ready: true, restart count 0
  Dec 16 17:16:17.305: INFO: 
  Logging pods the apiserver thinks is on node phoh7xai9ouk-2 before test
  Dec 16 17:16:17.321: INFO: rs-x7pc6 from disruption-7005 started at 2023-12-16 17:14:44 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.321: INFO: 	Container donothing ready: false, restart count 0
  Dec 16 17:16:17.321: INFO: kube-flannel-ds-p5tmh from kube-flannel started at 2023-12-16 16:11:26 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.321: INFO: 	Container kube-flannel ready: true, restart count 0
  Dec 16 17:16:17.321: INFO: coredns-5d78c9869d-848fd from kube-system started at 2023-12-16 16:11:25 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.321: INFO: 	Container coredns ready: true, restart count 0
  Dec 16 17:16:17.321: INFO: kube-addon-manager-phoh7xai9ouk-2 from kube-system started at 2023-12-16 16:09:29 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.321: INFO: 	Container kube-addon-manager ready: true, restart count 1
  Dec 16 17:16:17.321: INFO: kube-apiserver-phoh7xai9ouk-2 from kube-system started at 2023-12-16 16:09:29 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.321: INFO: 	Container kube-apiserver ready: true, restart count 1
  Dec 16 17:16:17.321: INFO: kube-controller-manager-phoh7xai9ouk-2 from kube-system started at 2023-12-16 16:09:29 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.321: INFO: 	Container kube-controller-manager ready: true, restart count 1
  Dec 16 17:16:17.321: INFO: kube-proxy-dzncl from kube-system started at 2023-12-16 16:11:26 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.321: INFO: 	Container kube-proxy ready: true, restart count 0
  Dec 16 17:16:17.321: INFO: kube-scheduler-phoh7xai9ouk-2 from kube-system started at 2023-12-16 16:09:29 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.321: INFO: 	Container kube-scheduler ready: true, restart count 1
  Dec 16 17:16:17.321: INFO: sonobuoy-systemd-logs-daemon-set-0d7ed218a9fd4126-2zwqp from sonobuoy started at 2023-12-16 16:12:30 +0000 UTC (2 container statuses recorded)
  Dec 16 17:16:17.321: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 16 17:16:17.321: INFO: 	Container systemd-logs ready: true, restart count 0
  Dec 16 17:16:17.321: INFO: 
  Logging pods the apiserver thinks is on node phoh7xai9ouk-3 before test
  Dec 16 17:16:17.347: INFO: pod-configmaps-bf539e57-6371-40b1-ac14-99a1cdf223a3 from configmap-4061 started at 2023-12-16 17:14:47 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.347: INFO: 	Container agnhost-container ready: true, restart count 0
  Dec 16 17:16:17.347: INFO: kube-flannel-ds-dk7xw from kube-flannel started at 2023-12-16 16:39:50 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.347: INFO: 	Container kube-flannel ready: true, restart count 0
  Dec 16 17:16:17.347: INFO: kube-proxy-w4mds from kube-system started at 2023-12-16 16:11:29 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.347: INFO: 	Container kube-proxy ready: true, restart count 0
  Dec 16 17:16:17.347: INFO: sonobuoy from sonobuoy started at 2023-12-16 16:12:19 +0000 UTC (1 container statuses recorded)
  Dec 16 17:16:17.347: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Dec 16 17:16:17.347: INFO: sonobuoy-e2e-job-8b3ba51fd9314720 from sonobuoy started at 2023-12-16 16:12:30 +0000 UTC (2 container statuses recorded)
  Dec 16 17:16:17.347: INFO: 	Container e2e ready: true, restart count 0
  Dec 16 17:16:17.347: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 16 17:16:17.347: INFO: sonobuoy-systemd-logs-daemon-set-0d7ed218a9fd4126-zq8f6 from sonobuoy started at 2023-12-16 16:12:30 +0000 UTC (2 container statuses recorded)
  Dec 16 17:16:17.347: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 16 17:16:17.347: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node phoh7xai9ouk-1 @ 12/16/23 17:16:17.411
  STEP: verifying the node has the label node phoh7xai9ouk-2 @ 12/16/23 17:16:17.453
  STEP: verifying the node has the label node phoh7xai9ouk-3 @ 12/16/23 17:16:17.494
  Dec 16 17:16:17.548: INFO: Pod pod-configmaps-bf539e57-6371-40b1-ac14-99a1cdf223a3 requesting resource cpu=0m on Node phoh7xai9ouk-3
  Dec 16 17:16:17.549: INFO: Pod rs-x7pc6 requesting resource cpu=0m on Node phoh7xai9ouk-2
  Dec 16 17:16:17.549: INFO: Pod kube-flannel-ds-5sbtb requesting resource cpu=100m on Node phoh7xai9ouk-1
  Dec 16 17:16:17.549: INFO: Pod kube-flannel-ds-dk7xw requesting resource cpu=100m on Node phoh7xai9ouk-3
  Dec 16 17:16:17.549: INFO: Pod kube-flannel-ds-p5tmh requesting resource cpu=100m on Node phoh7xai9ouk-2
  Dec 16 17:16:17.549: INFO: Pod coredns-5d78c9869d-848fd requesting resource cpu=100m on Node phoh7xai9ouk-2
  Dec 16 17:16:17.549: INFO: Pod coredns-5d78c9869d-scnc5 requesting resource cpu=100m on Node phoh7xai9ouk-1
  Dec 16 17:16:17.549: INFO: Pod kube-addon-manager-phoh7xai9ouk-1 requesting resource cpu=5m on Node phoh7xai9ouk-1
  Dec 16 17:16:17.549: INFO: Pod kube-addon-manager-phoh7xai9ouk-2 requesting resource cpu=5m on Node phoh7xai9ouk-2
  Dec 16 17:16:17.549: INFO: Pod kube-apiserver-phoh7xai9ouk-1 requesting resource cpu=250m on Node phoh7xai9ouk-1
  Dec 16 17:16:17.549: INFO: Pod kube-apiserver-phoh7xai9ouk-2 requesting resource cpu=250m on Node phoh7xai9ouk-2
  Dec 16 17:16:17.549: INFO: Pod kube-controller-manager-phoh7xai9ouk-1 requesting resource cpu=200m on Node phoh7xai9ouk-1
  Dec 16 17:16:17.549: INFO: Pod kube-controller-manager-phoh7xai9ouk-2 requesting resource cpu=200m on Node phoh7xai9ouk-2
  Dec 16 17:16:17.549: INFO: Pod kube-proxy-7c5h8 requesting resource cpu=0m on Node phoh7xai9ouk-1
  Dec 16 17:16:17.549: INFO: Pod kube-proxy-dzncl requesting resource cpu=0m on Node phoh7xai9ouk-2
  Dec 16 17:16:17.549: INFO: Pod kube-proxy-w4mds requesting resource cpu=0m on Node phoh7xai9ouk-3
  Dec 16 17:16:17.549: INFO: Pod kube-scheduler-phoh7xai9ouk-1 requesting resource cpu=100m on Node phoh7xai9ouk-1
  Dec 16 17:16:17.549: INFO: Pod kube-scheduler-phoh7xai9ouk-2 requesting resource cpu=100m on Node phoh7xai9ouk-2
  Dec 16 17:16:17.549: INFO: Pod sonobuoy requesting resource cpu=0m on Node phoh7xai9ouk-3
  Dec 16 17:16:17.549: INFO: Pod sonobuoy-e2e-job-8b3ba51fd9314720 requesting resource cpu=0m on Node phoh7xai9ouk-3
  Dec 16 17:16:17.549: INFO: Pod sonobuoy-systemd-logs-daemon-set-0d7ed218a9fd4126-2zwqp requesting resource cpu=0m on Node phoh7xai9ouk-2
  Dec 16 17:16:17.549: INFO: Pod sonobuoy-systemd-logs-daemon-set-0d7ed218a9fd4126-l6bt5 requesting resource cpu=0m on Node phoh7xai9ouk-1
  Dec 16 17:16:17.549: INFO: Pod sonobuoy-systemd-logs-daemon-set-0d7ed218a9fd4126-zq8f6 requesting resource cpu=0m on Node phoh7xai9ouk-3
  STEP: Starting Pods to consume most of the cluster CPU. @ 12/16/23 17:16:17.549
  Dec 16 17:16:17.549: INFO: Creating a pod which consumes cpu=591m on Node phoh7xai9ouk-1
  Dec 16 17:16:17.584: INFO: Creating a pod which consumes cpu=591m on Node phoh7xai9ouk-2
  Dec 16 17:16:17.669: INFO: Creating a pod which consumes cpu=1050m on Node phoh7xai9ouk-3
  E1216 17:16:18.055520      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:19.055720      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 12/16/23 17:16:19.771
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-0587db5d-bae3-41cf-9117-97d0c4cabc45.17a15f5accb87fb6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2830/filler-pod-0587db5d-bae3-41cf-9117-97d0c4cabc45 to phoh7xai9ouk-2] @ 12/16/23 17:16:19.785
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-0587db5d-bae3-41cf-9117-97d0c4cabc45.17a15f5aec282d8e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 12/16/23 17:16:19.786
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-0587db5d-bae3-41cf-9117-97d0c4cabc45.17a15f5af9bd157c], Reason = [Created], Message = [Created container filler-pod-0587db5d-bae3-41cf-9117-97d0c4cabc45] @ 12/16/23 17:16:19.787
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-0587db5d-bae3-41cf-9117-97d0c4cabc45.17a15f5afc20355a], Reason = [Started], Message = [Started container filler-pod-0587db5d-bae3-41cf-9117-97d0c4cabc45] @ 12/16/23 17:16:19.787
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-50e1730d-15f0-4af5-958b-08b8b04f52ce.17a15f5ac707970a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2830/filler-pod-50e1730d-15f0-4af5-958b-08b8b04f52ce to phoh7xai9ouk-1] @ 12/16/23 17:16:19.788
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-50e1730d-15f0-4af5-958b-08b8b04f52ce.17a15f5af1f76291], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 12/16/23 17:16:19.789
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-50e1730d-15f0-4af5-958b-08b8b04f52ce.17a15f5b0750db0a], Reason = [Created], Message = [Created container filler-pod-50e1730d-15f0-4af5-958b-08b8b04f52ce] @ 12/16/23 17:16:19.79
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-50e1730d-15f0-4af5-958b-08b8b04f52ce.17a15f5b0a1e7d7b], Reason = [Started], Message = [Started container filler-pod-50e1730d-15f0-4af5-958b-08b8b04f52ce] @ 12/16/23 17:16:19.791
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8963f322-e667-47ae-b322-2235f8485aa1.17a15f5ad0b81153], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2830/filler-pod-8963f322-e667-47ae-b322-2235f8485aa1 to phoh7xai9ouk-3] @ 12/16/23 17:16:19.791
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8963f322-e667-47ae-b322-2235f8485aa1.17a15f5af10ea87b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 12/16/23 17:16:19.793
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8963f322-e667-47ae-b322-2235f8485aa1.17a15f5afd5a21c4], Reason = [Created], Message = [Created container filler-pod-8963f322-e667-47ae-b322-2235f8485aa1] @ 12/16/23 17:16:19.794
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8963f322-e667-47ae-b322-2235f8485aa1.17a15f5aff2172d6], Reason = [Started], Message = [Started container filler-pod-8963f322-e667-47ae-b322-2235f8485aa1] @ 12/16/23 17:16:19.794
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.17a15f5b4aa9ebf4], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] @ 12/16/23 17:16:19.837
  E1216 17:16:20.055782      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node phoh7xai9ouk-1 @ 12/16/23 17:16:20.84
  STEP: verifying the node doesn't have the label node @ 12/16/23 17:16:20.896
  STEP: removing the label node off the node phoh7xai9ouk-2 @ 12/16/23 17:16:20.912
  STEP: verifying the node doesn't have the label node @ 12/16/23 17:16:20.952
  STEP: removing the label node off the node phoh7xai9ouk-3 @ 12/16/23 17:16:20.965
  STEP: verifying the node doesn't have the label node @ 12/16/23 17:16:20.998
  Dec 16 17:16:21.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-2830" for this suite. @ 12/16/23 17:16:21.025
• [3.889 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 12/16/23 17:16:21.049
  Dec 16 17:16:21.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename custom-resource-definition @ 12/16/23 17:16:21.051
  E1216 17:16:21.056415      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:16:21.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:16:21.089
  Dec 16 17:16:21.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:16:22.056620      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:16:22.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6996" for this suite. @ 12/16/23 17:16:22.18
• [1.146 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 12/16/23 17:16:22.207
  Dec 16 17:16:22.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename disruption @ 12/16/23 17:16:22.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:16:22.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:16:22.297
  STEP: Waiting for the pdb to be processed @ 12/16/23 17:16:22.315
  E1216 17:16:23.056695      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:24.057388      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating PodDisruptionBudget status @ 12/16/23 17:16:24.366
  STEP: Waiting for all pods to be running @ 12/16/23 17:16:24.383
  Dec 16 17:16:24.392: INFO: running pods: 0 < 1
  E1216 17:16:25.058282      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:26.059164      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 12/16/23 17:16:26.402
  STEP: Waiting for the pdb to be processed @ 12/16/23 17:16:26.428
  STEP: Patching PodDisruptionBudget status @ 12/16/23 17:16:26.448
  STEP: Waiting for the pdb to be processed @ 12/16/23 17:16:26.481
  Dec 16 17:16:26.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-6758" for this suite. @ 12/16/23 17:16:26.508
• [4.390 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 12/16/23 17:16:26.61
  Dec 16 17:16:26.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename resourcequota @ 12/16/23 17:16:26.615
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:16:26.842
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:16:26.855
  STEP: Creating a ResourceQuota with terminating scope @ 12/16/23 17:16:26.867
  STEP: Ensuring ResourceQuota status is calculated @ 12/16/23 17:16:26.903
  E1216 17:16:27.060375      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:28.060936      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 12/16/23 17:16:28.911
  STEP: Ensuring ResourceQuota status is calculated @ 12/16/23 17:16:28.925
  E1216 17:16:29.061231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:30.061490      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 12/16/23 17:16:30.939
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 12/16/23 17:16:30.991
  E1216 17:16:31.062163      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:32.063723      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 12/16/23 17:16:33.002
  E1216 17:16:33.063497      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:34.063726      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 12/16/23 17:16:35.012
  STEP: Ensuring resource quota status released the pod usage @ 12/16/23 17:16:35.04
  E1216 17:16:35.064151      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:36.065241      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 12/16/23 17:16:37.049
  E1216 17:16:37.066129      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 12/16/23 17:16:37.072
  E1216 17:16:38.066790      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:39.067807      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 12/16/23 17:16:39.081
  E1216 17:16:40.068400      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:41.068596      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 12/16/23 17:16:41.094
  STEP: Ensuring resource quota status released the pod usage @ 12/16/23 17:16:41.131
  E1216 17:16:42.068887      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:43.069084      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:16:43.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1021" for this suite. @ 12/16/23 17:16:43.151
• [16.563 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:748
  STEP: Creating a kubernetes client @ 12/16/23 17:16:43.175
  Dec 16 17:16:43.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename statefulset @ 12/16/23 17:16:43.18
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:16:43.226
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:16:43.233
  STEP: Creating service test in namespace statefulset-6205 @ 12/16/23 17:16:43.238
  STEP: Creating stateful set ss in namespace statefulset-6205 @ 12/16/23 17:16:43.254
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6205 @ 12/16/23 17:16:43.277
  Dec 16 17:16:43.289: INFO: Found 0 stateful pods, waiting for 1
  E1216 17:16:44.069312      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:45.069532      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:46.069774      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:47.069953      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:48.070856      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:49.071010      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:50.071182      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:51.071437      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:52.071671      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:53.072800      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:16:53.300: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 12/16/23 17:16:53.3
  Dec 16 17:16:53.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-6205 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Dec 16 17:16:53.658: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 16 17:16:53.659: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 16 17:16:53.659: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Dec 16 17:16:53.669: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E1216 17:16:54.073483      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:55.075469      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:56.075688      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:57.075826      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:58.076524      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:16:59.076887      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:00.077384      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:01.077640      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:02.077971      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:03.078929      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:03.680: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Dec 16 17:17:03.680: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 16 17:17:03.750: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999518s
  E1216 17:17:04.080473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:04.775: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.958421457s
  E1216 17:17:05.080852      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:05.788: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.93406146s
  E1216 17:17:06.084209      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:06.805: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.919952765s
  E1216 17:17:07.083828      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:07.817: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.902958027s
  E1216 17:17:08.084237      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:08.830: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.892436916s
  E1216 17:17:09.084837      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:09.843: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.878535446s
  E1216 17:17:10.085754      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:10.866: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.865958936s
  E1216 17:17:11.086564      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:11.877: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.84262103s
  E1216 17:17:12.087283      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:12.899: INFO: Verifying statefulset ss doesn't scale past 3 for another 831.07981ms
  E1216 17:17:13.087638      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6205 @ 12/16/23 17:17:13.901
  Dec 16 17:17:13.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-6205 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E1216 17:17:14.088693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:14.285: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Dec 16 17:17:14.285: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Dec 16 17:17:14.285: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Dec 16 17:17:14.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-6205 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Dec 16 17:17:14.574: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Dec 16 17:17:14.574: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Dec 16 17:17:14.574: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Dec 16 17:17:14.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-6205 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Dec 16 17:17:15.041: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Dec 16 17:17:15.041: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Dec 16 17:17:15.041: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Dec 16 17:17:15.050: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Dec 16 17:17:15.050: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Dec 16 17:17:15.050: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 12/16/23 17:17:15.05
  Dec 16 17:17:15.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-6205 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E1216 17:17:15.088791      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:15.358: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 16 17:17:15.358: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 16 17:17:15.358: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Dec 16 17:17:15.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-6205 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Dec 16 17:17:15.670: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 16 17:17:15.670: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 16 17:17:15.670: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Dec 16 17:17:15.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-6205 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Dec 16 17:17:16.008: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 16 17:17:16.008: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 16 17:17:16.008: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Dec 16 17:17:16.008: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 16 17:17:16.016: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  E1216 17:17:16.089451      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:17.090269      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:18.091399      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:19.091890      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:20.092059      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:21.092367      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:22.092518      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:23.092829      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:24.093242      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:25.093411      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:26.029: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Dec 16 17:17:26.030: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Dec 16 17:17:26.030: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Dec 16 17:17:26.058: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Dec 16 17:17:26.059: INFO: ss-0  phoh7xai9ouk-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:16:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:17:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:17:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:16:43 +0000 UTC  }]
  Dec 16 17:17:26.060: INFO: ss-1  phoh7xai9ouk-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:17:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:17:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:17:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:17:03 +0000 UTC  }]
  Dec 16 17:17:26.061: INFO: ss-2  phoh7xai9ouk-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:17:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:17:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:17:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:17:03 +0000 UTC  }]
  Dec 16 17:17:26.062: INFO: 
  Dec 16 17:17:26.062: INFO: StatefulSet ss has not reached scale 0, at 3
  E1216 17:17:26.094111      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:27.071: INFO: POD   NODE            PHASE      GRACE  CONDITIONS
  Dec 16 17:17:27.071: INFO: ss-0  phoh7xai9ouk-3  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:16:43 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:17:15 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:17:15 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:16:43 +0000 UTC  }]
  Dec 16 17:17:27.071: INFO: 
  Dec 16 17:17:27.071: INFO: StatefulSet ss has not reached scale 0, at 1
  E1216 17:17:27.094454      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:28.080: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.975346748s
  E1216 17:17:28.095180      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:29.090: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.96672554s
  E1216 17:17:29.095409      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:30.095365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:30.097: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.956836279s
  E1216 17:17:31.095947      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:31.105: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.949042361s
  E1216 17:17:32.096254      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:32.115: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.940928059s
  E1216 17:17:33.097154      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:33.124: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.930937748s
  E1216 17:17:34.097518      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:34.134: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.921569302s
  E1216 17:17:35.098529      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:35.147: INFO: Verifying statefulset ss doesn't scale past 0 for another 911.796611ms
  E1216 17:17:36.099304      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6205 @ 12/16/23 17:17:36.148
  Dec 16 17:17:36.160: INFO: Scaling statefulset ss to 0
  Dec 16 17:17:36.179: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 16 17:17:36.187: INFO: Deleting all statefulset in ns statefulset-6205
  Dec 16 17:17:36.192: INFO: Scaling statefulset ss to 0
  Dec 16 17:17:36.213: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 16 17:17:36.219: INFO: Deleting statefulset ss
  Dec 16 17:17:36.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6205" for this suite. @ 12/16/23 17:17:36.283
• [53.129 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 12/16/23 17:17:36.309
  Dec 16 17:17:36.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubectl @ 12/16/23 17:17:36.315
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:17:36.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:17:36.358
  Dec 16 17:17:36.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-9939 version'
  Dec 16 17:17:36.553: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Dec 16 17:17:36.553: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.8\", GitCommit:\"66fee42707cd7f5a89f1987f7cb81b02dd19161c\", GitTreeState:\"clean\", BuildDate:\"2023-11-15T16:59:43Z\", GoVersion:\"go1.20.11\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.8\", GitCommit:\"66fee42707cd7f5a89f1987f7cb81b02dd19161c\", GitTreeState:\"clean\", BuildDate:\"2023-11-15T16:50:09Z\", GoVersion:\"go1.20.11\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Dec 16 17:17:36.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9939" for this suite. @ 12/16/23 17:17:36.565
• [0.268 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 12/16/23 17:17:36.579
  Dec 16 17:17:36.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-runtime @ 12/16/23 17:17:36.584
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:17:36.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:17:36.671
  STEP: create the container @ 12/16/23 17:17:36.676
  W1216 17:17:36.711728      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 12/16/23 17:17:36.712
  E1216 17:17:37.099630      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:38.100973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:39.104513      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 12/16/23 17:17:39.764
  STEP: the container should be terminated @ 12/16/23 17:17:39.775
  STEP: the termination message should be set @ 12/16/23 17:17:39.775
  Dec 16 17:17:39.775: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 12/16/23 17:17:39.775
  Dec 16 17:17:39.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8250" for this suite. @ 12/16/23 17:17:39.84
• [3.284 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 12/16/23 17:17:39.864
  Dec 16 17:17:39.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 17:17:39.869
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:17:39.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:17:39.915
  STEP: Creating the pod @ 12/16/23 17:17:39.927
  E1216 17:17:40.102493      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:41.103805      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:42.104006      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:42.543: INFO: Successfully updated pod "labelsupdate1d8f5ea3-c906-4a6d-a6ca-032375c1156f"
  E1216 17:17:43.103875      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:44.104065      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:17:44.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9299" for this suite. @ 12/16/23 17:17:44.594
• [4.783 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 12/16/23 17:17:44.655
  Dec 16 17:17:44.655: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename emptydir @ 12/16/23 17:17:44.66
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:17:44.719
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:17:44.724
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 12/16/23 17:17:44.728
  E1216 17:17:45.104996      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:46.105249      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:47.106123      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:48.106734      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:17:48.782
  Dec 16 17:17:48.788: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-daa012e6-1cf0-457a-a6a3-4b8422387f0a container test-container: <nil>
  STEP: delete the pod @ 12/16/23 17:17:48.802
  Dec 16 17:17:48.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2719" for this suite. @ 12/16/23 17:17:48.841
• [4.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 12/16/23 17:17:48.871
  Dec 16 17:17:48.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename endpointslice @ 12/16/23 17:17:48.873
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:17:48.909
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:17:48.914
  E1216 17:17:49.107196      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:50.107428      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:51.107594      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:52.107862      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:53.108547      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 12/16/23 17:17:54.101
  E1216 17:17:54.109371      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:55.109401      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:56.110356      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:57.111047      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:58.111872      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:17:59.112406      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 12/16/23 17:17:59.118
  E1216 17:18:00.113169      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:01.113849      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:02.113759      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:03.114252      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:04.114488      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 12/16/23 17:18:04.135
  E1216 17:18:05.114923      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:06.115191      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:07.115375      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:08.116099      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:09.115764      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 12/16/23 17:18:09.151
  Dec 16 17:18:09.189: INFO: EndpointSlice for Service endpointslice-6815/example-named-port not found
  E1216 17:18:10.119269      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:11.120453      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:12.121137      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:13.129455      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:14.126758      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:15.126942      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:16.127066      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:17.127270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:18.127538      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:19.127839      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:18:19.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6815" for this suite. @ 12/16/23 17:18:19.229
• [30.378 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 12/16/23 17:18:19.253
  Dec 16 17:18:19.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubectl @ 12/16/23 17:18:19.258
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:18:19.302
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:18:19.307
  STEP: creating the pod @ 12/16/23 17:18:19.319
  Dec 16 17:18:19.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-5837 create -f -'
  E1216 17:18:20.128063      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:18:20.195: INFO: stderr: ""
  Dec 16 17:18:20.195: INFO: stdout: "pod/pause created\n"
  E1216 17:18:21.128459      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:22.128910      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 12/16/23 17:18:22.213
  Dec 16 17:18:22.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-5837 label pods pause testing-label=testing-label-value'
  Dec 16 17:18:22.425: INFO: stderr: ""
  Dec 16 17:18:22.425: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 12/16/23 17:18:22.425
  Dec 16 17:18:22.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-5837 get pod pause -L testing-label'
  Dec 16 17:18:22.622: INFO: stderr: ""
  Dec 16 17:18:22.622: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 12/16/23 17:18:22.622
  Dec 16 17:18:22.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-5837 label pods pause testing-label-'
  Dec 16 17:18:22.807: INFO: stderr: ""
  Dec 16 17:18:22.807: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 12/16/23 17:18:22.807
  Dec 16 17:18:22.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-5837 get pod pause -L testing-label'
  Dec 16 17:18:22.975: INFO: stderr: ""
  Dec 16 17:18:22.975: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
  STEP: using delete to clean up resources @ 12/16/23 17:18:22.976
  Dec 16 17:18:22.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-5837 delete --grace-period=0 --force -f -'
  E1216 17:18:23.129498      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:18:23.142: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Dec 16 17:18:23.142: INFO: stdout: "pod \"pause\" force deleted\n"
  Dec 16 17:18:23.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-5837 get rc,svc -l name=pause --no-headers'
  Dec 16 17:18:23.368: INFO: stderr: "No resources found in kubectl-5837 namespace.\n"
  Dec 16 17:18:23.368: INFO: stdout: ""
  Dec 16 17:18:23.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-5837 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Dec 16 17:18:23.556: INFO: stderr: ""
  Dec 16 17:18:23.556: INFO: stdout: ""
  Dec 16 17:18:23.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5837" for this suite. @ 12/16/23 17:18:23.569
• [4.330 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 12/16/23 17:18:23.585
  Dec 16 17:18:23.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename webhook @ 12/16/23 17:18:23.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:18:23.622
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:18:23.628
  STEP: Setting up server cert @ 12/16/23 17:18:23.68
  E1216 17:18:24.130016      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:25.131107      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/16/23 17:18:25.334
  STEP: Deploying the webhook pod @ 12/16/23 17:18:25.363
  STEP: Wait for the deployment to be ready @ 12/16/23 17:18:25.4
  Dec 16 17:18:25.420: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E1216 17:18:26.131481      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:27.131881      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/16/23 17:18:27.444
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 17:18:27.47
  E1216 17:18:28.133082      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:18:28.471: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 12/16/23 17:18:28.484
  STEP: create a pod that should be updated by the webhook @ 12/16/23 17:18:28.535
  Dec 16 17:18:28.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2378" for this suite. @ 12/16/23 17:18:28.886
  STEP: Destroying namespace "webhook-markers-8147" for this suite. @ 12/16/23 17:18:28.921
• [5.393 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 12/16/23 17:18:28.994
  Dec 16 17:18:28.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename webhook @ 12/16/23 17:18:29.015
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:18:29.059
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:18:29.067
  E1216 17:18:29.132827      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 12/16/23 17:18:29.133
  E1216 17:18:30.133765      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:31.133494      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/16/23 17:18:31.173
  STEP: Deploying the webhook pod @ 12/16/23 17:18:31.194
  STEP: Wait for the deployment to be ready @ 12/16/23 17:18:31.226
  Dec 16 17:18:31.243: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E1216 17:18:32.134050      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:33.134022      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/16/23 17:18:33.27
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 17:18:33.306
  E1216 17:18:34.134219      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:18:34.308: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 12/16/23 17:18:34.48
  STEP: Creating a configMap that should be mutated @ 12/16/23 17:18:34.531
  STEP: Deleting the collection of validation webhooks @ 12/16/23 17:18:34.632
  STEP: Creating a configMap that should not be mutated @ 12/16/23 17:18:34.779
  Dec 16 17:18:34.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-211" for this suite. @ 12/16/23 17:18:34.972
  STEP: Destroying namespace "webhook-markers-8559" for this suite. @ 12/16/23 17:18:34.993
• [6.025 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 12/16/23 17:18:35.025
  Dec 16 17:18:35.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename replicaset @ 12/16/23 17:18:35.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:18:35.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:18:35.1
  STEP: Create a Replicaset @ 12/16/23 17:18:35.119
  E1216 17:18:35.134458      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verify that the required pods have come up. @ 12/16/23 17:18:35.157
  Dec 16 17:18:35.169: INFO: Pod name sample-pod: Found 0 pods out of 1
  E1216 17:18:36.134716      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:37.134785      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:38.135928      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:39.136955      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:40.137130      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:18:40.178: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 12/16/23 17:18:40.178
  STEP: Getting /status @ 12/16/23 17:18:40.179
  Dec 16 17:18:40.191: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 12/16/23 17:18:40.191
  Dec 16 17:18:40.216: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 12/16/23 17:18:40.216
  Dec 16 17:18:40.223: INFO: Observed &ReplicaSet event: ADDED
  Dec 16 17:18:40.224: INFO: Observed &ReplicaSet event: MODIFIED
  Dec 16 17:18:40.224: INFO: Observed &ReplicaSet event: MODIFIED
  Dec 16 17:18:40.225: INFO: Observed &ReplicaSet event: MODIFIED
  Dec 16 17:18:40.225: INFO: Found replicaset test-rs in namespace replicaset-3042 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Dec 16 17:18:40.225: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 12/16/23 17:18:40.225
  Dec 16 17:18:40.225: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Dec 16 17:18:40.244: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 12/16/23 17:18:40.244
  Dec 16 17:18:40.249: INFO: Observed &ReplicaSet event: ADDED
  Dec 16 17:18:40.249: INFO: Observed &ReplicaSet event: MODIFIED
  Dec 16 17:18:40.249: INFO: Observed &ReplicaSet event: MODIFIED
  Dec 16 17:18:40.250: INFO: Observed &ReplicaSet event: MODIFIED
  Dec 16 17:18:40.250: INFO: Observed replicaset test-rs in namespace replicaset-3042 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Dec 16 17:18:40.250: INFO: Observed &ReplicaSet event: MODIFIED
  Dec 16 17:18:40.250: INFO: Found replicaset test-rs in namespace replicaset-3042 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Dec 16 17:18:40.250: INFO: Replicaset test-rs has a patched status
  Dec 16 17:18:40.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3042" for this suite. @ 12/16/23 17:18:40.266
• [5.260 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:638
  STEP: Creating a kubernetes client @ 12/16/23 17:18:40.289
  Dec 16 17:18:40.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename statefulset @ 12/16/23 17:18:40.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:18:40.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:18:40.329
  STEP: Creating service test in namespace statefulset-7176 @ 12/16/23 17:18:40.336
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 12/16/23 17:18:40.35
  STEP: Creating stateful set ss in namespace statefulset-7176 @ 12/16/23 17:18:40.363
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7176 @ 12/16/23 17:18:40.383
  Dec 16 17:18:40.392: INFO: Found 0 stateful pods, waiting for 1
  E1216 17:18:41.138249      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:42.138831      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:43.139011      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:44.140258      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:45.140311      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:46.140156      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:47.156392      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:48.148852      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:49.150165      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:50.150785      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:18:50.404: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 12/16/23 17:18:50.405
  Dec 16 17:18:50.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-7176 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Dec 16 17:18:50.750: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 16 17:18:50.750: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 16 17:18:50.750: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Dec 16 17:18:50.758: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E1216 17:18:51.151752      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:52.152491      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:53.153130      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:54.153222      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:55.153374      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:56.153721      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:57.153796      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:58.154241      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:18:59.154427      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:00.154685      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:00.773: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Dec 16 17:19:00.773: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 16 17:19:00.818: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999218s
  E1216 17:19:01.155765      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:01.826: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.989720916s
  E1216 17:19:02.156005      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:02.834: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.980686735s
  E1216 17:19:03.155940      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:03.844: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.973286005s
  E1216 17:19:04.157228      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:04.856: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.962936225s
  E1216 17:19:05.157999      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:05.866: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.950701635s
  E1216 17:19:06.157981      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:06.876: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.940907799s
  E1216 17:19:07.158050      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:07.886: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.930461522s
  E1216 17:19:08.158151      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:08.896: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.921107442s
  E1216 17:19:09.158386      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:09.906: INFO: Verifying statefulset ss doesn't scale past 1 for another 911.606471ms
  E1216 17:19:10.159064      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7176 @ 12/16/23 17:19:10.906
  Dec 16 17:19:10.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-7176 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E1216 17:19:11.159805      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:11.239: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Dec 16 17:19:11.239: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Dec 16 17:19:11.239: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Dec 16 17:19:11.248: INFO: Found 1 stateful pods, waiting for 3
  E1216 17:19:12.160498      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:13.161197      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:14.161482      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:15.163553      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:16.163794      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:17.163953      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:18.164654      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:19.165516      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:20.166238      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:21.166148      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:21.265: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Dec 16 17:19:21.265: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Dec 16 17:19:21.266: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 12/16/23 17:19:21.266
  STEP: Scale down will halt with unhealthy stateful pod @ 12/16/23 17:19:21.267
  Dec 16 17:19:21.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-7176 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Dec 16 17:19:21.629: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 16 17:19:21.629: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 16 17:19:21.629: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Dec 16 17:19:21.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-7176 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Dec 16 17:19:21.950: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 16 17:19:21.950: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 16 17:19:21.950: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Dec 16 17:19:21.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-7176 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E1216 17:19:22.167297      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:22.330: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 16 17:19:22.330: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 16 17:19:22.330: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Dec 16 17:19:22.330: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 16 17:19:22.348: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  E1216 17:19:23.167919      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:24.168043      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:25.168258      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:26.169143      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:27.170218      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:28.170447      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:29.171598      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:30.171837      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:31.172583      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:32.172827      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:32.367: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Dec 16 17:19:32.368: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Dec 16 17:19:32.368: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Dec 16 17:19:32.402: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999474s
  E1216 17:19:33.173571      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:33.411: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987665005s
  E1216 17:19:34.174078      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:34.420: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.979412028s
  E1216 17:19:35.174271      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:35.430: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.970164457s
  E1216 17:19:36.175246      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:36.441: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.9600312s
  E1216 17:19:37.175774      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:37.455: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.949475277s
  E1216 17:19:38.176078      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:38.467: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.934240922s
  E1216 17:19:39.177572      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:39.479: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.922146586s
  E1216 17:19:40.177667      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:40.491: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.910255565s
  E1216 17:19:41.177757      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:41.504: INFO: Verifying statefulset ss doesn't scale past 3 for another 898.209222ms
  E1216 17:19:42.177833      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7176 @ 12/16/23 17:19:42.505
  Dec 16 17:19:42.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-7176 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Dec 16 17:19:42.856: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Dec 16 17:19:42.856: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Dec 16 17:19:42.856: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Dec 16 17:19:42.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-7176 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E1216 17:19:43.178350      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:43.210: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Dec 16 17:19:43.210: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Dec 16 17:19:43.210: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Dec 16 17:19:43.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=statefulset-7176 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Dec 16 17:19:43.544: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Dec 16 17:19:43.544: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Dec 16 17:19:43.544: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Dec 16 17:19:43.544: INFO: Scaling statefulset ss to 0
  E1216 17:19:44.179196      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:45.191450      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:46.183976      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:47.186564      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:48.185810      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:49.187423      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:50.186566      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:51.186628      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:52.186808      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:53.187619      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 12/16/23 17:19:53.581
  Dec 16 17:19:53.581: INFO: Deleting all statefulset in ns statefulset-7176
  Dec 16 17:19:53.589: INFO: Scaling statefulset ss to 0
  Dec 16 17:19:53.628: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 16 17:19:53.636: INFO: Deleting statefulset ss
  Dec 16 17:19:53.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7176" for this suite. @ 12/16/23 17:19:53.695
• [73.419 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 12/16/23 17:19:53.712
  Dec 16 17:19:53.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubectl @ 12/16/23 17:19:53.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:19:53.76
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:19:53.768
  STEP: Starting the proxy @ 12/16/23 17:19:53.774
  Dec 16 17:19:53.777: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-6176 proxy --unix-socket=/tmp/kubectl-proxy-unix1644656500/test'
  STEP: retrieving proxy /api/ output @ 12/16/23 17:19:53.909
  Dec 16 17:19:53.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6176" for this suite. @ 12/16/23 17:19:53.922
• [0.225 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 12/16/23 17:19:53.941
  Dec 16 17:19:53.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename webhook @ 12/16/23 17:19:53.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:19:53.986
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:19:53.992
  STEP: Setting up server cert @ 12/16/23 17:19:54.054
  E1216 17:19:54.188295      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/16/23 17:19:55.062
  STEP: Deploying the webhook pod @ 12/16/23 17:19:55.086
  STEP: Wait for the deployment to be ready @ 12/16/23 17:19:55.121
  Dec 16 17:19:55.146: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E1216 17:19:55.190001      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:19:56.190270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/16/23 17:19:57.17
  E1216 17:19:57.190815      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 17:19:57.198
  E1216 17:19:58.191520      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:19:58.199: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 12/16/23 17:19:58.212
  STEP: create a configmap that should be updated by the webhook @ 12/16/23 17:19:58.252
  Dec 16 17:19:58.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-281" for this suite. @ 12/16/23 17:19:58.464
  STEP: Destroying namespace "webhook-markers-8533" for this suite. @ 12/16/23 17:19:58.479
• [4.550 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 12/16/23 17:19:58.493
  Dec 16 17:19:58.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename var-expansion @ 12/16/23 17:19:58.497
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:19:58.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:19:58.538
  STEP: Creating a pod to test substitution in container's args @ 12/16/23 17:19:58.543
  E1216 17:19:59.193911      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:00.194296      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:01.194721      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:02.195214      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:20:02.657
  Dec 16 17:20:02.665: INFO: Trying to get logs from node phoh7xai9ouk-3 pod var-expansion-f69c63cb-dee7-48b5-a305-60911b7a915b container dapi-container: <nil>
  STEP: delete the pod @ 12/16/23 17:20:02.728
  Dec 16 17:20:02.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3764" for this suite. @ 12/16/23 17:20:02.808
• [4.338 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 12/16/23 17:20:02.841
  Dec 16 17:20:02.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename limitrange @ 12/16/23 17:20:02.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:20:02.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:20:02.896
  STEP: Creating a LimitRange @ 12/16/23 17:20:02.907
  STEP: Setting up watch @ 12/16/23 17:20:02.907
  STEP: Submitting a LimitRange @ 12/16/23 17:20:03.022
  STEP: Verifying LimitRange creation was observed @ 12/16/23 17:20:03.037
  STEP: Fetching the LimitRange to ensure it has proper values @ 12/16/23 17:20:03.037
  Dec 16 17:20:03.048: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Dec 16 17:20:03.049: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 12/16/23 17:20:03.049
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 12/16/23 17:20:03.065
  Dec 16 17:20:03.077: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Dec 16 17:20:03.077: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 12/16/23 17:20:03.077
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 12/16/23 17:20:03.098
  Dec 16 17:20:03.122: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Dec 16 17:20:03.124: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 12/16/23 17:20:03.124
  STEP: Failing to create a Pod with more than max resources @ 12/16/23 17:20:03.139
  STEP: Updating a LimitRange @ 12/16/23 17:20:03.147
  STEP: Verifying LimitRange updating is effective @ 12/16/23 17:20:03.162
  E1216 17:20:03.195383      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:04.195802      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 12/16/23 17:20:05.174
  STEP: Failing to create a Pod with more than max resources @ 12/16/23 17:20:05.191
  E1216 17:20:05.197239      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a LimitRange @ 12/16/23 17:20:05.197
  STEP: Verifying the LimitRange was deleted @ 12/16/23 17:20:05.22
  E1216 17:20:06.197413      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:07.197711      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:08.198674      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:09.199656      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:10.200355      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:20:10.235: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 12/16/23 17:20:10.236
  Dec 16 17:20:10.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-1616" for this suite. @ 12/16/23 17:20:10.269
• [7.447 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 12/16/23 17:20:10.29
  Dec 16 17:20:10.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename emptydir @ 12/16/23 17:20:10.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:20:10.325
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:20:10.333
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 12/16/23 17:20:10.34
  E1216 17:20:11.202650      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:12.202802      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:13.203163      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:14.204172      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:20:14.415
  Dec 16 17:20:14.429: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-8504074a-9a1d-4521-aa39-83a8cd5a9376 container test-container: <nil>
  STEP: delete the pod @ 12/16/23 17:20:14.452
  Dec 16 17:20:14.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-995" for this suite. @ 12/16/23 17:20:14.524
• [4.258 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 12/16/23 17:20:14.555
  Dec 16 17:20:14.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename security-context-test @ 12/16/23 17:20:14.56
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:20:14.609
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:20:14.618
  E1216 17:20:15.204031      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:16.205328      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:17.205734      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:18.205641      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:20:18.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9051" for this suite. @ 12/16/23 17:20:18.716
• [4.179 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 12/16/23 17:20:18.738
  Dec 16 17:20:18.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename namespaces @ 12/16/23 17:20:18.743
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:20:18.778
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:20:18.792
  STEP: Creating a test namespace @ 12/16/23 17:20:18.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:20:18.836
  STEP: Creating a service in the namespace @ 12/16/23 17:20:18.842
  STEP: Deleting the namespace @ 12/16/23 17:20:18.876
  STEP: Waiting for the namespace to be removed. @ 12/16/23 17:20:18.905
  E1216 17:20:19.206402      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:20.206496      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:21.206797      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:22.206947      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:23.208080      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:24.209126      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 12/16/23 17:20:24.919
  STEP: Verifying there is no service in the namespace @ 12/16/23 17:20:24.955
  Dec 16 17:20:24.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8758" for this suite. @ 12/16/23 17:20:24.972
  STEP: Destroying namespace "nsdeletetest-7432" for this suite. @ 12/16/23 17:20:24.992
  Dec 16 17:20:25.001: INFO: Namespace nsdeletetest-7432 was already deleted
  STEP: Destroying namespace "nsdeletetest-1188" for this suite. @ 12/16/23 17:20:25.001
• [6.282 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 12/16/23 17:20:25.022
  Dec 16 17:20:25.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename pod-network-test @ 12/16/23 17:20:25.025
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:20:25.062
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:20:25.068
  STEP: Performing setup for networking test in namespace pod-network-test-202 @ 12/16/23 17:20:25.073
  STEP: creating a selector @ 12/16/23 17:20:25.074
  STEP: Creating the service pods in kubernetes @ 12/16/23 17:20:25.074
  Dec 16 17:20:25.074: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E1216 17:20:25.209942      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:26.210656      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:27.211013      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:28.212003      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:29.212433      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:30.213390      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:31.214350      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:32.214521      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:33.214707      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:34.215052      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:35.215209      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:36.216549      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:37.216920      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 12/16/23 17:20:37.297
  E1216 17:20:38.218203      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:39.218969      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:20:39.372: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Dec 16 17:20:39.372: INFO: Going to poll 10.233.64.40 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Dec 16 17:20:39.382: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.40:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-202 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:20:39.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:20:39.385: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:20:39.386: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-202/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.40%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Dec 16 17:20:39.596: INFO: Found all 1 expected endpoints: [netserver-0]
  Dec 16 17:20:39.597: INFO: Going to poll 10.233.65.38 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Dec 16 17:20:39.608: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.38:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-202 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:20:39.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:20:39.611: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:20:39.612: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-202/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.65.38%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Dec 16 17:20:39.747: INFO: Found all 1 expected endpoints: [netserver-1]
  Dec 16 17:20:39.748: INFO: Going to poll 10.233.66.187 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Dec 16 17:20:39.757: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.187:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-202 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:20:39.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:20:39.759: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:20:39.760: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-202/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.187%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Dec 16 17:20:39.884: INFO: Found all 1 expected endpoints: [netserver-2]
  Dec 16 17:20:39.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-202" for this suite. @ 12/16/23 17:20:39.899
• [14.896 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 12/16/23 17:20:39.925
  Dec 16 17:20:39.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename svcaccounts @ 12/16/23 17:20:39.928
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:20:39.961
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:20:39.966
  STEP: Creating a pod to test service account token:  @ 12/16/23 17:20:39.971
  E1216 17:20:40.219411      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:41.220454      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:42.221054      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:43.221724      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:20:44.021
  Dec 16 17:20:44.029: INFO: Trying to get logs from node phoh7xai9ouk-3 pod test-pod-a93f8fd8-391a-447f-b6b2-04da40d6d1e7 container agnhost-container: <nil>
  STEP: delete the pod @ 12/16/23 17:20:44.047
  Dec 16 17:20:44.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3895" for this suite. @ 12/16/23 17:20:44.1
• [4.192 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 12/16/23 17:20:44.121
  Dec 16 17:20:44.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename security-context-test @ 12/16/23 17:20:44.124
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:20:44.16
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:20:44.166
  E1216 17:20:44.223149      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:45.223249      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:46.230128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:47.230404      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:48.230960      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:20:48.270: INFO: Got logs for pod "busybox-privileged-false-24bf467d-0471-49c6-9865-e3c13b83e68d": "ip: RTNETLINK answers: Operation not permitted\n"
  Dec 16 17:20:48.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5756" for this suite. @ 12/16/23 17:20:48.285
• [4.185 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 12/16/23 17:20:48.309
  Dec 16 17:20:48.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename ingress @ 12/16/23 17:20:48.312
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:20:48.343
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:20:48.349
  STEP: getting /apis @ 12/16/23 17:20:48.355
  STEP: getting /apis/networking.k8s.io @ 12/16/23 17:20:48.362
  STEP: getting /apis/networking.k8s.iov1 @ 12/16/23 17:20:48.364
  STEP: creating @ 12/16/23 17:20:48.367
  STEP: getting @ 12/16/23 17:20:48.399
  STEP: listing @ 12/16/23 17:20:48.409
  STEP: watching @ 12/16/23 17:20:48.418
  Dec 16 17:20:48.418: INFO: starting watch
  STEP: cluster-wide listing @ 12/16/23 17:20:48.421
  STEP: cluster-wide watching @ 12/16/23 17:20:48.428
  Dec 16 17:20:48.428: INFO: starting watch
  STEP: patching @ 12/16/23 17:20:48.43
  STEP: updating @ 12/16/23 17:20:48.444
  Dec 16 17:20:48.464: INFO: waiting for watch events with expected annotations
  Dec 16 17:20:48.464: INFO: saw patched and updated annotations
  STEP: patching /status @ 12/16/23 17:20:48.465
  STEP: updating /status @ 12/16/23 17:20:48.479
  STEP: get /status @ 12/16/23 17:20:48.506
  STEP: deleting @ 12/16/23 17:20:48.515
  STEP: deleting a collection @ 12/16/23 17:20:48.55
  Dec 16 17:20:48.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-1465" for this suite. @ 12/16/23 17:20:48.666
• [0.367 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 12/16/23 17:20:48.687
  Dec 16 17:20:48.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename subpath @ 12/16/23 17:20:48.691
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:20:48.814
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:20:48.819
  STEP: Setting up data @ 12/16/23 17:20:48.826
  STEP: Creating pod pod-subpath-test-configmap-cx64 @ 12/16/23 17:20:48.846
  STEP: Creating a pod to test atomic-volume-subpath @ 12/16/23 17:20:48.846
  E1216 17:20:49.230951      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:50.231458      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:51.232153      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:52.233733      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:53.238698      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:54.238247      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:55.240356      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:56.241838      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:57.241204      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:58.243798      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:20:59.242127      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:00.243644      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:01.243446      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:02.243913      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:03.244883      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:04.245241      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:05.246292      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:06.246198      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:07.247214      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:08.247453      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:09.247666      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:10.247988      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:11.248389      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:12.249264      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:21:13.012
  Dec 16 17:21:13.020: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-subpath-test-configmap-cx64 container test-container-subpath-configmap-cx64: <nil>
  STEP: delete the pod @ 12/16/23 17:21:13.042
  STEP: Deleting pod pod-subpath-test-configmap-cx64 @ 12/16/23 17:21:13.082
  Dec 16 17:21:13.083: INFO: Deleting pod "pod-subpath-test-configmap-cx64" in namespace "subpath-963"
  Dec 16 17:21:13.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-963" for this suite. @ 12/16/23 17:21:13.106
• [24.433 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:350
  STEP: Creating a kubernetes client @ 12/16/23 17:21:13.121
  Dec 16 17:21:13.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename field-validation @ 12/16/23 17:21:13.128
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:21:13.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:21:13.198
  Dec 16 17:21:13.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  W1216 17:21:13.208414      13 field_validation.go:423] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc00156c250 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E1216 17:21:13.249907      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:14.250838      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:15.251907      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  W1216 17:21:16.007179      13 warnings.go:70] unknown field "alpha"
  W1216 17:21:16.007278      13 warnings.go:70] unknown field "beta"
  W1216 17:21:16.007309      13 warnings.go:70] unknown field "delta"
  W1216 17:21:16.007384      13 warnings.go:70] unknown field "epsilon"
  W1216 17:21:16.007487      13 warnings.go:70] unknown field "gamma"
  E1216 17:21:16.251980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:21:16.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8514" for this suite. @ 12/16/23 17:21:16.721
• [3.618 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 12/16/23 17:21:16.747
  Dec 16 17:21:16.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename ingressclass @ 12/16/23 17:21:16.751
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:21:16.812
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:21:16.819
  STEP: getting /apis @ 12/16/23 17:21:16.826
  STEP: getting /apis/networking.k8s.io @ 12/16/23 17:21:16.839
  STEP: getting /apis/networking.k8s.iov1 @ 12/16/23 17:21:16.842
  STEP: creating @ 12/16/23 17:21:16.845
  STEP: getting @ 12/16/23 17:21:16.885
  STEP: listing @ 12/16/23 17:21:16.901
  STEP: watching @ 12/16/23 17:21:16.915
  Dec 16 17:21:16.916: INFO: starting watch
  STEP: patching @ 12/16/23 17:21:16.922
  STEP: updating @ 12/16/23 17:21:16.941
  Dec 16 17:21:16.956: INFO: waiting for watch events with expected annotations
  Dec 16 17:21:16.956: INFO: saw patched and updated annotations
  STEP: deleting @ 12/16/23 17:21:16.956
  STEP: deleting a collection @ 12/16/23 17:21:17.005
  Dec 16 17:21:17.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-5105" for this suite. @ 12/16/23 17:21:17.063
• [0.337 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 12/16/23 17:21:17.085
  Dec 16 17:21:17.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename custom-resource-definition @ 12/16/23 17:21:17.087
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:21:17.127
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:21:17.134
  STEP: fetching the /apis discovery document @ 12/16/23 17:21:17.14
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 12/16/23 17:21:17.142
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 12/16/23 17:21:17.142
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 12/16/23 17:21:17.142
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 12/16/23 17:21:17.143
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 12/16/23 17:21:17.144
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 12/16/23 17:21:17.145
  Dec 16 17:21:17.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8992" for this suite. @ 12/16/23 17:21:17.158
• [0.100 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 12/16/23 17:21:17.19
  Dec 16 17:21:17.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-probe @ 12/16/23 17:21:17.193
  E1216 17:21:17.252806      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:21:17.262
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:21:17.269
  STEP: Creating pod busybox-630f50a0-7957-4a7a-84bd-48c489161663 in namespace container-probe-2909 @ 12/16/23 17:21:17.276
  E1216 17:21:18.253496      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:19.256094      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:21:19.315: INFO: Started pod busybox-630f50a0-7957-4a7a-84bd-48c489161663 in namespace container-probe-2909
  STEP: checking the pod's current state and verifying that restartCount is present @ 12/16/23 17:21:19.315
  Dec 16 17:21:19.321: INFO: Initial restart count of pod busybox-630f50a0-7957-4a7a-84bd-48c489161663 is 0
  E1216 17:21:20.254774      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:21.254827      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:22.256362      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:23.256156      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:24.256285      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:25.256252      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:26.256551      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:27.257185      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:28.257795      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:29.261323      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:30.260267      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:31.261428      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:32.261756      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:33.262015      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:34.262735      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:35.263091      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:36.263340      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:37.263507      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:38.264311      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:39.264517      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:40.265001      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:41.265813      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:42.266114      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:43.266030      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:44.266196      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:45.266723      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:46.267308      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:47.267226      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:48.267266      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:49.267762      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:50.273937      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:51.271776      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:52.278079      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:53.274051      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:54.274386      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:55.274548      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:56.274933      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:57.275214      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:58.276035      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:21:59.276317      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:00.276373      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:01.277509      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:02.278462      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:03.279371      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:04.279938      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:05.280665      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:06.280984      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:07.281145      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:08.281794      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:09.282299      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:22:09.593: INFO: Restart count of pod container-probe-2909/busybox-630f50a0-7957-4a7a-84bd-48c489161663 is now 1 (50.271156354s elapsed)
  Dec 16 17:22:09.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/16/23 17:22:09.609
  STEP: Destroying namespace "container-probe-2909" for this suite. @ 12/16/23 17:22:09.638
• [52.473 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 12/16/23 17:22:09.667
  Dec 16 17:22:09.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename containers @ 12/16/23 17:22:09.672
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:22:09.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:22:09.718
  STEP: Creating a pod to test override command @ 12/16/23 17:22:09.727
  E1216 17:22:10.282365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:11.282677      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:12.283709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:13.284522      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:22:13.783
  Dec 16 17:22:13.797: INFO: Trying to get logs from node phoh7xai9ouk-3 pod client-containers-05d2013d-2683-445a-b466-aed8b9c236e4 container agnhost-container: <nil>
  STEP: delete the pod @ 12/16/23 17:22:13.816
  Dec 16 17:22:13.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8511" for this suite. @ 12/16/23 17:22:13.873
• [4.226 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 12/16/23 17:22:13.895
  Dec 16 17:22:13.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 17:22:13.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:22:13.938
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:22:13.948
  STEP: Creating a pod to test downward API volume plugin @ 12/16/23 17:22:13.96
  E1216 17:22:14.285175      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:15.285574      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:16.285609      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:17.286566      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:22:18.024
  Dec 16 17:22:18.039: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downwardapi-volume-146793cd-1957-4796-b5f5-b8eb8f3bc505 container client-container: <nil>
  STEP: delete the pod @ 12/16/23 17:22:18.065
  Dec 16 17:22:18.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8972" for this suite. @ 12/16/23 17:22:18.112
• [4.232 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 12/16/23 17:22:18.13
  Dec 16 17:22:18.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename dns @ 12/16/23 17:22:18.133
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:22:18.195
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:22:18.202
  STEP: Creating a test headless service @ 12/16/23 17:22:18.21
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2299.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2299.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2299.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2299.svc.cluster.local;sleep 1; done
   @ 12/16/23 17:22:18.225
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2299.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2299.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2299.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2299.svc.cluster.local;sleep 1; done
   @ 12/16/23 17:22:18.225
  STEP: creating a pod to probe DNS @ 12/16/23 17:22:18.225
  STEP: submitting the pod to kubernetes @ 12/16/23 17:22:18.225
  E1216 17:22:18.287292      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:19.288107      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:20.289635      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 12/16/23 17:22:20.313
  STEP: looking for the results for each expected name from probers @ 12/16/23 17:22:20.323
  Dec 16 17:22:20.342: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:20.353: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:20.363: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:20.389: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:20.402: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:20.412: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:20.424: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:20.433: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:20.434: INFO: Lookups using dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2299.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2299.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local jessie_udp@dns-test-service-2.dns-2299.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2299.svc.cluster.local]

  E1216 17:22:21.290266      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:22.294437      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:23.294603      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:24.295356      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:25.295573      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:22:25.454: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:25.471: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:25.478: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:25.492: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:25.501: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:25.518: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:25.528: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:25.540: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:25.540: INFO: Lookups using dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2299.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2299.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local jessie_udp@dns-test-service-2.dns-2299.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2299.svc.cluster.local]

  E1216 17:22:26.296117      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:27.297596      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:28.297747      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:29.298856      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:30.299188      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:22:30.455: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:30.465: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:30.473: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:30.483: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:30.495: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:30.503: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:30.511: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:30.520: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:30.520: INFO: Lookups using dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2299.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2299.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local jessie_udp@dns-test-service-2.dns-2299.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2299.svc.cluster.local]

  E1216 17:22:31.299754      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:32.301160      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:33.301652      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:34.301780      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:35.301709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:22:35.449: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:35.462: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:35.476: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:35.491: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:35.509: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:35.524: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:35.538: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:35.550: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:35.550: INFO: Lookups using dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2299.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2299.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local jessie_udp@dns-test-service-2.dns-2299.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2299.svc.cluster.local]

  E1216 17:22:36.302109      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:37.303460      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:38.304509      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:39.304679      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:40.321670      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:22:40.453: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:40.461: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:40.471: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:40.478: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:40.486: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:40.492: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:40.499: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:40.505: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:40.505: INFO: Lookups using dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2299.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2299.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local jessie_udp@dns-test-service-2.dns-2299.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2299.svc.cluster.local]

  E1216 17:22:41.306101      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:42.308898      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:43.308984      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:44.316145      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:45.310352      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:22:45.453: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:45.462: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:45.474: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:45.482: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:45.490: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:45.501: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:45.509: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:45.525: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2299.svc.cluster.local from pod dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0: the server could not find the requested resource (get pods dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0)
  Dec 16 17:22:45.525: INFO: Lookups using dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2299.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2299.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2299.svc.cluster.local jessie_udp@dns-test-service-2.dns-2299.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2299.svc.cluster.local]

  E1216 17:22:46.310597      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:47.311072      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:48.312006      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:49.312264      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:50.312519      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:22:50.517: INFO: DNS probes using dns-2299/dns-test-4e71cf69-4a53-48b7-9cfd-a52db09fa2c0 succeeded

  Dec 16 17:22:50.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/16/23 17:22:50.536
  STEP: deleting the test headless service @ 12/16/23 17:22:50.605
  STEP: Destroying namespace "dns-2299" for this suite. @ 12/16/23 17:22:50.662
• [32.552 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 12/16/23 17:22:50.696
  Dec 16 17:22:50.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename services @ 12/16/23 17:22:50.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:22:50.765
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:22:50.776
  STEP: creating service multi-endpoint-test in namespace services-7159 @ 12/16/23 17:22:50.789
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7159 to expose endpoints map[] @ 12/16/23 17:22:50.825
  Dec 16 17:22:50.854: INFO: successfully validated that service multi-endpoint-test in namespace services-7159 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-7159 @ 12/16/23 17:22:50.854
  E1216 17:22:51.312885      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:52.313212      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7159 to expose endpoints map[pod1:[100]] @ 12/16/23 17:22:52.95
  Dec 16 17:22:52.977: INFO: successfully validated that service multi-endpoint-test in namespace services-7159 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-7159 @ 12/16/23 17:22:52.977
  E1216 17:22:53.314140      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:54.316983      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7159 to expose endpoints map[pod1:[100] pod2:[101]] @ 12/16/23 17:22:55.029
  Dec 16 17:22:55.068: INFO: successfully validated that service multi-endpoint-test in namespace services-7159 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 12/16/23 17:22:55.069
  Dec 16 17:22:55.069: INFO: Creating new exec pod
  E1216 17:22:55.315221      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:56.317292      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:22:57.317478      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:22:58.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-7159 exec execpod9hxn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  E1216 17:22:58.318521      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:22:58.543: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Dec 16 17:22:58.544: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 16 17:22:58.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-7159 exec execpod9hxn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.56.173 80'
  Dec 16 17:22:58.851: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.56.173 80\nConnection to 10.233.56.173 80 port [tcp/http] succeeded!\n"
  Dec 16 17:22:58.851: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 16 17:22:58.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-7159 exec execpod9hxn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Dec 16 17:22:59.167: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Dec 16 17:22:59.167: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 16 17:22:59.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-7159 exec execpod9hxn8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.56.173 81'
  E1216 17:22:59.319226      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:22:59.482: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.56.173 81\nConnection to 10.233.56.173 81 port [tcp/*] succeeded!\n"
  Dec 16 17:22:59.482: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-7159 @ 12/16/23 17:22:59.483
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7159 to expose endpoints map[pod2:[101]] @ 12/16/23 17:22:59.551
  Dec 16 17:22:59.593: INFO: successfully validated that service multi-endpoint-test in namespace services-7159 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-7159 @ 12/16/23 17:22:59.594
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7159 to expose endpoints map[] @ 12/16/23 17:22:59.654
  Dec 16 17:22:59.699: INFO: successfully validated that service multi-endpoint-test in namespace services-7159 exposes endpoints map[]
  Dec 16 17:22:59.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7159" for this suite. @ 12/16/23 17:22:59.757
• [9.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 12/16/23 17:22:59.786
  Dec 16 17:22:59.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename downward-api @ 12/16/23 17:22:59.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:22:59.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:22:59.837
  STEP: Creating a pod to test downward API volume plugin @ 12/16/23 17:22:59.844
  E1216 17:23:00.319917      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:01.321448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:02.321201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:03.336652      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:23:03.9
  Dec 16 17:23:03.917: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downwardapi-volume-c972ad34-427a-4633-9b13-7c20437938a9 container client-container: <nil>
  STEP: delete the pod @ 12/16/23 17:23:03.934
  Dec 16 17:23:03.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8359" for this suite. @ 12/16/23 17:23:03.985
• [4.211 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 12/16/23 17:23:04.008
  Dec 16 17:23:04.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename cronjob @ 12/16/23 17:23:04.014
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:23:04.056
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:23:04.064
  STEP: Creating a cronjob @ 12/16/23 17:23:04.076
  STEP: creating @ 12/16/23 17:23:04.076
  STEP: getting @ 12/16/23 17:23:04.1
  STEP: listing @ 12/16/23 17:23:04.108
  STEP: watching @ 12/16/23 17:23:04.119
  Dec 16 17:23:04.119: INFO: starting watch
  STEP: cluster-wide listing @ 12/16/23 17:23:04.121
  STEP: cluster-wide watching @ 12/16/23 17:23:04.129
  Dec 16 17:23:04.129: INFO: starting watch
  STEP: patching @ 12/16/23 17:23:04.133
  STEP: updating @ 12/16/23 17:23:04.156
  Dec 16 17:23:04.178: INFO: waiting for watch events with expected annotations
  Dec 16 17:23:04.178: INFO: saw patched and updated annotations
  STEP: patching /status @ 12/16/23 17:23:04.179
  STEP: updating /status @ 12/16/23 17:23:04.195
  STEP: get /status @ 12/16/23 17:23:04.212
  STEP: deleting @ 12/16/23 17:23:04.22
  STEP: deleting a collection @ 12/16/23 17:23:04.256
  Dec 16 17:23:04.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9038" for this suite. @ 12/16/23 17:23:04.308
• [0.316 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 12/16/23 17:23:04.328
  Dec 16 17:23:04.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename proxy @ 12/16/23 17:23:04.332
  E1216 17:23:04.334770      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:23:04.374
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:23:04.381
  Dec 16 17:23:04.390: INFO: Creating pod...
  E1216 17:23:05.335044      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:06.337563      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:23:06.434: INFO: Creating service...
  Dec 16 17:23:06.462: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy?method=DELETE
  Dec 16 17:23:06.489: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Dec 16 17:23:06.489: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy?method=OPTIONS
  Dec 16 17:23:06.498: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Dec 16 17:23:06.499: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy?method=PATCH
  Dec 16 17:23:06.508: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Dec 16 17:23:06.509: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy?method=POST
  Dec 16 17:23:06.526: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Dec 16 17:23:06.526: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy?method=PUT
  Dec 16 17:23:06.537: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Dec 16 17:23:06.537: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-269/services/e2e-proxy-test-service/proxy?method=DELETE
  Dec 16 17:23:06.553: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Dec 16 17:23:06.553: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-269/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Dec 16 17:23:06.583: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Dec 16 17:23:06.584: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-269/services/e2e-proxy-test-service/proxy?method=PATCH
  Dec 16 17:23:06.598: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Dec 16 17:23:06.598: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-269/services/e2e-proxy-test-service/proxy?method=POST
  Dec 16 17:23:06.610: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Dec 16 17:23:06.611: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-269/services/e2e-proxy-test-service/proxy?method=PUT
  Dec 16 17:23:06.647: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Dec 16 17:23:06.647: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy?method=GET
  Dec 16 17:23:06.656: INFO: http.Client request:GET StatusCode:301
  Dec 16 17:23:06.657: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-269/services/e2e-proxy-test-service/proxy?method=GET
  Dec 16 17:23:06.669: INFO: http.Client request:GET StatusCode:301
  Dec 16 17:23:06.670: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-269/pods/agnhost/proxy?method=HEAD
  Dec 16 17:23:06.677: INFO: http.Client request:HEAD StatusCode:301
  Dec 16 17:23:06.678: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-269/services/e2e-proxy-test-service/proxy?method=HEAD
  Dec 16 17:23:06.688: INFO: http.Client request:HEAD StatusCode:301
  Dec 16 17:23:06.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-269" for this suite. @ 12/16/23 17:23:06.705
• [2.393 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 12/16/23 17:23:06.721
  Dec 16 17:23:06.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename job @ 12/16/23 17:23:06.724
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:23:06.767
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:23:06.779
  STEP: Creating a suspended job @ 12/16/23 17:23:06.799
  STEP: Patching the Job @ 12/16/23 17:23:06.814
  STEP: Watching for Job to be patched @ 12/16/23 17:23:06.841
  Dec 16 17:23:06.845: INFO: Event ADDED observed for Job e2e-m5lhv in namespace job-5340 with labels: map[e2e-job-label:e2e-m5lhv] and annotations: map[batch.kubernetes.io/job-tracking:]
  Dec 16 17:23:06.846: INFO: Event MODIFIED found for Job e2e-m5lhv in namespace job-5340 with labels: map[e2e-job-label:e2e-m5lhv e2e-m5lhv:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 12/16/23 17:23:06.846
  STEP: Watching for Job to be updated @ 12/16/23 17:23:06.872
  Dec 16 17:23:06.876: INFO: Event MODIFIED found for Job e2e-m5lhv in namespace job-5340 with labels: map[e2e-job-label:e2e-m5lhv e2e-m5lhv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Dec 16 17:23:06.876: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 12/16/23 17:23:06.876
  Dec 16 17:23:06.883: INFO: Job: e2e-m5lhv as labels: map[e2e-job-label:e2e-m5lhv e2e-m5lhv:patched]
  STEP: Waiting for job to complete @ 12/16/23 17:23:06.883
  E1216 17:23:07.337454      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:08.338677      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:09.338961      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:10.338997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:11.339236      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:12.339781      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:13.340590      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:14.340928      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 12/16/23 17:23:14.895
  STEP: Watching for Job to be deleted @ 12/16/23 17:23:14.92
  Dec 16 17:23:14.929: INFO: Event MODIFIED observed for Job e2e-m5lhv in namespace job-5340 with labels: map[e2e-job-label:e2e-m5lhv e2e-m5lhv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Dec 16 17:23:14.930: INFO: Event MODIFIED observed for Job e2e-m5lhv in namespace job-5340 with labels: map[e2e-job-label:e2e-m5lhv e2e-m5lhv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Dec 16 17:23:14.931: INFO: Event MODIFIED observed for Job e2e-m5lhv in namespace job-5340 with labels: map[e2e-job-label:e2e-m5lhv e2e-m5lhv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Dec 16 17:23:14.932: INFO: Event MODIFIED observed for Job e2e-m5lhv in namespace job-5340 with labels: map[e2e-job-label:e2e-m5lhv e2e-m5lhv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Dec 16 17:23:14.933: INFO: Event MODIFIED observed for Job e2e-m5lhv in namespace job-5340 with labels: map[e2e-job-label:e2e-m5lhv e2e-m5lhv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Dec 16 17:23:14.933: INFO: Event DELETED found for Job e2e-m5lhv in namespace job-5340 with labels: map[e2e-job-label:e2e-m5lhv e2e-m5lhv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 12/16/23 17:23:14.933
  Dec 16 17:23:14.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5340" for this suite. @ 12/16/23 17:23:14.961
• [8.267 seconds]
------------------------------
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 12/16/23 17:23:14.996
  Dec 16 17:23:14.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename sysctl @ 12/16/23 17:23:15.005
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:23:15.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:23:15.039
  STEP: Creating a pod with one valid and two invalid sysctls @ 12/16/23 17:23:15.045
  Dec 16 17:23:15.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-5508" for this suite. @ 12/16/23 17:23:15.065
• [0.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 12/16/23 17:23:15.083
  Dec 16 17:23:15.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename subpath @ 12/16/23 17:23:15.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:23:15.114
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:23:15.121
  STEP: Setting up data @ 12/16/23 17:23:15.127
  STEP: Creating pod pod-subpath-test-downwardapi-m5h6 @ 12/16/23 17:23:15.157
  STEP: Creating a pod to test atomic-volume-subpath @ 12/16/23 17:23:15.157
  E1216 17:23:15.342119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:16.342458      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:17.342730      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:18.343911      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:19.346723      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:20.345088      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:21.346128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:22.346577      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:23.348377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:24.347354      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:25.349978      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:26.349967      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:27.350461      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:28.350677      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:29.351856      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:30.351939      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:31.352527      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:32.353020      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:33.354511      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:34.354470      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:35.355548      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:36.356039      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:37.356489      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:38.356699      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:23:39.354
  E1216 17:23:39.357004      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:23:39.364: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-subpath-test-downwardapi-m5h6 container test-container-subpath-downwardapi-m5h6: <nil>
  STEP: delete the pod @ 12/16/23 17:23:39.391
  STEP: Deleting pod pod-subpath-test-downwardapi-m5h6 @ 12/16/23 17:23:39.436
  Dec 16 17:23:39.436: INFO: Deleting pod "pod-subpath-test-downwardapi-m5h6" in namespace "subpath-1532"
  Dec 16 17:23:39.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1532" for this suite. @ 12/16/23 17:23:39.46
• [24.394 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 12/16/23 17:23:39.481
  Dec 16 17:23:39.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename resourcequota @ 12/16/23 17:23:39.486
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:23:39.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:23:39.538
  STEP: Counting existing ResourceQuota @ 12/16/23 17:23:39.548
  E1216 17:23:40.357360      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:41.357852      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:42.358054      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:43.358412      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:44.359200      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 12/16/23 17:23:44.558
  STEP: Ensuring resource quota status is calculated @ 12/16/23 17:23:44.574
  E1216 17:23:45.359822      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:46.360888      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 12/16/23 17:23:46.585
  STEP: Creating a NodePort Service @ 12/16/23 17:23:46.636
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 12/16/23 17:23:46.705
  STEP: Ensuring resource quota status captures service creation @ 12/16/23 17:23:46.764
  E1216 17:23:47.361396      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:48.361939      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 12/16/23 17:23:48.776
  STEP: Ensuring resource quota status released usage @ 12/16/23 17:23:48.873
  E1216 17:23:49.361971      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:50.362144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:23:50.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7045" for this suite. @ 12/16/23 17:23:50.892
• [11.425 seconds]
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 12/16/23 17:23:50.907
  Dec 16 17:23:50.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename gc @ 12/16/23 17:23:50.911
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:23:50.946
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:23:50.953
  STEP: create the deployment @ 12/16/23 17:23:50.958
  W1216 17:23:50.971539      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 12/16/23 17:23:50.971
  E1216 17:23:51.363160      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the deployment @ 12/16/23 17:23:51.499
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 12/16/23 17:23:51.517
  STEP: Gathering metrics @ 12/16/23 17:23:52.081
  Dec 16 17:23:52.300: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Dec 16 17:23:52.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3738" for this suite. @ 12/16/23 17:23:52.314
• [1.423 seconds]
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 12/16/23 17:23:52.331
  Dec 16 17:23:52.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename replication-controller @ 12/16/23 17:23:52.333
  E1216 17:23:52.365323      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:23:52.373
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:23:52.38
  Dec 16 17:23:52.388: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  E1216 17:23:53.365939      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 12/16/23 17:23:53.426
  STEP: Checking rc "condition-test" has the desired failure condition set @ 12/16/23 17:23:53.446
  E1216 17:23:54.366977      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 12/16/23 17:23:54.466
  Dec 16 17:23:54.489: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 12/16/23 17:23:54.489
  Dec 16 17:23:54.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8166" for this suite. @ 12/16/23 17:23:54.515
• [2.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 12/16/23 17:23:54.534
  Dec 16 17:23:54.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubectl @ 12/16/23 17:23:54.541
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:23:54.582
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:23:54.587
  STEP: creating all guestbook components @ 12/16/23 17:23:54.592
  Dec 16 17:23:54.592: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Dec 16 17:23:54.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-7944 create -f -'
  E1216 17:23:55.367670      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:23:56.368432      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:23:56.464: INFO: stderr: ""
  Dec 16 17:23:56.464: INFO: stdout: "service/agnhost-replica created\n"
  Dec 16 17:23:56.464: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Dec 16 17:23:56.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-7944 create -f -'
  Dec 16 17:23:57.215: INFO: stderr: ""
  Dec 16 17:23:57.215: INFO: stdout: "service/agnhost-primary created\n"
  Dec 16 17:23:57.215: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Dec 16 17:23:57.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-7944 create -f -'
  E1216 17:23:57.368711      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:23:57.993: INFO: stderr: ""
  Dec 16 17:23:57.993: INFO: stdout: "service/frontend created\n"
  Dec 16 17:23:57.994: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Dec 16 17:23:57.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-7944 create -f -'
  E1216 17:23:58.369422      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:23:58.749: INFO: stderr: ""
  Dec 16 17:23:58.749: INFO: stdout: "deployment.apps/frontend created\n"
  Dec 16 17:23:58.750: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Dec 16 17:23:58.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-7944 create -f -'
  E1216 17:23:59.369417      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:23:59.532: INFO: stderr: ""
  Dec 16 17:23:59.532: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Dec 16 17:23:59.532: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Dec 16 17:23:59.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-7944 create -f -'
  E1216 17:24:00.369625      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:24:00.655: INFO: stderr: ""
  Dec 16 17:24:00.655: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 12/16/23 17:24:00.655
  Dec 16 17:24:00.656: INFO: Waiting for all frontend pods to be Running.
  Dec 16 17:24:00.707: INFO: Waiting for frontend to serve content.
  Dec 16 17:24:00.768: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
  E1216 17:24:01.369802      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:02.370469      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:03.370975      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:04.371053      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:05.371607      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:24:05.811: INFO: Trying to add a new entry to the guestbook.
  Dec 16 17:24:05.851: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 12/16/23 17:24:05.882
  Dec 16 17:24:05.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-7944 delete --grace-period=0 --force -f -'
  Dec 16 17:24:06.140: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Dec 16 17:24:06.140: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 12/16/23 17:24:06.141
  Dec 16 17:24:06.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-7944 delete --grace-period=0 --force -f -'
  E1216 17:24:06.371918      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:24:06.383: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Dec 16 17:24:06.383: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 12/16/23 17:24:06.383
  Dec 16 17:24:06.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-7944 delete --grace-period=0 --force -f -'
  Dec 16 17:24:06.613: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Dec 16 17:24:06.613: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 12/16/23 17:24:06.613
  Dec 16 17:24:06.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-7944 delete --grace-period=0 --force -f -'
  Dec 16 17:24:06.793: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Dec 16 17:24:06.793: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 12/16/23 17:24:06.794
  Dec 16 17:24:06.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-7944 delete --grace-period=0 --force -f -'
  Dec 16 17:24:07.082: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Dec 16 17:24:07.082: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 12/16/23 17:24:07.083
  Dec 16 17:24:07.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-7944 delete --grace-period=0 --force -f -'
  Dec 16 17:24:07.362: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Dec 16 17:24:07.362: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Dec 16 17:24:07.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E1216 17:24:07.373472      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "kubectl-7944" for this suite. @ 12/16/23 17:24:07.386
• [12.921 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 12/16/23 17:24:07.461
  Dec 16 17:24:07.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubectl @ 12/16/23 17:24:07.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:24:07.605
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:24:07.622
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 12/16/23 17:24:07.653
  Dec 16 17:24:07.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-7492 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Dec 16 17:24:07.888: INFO: stderr: ""
  Dec 16 17:24:07.888: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 12/16/23 17:24:07.889
  Dec 16 17:24:07.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-7492 delete pods e2e-test-httpd-pod'
  E1216 17:24:08.374553      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:09.376615      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:10.376527      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:24:10.684: INFO: stderr: ""
  Dec 16 17:24:10.684: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Dec 16 17:24:10.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7492" for this suite. @ 12/16/23 17:24:10.705
• [3.262 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 12/16/23 17:24:10.725
  Dec 16 17:24:10.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename pods @ 12/16/23 17:24:10.73
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:24:10.76
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:24:10.767
  STEP: creating the pod @ 12/16/23 17:24:10.775
  STEP: submitting the pod to kubernetes @ 12/16/23 17:24:10.776
  E1216 17:24:11.378055      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:12.378355      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 12/16/23 17:24:12.826
  STEP: updating the pod @ 12/16/23 17:24:12.834
  Dec 16 17:24:13.367: INFO: Successfully updated pod "pod-update-5a5fcf68-269b-441e-a1d9-005b5b163534"
  STEP: verifying the updated pod is in kubernetes @ 12/16/23 17:24:13.376
  E1216 17:24:13.379319      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:24:13.384: INFO: Pod update OK
  Dec 16 17:24:13.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8648" for this suite. @ 12/16/23 17:24:13.397
• [2.686 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 12/16/23 17:24:13.418
  Dec 16 17:24:13.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename webhook @ 12/16/23 17:24:13.421
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:24:13.456
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:24:13.463
  STEP: Setting up server cert @ 12/16/23 17:24:13.514
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/16/23 17:24:13.967
  STEP: Deploying the webhook pod @ 12/16/23 17:24:13.983
  STEP: Wait for the deployment to be ready @ 12/16/23 17:24:14.008
  Dec 16 17:24:14.038: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E1216 17:24:14.380088      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:15.380436      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/16/23 17:24:16.115
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 17:24:16.159
  E1216 17:24:16.381290      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:24:17.162: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Dec 16 17:24:17.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:24:17.382070      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 12/16/23 17:24:17.707
  STEP: Creating a custom resource that should be denied by the webhook @ 12/16/23 17:24:17.757
  E1216 17:24:18.382786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:19.382862      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 12/16/23 17:24:19.928
  STEP: Updating the custom resource with disallowed data should be denied @ 12/16/23 17:24:19.945
  STEP: Deleting the custom resource should be denied @ 12/16/23 17:24:19.968
  STEP: Remove the offending key and value from the custom resource data @ 12/16/23 17:24:19.989
  STEP: Deleting the updated custom resource should be successful @ 12/16/23 17:24:20.024
  Dec 16 17:24:20.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E1216 17:24:20.383981      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-6284" for this suite. @ 12/16/23 17:24:20.843
  STEP: Destroying namespace "webhook-markers-5382" for this suite. @ 12/16/23 17:24:20.869
• [7.468 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 12/16/23 17:24:20.896
  Dec 16 17:24:20.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename deployment @ 12/16/23 17:24:20.899
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:24:20.995
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:24:21.001
  Dec 16 17:24:21.012: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Dec 16 17:24:21.044: INFO: Pod name sample-pod: Found 0 pods out of 1
  E1216 17:24:21.383960      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:22.386854      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:23.386840      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:24.387258      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:25.388422      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:24:26.055: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 12/16/23 17:24:26.056
  Dec 16 17:24:26.056: INFO: Creating deployment "test-rolling-update-deployment"
  Dec 16 17:24:26.069: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Dec 16 17:24:26.086: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E1216 17:24:26.390620      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:27.390191      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:24:28.105: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Dec 16 17:24:28.116: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Dec 16 17:24:28.144: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7836  dbd38c99-2101-488f-8bcc-c3e8648866e8 19676 1 2023-12-16 17:24:26 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-12-16 17:24:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-16 17:24:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047e66e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-12-16 17:24:26 +0000 UTC,LastTransitionTime:2023-12-16 17:24:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-12-16 17:24:27 +0000 UTC,LastTransitionTime:2023-12-16 17:24:26 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Dec 16 17:24:28.154: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-7836  2e61ef9f-4a17-4f5a-be27-569874526583 19666 1 2023-12-16 17:24:26 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment dbd38c99-2101-488f-8bcc-c3e8648866e8 0xc0047e6bc7 0xc0047e6bc8}] [] [{kube-controller-manager Update apps/v1 2023-12-16 17:24:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dbd38c99-2101-488f-8bcc-c3e8648866e8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-16 17:24:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047e6c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Dec 16 17:24:28.154: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Dec 16 17:24:28.154: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7836  19582c44-3e8b-4c50-ba26-9489f5c8dfb2 19675 2 2023-12-16 17:24:21 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment dbd38c99-2101-488f-8bcc-c3e8648866e8 0xc0047e6a97 0xc0047e6a98}] [] [{e2e.test Update apps/v1 2023-12-16 17:24:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-16 17:24:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dbd38c99-2101-488f-8bcc-c3e8648866e8\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-12-16 17:24:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0047e6b58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Dec 16 17:24:28.165: INFO: Pod "test-rolling-update-deployment-656d657cd8-gl2nd" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-gl2nd test-rolling-update-deployment-656d657cd8- deployment-7836  6d6e162a-16d6-43fc-b886-b903895d005a 19665 0 2023-12-16 17:24:26 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 2e61ef9f-4a17-4f5a-be27-569874526583 0xc0032e2f47 0xc0032e2f48}] [] [{kube-controller-manager Update v1 2023-12-16 17:24:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e61ef9f-4a17-4f5a-be27-569874526583\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:24:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.214\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2k82n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2k82n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:24:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:24:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:24:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:24:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.112,PodIP:10.233.66.214,StartTime:2023-12-16 17:24:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-16 17:24:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://5fc71227f2d0d86cd3e3d8ad4576e34b431300a95ce65883f00df90fab0c7f44,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.214,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:24:28.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7836" for this suite. @ 12/16/23 17:24:28.187
• [7.307 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 12/16/23 17:24:28.208
  Dec 16 17:24:28.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename crd-webhook @ 12/16/23 17:24:28.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:24:28.278
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:24:28.285
  STEP: Setting up server cert @ 12/16/23 17:24:28.296
  E1216 17:24:28.391025      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 12/16/23 17:24:29.178
  STEP: Deploying the custom resource conversion webhook pod @ 12/16/23 17:24:29.195
  STEP: Wait for the deployment to be ready @ 12/16/23 17:24:29.227
  Dec 16 17:24:29.243: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  E1216 17:24:29.392210      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:30.392590      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/16/23 17:24:31.278
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 17:24:31.301
  E1216 17:24:31.394827      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:24:32.302: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Dec 16 17:24:32.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:24:32.394532      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:33.396346      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:34.397082      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 12/16/23 17:24:35.216
  STEP: Create a v2 custom resource @ 12/16/23 17:24:35.265
  E1216 17:24:35.397256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: List CRs in v1 @ 12/16/23 17:24:35.629
  STEP: List CRs in v2 @ 12/16/23 17:24:35.644
  Dec 16 17:24:35.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-8863" for this suite. @ 12/16/23 17:24:36.36
• [8.180 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 12/16/23 17:24:36.392
  Dec 16 17:24:36.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 17:24:36.396
  E1216 17:24:36.397264      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:24:36.442
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:24:36.45
  STEP: Creating configMap with name configmap-projected-all-test-volume-ef2ee4a3-126a-485e-9f00-7772a10b965c @ 12/16/23 17:24:36.458
  STEP: Creating secret with name secret-projected-all-test-volume-2a35d906-1560-478d-b9a4-16e73baa283e @ 12/16/23 17:24:36.47
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 12/16/23 17:24:36.483
  E1216 17:24:37.398984      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:38.398590      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:39.399254      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:40.399964      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:24:40.562
  Dec 16 17:24:40.569: INFO: Trying to get logs from node phoh7xai9ouk-3 pod projected-volume-58f56561-0ea3-468f-a48e-5f521986c241 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 12/16/23 17:24:40.586
  Dec 16 17:24:40.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2669" for this suite. @ 12/16/23 17:24:40.723
• [4.349 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 12/16/23 17:24:40.741
  Dec 16 17:24:40.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename namespaces @ 12/16/23 17:24:40.743
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:24:40.773
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:24:40.779
  STEP: Read namespace status @ 12/16/23 17:24:40.793
  Dec 16 17:24:40.805: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 12/16/23 17:24:40.806
  Dec 16 17:24:40.816: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 12/16/23 17:24:40.816
  Dec 16 17:24:40.839: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Dec 16 17:24:40.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8421" for this suite. @ 12/16/23 17:24:40.85
• [0.125 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 12/16/23 17:24:40.867
  Dec 16 17:24:40.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-runtime @ 12/16/23 17:24:40.869
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:24:40.91
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:24:40.916
  STEP: create the container @ 12/16/23 17:24:40.922
  W1216 17:24:40.939264      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 12/16/23 17:24:40.939
  E1216 17:24:41.399628      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:42.400922      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:43.402259      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 12/16/23 17:24:44
  STEP: the container should be terminated @ 12/16/23 17:24:44.015
  STEP: the termination message should be set @ 12/16/23 17:24:44.015
  Dec 16 17:24:44.015: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 12/16/23 17:24:44.016
  Dec 16 17:24:44.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6011" for this suite. @ 12/16/23 17:24:44.093
• [3.240 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 12/16/23 17:24:44.11
  Dec 16 17:24:44.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename custom-resource-definition @ 12/16/23 17:24:44.114
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:24:44.15
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:24:44.16
  Dec 16 17:24:44.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:24:44.402169      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:24:44.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6646" for this suite. @ 12/16/23 17:24:44.797
• [0.705 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 12/16/23 17:24:44.817
  Dec 16 17:24:44.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 17:24:44.822
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:24:44.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:24:44.872
  STEP: Creating configMap with name projected-configmap-test-volume-map-b9c3f513-63cc-4da1-86c1-e7b5276038b1 @ 12/16/23 17:24:44.883
  STEP: Creating a pod to test consume configMaps @ 12/16/23 17:24:44.893
  E1216 17:24:45.402910      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:46.403565      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:47.404066      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:48.404262      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:24:48.963
  Dec 16 17:24:48.970: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-projected-configmaps-471c3e80-c73a-496a-b6e8-27b3af37e887 container agnhost-container: <nil>
  STEP: delete the pod @ 12/16/23 17:24:48.986
  Dec 16 17:24:49.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6201" for this suite. @ 12/16/23 17:24:49.026
• [4.221 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 12/16/23 17:24:49.042
  Dec 16 17:24:49.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename subjectreview @ 12/16/23 17:24:49.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:24:49.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:24:49.093
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-5505" @ 12/16/23 17:24:49.099
  Dec 16 17:24:49.117: INFO: saUsername: "system:serviceaccount:subjectreview-5505:e2e"
  Dec 16 17:24:49.117: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-5505"}
  Dec 16 17:24:49.118: INFO: saUID: "c5802bae-7ede-4f50-8025-3cbbf31d60ad"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-5505:e2e" @ 12/16/23 17:24:49.118
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-5505:e2e" @ 12/16/23 17:24:49.119
  Dec 16 17:24:49.127: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-5505:e2e" api 'list' configmaps in "subjectreview-5505" namespace @ 12/16/23 17:24:49.127
  Dec 16 17:24:49.131: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-5505:e2e" @ 12/16/23 17:24:49.131
  Dec 16 17:24:49.137: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Dec 16 17:24:49.137: INFO: LocalSubjectAccessReview has been verified
  Dec 16 17:24:49.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-5505" for this suite. @ 12/16/23 17:24:49.149
• [0.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 12/16/23 17:24:49.169
  Dec 16 17:24:49.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename containers @ 12/16/23 17:24:49.172
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:24:49.205
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:24:49.211
  STEP: Creating a pod to test override arguments @ 12/16/23 17:24:49.22
  E1216 17:24:49.406305      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:50.406377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:51.407540      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:52.408087      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:24:53.266
  Dec 16 17:24:53.272: INFO: Trying to get logs from node phoh7xai9ouk-3 pod client-containers-d1c410bd-703d-4b30-a901-4d0d3414b4e6 container agnhost-container: <nil>
  STEP: delete the pod @ 12/16/23 17:24:53.284
  Dec 16 17:24:53.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5259" for this suite. @ 12/16/23 17:24:53.327
• [4.175 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 12/16/23 17:24:53.346
  Dec 16 17:24:53.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 12/16/23 17:24:53.349
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:24:53.39
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:24:53.395
  STEP: Setting up the test @ 12/16/23 17:24:53.403
  STEP: Creating hostNetwork=false pod @ 12/16/23 17:24:53.403
  E1216 17:24:53.408176      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:54.409256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:55.409595      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 12/16/23 17:24:55.451
  E1216 17:24:56.412427      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:24:57.410290      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 12/16/23 17:24:57.505
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 12/16/23 17:24:57.505
  Dec 16 17:24:57.505: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2071 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:24:57.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:24:57.507: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:24:57.507: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2071/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Dec 16 17:24:57.679: INFO: Exec stderr: ""
  Dec 16 17:24:57.680: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2071 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:24:57.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:24:57.683: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:24:57.684: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2071/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Dec 16 17:24:57.845: INFO: Exec stderr: ""
  Dec 16 17:24:57.845: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2071 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:24:57.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:24:57.848: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:24:57.848: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2071/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Dec 16 17:24:57.996: INFO: Exec stderr: ""
  Dec 16 17:24:57.997: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2071 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:24:57.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:24:58.001: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:24:58.002: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2071/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Dec 16 17:24:58.114: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 12/16/23 17:24:58.114
  Dec 16 17:24:58.114: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2071 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:24:58.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:24:58.118: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:24:58.118: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2071/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Dec 16 17:24:58.235: INFO: Exec stderr: ""
  Dec 16 17:24:58.235: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2071 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:24:58.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:24:58.237: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:24:58.238: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2071/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Dec 16 17:24:58.354: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 12/16/23 17:24:58.355
  Dec 16 17:24:58.355: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2071 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:24:58.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:24:58.358: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:24:58.358: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2071/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  E1216 17:24:58.410817      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:24:58.501: INFO: Exec stderr: ""
  Dec 16 17:24:58.501: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2071 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:24:58.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:24:58.504: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:24:58.504: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2071/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Dec 16 17:24:58.629: INFO: Exec stderr: ""
  Dec 16 17:24:58.630: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2071 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:24:58.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:24:58.635: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:24:58.635: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2071/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Dec 16 17:24:58.808: INFO: Exec stderr: ""
  Dec 16 17:24:58.809: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2071 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:24:58.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:24:58.814: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:24:58.814: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2071/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Dec 16 17:24:58.927: INFO: Exec stderr: ""
  Dec 16 17:24:58.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-2071" for this suite. @ 12/16/23 17:24:58.944
• [5.623 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 12/16/23 17:24:58.981
  Dec 16 17:24:58.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename svcaccounts @ 12/16/23 17:24:58.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:24:59.031
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:24:59.039
  STEP: Creating ServiceAccount "e2e-sa-46ngf"  @ 12/16/23 17:24:59.046
  Dec 16 17:24:59.060: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-46ngf"  @ 12/16/23 17:24:59.061
  Dec 16 17:24:59.088: INFO: AutomountServiceAccountToken: true
  Dec 16 17:24:59.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2818" for this suite. @ 12/16/23 17:24:59.1
• [0.139 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 12/16/23 17:24:59.123
  Dec 16 17:24:59.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 17:24:59.125
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:24:59.166
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:24:59.178
  STEP: Creating projection with secret that has name projected-secret-test-map-81e72e6e-a62d-48e4-8ae0-e4cacdc5f3ec @ 12/16/23 17:24:59.185
  STEP: Creating a pod to test consume secrets @ 12/16/23 17:24:59.202
  E1216 17:24:59.411728      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:00.412322      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:01.413964      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:02.414412      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:25:03.255
  Dec 16 17:25:03.263: INFO: Trying to get logs from node phoh7xai9ouk-1 pod pod-projected-secrets-0feb9060-092d-4940-ad32-3d28b7eb865e container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 12/16/23 17:25:03.308
  Dec 16 17:25:03.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9112" for this suite. @ 12/16/23 17:25:03.354
• [4.245 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 12/16/23 17:25:03.376
  Dec 16 17:25:03.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename resourcequota @ 12/16/23 17:25:03.379
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:25:03.408
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:25:03.413
  E1216 17:25:03.413938      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Discovering how many secrets are in namespace by default @ 12/16/23 17:25:03.418
  E1216 17:25:04.415295      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:05.415941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:06.416871      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:07.417824      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:08.417971      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 12/16/23 17:25:08.426
  E1216 17:25:09.418221      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:10.420265      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:11.420556      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:12.421712      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:13.421756      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 12/16/23 17:25:13.434
  STEP: Ensuring resource quota status is calculated @ 12/16/23 17:25:13.452
  E1216 17:25:14.421920      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:15.422171      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 12/16/23 17:25:15.466
  STEP: Ensuring resource quota status captures secret creation @ 12/16/23 17:25:15.498
  E1216 17:25:16.422680      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:17.425734      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 12/16/23 17:25:17.51
  STEP: Ensuring resource quota status released usage @ 12/16/23 17:25:17.527
  E1216 17:25:18.424238      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:19.425224      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:25:19.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-343" for this suite. @ 12/16/23 17:25:19.562
• [16.210 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 12/16/23 17:25:19.589
  Dec 16 17:25:19.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 17:25:19.592
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:25:19.623
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:25:19.629
  STEP: Creating projection with secret that has name projected-secret-test-6db4638e-79ae-479c-b117-d7538080c9dc @ 12/16/23 17:25:19.633
  STEP: Creating a pod to test consume secrets @ 12/16/23 17:25:19.645
  E1216 17:25:20.425505      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:21.425864      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:22.426787      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:23.427012      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:25:23.7
  Dec 16 17:25:23.711: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-projected-secrets-3d880c75-4786-480d-9f4a-640f114472fa container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 12/16/23 17:25:23.73
  Dec 16 17:25:23.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5358" for this suite. @ 12/16/23 17:25:23.782
• [4.214 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 12/16/23 17:25:23.811
  Dec 16 17:25:23.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename var-expansion @ 12/16/23 17:25:23.814
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:25:23.845
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:25:23.853
  STEP: Creating a pod to test substitution in volume subpath @ 12/16/23 17:25:23.865
  E1216 17:25:24.427393      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:25.429261      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:26.429269      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:27.429656      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:25:27.927
  Dec 16 17:25:27.940: INFO: Trying to get logs from node phoh7xai9ouk-3 pod var-expansion-7652cec0-a3bf-46a1-8f4d-6b9c544a4f8f container dapi-container: <nil>
  STEP: delete the pod @ 12/16/23 17:25:27.954
  Dec 16 17:25:27.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3822" for this suite. @ 12/16/23 17:25:28.005
• [4.209 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 12/16/23 17:25:28.031
  Dec 16 17:25:28.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename containers @ 12/16/23 17:25:28.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:25:28.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:25:28.072
  E1216 17:25:28.429826      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:29.430648      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:25:30.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-3266" for this suite. @ 12/16/23 17:25:30.175
• [2.163 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 12/16/23 17:25:30.203
  Dec 16 17:25:30.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename secrets @ 12/16/23 17:25:30.206
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:25:30.272
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:25:30.28
  STEP: creating a secret @ 12/16/23 17:25:30.293
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 12/16/23 17:25:30.305
  STEP: patching the secret @ 12/16/23 17:25:30.314
  STEP: deleting the secret using a LabelSelector @ 12/16/23 17:25:30.338
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 12/16/23 17:25:30.358
  Dec 16 17:25:30.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2857" for this suite. @ 12/16/23 17:25:30.377
• [0.189 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 12/16/23 17:25:30.395
  Dec 16 17:25:30.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename emptydir @ 12/16/23 17:25:30.398
  E1216 17:25:30.431121      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:25:30.432
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:25:30.437
  STEP: Creating Pod @ 12/16/23 17:25:30.442
  E1216 17:25:31.431292      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:32.431962      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 12/16/23 17:25:32.478
  Dec 16 17:25:32.479: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-364 PodName:pod-sharedvolume-15caf3cb-9774-4bee-9272-2fe4b40d7dce ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:25:32.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:25:32.481: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:25:32.481: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-364/pods/pod-sharedvolume-15caf3cb-9774-4bee-9272-2fe4b40d7dce/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Dec 16 17:25:32.603: INFO: Exec stderr: ""
  Dec 16 17:25:32.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-364" for this suite. @ 12/16/23 17:25:32.636
• [2.255 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 12/16/23 17:25:32.652
  Dec 16 17:25:32.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename endpointslice @ 12/16/23 17:25:32.655
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:25:32.759
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:25:32.766
  E1216 17:25:33.432442      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:34.432922      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:35.433135      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:36.433104      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:25:36.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7270" for this suite. @ 12/16/23 17:25:36.938
• [4.306 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 12/16/23 17:25:36.967
  Dec 16 17:25:36.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename security-context @ 12/16/23 17:25:36.969
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:25:37.038
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:25:37.054
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 12/16/23 17:25:37.059
  E1216 17:25:37.434257      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:38.435046      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:39.435469      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:40.436132      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:25:41.127
  Dec 16 17:25:41.141: INFO: Trying to get logs from node phoh7xai9ouk-3 pod security-context-ba5cbb92-4f8f-4bfe-90f7-d8dafb67b591 container test-container: <nil>
  STEP: delete the pod @ 12/16/23 17:25:41.159
  Dec 16 17:25:41.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-5697" for this suite. @ 12/16/23 17:25:41.219
• [4.274 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 12/16/23 17:25:41.244
  Dec 16 17:25:41.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename watch @ 12/16/23 17:25:41.248
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:25:41.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:25:41.302
  STEP: creating a watch on configmaps with a certain label @ 12/16/23 17:25:41.308
  STEP: creating a new configmap @ 12/16/23 17:25:41.31
  STEP: modifying the configmap once @ 12/16/23 17:25:41.321
  STEP: changing the label value of the configmap @ 12/16/23 17:25:41.339
  STEP: Expecting to observe a delete notification for the watched object @ 12/16/23 17:25:41.365
  Dec 16 17:25:41.366: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9983  4baf9ff0-c256-4098-a25d-c7e2cfc7d785 20291 0 2023-12-16 17:25:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-12-16 17:25:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 16 17:25:41.366: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9983  4baf9ff0-c256-4098-a25d-c7e2cfc7d785 20292 0 2023-12-16 17:25:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-12-16 17:25:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 16 17:25:41.370: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9983  4baf9ff0-c256-4098-a25d-c7e2cfc7d785 20293 0 2023-12-16 17:25:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-12-16 17:25:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 12/16/23 17:25:41.371
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 12/16/23 17:25:41.398
  E1216 17:25:41.437471      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:42.438460      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:43.439545      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:44.440142      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:45.440595      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:46.444221      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:47.442065      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:48.447734      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:49.445028      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:50.446016      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 12/16/23 17:25:51.399
  STEP: modifying the configmap a third time @ 12/16/23 17:25:51.438
  E1216 17:25:51.446357      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the configmap @ 12/16/23 17:25:51.462
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 12/16/23 17:25:51.479
  Dec 16 17:25:51.480: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9983  4baf9ff0-c256-4098-a25d-c7e2cfc7d785 20324 0 2023-12-16 17:25:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-12-16 17:25:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 16 17:25:51.481: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9983  4baf9ff0-c256-4098-a25d-c7e2cfc7d785 20325 0 2023-12-16 17:25:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-12-16 17:25:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 16 17:25:51.481: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9983  4baf9ff0-c256-4098-a25d-c7e2cfc7d785 20326 0 2023-12-16 17:25:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-12-16 17:25:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 16 17:25:51.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9983" for this suite. @ 12/16/23 17:25:51.506
• [10.277 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 12/16/23 17:25:51.525
  Dec 16 17:25:51.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename downward-api @ 12/16/23 17:25:51.53
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:25:51.561
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:25:51.567
  STEP: Creating a pod to test downward API volume plugin @ 12/16/23 17:25:51.573
  E1216 17:25:52.447224      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:53.448029      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:25:53.615
  Dec 16 17:25:53.625: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downwardapi-volume-5783f5cc-f8e8-446b-816a-c8b8a604e06f container client-container: <nil>
  STEP: delete the pod @ 12/16/23 17:25:53.642
  Dec 16 17:25:53.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9690" for this suite. @ 12/16/23 17:25:53.699
• [2.192 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 12/16/23 17:25:53.729
  Dec 16 17:25:53.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename services @ 12/16/23 17:25:53.731
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:25:53.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:25:53.809
  STEP: creating service in namespace services-4273 @ 12/16/23 17:25:53.824
  STEP: creating service affinity-nodeport in namespace services-4273 @ 12/16/23 17:25:53.825
  STEP: creating replication controller affinity-nodeport in namespace services-4273 @ 12/16/23 17:25:53.875
  I1216 17:25:53.913122      13 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-4273, replica count: 3
  E1216 17:25:54.448479      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:55.448841      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:56.449822      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:25:56.966020      13 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Dec 16 17:25:57.005: INFO: Creating new exec pod
  E1216 17:25:57.450800      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:58.452233      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:25:59.452591      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:26:00.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-4273 exec execpod-affinitywcd9h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  E1216 17:26:00.452897      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:26:00.523: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Dec 16 17:26:00.523: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 16 17:26:00.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-4273 exec execpod-affinitywcd9h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.37.247 80'
  Dec 16 17:26:00.845: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.37.247 80\nConnection to 10.233.37.247 80 port [tcp/http] succeeded!\n"
  Dec 16 17:26:00.845: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 16 17:26:00.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-4273 exec execpod-affinitywcd9h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.112 30253'
  Dec 16 17:26:01.202: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.112 30253\nConnection to 192.168.121.112 30253 port [tcp/*] succeeded!\n"
  Dec 16 17:26:01.202: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 16 17:26:01.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-4273 exec execpod-affinitywcd9h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.172 30253'
  E1216 17:26:01.454242      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:26:01.530: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.172 30253\nConnection to 192.168.121.172 30253 port [tcp/*] succeeded!\n"
  Dec 16 17:26:01.530: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 16 17:26:01.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-4273 exec execpod-affinitywcd9h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.172:30253/ ; done'
  Dec 16 17:26:02.157: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:30253/\n"
  Dec 16 17:26:02.157: INFO: stdout: "\naffinity-nodeport-gvl98\naffinity-nodeport-gvl98\naffinity-nodeport-gvl98\naffinity-nodeport-gvl98\naffinity-nodeport-gvl98\naffinity-nodeport-gvl98\naffinity-nodeport-gvl98\naffinity-nodeport-gvl98\naffinity-nodeport-gvl98\naffinity-nodeport-gvl98\naffinity-nodeport-gvl98\naffinity-nodeport-gvl98\naffinity-nodeport-gvl98\naffinity-nodeport-gvl98\naffinity-nodeport-gvl98\naffinity-nodeport-gvl98"
  Dec 16 17:26:02.157: INFO: Received response from host: affinity-nodeport-gvl98
  Dec 16 17:26:02.159: INFO: Received response from host: affinity-nodeport-gvl98
  Dec 16 17:26:02.159: INFO: Received response from host: affinity-nodeport-gvl98
  Dec 16 17:26:02.159: INFO: Received response from host: affinity-nodeport-gvl98
  Dec 16 17:26:02.159: INFO: Received response from host: affinity-nodeport-gvl98
  Dec 16 17:26:02.159: INFO: Received response from host: affinity-nodeport-gvl98
  Dec 16 17:26:02.159: INFO: Received response from host: affinity-nodeport-gvl98
  Dec 16 17:26:02.159: INFO: Received response from host: affinity-nodeport-gvl98
  Dec 16 17:26:02.159: INFO: Received response from host: affinity-nodeport-gvl98
  Dec 16 17:26:02.159: INFO: Received response from host: affinity-nodeport-gvl98
  Dec 16 17:26:02.159: INFO: Received response from host: affinity-nodeport-gvl98
  Dec 16 17:26:02.159: INFO: Received response from host: affinity-nodeport-gvl98
  Dec 16 17:26:02.159: INFO: Received response from host: affinity-nodeport-gvl98
  Dec 16 17:26:02.159: INFO: Received response from host: affinity-nodeport-gvl98
  Dec 16 17:26:02.159: INFO: Received response from host: affinity-nodeport-gvl98
  Dec 16 17:26:02.159: INFO: Received response from host: affinity-nodeport-gvl98
  Dec 16 17:26:02.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 16 17:26:02.190: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-4273, will wait for the garbage collector to delete the pods @ 12/16/23 17:26:02.227
  Dec 16 17:26:02.351: INFO: Deleting ReplicationController affinity-nodeport took: 61.452982ms
  Dec 16 17:26:02.452: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.866534ms
  E1216 17:26:02.454904      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:03.455713      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:04.456438      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-4273" for this suite. @ 12/16/23 17:26:05.215
• [11.504 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 12/16/23 17:26:05.237
  Dec 16 17:26:05.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename field-validation @ 12/16/23 17:26:05.243
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:26:05.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:26:05.283
  Dec 16 17:26:05.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:26:05.457448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:06.458290      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:07.458506      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  W1216 17:26:08.210762      13 warnings.go:70] unknown field "alpha"
  W1216 17:26:08.210818      13 warnings.go:70] unknown field "beta"
  W1216 17:26:08.210829      13 warnings.go:70] unknown field "delta"
  W1216 17:26:08.210839      13 warnings.go:70] unknown field "epsilon"
  W1216 17:26:08.210849      13 warnings.go:70] unknown field "gamma"
  E1216 17:26:08.459009      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:26:09.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7617" for this suite. @ 12/16/23 17:26:09.361
• [4.144 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 12/16/23 17:26:09.384
  Dec 16 17:26:09.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubelet-test @ 12/16/23 17:26:09.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:26:09.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:26:09.431
  E1216 17:26:09.460649      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:26:09.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1906" for this suite. @ 12/16/23 17:26:09.531
• [0.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 12/16/23 17:26:09.574
  Dec 16 17:26:09.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename svcaccounts @ 12/16/23 17:26:09.577
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:26:09.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:26:09.62
  Dec 16 17:26:09.672: INFO: created pod pod-service-account-defaultsa
  Dec 16 17:26:09.672: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Dec 16 17:26:09.689: INFO: created pod pod-service-account-mountsa
  Dec 16 17:26:09.690: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Dec 16 17:26:09.772: INFO: created pod pod-service-account-nomountsa
  Dec 16 17:26:09.772: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Dec 16 17:26:09.798: INFO: created pod pod-service-account-defaultsa-mountspec
  Dec 16 17:26:09.798: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Dec 16 17:26:09.840: INFO: created pod pod-service-account-mountsa-mountspec
  Dec 16 17:26:09.841: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Dec 16 17:26:09.855: INFO: created pod pod-service-account-nomountsa-mountspec
  Dec 16 17:26:09.855: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Dec 16 17:26:09.877: INFO: created pod pod-service-account-defaultsa-nomountspec
  Dec 16 17:26:09.878: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Dec 16 17:26:09.888: INFO: created pod pod-service-account-mountsa-nomountspec
  Dec 16 17:26:09.888: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Dec 16 17:26:09.922: INFO: created pod pod-service-account-nomountsa-nomountspec
  Dec 16 17:26:09.922: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Dec 16 17:26:09.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1458" for this suite. @ 12/16/23 17:26:09.96
• [0.512 seconds]
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 12/16/23 17:26:10.087
  Dec 16 17:26:10.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename var-expansion @ 12/16/23 17:26:10.089
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:26:10.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:26:10.178
  STEP: Creating a pod to test substitution in container's command @ 12/16/23 17:26:10.185
  E1216 17:26:10.461812      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:11.464152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:12.466198      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:13.467128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:14.467138      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:15.470115      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:26:16.351
  Dec 16 17:26:16.365: INFO: Trying to get logs from node phoh7xai9ouk-2 pod var-expansion-182943df-0e22-4619-b1d8-74fa2225cb70 container dapi-container: <nil>
  STEP: delete the pod @ 12/16/23 17:26:16.463
  E1216 17:26:16.468909      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:26:16.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-278" for this suite. @ 12/16/23 17:26:16.602
• [6.573 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 12/16/23 17:26:16.67
  Dec 16 17:26:16.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename field-validation @ 12/16/23 17:26:16.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:26:17.057
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:26:17.065
  STEP: apply creating a deployment @ 12/16/23 17:26:17.087
  Dec 16 17:26:17.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1949" for this suite. @ 12/16/23 17:26:17.19
• [0.545 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 12/16/23 17:26:17.224
  Dec 16 17:26:17.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename field-validation @ 12/16/23 17:26:17.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:26:17.334
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:26:17.342
  STEP: apply creating a deployment @ 12/16/23 17:26:17.351
  Dec 16 17:26:17.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-3268" for this suite. @ 12/16/23 17:26:17.401
• [0.191 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 12/16/23 17:26:17.418
  Dec 16 17:26:17.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename watch @ 12/16/23 17:26:17.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:26:17.459
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:26:17.466
  E1216 17:26:17.468844      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a watch on configmaps @ 12/16/23 17:26:17.474
  STEP: creating a new configmap @ 12/16/23 17:26:17.477
  STEP: modifying the configmap once @ 12/16/23 17:26:17.488
  STEP: closing the watch once it receives two notifications @ 12/16/23 17:26:17.505
  Dec 16 17:26:17.505: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-710  1e7d2d84-ebc0-4d6e-bc1f-e719b50ec346 20677 0 2023-12-16 17:26:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-12-16 17:26:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 16 17:26:17.506: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-710  1e7d2d84-ebc0-4d6e-bc1f-e719b50ec346 20678 0 2023-12-16 17:26:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-12-16 17:26:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 12/16/23 17:26:17.506
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 12/16/23 17:26:17.534
  STEP: deleting the configmap @ 12/16/23 17:26:17.537
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 12/16/23 17:26:17.566
  Dec 16 17:26:17.567: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-710  1e7d2d84-ebc0-4d6e-bc1f-e719b50ec346 20679 0 2023-12-16 17:26:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-12-16 17:26:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 16 17:26:17.568: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-710  1e7d2d84-ebc0-4d6e-bc1f-e719b50ec346 20680 0 2023-12-16 17:26:17 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-12-16 17:26:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 16 17:26:17.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-710" for this suite. @ 12/16/23 17:26:17.588
• [0.192 seconds]
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 12/16/23 17:26:17.61
  Dec 16 17:26:17.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename gc @ 12/16/23 17:26:17.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:26:17.654
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:26:17.66
  STEP: create the rc1 @ 12/16/23 17:26:17.681
  STEP: create the rc2 @ 12/16/23 17:26:17.694
  E1216 17:26:18.469252      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:19.469346      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:20.470257      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:21.470707      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:22.470884      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:23.472014      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 12/16/23 17:26:23.815
  E1216 17:26:24.473231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:25.474139      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:26.474916      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:27.475655      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc simpletest-rc-to-be-deleted @ 12/16/23 17:26:27.937
  STEP: wait for the rc to be deleted @ 12/16/23 17:26:28.036
  E1216 17:26:28.477043      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:29.479349      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:30.479004      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:31.479559      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:32.479904      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:26:33.110: INFO: 71 pods remaining
  Dec 16 17:26:33.110: INFO: 71 pods has nil DeletionTimestamp
  Dec 16 17:26:33.111: INFO: 
  E1216 17:26:33.480544      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:34.481588      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:35.485505      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:36.485672      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:37.485934      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 12/16/23 17:26:38.09
  Dec 16 17:26:38.399: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Dec 16 17:26:38.418: INFO: Deleting pod "simpletest-rc-to-be-deleted-2dkdq" in namespace "gc-5309"
  E1216 17:26:38.486635      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:26:38.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-489z7" in namespace "gc-5309"
  Dec 16 17:26:38.570: INFO: Deleting pod "simpletest-rc-to-be-deleted-4gqww" in namespace "gc-5309"
  Dec 16 17:26:38.655: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rfll" in namespace "gc-5309"
  Dec 16 17:26:38.703: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vh6r" in namespace "gc-5309"
  Dec 16 17:26:38.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-5868h" in namespace "gc-5309"
  Dec 16 17:26:38.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-5ffzl" in namespace "gc-5309"
  Dec 16 17:26:38.952: INFO: Deleting pod "simpletest-rc-to-be-deleted-5j555" in namespace "gc-5309"
  Dec 16 17:26:39.068: INFO: Deleting pod "simpletest-rc-to-be-deleted-62q7q" in namespace "gc-5309"
  Dec 16 17:26:39.137: INFO: Deleting pod "simpletest-rc-to-be-deleted-6bqvj" in namespace "gc-5309"
  Dec 16 17:26:39.213: INFO: Deleting pod "simpletest-rc-to-be-deleted-72rl5" in namespace "gc-5309"
  Dec 16 17:26:39.320: INFO: Deleting pod "simpletest-rc-to-be-deleted-74ldm" in namespace "gc-5309"
  Dec 16 17:26:39.370: INFO: Deleting pod "simpletest-rc-to-be-deleted-776ph" in namespace "gc-5309"
  Dec 16 17:26:39.467: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jhrv" in namespace "gc-5309"
  E1216 17:26:39.486821      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:26:39.559: INFO: Deleting pod "simpletest-rc-to-be-deleted-7llbn" in namespace "gc-5309"
  Dec 16 17:26:39.652: INFO: Deleting pod "simpletest-rc-to-be-deleted-8lt8g" in namespace "gc-5309"
  Dec 16 17:26:39.708: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mzjd" in namespace "gc-5309"
  Dec 16 17:26:39.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ngzb" in namespace "gc-5309"
  Dec 16 17:26:39.812: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rr97" in namespace "gc-5309"
  Dec 16 17:26:39.869: INFO: Deleting pod "simpletest-rc-to-be-deleted-9597w" in namespace "gc-5309"
  Dec 16 17:26:39.941: INFO: Deleting pod "simpletest-rc-to-be-deleted-96s6f" in namespace "gc-5309"
  Dec 16 17:26:40.037: INFO: Deleting pod "simpletest-rc-to-be-deleted-9c5w4" in namespace "gc-5309"
  Dec 16 17:26:40.156: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vdxn" in namespace "gc-5309"
  Dec 16 17:26:40.243: INFO: Deleting pod "simpletest-rc-to-be-deleted-9z8l6" in namespace "gc-5309"
  Dec 16 17:26:40.344: INFO: Deleting pod "simpletest-rc-to-be-deleted-bggqn" in namespace "gc-5309"
  Dec 16 17:26:40.417: INFO: Deleting pod "simpletest-rc-to-be-deleted-bhx6h" in namespace "gc-5309"
  Dec 16 17:26:40.481: INFO: Deleting pod "simpletest-rc-to-be-deleted-bj8n5" in namespace "gc-5309"
  E1216 17:26:40.487501      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:26:40.530: INFO: Deleting pod "simpletest-rc-to-be-deleted-bz4z4" in namespace "gc-5309"
  Dec 16 17:26:40.576: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5mm7" in namespace "gc-5309"
  Dec 16 17:26:40.722: INFO: Deleting pod "simpletest-rc-to-be-deleted-cfqzr" in namespace "gc-5309"
  Dec 16 17:26:40.880: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckjqv" in namespace "gc-5309"
  Dec 16 17:26:41.119: INFO: Deleting pod "simpletest-rc-to-be-deleted-dbtlw" in namespace "gc-5309"
  Dec 16 17:26:41.221: INFO: Deleting pod "simpletest-rc-to-be-deleted-drxqk" in namespace "gc-5309"
  Dec 16 17:26:41.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-ds8pq" in namespace "gc-5309"
  Dec 16 17:26:41.337: INFO: Deleting pod "simpletest-rc-to-be-deleted-dzdkz" in namespace "gc-5309"
  Dec 16 17:26:41.375: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9q59" in namespace "gc-5309"
  Dec 16 17:26:41.444: INFO: Deleting pod "simpletest-rc-to-be-deleted-fg6z4" in namespace "gc-5309"
  Dec 16 17:26:41.481: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkwnr" in namespace "gc-5309"
  E1216 17:26:41.489636      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:26:41.524: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqkqt" in namespace "gc-5309"
  Dec 16 17:26:41.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzxb7" in namespace "gc-5309"
  Dec 16 17:26:41.624: INFO: Deleting pod "simpletest-rc-to-be-deleted-grpkv" in namespace "gc-5309"
  Dec 16 17:26:41.684: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtnm7" in namespace "gc-5309"
  Dec 16 17:26:41.747: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtzwb" in namespace "gc-5309"
  Dec 16 17:26:41.813: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2ql9" in namespace "gc-5309"
  Dec 16 17:26:41.853: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6vg5" in namespace "gc-5309"
  Dec 16 17:26:41.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvct2" in namespace "gc-5309"
  Dec 16 17:26:42.026: INFO: Deleting pod "simpletest-rc-to-be-deleted-jvvwd" in namespace "gc-5309"
  Dec 16 17:26:42.083: INFO: Deleting pod "simpletest-rc-to-be-deleted-kcr2z" in namespace "gc-5309"
  Dec 16 17:26:42.186: INFO: Deleting pod "simpletest-rc-to-be-deleted-kjfzd" in namespace "gc-5309"
  Dec 16 17:26:42.318: INFO: Deleting pod "simpletest-rc-to-be-deleted-kwpcq" in namespace "gc-5309"
  Dec 16 17:26:42.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5309" for this suite. @ 12/16/23 17:26:42.435
• [24.877 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  E1216 17:26:42.488615      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a kubernetes client @ 12/16/23 17:26:42.489
  Dec 16 17:26:42.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename custom-resource-definition @ 12/16/23 17:26:42.495
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:26:42.561
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:26:42.568
  Dec 16 17:26:42.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:26:43.488798      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:44.488864      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:45.489034      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:46.489270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:47.489754      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:48.490948      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:49.491565      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:26:50.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5746" for this suite. @ 12/16/23 17:26:50.167
• [7.884 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:622
  STEP: Creating a kubernetes client @ 12/16/23 17:26:50.374
  Dec 16 17:26:50.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename field-validation @ 12/16/23 17:26:50.378
  E1216 17:26:50.491945      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:26:50.522
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:26:50.529
  Dec 16 17:26:50.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:26:51.493194      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:52.493570      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:53.494578      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  W1216 17:26:53.673113      13 warnings.go:70] unknown field "alpha"
  W1216 17:26:53.673168      13 warnings.go:70] unknown field "beta"
  W1216 17:26:53.673181      13 warnings.go:70] unknown field "delta"
  W1216 17:26:53.673191      13 warnings.go:70] unknown field "epsilon"
  W1216 17:26:53.673202      13 warnings.go:70] unknown field "gamma"
  Dec 16 17:26:54.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E1216 17:26:54.495404      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "field-validation-1149" for this suite. @ 12/16/23 17:26:54.723
• [4.437 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 12/16/23 17:26:54.815
  Dec 16 17:26:54.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 12/16/23 17:26:54.818
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:26:54.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:26:54.933
  STEP: create the container to handle the HTTPGet hook request. @ 12/16/23 17:26:54.954
  E1216 17:26:55.495434      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:56.495614      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 12/16/23 17:26:57.12
  E1216 17:26:57.496722      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:26:58.497058      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 12/16/23 17:26:59.256
  STEP: delete the pod with lifecycle hook @ 12/16/23 17:26:59.316
  E1216 17:26:59.498345      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:00.499424      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:27:01.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3829" for this suite. @ 12/16/23 17:27:01.421
• [6.660 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 12/16/23 17:27:01.484
  Dec 16 17:27:01.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename secrets @ 12/16/23 17:27:01.489
  E1216 17:27:01.499734      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:27:01.534
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:27:01.545
  STEP: Creating secret with name s-test-opt-del-f5da7fa8-273f-436e-b624-774bc77c33d3 @ 12/16/23 17:27:01.569
  STEP: Creating secret with name s-test-opt-upd-64f25fc0-4169-4daa-82fc-0ce8f18455e0 @ 12/16/23 17:27:01.578
  STEP: Creating the pod @ 12/16/23 17:27:01.588
  E1216 17:27:02.500154      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:03.500163      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-f5da7fa8-273f-436e-b624-774bc77c33d3 @ 12/16/23 17:27:03.75
  STEP: Updating secret s-test-opt-upd-64f25fc0-4169-4daa-82fc-0ce8f18455e0 @ 12/16/23 17:27:03.766
  STEP: Creating secret with name s-test-opt-create-639e582b-f993-442c-9a4e-c9899d13189b @ 12/16/23 17:27:03.782
  STEP: waiting to observe update in volume @ 12/16/23 17:27:03.799
  E1216 17:27:04.500590      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:05.501054      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:27:05.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8239" for this suite. @ 12/16/23 17:27:05.932
• [4.470 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 12/16/23 17:27:05.963
  Dec 16 17:27:05.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename webhook @ 12/16/23 17:27:05.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:27:06.019
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:27:06.025
  STEP: Setting up server cert @ 12/16/23 17:27:06.077
  E1216 17:27:06.501158      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/16/23 17:27:07.249
  STEP: Deploying the webhook pod @ 12/16/23 17:27:07.313
  STEP: Wait for the deployment to be ready @ 12/16/23 17:27:07.355
  Dec 16 17:27:07.374: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E1216 17:27:07.502359      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:08.502708      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/16/23 17:27:09.398
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 17:27:09.423
  E1216 17:27:09.504677      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:27:10.424: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 12/16/23 17:27:10.434
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 12/16/23 17:27:10.485
  E1216 17:27:10.504874      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 12/16/23 17:27:10.525
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 12/16/23 17:27:10.548
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 12/16/23 17:27:10.579
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 12/16/23 17:27:10.622
  Dec 16 17:27:10.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1394" for this suite. @ 12/16/23 17:27:10.843
  STEP: Destroying namespace "webhook-markers-8171" for this suite. @ 12/16/23 17:27:10.858
• [4.915 seconds]
------------------------------
S
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 12/16/23 17:27:10.88
  Dec 16 17:27:10.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename runtimeclass @ 12/16/23 17:27:10.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:27:10.92
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:27:10.925
  Dec 16 17:27:10.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9690" for this suite. @ 12/16/23 17:27:10.957
• [0.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 12/16/23 17:27:10.978
  Dec 16 17:27:10.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 12/16/23 17:27:10.98
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:27:11.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:27:11.023
  STEP: creating a target pod @ 12/16/23 17:27:11.031
  E1216 17:27:11.505592      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:12.505859      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 12/16/23 17:27:13.087
  E1216 17:27:13.505971      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:14.506374      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 12/16/23 17:27:15.143
  Dec 16 17:27:15.144: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5603 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 16 17:27:15.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  Dec 16 17:27:15.147: INFO: ExecWithOptions: Clientset creation
  Dec 16 17:27:15.147: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-5603/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Dec 16 17:27:15.301: INFO: Exec stderr: ""
  Dec 16 17:27:15.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-5603" for this suite. @ 12/16/23 17:27:15.341
• [4.383 seconds]
------------------------------
SSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 12/16/23 17:27:15.364
  Dec 16 17:27:15.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename events @ 12/16/23 17:27:15.368
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:27:15.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:27:15.419
  STEP: Create set of events @ 12/16/23 17:27:15.425
  Dec 16 17:27:15.439: INFO: created test-event-1
  Dec 16 17:27:15.448: INFO: created test-event-2
  Dec 16 17:27:15.459: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 12/16/23 17:27:15.46
  STEP: delete collection of events @ 12/16/23 17:27:15.472
  Dec 16 17:27:15.472: INFO: requesting DeleteCollection of events
  E1216 17:27:15.507320      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check that the list of events matches the requested quantity @ 12/16/23 17:27:15.527
  Dec 16 17:27:15.527: INFO: requesting list of events to confirm quantity
  Dec 16 17:27:15.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8054" for this suite. @ 12/16/23 17:27:15.543
• [0.193 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 12/16/23 17:27:15.562
  Dec 16 17:27:15.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename svc-latency @ 12/16/23 17:27:15.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:27:15.591
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:27:15.596
  Dec 16 17:27:15.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-7043 @ 12/16/23 17:27:15.606
  I1216 17:27:15.618564      13 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7043, replica count: 1
  E1216 17:27:16.507365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:27:16.670227      13 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E1216 17:27:17.508231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:27:17.671120      13 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Dec 16 17:27:17.868: INFO: Created: latency-svc-nj92t
  Dec 16 17:27:17.910: INFO: Got endpoints: latency-svc-nj92t [137.748274ms]
  Dec 16 17:27:17.962: INFO: Created: latency-svc-rb4ff
  Dec 16 17:27:17.976: INFO: Got endpoints: latency-svc-rb4ff [65.360133ms]
  Dec 16 17:27:18.002: INFO: Created: latency-svc-khd9g
  Dec 16 17:27:18.010: INFO: Got endpoints: latency-svc-khd9g [96.92891ms]
  Dec 16 17:27:18.020: INFO: Created: latency-svc-wh67t
  Dec 16 17:27:18.041: INFO: Created: latency-svc-jd4j8
  Dec 16 17:27:18.068: INFO: Created: latency-svc-8k6l6
  Dec 16 17:27:18.075: INFO: Got endpoints: latency-svc-jd4j8 [164.034582ms]
  Dec 16 17:27:18.076: INFO: Got endpoints: latency-svc-wh67t [163.343241ms]
  Dec 16 17:27:18.096: INFO: Created: latency-svc-xlp6n
  Dec 16 17:27:18.116: INFO: Got endpoints: latency-svc-xlp6n [204.139314ms]
  Dec 16 17:27:18.116: INFO: Got endpoints: latency-svc-8k6l6 [204.635083ms]
  Dec 16 17:27:18.146: INFO: Created: latency-svc-jmrjk
  Dec 16 17:27:18.148: INFO: Created: latency-svc-9dl59
  Dec 16 17:27:18.150: INFO: Got endpoints: latency-svc-9dl59 [237.78556ms]
  Dec 16 17:27:18.172: INFO: Created: latency-svc-zrn5l
  Dec 16 17:27:18.177: INFO: Got endpoints: latency-svc-jmrjk [263.975324ms]
  Dec 16 17:27:18.227: INFO: Got endpoints: latency-svc-zrn5l [312.434645ms]
  Dec 16 17:27:18.498: INFO: Created: latency-svc-m78mk
  Dec 16 17:27:18.500: INFO: Created: latency-svc-r4bx8
  Dec 16 17:27:18.501: INFO: Created: latency-svc-zhz8b
  Dec 16 17:27:18.502: INFO: Created: latency-svc-2vqs9
  Dec 16 17:27:18.506: INFO: Created: latency-svc-bjjh4
  Dec 16 17:27:18.507: INFO: Created: latency-svc-h2sqn
  E1216 17:27:18.509230      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:27:18.521: INFO: Created: latency-svc-8l2x7
  Dec 16 17:27:18.526: INFO: Created: latency-svc-ccg9n
  Dec 16 17:27:18.526: INFO: Created: latency-svc-g8dgz
  Dec 16 17:27:18.528: INFO: Created: latency-svc-qs2fs
  Dec 16 17:27:18.528: INFO: Created: latency-svc-bdpvk
  Dec 16 17:27:18.530: INFO: Created: latency-svc-h8njp
  Dec 16 17:27:18.533: INFO: Created: latency-svc-t7brq
  Dec 16 17:27:18.540: INFO: Created: latency-svc-mxwg6
  Dec 16 17:27:18.541: INFO: Created: latency-svc-cvwcf
  Dec 16 17:27:18.543: INFO: Got endpoints: latency-svc-m78mk [315.078941ms]
  Dec 16 17:27:18.564: INFO: Got endpoints: latency-svc-bjjh4 [649.630318ms]
  Dec 16 17:27:18.565: INFO: Got endpoints: latency-svc-r4bx8 [414.164112ms]
  Dec 16 17:27:18.565: INFO: Got endpoints: latency-svc-h2sqn [650.175762ms]
  Dec 16 17:27:18.576: INFO: Got endpoints: latency-svc-g8dgz [500.817842ms]
  Dec 16 17:27:18.577: INFO: Got endpoints: latency-svc-ccg9n [663.028419ms]
  Dec 16 17:27:18.591: INFO: Got endpoints: latency-svc-2vqs9 [677.633169ms]
  Dec 16 17:27:18.592: INFO: Got endpoints: latency-svc-zhz8b [677.216662ms]
  Dec 16 17:27:18.626: INFO: Got endpoints: latency-svc-bdpvk [510.170406ms]
  Dec 16 17:27:18.627: INFO: Got endpoints: latency-svc-qs2fs [711.948278ms]
  Dec 16 17:27:18.636: INFO: Created: latency-svc-grqvc
  Dec 16 17:27:18.658: INFO: Got endpoints: latency-svc-t7brq [681.24445ms]
  Dec 16 17:27:18.659: INFO: Got endpoints: latency-svc-cvwcf [481.107367ms]
  Dec 16 17:27:18.666: INFO: Created: latency-svc-27q6r
  Dec 16 17:27:18.681: INFO: Got endpoints: latency-svc-8l2x7 [564.520812ms]
  Dec 16 17:27:18.681: INFO: Got endpoints: latency-svc-h8njp [605.644482ms]
  Dec 16 17:27:18.712: INFO: Got endpoints: latency-svc-grqvc [169.324516ms]
  Dec 16 17:27:18.713: INFO: Got endpoints: latency-svc-mxwg6 [702.712051ms]
  Dec 16 17:27:18.713: INFO: Got endpoints: latency-svc-27q6r [137.258717ms]
  Dec 16 17:27:18.746: INFO: Created: latency-svc-bj7ck
  Dec 16 17:27:18.760: INFO: Got endpoints: latency-svc-bj7ck [182.330681ms]
  Dec 16 17:27:18.965: INFO: Created: latency-svc-w4nd4
  Dec 16 17:27:18.994: INFO: Created: latency-svc-mckwn
  Dec 16 17:27:18.995: INFO: Created: latency-svc-2m85j
  Dec 16 17:27:19.005: INFO: Created: latency-svc-shbgt
  Dec 16 17:27:19.006: INFO: Created: latency-svc-2l86r
  Dec 16 17:27:19.006: INFO: Created: latency-svc-f4nwd
  Dec 16 17:27:19.031: INFO: Created: latency-svc-gs4j4
  Dec 16 17:27:19.043: INFO: Created: latency-svc-vh24f
  Dec 16 17:27:19.043: INFO: Created: latency-svc-fcdgn
  Dec 16 17:27:19.043: INFO: Created: latency-svc-8jntx
  Dec 16 17:27:19.044: INFO: Created: latency-svc-z5w24
  Dec 16 17:27:19.044: INFO: Created: latency-svc-d6t7s
  Dec 16 17:27:19.043: INFO: Created: latency-svc-czc55
  Dec 16 17:27:19.043: INFO: Created: latency-svc-lfcv7
  Dec 16 17:27:19.044: INFO: Created: latency-svc-rbd7w
  Dec 16 17:27:19.050: INFO: Got endpoints: latency-svc-2m85j [458.379317ms]
  Dec 16 17:27:19.050: INFO: Got endpoints: latency-svc-mckwn [485.336746ms]
  Dec 16 17:27:19.050: INFO: Got endpoints: latency-svc-w4nd4 [423.782176ms]
  Dec 16 17:27:19.051: INFO: Got endpoints: latency-svc-shbgt [291.50427ms]
  Dec 16 17:27:19.071: INFO: Got endpoints: latency-svc-2l86r [505.793198ms]
  Dec 16 17:27:19.091: INFO: Got endpoints: latency-svc-f4nwd [377.719648ms]
  Dec 16 17:27:19.093: INFO: Got endpoints: latency-svc-lfcv7 [466.254225ms]
  Dec 16 17:27:19.098: INFO: Created: latency-svc-g5gnj
  Dec 16 17:27:19.110: INFO: Got endpoints: latency-svc-gs4j4 [428.936851ms]
  Dec 16 17:27:19.132: INFO: Got endpoints: latency-svc-vh24f [451.632163ms]
  Dec 16 17:27:19.133: INFO: Got endpoints: latency-svc-fcdgn [474.163143ms]
  Dec 16 17:27:19.139: INFO: Created: latency-svc-t25kv
  Dec 16 17:27:19.163: INFO: Got endpoints: latency-svc-z5w24 [599.177688ms]
  Dec 16 17:27:19.164: INFO: Got endpoints: latency-svc-d6t7s [571.832077ms]
  Dec 16 17:27:19.165: INFO: Got endpoints: latency-svc-rbd7w [452.017622ms]
  Dec 16 17:27:19.188: INFO: Got endpoints: latency-svc-g5gnj [136.380585ms]
  Dec 16 17:27:19.197: INFO: Got endpoints: latency-svc-t25kv [146.564461ms]
  Dec 16 17:27:19.197: INFO: Got endpoints: latency-svc-czc55 [539.406405ms]
  Dec 16 17:27:19.198: INFO: Got endpoints: latency-svc-8jntx [485.332533ms]
  Dec 16 17:27:19.336: INFO: Created: latency-svc-7kvh2
  Dec 16 17:27:19.337: INFO: Created: latency-svc-tjtzk
  Dec 16 17:27:19.339: INFO: Created: latency-svc-ddvcb
  Dec 16 17:27:19.344: INFO: Created: latency-svc-xx9t8
  Dec 16 17:27:19.344: INFO: Created: latency-svc-gxshc
  Dec 16 17:27:19.347: INFO: Created: latency-svc-t5sdl
  Dec 16 17:27:19.348: INFO: Created: latency-svc-nbc57
  Dec 16 17:27:19.348: INFO: Created: latency-svc-p82fc
  Dec 16 17:27:19.349: INFO: Created: latency-svc-x9mj9
  Dec 16 17:27:19.349: INFO: Created: latency-svc-v5d2t
  Dec 16 17:27:19.350: INFO: Created: latency-svc-5w8vv
  Dec 16 17:27:19.350: INFO: Created: latency-svc-82q5l
  Dec 16 17:27:19.384: INFO: Created: latency-svc-rmp8t
  Dec 16 17:27:19.384: INFO: Created: latency-svc-gf2gw
  Dec 16 17:27:19.384: INFO: Created: latency-svc-l44l4
  Dec 16 17:27:19.387: INFO: Got endpoints: latency-svc-7kvh2 [315.096483ms]
  Dec 16 17:27:19.411: INFO: Got endpoints: latency-svc-xx9t8 [213.100215ms]
  Dec 16 17:27:19.420: INFO: Got endpoints: latency-svc-82q5l [329.465108ms]
  Dec 16 17:27:19.421: INFO: Got endpoints: latency-svc-nbc57 [256.815956ms]
  Dec 16 17:27:19.437: INFO: Got endpoints: latency-svc-v5d2t [344.250941ms]
  Dec 16 17:27:19.446: INFO: Got endpoints: latency-svc-5w8vv [394.924285ms]
  Dec 16 17:27:19.469: INFO: Created: latency-svc-xvcdv
  Dec 16 17:27:19.493: INFO: Created: latency-svc-d8jr7
  Dec 16 17:27:19.506: INFO: Got endpoints: latency-svc-t5sdl [308.73292ms]
  Dec 16 17:27:19.507: INFO: Got endpoints: latency-svc-ddvcb [318.592501ms]
  Dec 16 17:27:19.507: INFO: Got endpoints: latency-svc-x9mj9 [309.228976ms]
  E1216 17:27:19.510036      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:27:19.511: INFO: Got endpoints: latency-svc-rmp8t [377.714648ms]
  Dec 16 17:27:19.512: INFO: Got endpoints: latency-svc-gf2gw [401.119844ms]
  Dec 16 17:27:19.547: INFO: Created: latency-svc-nw6bl
  Dec 16 17:27:19.558: INFO: Got endpoints: latency-svc-p82fc [425.389329ms]
  Dec 16 17:27:19.559: INFO: Got endpoints: latency-svc-l44l4 [392.519747ms]
  Dec 16 17:27:19.559: INFO: Got endpoints: latency-svc-gxshc [395.683161ms]
  Dec 16 17:27:19.585: INFO: Created: latency-svc-bv4l6
  Dec 16 17:27:19.600: INFO: Got endpoints: latency-svc-tjtzk [550.118151ms]
  Dec 16 17:27:19.605: INFO: Got endpoints: latency-svc-xvcdv [217.339157ms]
  Dec 16 17:27:19.627: INFO: Created: latency-svc-hfn4h
  Dec 16 17:27:19.637: INFO: Created: latency-svc-frlxn
  Dec 16 17:27:19.645: INFO: Got endpoints: latency-svc-d8jr7 [233.814791ms]
  Dec 16 17:27:19.659: INFO: Created: latency-svc-tqbkb
  Dec 16 17:27:19.683: INFO: Got endpoints: latency-svc-nw6bl [262.806434ms]
  Dec 16 17:27:19.686: INFO: Created: latency-svc-b6zjc
  Dec 16 17:27:19.719: INFO: Created: latency-svc-skt49
  Dec 16 17:27:19.734: INFO: Got endpoints: latency-svc-bv4l6 [312.157022ms]
  Dec 16 17:27:19.735: INFO: Created: latency-svc-nvmts
  Dec 16 17:27:19.747: INFO: Created: latency-svc-jhwn2
  Dec 16 17:27:19.759: INFO: Created: latency-svc-jcvmd
  Dec 16 17:27:19.784: INFO: Created: latency-svc-zcvp7
  Dec 16 17:27:19.785: INFO: Got endpoints: latency-svc-hfn4h [347.067939ms]
  Dec 16 17:27:19.797: INFO: Created: latency-svc-xlnwr
  Dec 16 17:27:19.818: INFO: Created: latency-svc-zj56t
  Dec 16 17:27:19.839: INFO: Created: latency-svc-cvvxf
  Dec 16 17:27:19.849: INFO: Got endpoints: latency-svc-frlxn [401.812363ms]
  Dec 16 17:27:19.886: INFO: Created: latency-svc-r24zf
  Dec 16 17:27:19.908: INFO: Got endpoints: latency-svc-tqbkb [402.215411ms]
  Dec 16 17:27:19.936: INFO: Created: latency-svc-f6cxv
  Dec 16 17:27:19.952: INFO: Created: latency-svc-6fd5l
  Dec 16 17:27:19.955: INFO: Got endpoints: latency-svc-b6zjc [447.531388ms]
  Dec 16 17:27:19.976: INFO: Got endpoints: latency-svc-skt49 [469.754768ms]
  Dec 16 17:27:19.979: INFO: Created: latency-svc-b7fwv
  Dec 16 17:27:19.998: INFO: Created: latency-svc-vt2hq
  Dec 16 17:27:20.019: INFO: Created: latency-svc-6n424
  Dec 16 17:27:20.036: INFO: Created: latency-svc-trzbp
  Dec 16 17:27:20.041: INFO: Got endpoints: latency-svc-nvmts [530.263763ms]
  Dec 16 17:27:20.076: INFO: Created: latency-svc-ph68s
  Dec 16 17:27:20.090: INFO: Got endpoints: latency-svc-jhwn2 [577.818937ms]
  Dec 16 17:27:20.101: INFO: Created: latency-svc-w9jsz
  Dec 16 17:27:20.115: INFO: Got endpoints: latency-svc-jcvmd [557.067814ms]
  Dec 16 17:27:20.134: INFO: Created: latency-svc-pmvph
  Dec 16 17:27:20.147: INFO: Created: latency-svc-2xlnt
  Dec 16 17:27:20.174: INFO: Got endpoints: latency-svc-zcvp7 [614.56243ms]
  Dec 16 17:27:20.199: INFO: Created: latency-svc-fdwrl
  Dec 16 17:27:20.226: INFO: Got endpoints: latency-svc-xlnwr [667.69622ms]
  Dec 16 17:27:20.253: INFO: Created: latency-svc-rjq9d
  Dec 16 17:27:20.276: INFO: Got endpoints: latency-svc-zj56t [670.960102ms]
  Dec 16 17:27:20.303: INFO: Created: latency-svc-rkg95
  Dec 16 17:27:20.323: INFO: Got endpoints: latency-svc-cvvxf [721.963226ms]
  Dec 16 17:27:20.377: INFO: Got endpoints: latency-svc-r24zf [731.358728ms]
  Dec 16 17:27:20.384: INFO: Created: latency-svc-25rbt
  Dec 16 17:27:20.419: INFO: Created: latency-svc-np5g8
  Dec 16 17:27:20.435: INFO: Got endpoints: latency-svc-f6cxv [752.176523ms]
  Dec 16 17:27:20.482: INFO: Created: latency-svc-pgv4b
  Dec 16 17:27:20.491: INFO: Got endpoints: latency-svc-6fd5l [757.152454ms]
  E1216 17:27:20.510240      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:27:20.528: INFO: Created: latency-svc-xb7n2
  Dec 16 17:27:20.536: INFO: Got endpoints: latency-svc-b7fwv [750.874652ms]
  Dec 16 17:27:20.561: INFO: Created: latency-svc-t2942
  Dec 16 17:27:20.584: INFO: Got endpoints: latency-svc-vt2hq [735.12681ms]
  Dec 16 17:27:20.690: INFO: Created: latency-svc-87fxh
  Dec 16 17:27:20.693: INFO: Got endpoints: latency-svc-6n424 [785.261488ms]
  Dec 16 17:27:20.698: INFO: Got endpoints: latency-svc-trzbp [743.152255ms]
  Dec 16 17:27:20.780: INFO: Got endpoints: latency-svc-ph68s [803.331409ms]
  Dec 16 17:27:20.820: INFO: Got endpoints: latency-svc-w9jsz [778.688093ms]
  Dec 16 17:27:20.836: INFO: Got endpoints: latency-svc-pmvph [746.631226ms]
  Dec 16 17:27:20.848: INFO: Created: latency-svc-n6vkw
  Dec 16 17:27:20.872: INFO: Created: latency-svc-wqqvx
  Dec 16 17:27:20.884: INFO: Got endpoints: latency-svc-2xlnt [768.547471ms]
  Dec 16 17:27:20.888: INFO: Created: latency-svc-ch4xn
  Dec 16 17:27:20.930: INFO: Created: latency-svc-th6qk
  Dec 16 17:27:20.931: INFO: Got endpoints: latency-svc-fdwrl [757.498863ms]
  Dec 16 17:27:20.940: INFO: Created: latency-svc-s4bbj
  Dec 16 17:27:20.972: INFO: Created: latency-svc-fx9zm
  Dec 16 17:27:20.988: INFO: Created: latency-svc-jchx2
  Dec 16 17:27:20.992: INFO: Got endpoints: latency-svc-rjq9d [765.210715ms]
  Dec 16 17:27:21.027: INFO: Got endpoints: latency-svc-rkg95 [750.515371ms]
  Dec 16 17:27:21.039: INFO: Created: latency-svc-95lp6
  Dec 16 17:27:21.056: INFO: Created: latency-svc-fbkpv
  Dec 16 17:27:21.071: INFO: Got endpoints: latency-svc-25rbt [748.103013ms]
  Dec 16 17:27:21.107: INFO: Created: latency-svc-x7rws
  Dec 16 17:27:21.126: INFO: Got endpoints: latency-svc-np5g8 [748.795823ms]
  Dec 16 17:27:21.148: INFO: Created: latency-svc-jvnp4
  Dec 16 17:27:21.170: INFO: Got endpoints: latency-svc-pgv4b [734.378772ms]
  Dec 16 17:27:21.194: INFO: Created: latency-svc-q5fdf
  Dec 16 17:27:21.226: INFO: Got endpoints: latency-svc-xb7n2 [734.736811ms]
  Dec 16 17:27:21.293: INFO: Created: latency-svc-zhrwn
  Dec 16 17:27:21.303: INFO: Got endpoints: latency-svc-t2942 [767.128922ms]
  Dec 16 17:27:21.337: INFO: Got endpoints: latency-svc-87fxh [752.746501ms]
  Dec 16 17:27:21.380: INFO: Created: latency-svc-tfnf5
  Dec 16 17:27:21.402: INFO: Got endpoints: latency-svc-n6vkw [708.66549ms]
  Dec 16 17:27:21.419: INFO: Got endpoints: latency-svc-wqqvx [720.783152ms]
  Dec 16 17:27:21.441: INFO: Created: latency-svc-4zmp9
  Dec 16 17:27:21.469: INFO: Created: latency-svc-tqlz2
  Dec 16 17:27:21.479: INFO: Got endpoints: latency-svc-ch4xn [698.597775ms]
  Dec 16 17:27:21.505: INFO: Created: latency-svc-9t28c
  E1216 17:27:21.511403      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:27:21.539: INFO: Created: latency-svc-75z58
  Dec 16 17:27:21.543: INFO: Got endpoints: latency-svc-th6qk [722.801042ms]
  Dec 16 17:27:21.576: INFO: Got endpoints: latency-svc-s4bbj [739.021096ms]
  Dec 16 17:27:21.584: INFO: Created: latency-svc-c6x2k
  Dec 16 17:27:21.626: INFO: Created: latency-svc-b469t
  Dec 16 17:27:21.635: INFO: Got endpoints: latency-svc-fx9zm [750.892782ms]
  Dec 16 17:27:21.675: INFO: Created: latency-svc-s4dhx
  Dec 16 17:27:21.700: INFO: Got endpoints: latency-svc-jchx2 [768.640716ms]
  Dec 16 17:27:21.764: INFO: Got endpoints: latency-svc-95lp6 [772.234637ms]
  Dec 16 17:27:21.809: INFO: Got endpoints: latency-svc-fbkpv [781.353561ms]
  Dec 16 17:27:21.828: INFO: Created: latency-svc-zxcpd
  Dec 16 17:27:21.851: INFO: Got endpoints: latency-svc-x7rws [779.05933ms]
  Dec 16 17:27:21.864: INFO: Created: latency-svc-76ztt
  Dec 16 17:27:21.893: INFO: Created: latency-svc-gzp4z
  Dec 16 17:27:21.923: INFO: Got endpoints: latency-svc-jvnp4 [796.632128ms]
  Dec 16 17:27:22.012: INFO: Created: latency-svc-wbp94
  Dec 16 17:27:22.024: INFO: Got endpoints: latency-svc-q5fdf [853.505015ms]
  Dec 16 17:27:22.035: INFO: Got endpoints: latency-svc-zhrwn [809.091521ms]
  Dec 16 17:27:22.054: INFO: Got endpoints: latency-svc-tfnf5 [750.729646ms]
  Dec 16 17:27:22.089: INFO: Created: latency-svc-7g4r4
  Dec 16 17:27:22.090: INFO: Got endpoints: latency-svc-4zmp9 [752.098105ms]
  Dec 16 17:27:22.106: INFO: Created: latency-svc-9smrh
  Dec 16 17:27:22.131: INFO: Got endpoints: latency-svc-tqlz2 [728.392646ms]
  Dec 16 17:27:22.142: INFO: Created: latency-svc-zlxqz
  Dec 16 17:27:22.159: INFO: Created: latency-svc-fk9n5
  Dec 16 17:27:22.171: INFO: Created: latency-svc-fstm6
  Dec 16 17:27:22.173: INFO: Got endpoints: latency-svc-9t28c [754.42201ms]
  Dec 16 17:27:22.256: INFO: Got endpoints: latency-svc-75z58 [776.317329ms]
  Dec 16 17:27:22.301: INFO: Created: latency-svc-zs7r9
  Dec 16 17:27:22.305: INFO: Got endpoints: latency-svc-c6x2k [761.81314ms]
  Dec 16 17:27:22.330: INFO: Created: latency-svc-kfgvp
  Dec 16 17:27:22.343: INFO: Got endpoints: latency-svc-b469t [766.44945ms]
  Dec 16 17:27:22.352: INFO: Created: latency-svc-nbpw9
  Dec 16 17:27:22.377: INFO: Created: latency-svc-t5zbj
  Dec 16 17:27:22.390: INFO: Got endpoints: latency-svc-s4dhx [754.485592ms]
  Dec 16 17:27:22.407: INFO: Created: latency-svc-78gk6
  Dec 16 17:27:22.456: INFO: Got endpoints: latency-svc-zxcpd [754.673123ms]
  Dec 16 17:27:22.460: INFO: Created: latency-svc-ftvgk
  Dec 16 17:27:22.479: INFO: Got endpoints: latency-svc-76ztt [715.021708ms]
  Dec 16 17:27:22.501: INFO: Created: latency-svc-b42sm
  E1216 17:27:22.511858      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:27:22.512: INFO: Created: latency-svc-9cm6m
  Dec 16 17:27:22.528: INFO: Got endpoints: latency-svc-gzp4z [719.203254ms]
  Dec 16 17:27:22.553: INFO: Created: latency-svc-65dtr
  Dec 16 17:27:22.571: INFO: Got endpoints: latency-svc-wbp94 [719.976662ms]
  Dec 16 17:27:22.596: INFO: Created: latency-svc-7tkq2
  Dec 16 17:27:22.620: INFO: Got endpoints: latency-svc-7g4r4 [697.498704ms]
  Dec 16 17:27:22.653: INFO: Created: latency-svc-vkvmc
  Dec 16 17:27:22.673: INFO: Got endpoints: latency-svc-9smrh [648.303311ms]
  Dec 16 17:27:22.697: INFO: Created: latency-svc-2s7n6
  Dec 16 17:27:22.723: INFO: Got endpoints: latency-svc-zlxqz [687.48789ms]
  Dec 16 17:27:22.749: INFO: Created: latency-svc-47j95
  Dec 16 17:27:22.774: INFO: Got endpoints: latency-svc-fk9n5 [718.57652ms]
  Dec 16 17:27:22.824: INFO: Created: latency-svc-c85jp
  Dec 16 17:27:22.826: INFO: Got endpoints: latency-svc-fstm6 [735.089819ms]
  Dec 16 17:27:22.848: INFO: Created: latency-svc-rhdsp
  Dec 16 17:27:22.872: INFO: Got endpoints: latency-svc-zs7r9 [739.919902ms]
  Dec 16 17:27:22.902: INFO: Created: latency-svc-mrb6w
  Dec 16 17:27:22.920: INFO: Got endpoints: latency-svc-kfgvp [746.847811ms]
  Dec 16 17:27:22.972: INFO: Got endpoints: latency-svc-nbpw9 [716.387722ms]
  Dec 16 17:27:22.975: INFO: Created: latency-svc-bjvkj
  Dec 16 17:27:23.011: INFO: Created: latency-svc-vp9l2
  Dec 16 17:27:23.030: INFO: Got endpoints: latency-svc-t5zbj [724.480181ms]
  Dec 16 17:27:23.061: INFO: Created: latency-svc-gzkpt
  Dec 16 17:27:23.090: INFO: Got endpoints: latency-svc-78gk6 [746.595897ms]
  Dec 16 17:27:23.139: INFO: Got endpoints: latency-svc-ftvgk [748.618638ms]
  Dec 16 17:27:23.151: INFO: Created: latency-svc-vhpdf
  Dec 16 17:27:23.169: INFO: Created: latency-svc-dk276
  Dec 16 17:27:23.181: INFO: Got endpoints: latency-svc-b42sm [725.668489ms]
  Dec 16 17:27:23.216: INFO: Created: latency-svc-dpzgk
  Dec 16 17:27:23.228: INFO: Got endpoints: latency-svc-9cm6m [749.123335ms]
  Dec 16 17:27:23.272: INFO: Created: latency-svc-6sp9c
  Dec 16 17:27:23.276: INFO: Got endpoints: latency-svc-65dtr [747.071262ms]
  Dec 16 17:27:23.306: INFO: Created: latency-svc-dwcpl
  Dec 16 17:27:23.325: INFO: Got endpoints: latency-svc-7tkq2 [753.386127ms]
  Dec 16 17:27:23.353: INFO: Created: latency-svc-gdmlp
  Dec 16 17:27:23.366: INFO: Got endpoints: latency-svc-vkvmc [745.380916ms]
  Dec 16 17:27:23.394: INFO: Created: latency-svc-xg6s2
  Dec 16 17:27:23.420: INFO: Got endpoints: latency-svc-2s7n6 [746.67762ms]
  Dec 16 17:27:23.448: INFO: Created: latency-svc-nz6c4
  Dec 16 17:27:23.470: INFO: Got endpoints: latency-svc-47j95 [746.514127ms]
  Dec 16 17:27:23.494: INFO: Created: latency-svc-h96wv
  E1216 17:27:23.512223      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:27:23.524: INFO: Got endpoints: latency-svc-c85jp [749.092814ms]
  Dec 16 17:27:23.580: INFO: Created: latency-svc-pf9bd
  Dec 16 17:27:23.588: INFO: Got endpoints: latency-svc-rhdsp [761.870217ms]
  Dec 16 17:27:23.625: INFO: Got endpoints: latency-svc-mrb6w [753.510492ms]
  Dec 16 17:27:23.636: INFO: Created: latency-svc-gf7n9
  Dec 16 17:27:23.666: INFO: Created: latency-svc-x48bq
  Dec 16 17:27:23.681: INFO: Got endpoints: latency-svc-bjvkj [760.854214ms]
  Dec 16 17:27:23.719: INFO: Created: latency-svc-chjs2
  Dec 16 17:27:23.727: INFO: Got endpoints: latency-svc-vp9l2 [754.820902ms]
  Dec 16 17:27:23.747: INFO: Created: latency-svc-hphm6
  Dec 16 17:27:23.768: INFO: Got endpoints: latency-svc-gzkpt [738.197904ms]
  Dec 16 17:27:23.791: INFO: Created: latency-svc-659pv
  Dec 16 17:27:23.831: INFO: Got endpoints: latency-svc-vhpdf [740.275625ms]
  Dec 16 17:27:23.848: INFO: Created: latency-svc-zx8s8
  Dec 16 17:27:23.866: INFO: Got endpoints: latency-svc-dk276 [727.51589ms]
  Dec 16 17:27:23.898: INFO: Created: latency-svc-5v2cn
  Dec 16 17:27:23.934: INFO: Got endpoints: latency-svc-dpzgk [752.409574ms]
  Dec 16 17:27:23.953: INFO: Created: latency-svc-cc7pp
  Dec 16 17:27:23.972: INFO: Got endpoints: latency-svc-6sp9c [743.661131ms]
  Dec 16 17:27:23.991: INFO: Created: latency-svc-mvl72
  Dec 16 17:27:24.023: INFO: Got endpoints: latency-svc-dwcpl [747.499135ms]
  Dec 16 17:27:24.058: INFO: Created: latency-svc-vhjlt
  Dec 16 17:27:24.077: INFO: Got endpoints: latency-svc-gdmlp [752.115949ms]
  Dec 16 17:27:24.105: INFO: Created: latency-svc-rf2j7
  Dec 16 17:27:24.134: INFO: Got endpoints: latency-svc-xg6s2 [767.769337ms]
  Dec 16 17:27:24.160: INFO: Created: latency-svc-wxlqg
  Dec 16 17:27:24.172: INFO: Got endpoints: latency-svc-nz6c4 [750.668897ms]
  Dec 16 17:27:24.194: INFO: Created: latency-svc-qjw25
  Dec 16 17:27:24.220: INFO: Got endpoints: latency-svc-h96wv [750.370564ms]
  Dec 16 17:27:24.246: INFO: Created: latency-svc-cv2sp
  Dec 16 17:27:24.271: INFO: Got endpoints: latency-svc-pf9bd [747.382362ms]
  Dec 16 17:27:24.290: INFO: Created: latency-svc-64n2n
  Dec 16 17:27:24.318: INFO: Got endpoints: latency-svc-gf7n9 [730.098128ms]
  Dec 16 17:27:24.350: INFO: Created: latency-svc-2rb2d
  Dec 16 17:27:24.374: INFO: Got endpoints: latency-svc-x48bq [748.424828ms]
  Dec 16 17:27:24.404: INFO: Created: latency-svc-z28dt
  Dec 16 17:27:24.420: INFO: Got endpoints: latency-svc-chjs2 [738.167234ms]
  Dec 16 17:27:24.451: INFO: Created: latency-svc-djvgk
  Dec 16 17:27:24.468: INFO: Got endpoints: latency-svc-hphm6 [739.876878ms]
  Dec 16 17:27:24.494: INFO: Created: latency-svc-59zlf
  E1216 17:27:24.514414      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:27:24.518: INFO: Got endpoints: latency-svc-659pv [749.969995ms]
  Dec 16 17:27:24.546: INFO: Created: latency-svc-sldrc
  Dec 16 17:27:24.571: INFO: Got endpoints: latency-svc-zx8s8 [740.589499ms]
  Dec 16 17:27:24.631: INFO: Created: latency-svc-lzlht
  Dec 16 17:27:24.670: INFO: Got endpoints: latency-svc-5v2cn [803.723782ms]
  Dec 16 17:27:24.685: INFO: Got endpoints: latency-svc-cc7pp [751.476958ms]
  Dec 16 17:27:24.717: INFO: Created: latency-svc-tlxkr
  Dec 16 17:27:24.751: INFO: Got endpoints: latency-svc-mvl72 [778.234555ms]
  Dec 16 17:27:24.767: INFO: Created: latency-svc-9ns6l
  Dec 16 17:27:24.801: INFO: Got endpoints: latency-svc-vhjlt [777.420912ms]
  Dec 16 17:27:24.809: INFO: Created: latency-svc-5khhz
  Dec 16 17:27:24.823: INFO: Got endpoints: latency-svc-rf2j7 [745.113019ms]
  Dec 16 17:27:24.843: INFO: Created: latency-svc-qr674
  Dec 16 17:27:24.861: INFO: Created: latency-svc-p9c6b
  Dec 16 17:27:24.889: INFO: Got endpoints: latency-svc-wxlqg [754.704871ms]
  Dec 16 17:27:24.925: INFO: Got endpoints: latency-svc-qjw25 [752.900237ms]
  Dec 16 17:27:24.958: INFO: Created: latency-svc-xg9wf
  Dec 16 17:27:24.967: INFO: Got endpoints: latency-svc-cv2sp [746.296099ms]
  Dec 16 17:27:24.982: INFO: Created: latency-svc-x6qfw
  Dec 16 17:27:25.003: INFO: Created: latency-svc-hqh4c
  Dec 16 17:27:25.042: INFO: Got endpoints: latency-svc-64n2n [770.546522ms]
  Dec 16 17:27:25.106: INFO: Created: latency-svc-8b7r4
  Dec 16 17:27:25.125: INFO: Got endpoints: latency-svc-2rb2d [806.886639ms]
  Dec 16 17:27:25.146: INFO: Got endpoints: latency-svc-z28dt [771.729986ms]
  Dec 16 17:27:25.199: INFO: Got endpoints: latency-svc-djvgk [778.277895ms]
  Dec 16 17:27:25.231: INFO: Created: latency-svc-92wnw
  Dec 16 17:27:25.244: INFO: Created: latency-svc-gbr5z
  Dec 16 17:27:25.247: INFO: Created: latency-svc-zmgcr
  Dec 16 17:27:25.255: INFO: Got endpoints: latency-svc-59zlf [786.406729ms]
  Dec 16 17:27:25.270: INFO: Got endpoints: latency-svc-sldrc [751.491247ms]
  Dec 16 17:27:25.284: INFO: Created: latency-svc-2rl2g
  Dec 16 17:27:25.297: INFO: Created: latency-svc-zqk4q
  Dec 16 17:27:25.328: INFO: Got endpoints: latency-svc-lzlht [756.311535ms]
  Dec 16 17:27:25.357: INFO: Created: latency-svc-ddvx4
  Dec 16 17:27:25.377: INFO: Got endpoints: latency-svc-tlxkr [706.457203ms]
  Dec 16 17:27:25.415: INFO: Created: latency-svc-84f6s
  Dec 16 17:27:25.431: INFO: Got endpoints: latency-svc-9ns6l [745.343255ms]
  Dec 16 17:27:25.466: INFO: Created: latency-svc-c9gd7
  Dec 16 17:27:25.484: INFO: Got endpoints: latency-svc-5khhz [732.949347ms]
  Dec 16 17:27:25.509: INFO: Created: latency-svc-df7dn
  E1216 17:27:25.513648      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:27:25.522: INFO: Got endpoints: latency-svc-qr674 [721.63871ms]
  Dec 16 17:27:25.563: INFO: Created: latency-svc-wznxk
  Dec 16 17:27:25.575: INFO: Got endpoints: latency-svc-p9c6b [751.884901ms]
  Dec 16 17:27:25.612: INFO: Created: latency-svc-lqq6g
  Dec 16 17:27:25.625: INFO: Got endpoints: latency-svc-xg9wf [736.439921ms]
  Dec 16 17:27:25.649: INFO: Created: latency-svc-bhkl7
  Dec 16 17:27:25.673: INFO: Got endpoints: latency-svc-x6qfw [748.406001ms]
  Dec 16 17:27:25.694: INFO: Created: latency-svc-dqnbj
  Dec 16 17:27:25.714: INFO: Got endpoints: latency-svc-hqh4c [747.053751ms]
  Dec 16 17:27:25.747: INFO: Created: latency-svc-fvkkv
  Dec 16 17:27:25.770: INFO: Got endpoints: latency-svc-8b7r4 [727.422376ms]
  Dec 16 17:27:25.808: INFO: Created: latency-svc-jmtkx
  Dec 16 17:27:25.821: INFO: Got endpoints: latency-svc-92wnw [675.162147ms]
  Dec 16 17:27:25.847: INFO: Created: latency-svc-mlwqp
  Dec 16 17:27:25.871: INFO: Got endpoints: latency-svc-gbr5z [743.542415ms]
  Dec 16 17:27:25.923: INFO: Got endpoints: latency-svc-zmgcr [724.454624ms]
  Dec 16 17:27:25.973: INFO: Got endpoints: latency-svc-2rl2g [717.428852ms]
  Dec 16 17:27:26.025: INFO: Got endpoints: latency-svc-zqk4q [754.465437ms]
  Dec 16 17:27:26.070: INFO: Got endpoints: latency-svc-ddvx4 [741.55996ms]
  Dec 16 17:27:26.126: INFO: Got endpoints: latency-svc-84f6s [749.423306ms]
  Dec 16 17:27:26.173: INFO: Got endpoints: latency-svc-c9gd7 [741.940138ms]
  Dec 16 17:27:26.230: INFO: Got endpoints: latency-svc-df7dn [745.23998ms]
  Dec 16 17:27:26.303: INFO: Got endpoints: latency-svc-wznxk [780.075664ms]
  Dec 16 17:27:26.326: INFO: Got endpoints: latency-svc-lqq6g [751.223583ms]
  Dec 16 17:27:26.372: INFO: Got endpoints: latency-svc-bhkl7 [746.729979ms]
  Dec 16 17:27:26.428: INFO: Got endpoints: latency-svc-dqnbj [754.676386ms]
  Dec 16 17:27:26.467: INFO: Got endpoints: latency-svc-fvkkv [752.456222ms]
  E1216 17:27:26.514603      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:27:26.524: INFO: Got endpoints: latency-svc-jmtkx [753.331859ms]
  Dec 16 17:27:26.590: INFO: Got endpoints: latency-svc-mlwqp [768.470589ms]
  Dec 16 17:27:26.590: INFO: Latencies: [65.360133ms 96.92891ms 136.380585ms 137.258717ms 146.564461ms 163.343241ms 164.034582ms 169.324516ms 182.330681ms 204.139314ms 204.635083ms 213.100215ms 217.339157ms 233.814791ms 237.78556ms 256.815956ms 262.806434ms 263.975324ms 291.50427ms 308.73292ms 309.228976ms 312.157022ms 312.434645ms 315.078941ms 315.096483ms 318.592501ms 329.465108ms 344.250941ms 347.067939ms 377.714648ms 377.719648ms 392.519747ms 394.924285ms 395.683161ms 401.119844ms 401.812363ms 402.215411ms 414.164112ms 423.782176ms 425.389329ms 428.936851ms 447.531388ms 451.632163ms 452.017622ms 458.379317ms 466.254225ms 469.754768ms 474.163143ms 481.107367ms 485.332533ms 485.336746ms 500.817842ms 505.793198ms 510.170406ms 530.263763ms 539.406405ms 550.118151ms 557.067814ms 564.520812ms 571.832077ms 577.818937ms 599.177688ms 605.644482ms 614.56243ms 648.303311ms 649.630318ms 650.175762ms 663.028419ms 667.69622ms 670.960102ms 675.162147ms 677.216662ms 677.633169ms 681.24445ms 687.48789ms 697.498704ms 698.597775ms 702.712051ms 706.457203ms 708.66549ms 711.948278ms 715.021708ms 716.387722ms 717.428852ms 718.57652ms 719.203254ms 719.976662ms 720.783152ms 721.63871ms 721.963226ms 722.801042ms 724.454624ms 724.480181ms 725.668489ms 727.422376ms 727.51589ms 728.392646ms 730.098128ms 731.358728ms 732.949347ms 734.378772ms 734.736811ms 735.089819ms 735.12681ms 736.439921ms 738.167234ms 738.197904ms 739.021096ms 739.876878ms 739.919902ms 740.275625ms 740.589499ms 741.55996ms 741.940138ms 743.152255ms 743.542415ms 743.661131ms 745.113019ms 745.23998ms 745.343255ms 745.380916ms 746.296099ms 746.514127ms 746.595897ms 746.631226ms 746.67762ms 746.729979ms 746.847811ms 747.053751ms 747.071262ms 747.382362ms 747.499135ms 748.103013ms 748.406001ms 748.424828ms 748.618638ms 748.795823ms 749.092814ms 749.123335ms 749.423306ms 749.969995ms 750.370564ms 750.515371ms 750.668897ms 750.729646ms 750.874652ms 750.892782ms 751.223583ms 751.476958ms 751.491247ms 751.884901ms 752.098105ms 752.115949ms 752.176523ms 752.409574ms 752.456222ms 752.746501ms 752.900237ms 753.331859ms 753.386127ms 753.510492ms 754.42201ms 754.465437ms 754.485592ms 754.673123ms 754.676386ms 754.704871ms 754.820902ms 756.311535ms 757.152454ms 757.498863ms 760.854214ms 761.81314ms 761.870217ms 765.210715ms 766.44945ms 767.128922ms 767.769337ms 768.470589ms 768.547471ms 768.640716ms 770.546522ms 771.729986ms 772.234637ms 776.317329ms 777.420912ms 778.234555ms 778.277895ms 778.688093ms 779.05933ms 780.075664ms 781.353561ms 785.261488ms 786.406729ms 796.632128ms 803.331409ms 803.723782ms 806.886639ms 809.091521ms 853.505015ms]
  Dec 16 17:27:26.590: INFO: 50 %ile: 734.378772ms
  Dec 16 17:27:26.590: INFO: 90 %ile: 768.640716ms
  Dec 16 17:27:26.590: INFO: 99 %ile: 809.091521ms
  Dec 16 17:27:26.590: INFO: Total sample count: 200
  Dec 16 17:27:26.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-7043" for this suite. @ 12/16/23 17:27:26.61
• [11.067 seconds]
------------------------------
SS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 12/16/23 17:27:26.63
  Dec 16 17:27:26.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename podtemplate @ 12/16/23 17:27:26.634
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:27:26.717
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:27:26.724
  Dec 16 17:27:26.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-9191" for this suite. @ 12/16/23 17:27:26.853
• [0.237 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 12/16/23 17:27:26.872
  Dec 16 17:27:26.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename webhook @ 12/16/23 17:27:26.875
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:27:26.927
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:27:26.945
  STEP: Setting up server cert @ 12/16/23 17:27:27.005
  E1216 17:27:27.514839      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/16/23 17:27:27.798
  STEP: Deploying the webhook pod @ 12/16/23 17:27:27.806
  STEP: Wait for the deployment to be ready @ 12/16/23 17:27:27.833
  Dec 16 17:27:27.885: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E1216 17:27:28.515336      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:29.515911      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/16/23 17:27:29.911
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 17:27:29.944
  E1216 17:27:30.517027      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:27:30.945: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 12/16/23 17:27:31.121
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 12/16/23 17:27:31.218
  STEP: Deleting the collection of validation webhooks @ 12/16/23 17:27:31.301
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 12/16/23 17:27:31.411
  Dec 16 17:27:31.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E1216 17:27:31.517414      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-4800" for this suite. @ 12/16/23 17:27:31.6
  STEP: Destroying namespace "webhook-markers-1869" for this suite. @ 12/16/23 17:27:31.617
• [4.756 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 12/16/23 17:27:31.629
  Dec 16 17:27:31.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename resourcequota @ 12/16/23 17:27:31.633
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:27:31.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:27:31.679
  STEP: Creating a ResourceQuota @ 12/16/23 17:27:31.686
  STEP: Getting a ResourceQuota @ 12/16/23 17:27:31.702
  STEP: Listing all ResourceQuotas with LabelSelector @ 12/16/23 17:27:31.708
  STEP: Patching the ResourceQuota @ 12/16/23 17:27:31.729
  STEP: Deleting a Collection of ResourceQuotas @ 12/16/23 17:27:31.758
  STEP: Verifying the deleted ResourceQuota @ 12/16/23 17:27:31.785
  Dec 16 17:27:31.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3597" for this suite. @ 12/16/23 17:27:31.805
• [0.201 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 12/16/23 17:27:31.847
  Dec 16 17:27:31.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubelet-test @ 12/16/23 17:27:31.852
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:27:31.893
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:27:31.916
  E1216 17:27:32.518125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:33.519823      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:27:34.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-9500" for this suite. @ 12/16/23 17:27:34.061
• [2.254 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 12/16/23 17:27:34.103
  Dec 16 17:27:34.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename gc @ 12/16/23 17:27:34.106
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:27:34.139
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:27:34.151
  STEP: create the rc @ 12/16/23 17:27:34.179
  W1216 17:27:34.196896      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E1216 17:27:34.519224      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:35.519587      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:36.520174      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:37.521380      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:38.523230      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:39.522795      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 12/16/23 17:27:40.36
  STEP: wait for the rc to be deleted @ 12/16/23 17:27:40.446
  E1216 17:27:40.523843      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:41.524282      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:42.524699      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:43.525225      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:44.527115      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 12/16/23 17:27:45.507
  E1216 17:27:45.526463      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:46.527363      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:47.528137      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:48.528545      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:49.529049      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:50.530253      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:51.531166      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:52.532120      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:53.532697      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:54.533056      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:55.533742      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:56.534767      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:57.535579      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:58.535834      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:27:59.536340      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:00.536229      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:01.536354      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:02.536571      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:03.536846      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:04.537159      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:05.538061      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:06.538426      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:07.538570      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:08.538869      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:09.539238      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:10.540212      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:11.541312      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:12.541302      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:13.542484      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:14.542386      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:15.543203      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 12/16/23 17:28:15.551
  Dec 16 17:28:15.777: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Dec 16 17:28:15.777: INFO: Deleting pod "simpletest.rc-22zj5" in namespace "gc-8575"
  Dec 16 17:28:15.819: INFO: Deleting pod "simpletest.rc-25k7l" in namespace "gc-8575"
  Dec 16 17:28:15.872: INFO: Deleting pod "simpletest.rc-27qvt" in namespace "gc-8575"
  Dec 16 17:28:15.904: INFO: Deleting pod "simpletest.rc-2cztw" in namespace "gc-8575"
  Dec 16 17:28:16.003: INFO: Deleting pod "simpletest.rc-2s2wd" in namespace "gc-8575"
  Dec 16 17:28:16.046: INFO: Deleting pod "simpletest.rc-2vrln" in namespace "gc-8575"
  Dec 16 17:28:16.097: INFO: Deleting pod "simpletest.rc-4f5fb" in namespace "gc-8575"
  Dec 16 17:28:16.147: INFO: Deleting pod "simpletest.rc-4p8fb" in namespace "gc-8575"
  Dec 16 17:28:16.230: INFO: Deleting pod "simpletest.rc-4t9xp" in namespace "gc-8575"
  Dec 16 17:28:16.298: INFO: Deleting pod "simpletest.rc-4xvh2" in namespace "gc-8575"
  Dec 16 17:28:16.374: INFO: Deleting pod "simpletest.rc-5dwhz" in namespace "gc-8575"
  Dec 16 17:28:16.498: INFO: Deleting pod "simpletest.rc-5qmjj" in namespace "gc-8575"
  Dec 16 17:28:16.543: INFO: Deleting pod "simpletest.rc-5s2kb" in namespace "gc-8575"
  E1216 17:28:16.544296      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:28:16.772: INFO: Deleting pod "simpletest.rc-5s8bg" in namespace "gc-8575"
  Dec 16 17:28:17.170: INFO: Deleting pod "simpletest.rc-62nq2" in namespace "gc-8575"
  Dec 16 17:28:17.310: INFO: Deleting pod "simpletest.rc-66jg9" in namespace "gc-8575"
  Dec 16 17:28:17.352: INFO: Deleting pod "simpletest.rc-6b65s" in namespace "gc-8575"
  Dec 16 17:28:17.450: INFO: Deleting pod "simpletest.rc-6c75c" in namespace "gc-8575"
  Dec 16 17:28:17.503: INFO: Deleting pod "simpletest.rc-6fzzf" in namespace "gc-8575"
  E1216 17:28:17.544945      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:28:17.642: INFO: Deleting pod "simpletest.rc-6gcnj" in namespace "gc-8575"
  Dec 16 17:28:17.896: INFO: Deleting pod "simpletest.rc-6jtls" in namespace "gc-8575"
  Dec 16 17:28:18.009: INFO: Deleting pod "simpletest.rc-6r5fl" in namespace "gc-8575"
  Dec 16 17:28:18.141: INFO: Deleting pod "simpletest.rc-6ss7l" in namespace "gc-8575"
  Dec 16 17:28:18.315: INFO: Deleting pod "simpletest.rc-76cg9" in namespace "gc-8575"
  Dec 16 17:28:18.437: INFO: Deleting pod "simpletest.rc-76xzw" in namespace "gc-8575"
  Dec 16 17:28:18.505: INFO: Deleting pod "simpletest.rc-7gz22" in namespace "gc-8575"
  E1216 17:28:18.545929      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:28:18.546: INFO: Deleting pod "simpletest.rc-7hd7k" in namespace "gc-8575"
  Dec 16 17:28:18.593: INFO: Deleting pod "simpletest.rc-7jlft" in namespace "gc-8575"
  Dec 16 17:28:18.630: INFO: Deleting pod "simpletest.rc-8v468" in namespace "gc-8575"
  Dec 16 17:28:18.715: INFO: Deleting pod "simpletest.rc-8vlpx" in namespace "gc-8575"
  Dec 16 17:28:18.778: INFO: Deleting pod "simpletest.rc-99c59" in namespace "gc-8575"
  Dec 16 17:28:18.892: INFO: Deleting pod "simpletest.rc-9m9w4" in namespace "gc-8575"
  Dec 16 17:28:18.983: INFO: Deleting pod "simpletest.rc-9px7s" in namespace "gc-8575"
  Dec 16 17:28:19.078: INFO: Deleting pod "simpletest.rc-9t9gr" in namespace "gc-8575"
  Dec 16 17:28:19.179: INFO: Deleting pod "simpletest.rc-9v7rz" in namespace "gc-8575"
  Dec 16 17:28:19.246: INFO: Deleting pod "simpletest.rc-b25z8" in namespace "gc-8575"
  Dec 16 17:28:19.462: INFO: Deleting pod "simpletest.rc-b8lhf" in namespace "gc-8575"
  E1216 17:28:19.548412      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:28:19.551: INFO: Deleting pod "simpletest.rc-bjcp7" in namespace "gc-8575"
  Dec 16 17:28:19.626: INFO: Deleting pod "simpletest.rc-br659" in namespace "gc-8575"
  Dec 16 17:28:19.693: INFO: Deleting pod "simpletest.rc-cb8cc" in namespace "gc-8575"
  Dec 16 17:28:19.778: INFO: Deleting pod "simpletest.rc-ch2nl" in namespace "gc-8575"
  Dec 16 17:28:19.953: INFO: Deleting pod "simpletest.rc-ckqsw" in namespace "gc-8575"
  Dec 16 17:28:20.055: INFO: Deleting pod "simpletest.rc-cp6t4" in namespace "gc-8575"
  Dec 16 17:28:20.133: INFO: Deleting pod "simpletest.rc-cpr7f" in namespace "gc-8575"
  Dec 16 17:28:20.224: INFO: Deleting pod "simpletest.rc-cqm95" in namespace "gc-8575"
  Dec 16 17:28:20.329: INFO: Deleting pod "simpletest.rc-cv7q5" in namespace "gc-8575"
  Dec 16 17:28:20.405: INFO: Deleting pod "simpletest.rc-cw54t" in namespace "gc-8575"
  Dec 16 17:28:20.528: INFO: Deleting pod "simpletest.rc-db4qh" in namespace "gc-8575"
  E1216 17:28:20.548709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:28:20.575: INFO: Deleting pod "simpletest.rc-dcxtd" in namespace "gc-8575"
  Dec 16 17:28:20.691: INFO: Deleting pod "simpletest.rc-fmfq9" in namespace "gc-8575"
  Dec 16 17:28:21.210: INFO: Deleting pod "simpletest.rc-fp5mw" in namespace "gc-8575"
  Dec 16 17:28:21.443: INFO: Deleting pod "simpletest.rc-fscw4" in namespace "gc-8575"
  E1216 17:28:21.549547      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:28:21.724: INFO: Deleting pod "simpletest.rc-h9wcq" in namespace "gc-8575"
  Dec 16 17:28:21.804: INFO: Deleting pod "simpletest.rc-hl2w2" in namespace "gc-8575"
  Dec 16 17:28:21.895: INFO: Deleting pod "simpletest.rc-hx4kc" in namespace "gc-8575"
  Dec 16 17:28:22.012: INFO: Deleting pod "simpletest.rc-jwwjt" in namespace "gc-8575"
  Dec 16 17:28:22.132: INFO: Deleting pod "simpletest.rc-k6b6w" in namespace "gc-8575"
  Dec 16 17:28:22.175: INFO: Deleting pod "simpletest.rc-kfw6v" in namespace "gc-8575"
  Dec 16 17:28:22.502: INFO: Deleting pod "simpletest.rc-kthzg" in namespace "gc-8575"
  E1216 17:28:22.549828      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:28:22.578: INFO: Deleting pod "simpletest.rc-kzf6j" in namespace "gc-8575"
  Dec 16 17:28:22.723: INFO: Deleting pod "simpletest.rc-l5bvj" in namespace "gc-8575"
  Dec 16 17:28:22.819: INFO: Deleting pod "simpletest.rc-l7sxt" in namespace "gc-8575"
  Dec 16 17:28:22.874: INFO: Deleting pod "simpletest.rc-ljnvh" in namespace "gc-8575"
  Dec 16 17:28:23.031: INFO: Deleting pod "simpletest.rc-ll49n" in namespace "gc-8575"
  Dec 16 17:28:23.089: INFO: Deleting pod "simpletest.rc-lpqqm" in namespace "gc-8575"
  Dec 16 17:28:23.136: INFO: Deleting pod "simpletest.rc-lvzcf" in namespace "gc-8575"
  Dec 16 17:28:23.455: INFO: Deleting pod "simpletest.rc-mh24m" in namespace "gc-8575"
  Dec 16 17:28:23.528: INFO: Deleting pod "simpletest.rc-mjmhk" in namespace "gc-8575"
  E1216 17:28:23.550453      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:28:23.646: INFO: Deleting pod "simpletest.rc-mrgq7" in namespace "gc-8575"
  Dec 16 17:28:23.717: INFO: Deleting pod "simpletest.rc-msqb8" in namespace "gc-8575"
  Dec 16 17:28:23.821: INFO: Deleting pod "simpletest.rc-mwv29" in namespace "gc-8575"
  Dec 16 17:28:23.961: INFO: Deleting pod "simpletest.rc-njh5b" in namespace "gc-8575"
  Dec 16 17:28:24.028: INFO: Deleting pod "simpletest.rc-nqscj" in namespace "gc-8575"
  Dec 16 17:28:24.194: INFO: Deleting pod "simpletest.rc-p2rg8" in namespace "gc-8575"
  Dec 16 17:28:24.285: INFO: Deleting pod "simpletest.rc-pcdkc" in namespace "gc-8575"
  Dec 16 17:28:24.380: INFO: Deleting pod "simpletest.rc-psfhv" in namespace "gc-8575"
  E1216 17:28:24.550509      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:28:24.572: INFO: Deleting pod "simpletest.rc-qcrtw" in namespace "gc-8575"
  Dec 16 17:28:24.656: INFO: Deleting pod "simpletest.rc-qn4zz" in namespace "gc-8575"
  Dec 16 17:28:25.077: INFO: Deleting pod "simpletest.rc-qxhrd" in namespace "gc-8575"
  Dec 16 17:28:25.187: INFO: Deleting pod "simpletest.rc-r78hr" in namespace "gc-8575"
  Dec 16 17:28:25.230: INFO: Deleting pod "simpletest.rc-rdjw7" in namespace "gc-8575"
  Dec 16 17:28:25.344: INFO: Deleting pod "simpletest.rc-rj8wg" in namespace "gc-8575"
  Dec 16 17:28:25.390: INFO: Deleting pod "simpletest.rc-rjflt" in namespace "gc-8575"
  E1216 17:28:25.551656      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:28:25.561: INFO: Deleting pod "simpletest.rc-rlj8f" in namespace "gc-8575"
  Dec 16 17:28:25.678: INFO: Deleting pod "simpletest.rc-rlwmb" in namespace "gc-8575"
  Dec 16 17:28:25.758: INFO: Deleting pod "simpletest.rc-rxwz6" in namespace "gc-8575"
  Dec 16 17:28:25.840: INFO: Deleting pod "simpletest.rc-s29z9" in namespace "gc-8575"
  Dec 16 17:28:25.944: INFO: Deleting pod "simpletest.rc-s79dp" in namespace "gc-8575"
  Dec 16 17:28:26.020: INFO: Deleting pod "simpletest.rc-s9nck" in namespace "gc-8575"
  Dec 16 17:28:26.135: INFO: Deleting pod "simpletest.rc-sh8tt" in namespace "gc-8575"
  Dec 16 17:28:26.244: INFO: Deleting pod "simpletest.rc-sjbrw" in namespace "gc-8575"
  Dec 16 17:28:26.302: INFO: Deleting pod "simpletest.rc-smsb6" in namespace "gc-8575"
  Dec 16 17:28:26.484: INFO: Deleting pod "simpletest.rc-sqnlb" in namespace "gc-8575"
  E1216 17:28:26.552220      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:28:26.687: INFO: Deleting pod "simpletest.rc-ss6zn" in namespace "gc-8575"
  Dec 16 17:28:26.818: INFO: Deleting pod "simpletest.rc-tjpgq" in namespace "gc-8575"
  Dec 16 17:28:26.912: INFO: Deleting pod "simpletest.rc-tnr2x" in namespace "gc-8575"
  Dec 16 17:28:27.001: INFO: Deleting pod "simpletest.rc-tsmwz" in namespace "gc-8575"
  Dec 16 17:28:27.124: INFO: Deleting pod "simpletest.rc-twj88" in namespace "gc-8575"
  Dec 16 17:28:27.244: INFO: Deleting pod "simpletest.rc-xgsxc" in namespace "gc-8575"
  Dec 16 17:28:27.297: INFO: Deleting pod "simpletest.rc-z599q" in namespace "gc-8575"
  Dec 16 17:28:27.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8575" for this suite. @ 12/16/23 17:28:27.38
• [53.322 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 12/16/23 17:28:27.433
  Dec 16 17:28:27.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename services @ 12/16/23 17:28:27.445
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:28:27.494
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:28:27.502
  STEP: creating service in namespace services-7850 @ 12/16/23 17:28:27.514
  STEP: creating service affinity-clusterip in namespace services-7850 @ 12/16/23 17:28:27.514
  E1216 17:28:27.553410      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating replication controller affinity-clusterip in namespace services-7850 @ 12/16/23 17:28:27.575
  I1216 17:28:27.594336      13 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-7850, replica count: 3
  E1216 17:28:28.553544      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:29.554416      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:30.555305      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:28:30.645372      13 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E1216 17:28:31.555717      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:32.556380      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:33.556689      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:28:33.646270      13 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E1216 17:28:34.557460      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:35.557426      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:36.558336      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:28:36.647122      13 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Dec 16 17:28:36.693: INFO: Creating new exec pod
  E1216 17:28:37.559167      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:38.561145      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:39.560326      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:28:39.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-7850 exec execpod-affinityffb6s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Dec 16 17:28:40.154: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Dec 16 17:28:40.154: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 16 17:28:40.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-7850 exec execpod-affinityffb6s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.17.51 80'
  Dec 16 17:28:40.442: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.17.51 80\nConnection to 10.233.17.51 80 port [tcp/http] succeeded!\n"
  Dec 16 17:28:40.442: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 16 17:28:40.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-7850 exec execpod-affinityffb6s -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.17.51:80/ ; done'
  E1216 17:28:40.560591      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:28:41.050: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.17.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.17.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.17.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.17.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.17.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.17.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.17.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.17.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.17.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.17.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.17.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.17.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.17.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.17.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.17.51:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.17.51:80/\n"
  Dec 16 17:28:41.050: INFO: stdout: "\naffinity-clusterip-5rc7b\naffinity-clusterip-5rc7b\naffinity-clusterip-5rc7b\naffinity-clusterip-5rc7b\naffinity-clusterip-5rc7b\naffinity-clusterip-5rc7b\naffinity-clusterip-5rc7b\naffinity-clusterip-5rc7b\naffinity-clusterip-5rc7b\naffinity-clusterip-5rc7b\naffinity-clusterip-5rc7b\naffinity-clusterip-5rc7b\naffinity-clusterip-5rc7b\naffinity-clusterip-5rc7b\naffinity-clusterip-5rc7b\naffinity-clusterip-5rc7b"
  Dec 16 17:28:41.050: INFO: Received response from host: affinity-clusterip-5rc7b
  Dec 16 17:28:41.050: INFO: Received response from host: affinity-clusterip-5rc7b
  Dec 16 17:28:41.050: INFO: Received response from host: affinity-clusterip-5rc7b
  Dec 16 17:28:41.050: INFO: Received response from host: affinity-clusterip-5rc7b
  Dec 16 17:28:41.050: INFO: Received response from host: affinity-clusterip-5rc7b
  Dec 16 17:28:41.050: INFO: Received response from host: affinity-clusterip-5rc7b
  Dec 16 17:28:41.050: INFO: Received response from host: affinity-clusterip-5rc7b
  Dec 16 17:28:41.050: INFO: Received response from host: affinity-clusterip-5rc7b
  Dec 16 17:28:41.050: INFO: Received response from host: affinity-clusterip-5rc7b
  Dec 16 17:28:41.050: INFO: Received response from host: affinity-clusterip-5rc7b
  Dec 16 17:28:41.050: INFO: Received response from host: affinity-clusterip-5rc7b
  Dec 16 17:28:41.050: INFO: Received response from host: affinity-clusterip-5rc7b
  Dec 16 17:28:41.050: INFO: Received response from host: affinity-clusterip-5rc7b
  Dec 16 17:28:41.050: INFO: Received response from host: affinity-clusterip-5rc7b
  Dec 16 17:28:41.051: INFO: Received response from host: affinity-clusterip-5rc7b
  Dec 16 17:28:41.051: INFO: Received response from host: affinity-clusterip-5rc7b
  Dec 16 17:28:41.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 16 17:28:41.066: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-7850, will wait for the garbage collector to delete the pods @ 12/16/23 17:28:41.1
  Dec 16 17:28:41.195: INFO: Deleting ReplicationController affinity-clusterip took: 15.011234ms
  Dec 16 17:28:41.396: INFO: Terminating ReplicationController affinity-clusterip pods took: 200.590238ms
  E1216 17:28:41.561412      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:42.562341      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-7850" for this suite. @ 12/16/23 17:28:43.541
• [16.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 12/16/23 17:28:43.556
  Dec 16 17:28:43.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename configmap @ 12/16/23 17:28:43.56
  E1216 17:28:43.562494      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:28:43.59
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:28:43.595
  STEP: Creating configMap with name configmap-test-volume-map-d7524910-33df-4067-98eb-cffb937a1a85 @ 12/16/23 17:28:43.602
  STEP: Creating a pod to test consume configMaps @ 12/16/23 17:28:43.615
  E1216 17:28:44.563196      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:45.564187      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:46.563717      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:47.564385      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:28:47.674
  Dec 16 17:28:47.682: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-configmaps-0477be87-20c5-40a3-bc6a-d307d3c727f4 container agnhost-container: <nil>
  STEP: delete the pod @ 12/16/23 17:28:47.698
  Dec 16 17:28:47.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2549" for this suite. @ 12/16/23 17:28:47.75
• [4.211 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 12/16/23 17:28:47.771
  Dec 16 17:28:47.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename dns @ 12/16/23 17:28:47.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:28:47.812
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:28:47.818
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 12/16/23 17:28:47.822
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 12/16/23 17:28:47.822
  STEP: creating a pod to probe DNS @ 12/16/23 17:28:47.822
  STEP: submitting the pod to kubernetes @ 12/16/23 17:28:47.822
  E1216 17:28:48.564717      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:49.565107      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 12/16/23 17:28:49.861
  STEP: looking for the results for each expected name from probers @ 12/16/23 17:28:49.867
  Dec 16 17:28:49.909: INFO: DNS probes using dns-3516/dns-test-7959581c-8c45-47c2-abf4-2418f3c72f82 succeeded

  Dec 16 17:28:49.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/16/23 17:28:49.931
  STEP: Destroying namespace "dns-3516" for this suite. @ 12/16/23 17:28:49.964
• [2.208 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 12/16/23 17:28:49.989
  Dec 16 17:28:49.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-probe @ 12/16/23 17:28:49.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:28:50.019
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:28:50.025
  STEP: Creating pod liveness-abca8afd-1a64-4a59-93b5-66e3722ad0b0 in namespace container-probe-3422 @ 12/16/23 17:28:50.031
  E1216 17:28:50.565366      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:51.565505      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:28:52.064: INFO: Started pod liveness-abca8afd-1a64-4a59-93b5-66e3722ad0b0 in namespace container-probe-3422
  STEP: checking the pod's current state and verifying that restartCount is present @ 12/16/23 17:28:52.064
  Dec 16 17:28:52.070: INFO: Initial restart count of pod liveness-abca8afd-1a64-4a59-93b5-66e3722ad0b0 is 0
  E1216 17:28:52.565853      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:53.566102      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:54.566447      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:55.567319      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:56.567519      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:57.568000      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:58.568166      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:28:59.569164      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:00.570072      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:01.570312      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:02.572364      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:03.572714      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:04.573097      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:05.573207      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:06.575303      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:07.574430      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:08.575476      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:09.576245      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:10.576991      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:11.577301      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:12.578333      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:13.578447      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:14.578549      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:15.578967      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:16.579094      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:17.579838      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:18.580860      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:19.581261      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:20.581665      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:21.582604      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:22.583805      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:23.583904      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:24.584612      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:25.585156      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:26.586318      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:27.586818      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:28.587109      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:29.587591      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:30.587944      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:31.588141      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:32.588501      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:33.588529      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:34.589409      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:35.589704      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:36.589887      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:37.590453      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:38.591316      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:39.591743      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:40.592669      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:41.593744      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:42.594042      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:43.594145      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:44.594438      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:45.595462      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:46.595801      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:47.596392      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:48.596254      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:49.597062      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:50.597118      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:51.597618      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:52.598319      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:53.599243      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:54.600212      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:55.601060      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:56.600908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:57.601525      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:58.601754      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:29:59.602742      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:00.602986      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:01.603294      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:02.604181      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:03.604341      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:04.605086      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:05.605267      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:06.608705      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:07.608582      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:08.608807      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:09.609130      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:10.609968      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:11.610724      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:12.611855      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:13.611981      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:14.613208      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:15.614090      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:16.614571      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:17.615472      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:18.615671      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:19.616550      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:20.616975      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:21.617940      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:22.618894      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:23.619235      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:24.619914      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:25.620482      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:26.620509      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:27.621056      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:28.622192      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:29.622445      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:30.623140      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:31.623906      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:32.624114      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:33.624449      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:34.624576      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:35.624652      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:36.625293      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:37.625825      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:38.626241      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:39.627469      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:40.627834      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:41.628466      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:42.629249      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:43.630210      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:44.636986      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:45.637403      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:46.637694      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:47.637871      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:48.641030      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:49.641305      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:50.641695      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:51.642704      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:52.643327      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:53.643398      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:54.643973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:55.644470      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:56.644495      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:57.645341      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:58.646005      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:30:59.646214      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:00.647213      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:01.647491      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:02.648430      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:03.649018      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:04.649071      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:05.650224      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:06.651173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:07.651306      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:08.651848      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:09.651868      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:10.653210      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:11.653028      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:12.653273      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:13.654110      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:14.654742      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:15.654872      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:16.655464      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:17.655638      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:18.655913      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:19.656270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:20.656380      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:21.657047      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:22.657708      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:23.658360      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:24.658525      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:25.658765      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:26.658803      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:27.659616      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:28.660179      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:29.660777      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:30.661722      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:31.662345      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:32.662611      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:33.662692      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:34.663116      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:35.663188      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:36.663245      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:37.663629      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:38.663937      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:39.665000      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:40.665124      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:41.665285      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:42.665977      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:43.666600      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:44.667706      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:45.669298      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:46.668499      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:47.669487      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:48.670111      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:49.670795      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:50.686668      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:51.677694      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:52.677829      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:53.678788      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:54.679039      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:55.679555      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:56.681765      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:57.683586      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:58.682675      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:31:59.682613      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:00.683344      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:01.684170      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:02.684596      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:03.685477      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:04.685716      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:05.686601      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:06.686826      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:07.688001      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:08.688996      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:09.689682      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:10.689939      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:11.690347      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:12.690542      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:13.691372      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:14.692056      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:15.693160      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:16.693450      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:17.694484      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:18.694781      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:19.696233      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:20.696942      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:21.697984      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:22.713646      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:23.705475      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:24.705122      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:25.707463      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:26.706012      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:27.706991      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:28.707567      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:29.707430      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:30.708353      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:31.712096      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:32.709135      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:33.710318      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:34.710304      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:35.713290      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:36.712029      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:37.714668      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:38.714476      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:39.714114      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:40.714181      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:41.715230      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:42.715645      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:43.716650      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:44.717200      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:45.718256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:46.718501      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:47.719420      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:48.720287      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:49.721072      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:50.721674      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:51.722836      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:52.723489      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:32:53.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/16/23 17:32:53.524
  STEP: Destroying namespace "container-probe-3422" for this suite. @ 12/16/23 17:32:53.559
• [243.586 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 12/16/23 17:32:53.589
  Dec 16 17:32:53.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename dns @ 12/16/23 17:32:53.596
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:32:53.65
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:32:53.655
  STEP: Creating a test headless service @ 12/16/23 17:32:53.665
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2213.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2213.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 12/16/23 17:32:53.685
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2213.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2213.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 12/16/23 17:32:53.685
  STEP: creating a pod to probe DNS @ 12/16/23 17:32:53.685
  STEP: submitting the pod to kubernetes @ 12/16/23 17:32:53.686
  E1216 17:32:53.724140      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:54.725576      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:55.731078      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:56.729042      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:57.729307      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 12/16/23 17:32:57.765
  STEP: looking for the results for each expected name from probers @ 12/16/23 17:32:57.775
  Dec 16 17:32:57.819: INFO: Unable to read jessie_hosts@dns-querier-2 from pod dns-2213/dns-test-6140e4f3-14c4-4531-9552-10b5ee68aede: the server could not find the requested resource (get pods dns-test-6140e4f3-14c4-4531-9552-10b5ee68aede)
  Dec 16 17:32:57.819: INFO: Lookups using dns-2213/dns-test-6140e4f3-14c4-4531-9552-10b5ee68aede failed for: [jessie_hosts@dns-querier-2]

  E1216 17:32:58.729400      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:32:59.733351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:00.730992      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:01.731645      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:02.732441      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:33:02.855: INFO: DNS probes using dns-2213/dns-test-6140e4f3-14c4-4531-9552-10b5ee68aede succeeded

  Dec 16 17:33:02.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/16/23 17:33:02.867
  STEP: deleting the test headless service @ 12/16/23 17:33:02.918
  STEP: Destroying namespace "dns-2213" for this suite. @ 12/16/23 17:33:02.97
• [9.403 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 12/16/23 17:33:02.997
  Dec 16 17:33:02.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename webhook @ 12/16/23 17:33:03.006
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:33:03.046
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:33:03.053
  STEP: Setting up server cert @ 12/16/23 17:33:03.1
  E1216 17:33:03.734086      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/16/23 17:33:04.409
  STEP: Deploying the webhook pod @ 12/16/23 17:33:04.435
  STEP: Wait for the deployment to be ready @ 12/16/23 17:33:04.474
  Dec 16 17:33:04.495: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E1216 17:33:04.735331      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:05.734777      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/16/23 17:33:06.525
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 17:33:06.555
  E1216 17:33:06.735239      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:33:07.556: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Dec 16 17:33:07.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:33:07.737128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2149-crds.webhook.example.com via the AdmissionRegistration API @ 12/16/23 17:33:08.094
  STEP: Creating a custom resource while v1 is storage version @ 12/16/23 17:33:08.137
  E1216 17:33:08.737845      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:09.738365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 12/16/23 17:33:10.409
  STEP: Patching the custom resource while v2 is storage version @ 12/16/23 17:33:10.45
  Dec 16 17:33:10.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E1216 17:33:10.739359      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-8973" for this suite. @ 12/16/23 17:33:11.263
  STEP: Destroying namespace "webhook-markers-8719" for this suite. @ 12/16/23 17:33:11.28
• [8.295 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 12/16/23 17:33:11.302
  Dec 16 17:33:11.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename custom-resource-definition @ 12/16/23 17:33:11.305
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:33:11.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:33:11.339
  Dec 16 17:33:11.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:33:11.739544      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:12.740834      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:13.741201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:14.741859      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:33:14.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8267" for this suite. @ 12/16/23 17:33:14.963
• [3.678 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 12/16/23 17:33:14.99
  Dec 16 17:33:14.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename services @ 12/16/23 17:33:14.992
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:33:15.023
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:33:15.029
  STEP: creating a Service @ 12/16/23 17:33:15.042
  STEP: watching for the Service to be added @ 12/16/23 17:33:15.068
  Dec 16 17:33:15.072: INFO: Found Service test-service-9kssh in namespace services-6743 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Dec 16 17:33:15.072: INFO: Service test-service-9kssh created
  STEP: Getting /status @ 12/16/23 17:33:15.072
  Dec 16 17:33:15.085: INFO: Service test-service-9kssh has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 12/16/23 17:33:15.085
  STEP: watching for the Service to be patched @ 12/16/23 17:33:15.109
  Dec 16 17:33:15.113: INFO: observed Service test-service-9kssh in namespace services-6743 with annotations: map[] & LoadBalancer: {[]}
  Dec 16 17:33:15.113: INFO: Found Service test-service-9kssh in namespace services-6743 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Dec 16 17:33:15.113: INFO: Service test-service-9kssh has service status patched
  STEP: updating the ServiceStatus @ 12/16/23 17:33:15.113
  Dec 16 17:33:15.131: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 12/16/23 17:33:15.132
  Dec 16 17:33:15.137: INFO: Observed Service test-service-9kssh in namespace services-6743 with annotations: map[] & Conditions: {[]}
  Dec 16 17:33:15.137: INFO: Observed event: &Service{ObjectMeta:{test-service-9kssh  services-6743  d7c69305-aee5-479c-881b-c893367e3e3a 27242 0 2023-12-16 17:33:15 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-12-16 17:33:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-12-16 17:33:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.36.20,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.36.20],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Dec 16 17:33:15.137: INFO: Found Service test-service-9kssh in namespace services-6743 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Dec 16 17:33:15.137: INFO: Service test-service-9kssh has service status updated
  STEP: patching the service @ 12/16/23 17:33:15.137
  STEP: watching for the Service to be patched @ 12/16/23 17:33:15.159
  Dec 16 17:33:15.164: INFO: observed Service test-service-9kssh in namespace services-6743 with labels: map[test-service-static:true]
  Dec 16 17:33:15.164: INFO: observed Service test-service-9kssh in namespace services-6743 with labels: map[test-service-static:true]
  Dec 16 17:33:15.164: INFO: observed Service test-service-9kssh in namespace services-6743 with labels: map[test-service-static:true]
  Dec 16 17:33:15.165: INFO: Found Service test-service-9kssh in namespace services-6743 with labels: map[test-service:patched test-service-static:true]
  Dec 16 17:33:15.165: INFO: Service test-service-9kssh patched
  STEP: deleting the service @ 12/16/23 17:33:15.166
  STEP: watching for the Service to be deleted @ 12/16/23 17:33:15.197
  Dec 16 17:33:15.202: INFO: Observed event: ADDED
  Dec 16 17:33:15.202: INFO: Observed event: MODIFIED
  Dec 16 17:33:15.203: INFO: Observed event: MODIFIED
  Dec 16 17:33:15.203: INFO: Observed event: MODIFIED
  Dec 16 17:33:15.203: INFO: Found Service test-service-9kssh in namespace services-6743 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Dec 16 17:33:15.204: INFO: Service test-service-9kssh deleted
  Dec 16 17:33:15.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6743" for this suite. @ 12/16/23 17:33:15.211
• [0.237 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 12/16/23 17:33:15.227
  Dec 16 17:33:15.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 17:33:15.231
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:33:15.26
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:33:15.275
  STEP: Creating a pod to test downward API volume plugin @ 12/16/23 17:33:15.286
  E1216 17:33:15.743951      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:16.744157      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:17.744260      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:18.744515      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:33:19.389
  Dec 16 17:33:19.399: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downwardapi-volume-e39461ef-384c-48de-8dd2-aef7066c75e0 container client-container: <nil>
  STEP: delete the pod @ 12/16/23 17:33:19.451
  Dec 16 17:33:19.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1184" for this suite. @ 12/16/23 17:33:19.521
• [4.311 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 12/16/23 17:33:19.539
  Dec 16 17:33:19.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename configmap @ 12/16/23 17:33:19.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:33:19.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:33:19.579
  STEP: creating a ConfigMap @ 12/16/23 17:33:19.585
  STEP: fetching the ConfigMap @ 12/16/23 17:33:19.598
  STEP: patching the ConfigMap @ 12/16/23 17:33:19.606
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 12/16/23 17:33:19.623
  STEP: deleting the ConfigMap by collection with a label selector @ 12/16/23 17:33:19.631
  STEP: listing all ConfigMaps in test namespace @ 12/16/23 17:33:19.653
  Dec 16 17:33:19.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2365" for this suite. @ 12/16/23 17:33:19.671
• [0.144 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 12/16/23 17:33:19.685
  Dec 16 17:33:19.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename crd-webhook @ 12/16/23 17:33:19.688
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:33:19.73
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:33:19.735
  STEP: Setting up server cert @ 12/16/23 17:33:19.743
  E1216 17:33:19.745113      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:20.746158      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 12/16/23 17:33:21.522
  STEP: Deploying the custom resource conversion webhook pod @ 12/16/23 17:33:21.538
  STEP: Wait for the deployment to be ready @ 12/16/23 17:33:21.567
  Dec 16 17:33:21.606: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E1216 17:33:21.747852      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:22.748902      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/16/23 17:33:23.631
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 17:33:23.655
  E1216 17:33:23.749538      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:33:24.655: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Dec 16 17:33:24.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:33:24.749680      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:25.749946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:26.750607      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 12/16/23 17:33:27.545
  STEP: v2 custom resource should be converted @ 12/16/23 17:33:27.56
  Dec 16 17:33:27.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E1216 17:33:27.750762      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-18" for this suite. @ 12/16/23 17:33:28.23
• [8.566 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 12/16/23 17:33:28.257
  Dec 16 17:33:28.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename downward-api @ 12/16/23 17:33:28.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:33:28.34
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:33:28.344
  STEP: Creating a pod to test downward API volume plugin @ 12/16/23 17:33:28.35
  E1216 17:33:28.750951      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:29.752061      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:30.753870      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:31.754363      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:33:32.419
  Dec 16 17:33:32.426: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downwardapi-volume-2d039b32-c244-4f54-b21e-aa1ce78f09dc container client-container: <nil>
  STEP: delete the pod @ 12/16/23 17:33:32.447
  Dec 16 17:33:32.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5561" for this suite. @ 12/16/23 17:33:32.499
• [4.256 seconds]
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 12/16/23 17:33:32.517
  Dec 16 17:33:32.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename proxy @ 12/16/23 17:33:32.526
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:33:32.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:33:32.574
  STEP: starting an echo server on multiple ports @ 12/16/23 17:33:32.649
  STEP: creating replication controller proxy-service-9k5h7 in namespace proxy-9736 @ 12/16/23 17:33:32.649
  I1216 17:33:32.713452      13 runners.go:194] Created replication controller with name: proxy-service-9k5h7, namespace: proxy-9736, replica count: 1
  E1216 17:33:32.754567      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:33:33.767529      13 runners.go:194] proxy-service-9k5h7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E1216 17:33:33.770223      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:34.764426      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:33:34.768121      13 runners.go:194] proxy-service-9k5h7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E1216 17:33:35.765729      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:33:35.768684      13 runners.go:194] proxy-service-9k5h7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E1216 17:33:36.766486      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:33:36.769832      13 runners.go:194] proxy-service-9k5h7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E1216 17:33:37.766878      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:33:37.771604      13 runners.go:194] proxy-service-9k5h7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E1216 17:33:38.768502      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:33:38.772689      13 runners.go:194] proxy-service-9k5h7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E1216 17:33:39.768831      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:33:39.773299      13 runners.go:194] proxy-service-9k5h7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E1216 17:33:40.769164      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:33:40.773877      13 runners.go:194] proxy-service-9k5h7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E1216 17:33:41.770277      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:33:41.775808      13 runners.go:194] proxy-service-9k5h7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E1216 17:33:42.770153      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:33:42.775957      13 runners.go:194] proxy-service-9k5h7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E1216 17:33:43.770821      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:33:43.776701      13 runners.go:194] proxy-service-9k5h7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Dec 16 17:33:43.791: INFO: setup took 11.211240587s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 12/16/23 17:33:43.791
  Dec 16 17:33:43.820: INFO: (0) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 28.445633ms)
  Dec 16 17:33:43.834: INFO: (0) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 40.468511ms)
  Dec 16 17:33:43.843: INFO: (0) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 48.891701ms)
  Dec 16 17:33:43.843: INFO: (0) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 46.112213ms)
  Dec 16 17:33:43.844: INFO: (0) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 45.927872ms)
  Dec 16 17:33:43.844: INFO: (0) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 48.322039ms)
  Dec 16 17:33:43.845: INFO: (0) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 50.573021ms)
  Dec 16 17:33:43.845: INFO: (0) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 49.255938ms)
  Dec 16 17:33:43.848: INFO: (0) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 50.781581ms)
  Dec 16 17:33:43.848: INFO: (0) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 50.531727ms)
  Dec 16 17:33:43.849: INFO: (0) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 55.682767ms)
  Dec 16 17:33:43.857: INFO: (0) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 59.598126ms)
  Dec 16 17:33:43.862: INFO: (0) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 66.969813ms)
  Dec 16 17:33:43.863: INFO: (0) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 69.450981ms)
  Dec 16 17:33:43.863: INFO: (0) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 67.106987ms)
  Dec 16 17:33:43.863: INFO: (0) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 66.166517ms)
  Dec 16 17:33:43.884: INFO: (1) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 20.125236ms)
  Dec 16 17:33:43.889: INFO: (1) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 23.731673ms)
  Dec 16 17:33:43.891: INFO: (1) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 26.911271ms)
  Dec 16 17:33:43.891: INFO: (1) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 26.304507ms)
  Dec 16 17:33:43.894: INFO: (1) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 28.257518ms)
  Dec 16 17:33:43.895: INFO: (1) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 28.863854ms)
  Dec 16 17:33:43.895: INFO: (1) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 29.995559ms)
  Dec 16 17:33:43.895: INFO: (1) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 29.920327ms)
  Dec 16 17:33:43.895: INFO: (1) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 30.557945ms)
  Dec 16 17:33:43.895: INFO: (1) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 30.226664ms)
  Dec 16 17:33:43.896: INFO: (1) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 30.515235ms)
  Dec 16 17:33:43.906: INFO: (1) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 41.722021ms)
  Dec 16 17:33:43.907: INFO: (1) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 41.851422ms)
  Dec 16 17:33:43.907: INFO: (1) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 41.066333ms)
  Dec 16 17:33:43.907: INFO: (1) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 41.186746ms)
  Dec 16 17:33:43.908: INFO: (1) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 42.034904ms)
  Dec 16 17:33:43.926: INFO: (2) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 16.502215ms)
  Dec 16 17:33:43.929: INFO: (2) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 21.134042ms)
  Dec 16 17:33:43.931: INFO: (2) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 20.919835ms)
  Dec 16 17:33:43.934: INFO: (2) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 23.586281ms)
  Dec 16 17:33:43.935: INFO: (2) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 26.914625ms)
  Dec 16 17:33:43.937: INFO: (2) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 26.73378ms)
  Dec 16 17:33:43.938: INFO: (2) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 27.264444ms)
  Dec 16 17:33:43.938: INFO: (2) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 29.966968ms)
  Dec 16 17:33:43.938: INFO: (2) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 29.431881ms)
  Dec 16 17:33:43.938: INFO: (2) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 28.056879ms)
  Dec 16 17:33:43.938: INFO: (2) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 30.077865ms)
  Dec 16 17:33:43.951: INFO: (2) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 41.783178ms)
  Dec 16 17:33:43.952: INFO: (2) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 44.106716ms)
  Dec 16 17:33:43.953: INFO: (2) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 44.200028ms)
  Dec 16 17:33:43.953: INFO: (2) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 44.454181ms)
  Dec 16 17:33:43.956: INFO: (2) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 46.39674ms)
  Dec 16 17:33:43.982: INFO: (3) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 25.13853ms)
  Dec 16 17:33:43.982: INFO: (3) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 25.478674ms)
  Dec 16 17:33:43.985: INFO: (3) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 27.295465ms)
  Dec 16 17:33:43.986: INFO: (3) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 28.826847ms)
  Dec 16 17:33:43.986: INFO: (3) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 28.406854ms)
  Dec 16 17:33:44.003: INFO: (3) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 46.091485ms)
  Dec 16 17:33:44.007: INFO: (3) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 49.433558ms)
  Dec 16 17:33:44.006: INFO: (3) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 48.636121ms)
  Dec 16 17:33:44.006: INFO: (3) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 48.13974ms)
  Dec 16 17:33:44.007: INFO: (3) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 48.904229ms)
  Dec 16 17:33:44.007: INFO: (3) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 48.16285ms)
  Dec 16 17:33:44.007: INFO: (3) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 48.042837ms)
  Dec 16 17:33:44.007: INFO: (3) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 49.689058ms)
  Dec 16 17:33:44.008: INFO: (3) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 50.572924ms)
  Dec 16 17:33:44.008: INFO: (3) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 49.553053ms)
  Dec 16 17:33:44.008: INFO: (3) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 49.880653ms)
  Dec 16 17:33:44.027: INFO: (4) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 16.539436ms)
  Dec 16 17:33:44.029: INFO: (4) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 16.950971ms)
  Dec 16 17:33:44.029: INFO: (4) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 17.658713ms)
  Dec 16 17:33:44.033: INFO: (4) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 22.670085ms)
  Dec 16 17:33:44.033: INFO: (4) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 22.07431ms)
  Dec 16 17:33:44.033: INFO: (4) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 21.43453ms)
  Dec 16 17:33:44.036: INFO: (4) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 21.983801ms)
  Dec 16 17:33:44.038: INFO: (4) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 25.853962ms)
  Dec 16 17:33:44.044: INFO: (4) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 30.113851ms)
  Dec 16 17:33:44.045: INFO: (4) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 30.337119ms)
  Dec 16 17:33:44.045: INFO: (4) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 31.545999ms)
  Dec 16 17:33:44.045: INFO: (4) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 30.688021ms)
  Dec 16 17:33:44.046: INFO: (4) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 30.921649ms)
  Dec 16 17:33:44.046: INFO: (4) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 35.731352ms)
  Dec 16 17:33:44.046: INFO: (4) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 31.845874ms)
  Dec 16 17:33:44.046: INFO: (4) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 34.832051ms)
  Dec 16 17:33:44.068: INFO: (5) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 20.642567ms)
  Dec 16 17:33:44.068: INFO: (5) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 21.55457ms)
  Dec 16 17:33:44.070: INFO: (5) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 21.372524ms)
  Dec 16 17:33:44.072: INFO: (5) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 23.348343ms)
  Dec 16 17:33:44.073: INFO: (5) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 25.334203ms)
  Dec 16 17:33:44.073: INFO: (5) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 26.663984ms)
  Dec 16 17:33:44.075: INFO: (5) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 28.073947ms)
  Dec 16 17:33:44.075: INFO: (5) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 28.629777ms)
  Dec 16 17:33:44.075: INFO: (5) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 26.491434ms)
  Dec 16 17:33:44.084: INFO: (5) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 35.009847ms)
  Dec 16 17:33:44.084: INFO: (5) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 34.998574ms)
  Dec 16 17:33:44.084: INFO: (5) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 34.856027ms)
  Dec 16 17:33:44.084: INFO: (5) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 34.809569ms)
  Dec 16 17:33:44.085: INFO: (5) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 35.52305ms)
  Dec 16 17:33:44.087: INFO: (5) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 38.521264ms)
  Dec 16 17:33:44.090: INFO: (5) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 40.123443ms)
  Dec 16 17:33:44.101: INFO: (6) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 11.420184ms)
  Dec 16 17:33:44.101: INFO: (6) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 11.562776ms)
  Dec 16 17:33:44.115: INFO: (6) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 24.196944ms)
  Dec 16 17:33:44.115: INFO: (6) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 24.518059ms)
  Dec 16 17:33:44.116: INFO: (6) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 25.372241ms)
  Dec 16 17:33:44.117: INFO: (6) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 26.450188ms)
  Dec 16 17:33:44.124: INFO: (6) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 33.582697ms)
  Dec 16 17:33:44.125: INFO: (6) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 33.835825ms)
  Dec 16 17:33:44.126: INFO: (6) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 35.094676ms)
  Dec 16 17:33:44.126: INFO: (6) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 35.415553ms)
  Dec 16 17:33:44.127: INFO: (6) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 35.754957ms)
  Dec 16 17:33:44.127: INFO: (6) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 36.947956ms)
  Dec 16 17:33:44.127: INFO: (6) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 36.557826ms)
  Dec 16 17:33:44.127: INFO: (6) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 36.487927ms)
  Dec 16 17:33:44.127: INFO: (6) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 36.408007ms)
  Dec 16 17:33:44.127: INFO: (6) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 36.386335ms)
  Dec 16 17:33:44.146: INFO: (7) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 15.939947ms)
  Dec 16 17:33:44.153: INFO: (7) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 23.226788ms)
  Dec 16 17:33:44.153: INFO: (7) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 22.143387ms)
  Dec 16 17:33:44.154: INFO: (7) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 23.886719ms)
  Dec 16 17:33:44.158: INFO: (7) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 28.72486ms)
  Dec 16 17:33:44.159: INFO: (7) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 29.027668ms)
  Dec 16 17:33:44.161: INFO: (7) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 31.007795ms)
  Dec 16 17:33:44.161: INFO: (7) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 32.04426ms)
  Dec 16 17:33:44.161: INFO: (7) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 30.573234ms)
  Dec 16 17:33:44.162: INFO: (7) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 30.941747ms)
  Dec 16 17:33:44.162: INFO: (7) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 30.507771ms)
  Dec 16 17:33:44.164: INFO: (7) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 33.870734ms)
  Dec 16 17:33:44.166: INFO: (7) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 35.554031ms)
  Dec 16 17:33:44.166: INFO: (7) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 35.093531ms)
  Dec 16 17:33:44.166: INFO: (7) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 34.494915ms)
  Dec 16 17:33:44.171: INFO: (7) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 38.965856ms)
  Dec 16 17:33:44.188: INFO: (8) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 16.81977ms)
  Dec 16 17:33:44.192: INFO: (8) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 20.767587ms)
  Dec 16 17:33:44.194: INFO: (8) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 21.94027ms)
  Dec 16 17:33:44.204: INFO: (8) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 31.071917ms)
  Dec 16 17:33:44.205: INFO: (8) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 33.046093ms)
  Dec 16 17:33:44.205: INFO: (8) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 32.284345ms)
  Dec 16 17:33:44.205: INFO: (8) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 32.259831ms)
  Dec 16 17:33:44.205: INFO: (8) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 33.677561ms)
  Dec 16 17:33:44.205: INFO: (8) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 34.272986ms)
  Dec 16 17:33:44.205: INFO: (8) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 32.734369ms)
  Dec 16 17:33:44.207: INFO: (8) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 33.567001ms)
  Dec 16 17:33:44.207: INFO: (8) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 35.042115ms)
  Dec 16 17:33:44.207: INFO: (8) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 35.414357ms)
  Dec 16 17:33:44.208: INFO: (8) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 34.808071ms)
  Dec 16 17:33:44.213: INFO: (8) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 40.493197ms)
  Dec 16 17:33:44.214: INFO: (8) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 41.101719ms)
  Dec 16 17:33:44.237: INFO: (9) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 22.647507ms)
  Dec 16 17:33:44.241: INFO: (9) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 24.774753ms)
  Dec 16 17:33:44.241: INFO: (9) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 25.762907ms)
  Dec 16 17:33:44.242: INFO: (9) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 27.033497ms)
  Dec 16 17:33:44.242: INFO: (9) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 26.732329ms)
  Dec 16 17:33:44.242: INFO: (9) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 26.587228ms)
  Dec 16 17:33:44.242: INFO: (9) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 26.909622ms)
  Dec 16 17:33:44.242: INFO: (9) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 26.378054ms)
  Dec 16 17:33:44.245: INFO: (9) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 29.886638ms)
  Dec 16 17:33:44.245: INFO: (9) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 29.317726ms)
  Dec 16 17:33:44.246: INFO: (9) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 29.651295ms)
  Dec 16 17:33:44.246: INFO: (9) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 31.496162ms)
  Dec 16 17:33:44.247: INFO: (9) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 30.760062ms)
  Dec 16 17:33:44.247: INFO: (9) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 30.939256ms)
  Dec 16 17:33:44.248: INFO: (9) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 32.211552ms)
  Dec 16 17:33:44.252: INFO: (9) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 35.446101ms)
  Dec 16 17:33:44.261: INFO: (10) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 8.534418ms)
  Dec 16 17:33:44.273: INFO: (10) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 20.686039ms)
  Dec 16 17:33:44.276: INFO: (10) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 23.987024ms)
  Dec 16 17:33:44.277: INFO: (10) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 22.968985ms)
  Dec 16 17:33:44.278: INFO: (10) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 24.500142ms)
  Dec 16 17:33:44.282: INFO: (10) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 27.481684ms)
  Dec 16 17:33:44.282: INFO: (10) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 28.924483ms)
  Dec 16 17:33:44.282: INFO: (10) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 29.537929ms)
  Dec 16 17:33:44.284: INFO: (10) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 29.993865ms)
  Dec 16 17:33:44.284: INFO: (10) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 30.795689ms)
  Dec 16 17:33:44.285: INFO: (10) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 30.456642ms)
  Dec 16 17:33:44.285: INFO: (10) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 30.680283ms)
  Dec 16 17:33:44.286: INFO: (10) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 32.144865ms)
  Dec 16 17:33:44.286: INFO: (10) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 32.445892ms)
  Dec 16 17:33:44.288: INFO: (10) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 34.833907ms)
  Dec 16 17:33:44.289: INFO: (10) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 34.783862ms)
  Dec 16 17:33:44.302: INFO: (11) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 12.832233ms)
  Dec 16 17:33:44.305: INFO: (11) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 16.410465ms)
  Dec 16 17:33:44.309: INFO: (11) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 19.638764ms)
  Dec 16 17:33:44.310: INFO: (11) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 20.035541ms)
  Dec 16 17:33:44.310: INFO: (11) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 20.626736ms)
  Dec 16 17:33:44.310: INFO: (11) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 20.582458ms)
  Dec 16 17:33:44.311: INFO: (11) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 20.455925ms)
  Dec 16 17:33:44.313: INFO: (11) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 23.301722ms)
  Dec 16 17:33:44.318: INFO: (11) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 28.747487ms)
  Dec 16 17:33:44.319: INFO: (11) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 28.920534ms)
  Dec 16 17:33:44.319: INFO: (11) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 28.420383ms)
  Dec 16 17:33:44.319: INFO: (11) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 28.804458ms)
  Dec 16 17:33:44.321: INFO: (11) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 30.398657ms)
  Dec 16 17:33:44.321: INFO: (11) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 30.836747ms)
  Dec 16 17:33:44.323: INFO: (11) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 32.281509ms)
  Dec 16 17:33:44.323: INFO: (11) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 32.930401ms)
  Dec 16 17:33:44.339: INFO: (12) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 15.324537ms)
  Dec 16 17:33:44.339: INFO: (12) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 16.113552ms)
  Dec 16 17:33:44.356: INFO: (12) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 31.455831ms)
  Dec 16 17:33:44.357: INFO: (12) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 33.122155ms)
  Dec 16 17:33:44.357: INFO: (12) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 31.673934ms)
  Dec 16 17:33:44.357: INFO: (12) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 32.207237ms)
  Dec 16 17:33:44.357: INFO: (12) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 32.463518ms)
  Dec 16 17:33:44.360: INFO: (12) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 34.034314ms)
  Dec 16 17:33:44.360: INFO: (12) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 35.545782ms)
  Dec 16 17:33:44.360: INFO: (12) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 34.784869ms)
  Dec 16 17:33:44.360: INFO: (12) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 34.907018ms)
  Dec 16 17:33:44.361: INFO: (12) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 36.804394ms)
  Dec 16 17:33:44.362: INFO: (12) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 37.067878ms)
  Dec 16 17:33:44.362: INFO: (12) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 38.34592ms)
  Dec 16 17:33:44.371: INFO: (12) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 46.646127ms)
  Dec 16 17:33:44.373: INFO: (12) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 48.1342ms)
  Dec 16 17:33:44.390: INFO: (13) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 16.615014ms)
  Dec 16 17:33:44.390: INFO: (13) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 16.736374ms)
  Dec 16 17:33:44.390: INFO: (13) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 17.034306ms)
  Dec 16 17:33:44.390: INFO: (13) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 16.254864ms)
  Dec 16 17:33:44.391: INFO: (13) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 17.547815ms)
  Dec 16 17:33:44.399: INFO: (13) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 23.703115ms)
  Dec 16 17:33:44.401: INFO: (13) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 26.194602ms)
  Dec 16 17:33:44.402: INFO: (13) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 27.000775ms)
  Dec 16 17:33:44.403: INFO: (13) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 26.747307ms)
  Dec 16 17:33:44.403: INFO: (13) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 27.505657ms)
  Dec 16 17:33:44.404: INFO: (13) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 28.389992ms)
  Dec 16 17:33:44.407: INFO: (13) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 32.085741ms)
  Dec 16 17:33:44.407: INFO: (13) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 33.285287ms)
  Dec 16 17:33:44.408: INFO: (13) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 31.158041ms)
  Dec 16 17:33:44.412: INFO: (13) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 35.223339ms)
  Dec 16 17:33:44.415: INFO: (13) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 38.986846ms)
  Dec 16 17:33:44.432: INFO: (14) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 17.469593ms)
  Dec 16 17:33:44.438: INFO: (14) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 22.629431ms)
  Dec 16 17:33:44.444: INFO: (14) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 27.968399ms)
  Dec 16 17:33:44.445: INFO: (14) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 28.283685ms)
  Dec 16 17:33:44.446: INFO: (14) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 29.002927ms)
  Dec 16 17:33:44.446: INFO: (14) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 30.295084ms)
  Dec 16 17:33:44.446: INFO: (14) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 30.384429ms)
  Dec 16 17:33:44.447: INFO: (14) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 30.431958ms)
  Dec 16 17:33:44.447: INFO: (14) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 29.884241ms)
  Dec 16 17:33:44.447: INFO: (14) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 30.212269ms)
  Dec 16 17:33:44.450: INFO: (14) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 33.440519ms)
  Dec 16 17:33:44.450: INFO: (14) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 33.865842ms)
  Dec 16 17:33:44.451: INFO: (14) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 35.338046ms)
  Dec 16 17:33:44.455: INFO: (14) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 39.436009ms)
  Dec 16 17:33:44.456: INFO: (14) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 38.929734ms)
  Dec 16 17:33:44.461: INFO: (14) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 43.591918ms)
  Dec 16 17:33:44.481: INFO: (15) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 19.090174ms)
  Dec 16 17:33:44.482: INFO: (15) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 20.681993ms)
  Dec 16 17:33:44.485: INFO: (15) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 22.336302ms)
  Dec 16 17:33:44.485: INFO: (15) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 21.29512ms)
  Dec 16 17:33:44.486: INFO: (15) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 23.504199ms)
  Dec 16 17:33:44.490: INFO: (15) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 25.873698ms)
  Dec 16 17:33:44.491: INFO: (15) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 26.399144ms)
  Dec 16 17:33:44.491: INFO: (15) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 28.508623ms)
  Dec 16 17:33:44.499: INFO: (15) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 33.848794ms)
  Dec 16 17:33:44.499: INFO: (15) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 36.450506ms)
  Dec 16 17:33:44.501: INFO: (15) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 36.918837ms)
  Dec 16 17:33:44.502: INFO: (15) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 39.319659ms)
  Dec 16 17:33:44.502: INFO: (15) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 37.685486ms)
  Dec 16 17:33:44.502: INFO: (15) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 38.214096ms)
  Dec 16 17:33:44.502: INFO: (15) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 37.965019ms)
  Dec 16 17:33:44.505: INFO: (15) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 40.417876ms)
  Dec 16 17:33:44.527: INFO: (16) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 20.389674ms)
  Dec 16 17:33:44.530: INFO: (16) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 23.298621ms)
  Dec 16 17:33:44.532: INFO: (16) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 25.88426ms)
  Dec 16 17:33:44.534: INFO: (16) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 28.714936ms)
  Dec 16 17:33:44.535: INFO: (16) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 27.712981ms)
  Dec 16 17:33:44.535: INFO: (16) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 28.317381ms)
  Dec 16 17:33:44.536: INFO: (16) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 30.436715ms)
  Dec 16 17:33:44.537: INFO: (16) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 29.866153ms)
  Dec 16 17:33:44.538: INFO: (16) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 30.371554ms)
  Dec 16 17:33:44.538: INFO: (16) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 32.672256ms)
  Dec 16 17:33:44.544: INFO: (16) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 37.820252ms)
  Dec 16 17:33:44.551: INFO: (16) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 44.619909ms)
  Dec 16 17:33:44.551: INFO: (16) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 43.441717ms)
  Dec 16 17:33:44.552: INFO: (16) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 44.759293ms)
  Dec 16 17:33:44.553: INFO: (16) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 45.22325ms)
  Dec 16 17:33:44.555: INFO: (16) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 48.561692ms)
  Dec 16 17:33:44.570: INFO: (17) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 14.96614ms)
  Dec 16 17:33:44.573: INFO: (17) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 17.065123ms)
  Dec 16 17:33:44.579: INFO: (17) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 22.237664ms)
  Dec 16 17:33:44.579: INFO: (17) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 22.606843ms)
  Dec 16 17:33:44.580: INFO: (17) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 25.378679ms)
  Dec 16 17:33:44.581: INFO: (17) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 24.608208ms)
  Dec 16 17:33:44.582: INFO: (17) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 25.427067ms)
  Dec 16 17:33:44.582: INFO: (17) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 26.397986ms)
  Dec 16 17:33:44.587: INFO: (17) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 29.388993ms)
  Dec 16 17:33:44.592: INFO: (17) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 34.673294ms)
  Dec 16 17:33:44.592: INFO: (17) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 34.599655ms)
  Dec 16 17:33:44.595: INFO: (17) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 36.911946ms)
  Dec 16 17:33:44.596: INFO: (17) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 37.663049ms)
  Dec 16 17:33:44.597: INFO: (17) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 38.903655ms)
  Dec 16 17:33:44.601: INFO: (17) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 43.480993ms)
  Dec 16 17:33:44.601: INFO: (17) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 42.859688ms)
  Dec 16 17:33:44.639: INFO: (18) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 36.798996ms)
  Dec 16 17:33:44.662: INFO: (18) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 57.31418ms)
  Dec 16 17:33:44.663: INFO: (18) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 61.110236ms)
  Dec 16 17:33:44.663: INFO: (18) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 59.559301ms)
  Dec 16 17:33:44.684: INFO: (18) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 82.393221ms)
  Dec 16 17:33:44.693: INFO: (18) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 88.50556ms)
  Dec 16 17:33:44.695: INFO: (18) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 90.278787ms)
  Dec 16 17:33:44.696: INFO: (18) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 90.395099ms)
  Dec 16 17:33:44.703: INFO: (18) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 100.908102ms)
  Dec 16 17:33:44.703: INFO: (18) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 101.044797ms)
  Dec 16 17:33:44.718: INFO: (18) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 113.782108ms)
  Dec 16 17:33:44.719: INFO: (18) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 116.398874ms)
  Dec 16 17:33:44.719: INFO: (18) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 114.273496ms)
  Dec 16 17:33:44.726: INFO: (18) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 122.248702ms)
  Dec 16 17:33:44.726: INFO: (18) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 121.487391ms)
  Dec 16 17:33:44.728: INFO: (18) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 126.102185ms)
  Dec 16 17:33:44.746: INFO: (19) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw/proxy/rewriteme">test</a> (200; 16.579769ms)
  Dec 16 17:33:44.749: INFO: (19) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:1080/proxy/rewriteme">... (200; 19.964519ms)
  Dec 16 17:33:44.750: INFO: (19) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:443/proxy/tlsrewritem... (200; 20.850324ms)
  Dec 16 17:33:44.751: INFO: (19) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:160/proxy/: foo (200; 21.25979ms)
  Dec 16 17:33:44.751: INFO: (19) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:160/proxy/: foo (200; 21.829774ms)
  Dec 16 17:33:44.751: INFO: (19) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/: <a href="/api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:1080/proxy/rewriteme">test<... (200; 22.458746ms)
  Dec 16 17:33:44.755: INFO: (19) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname1/proxy/: foo (200; 25.117618ms)
  Dec 16 17:33:44.756: INFO: (19) /api/v1/namespaces/proxy-9736/services/proxy-service-9k5h7:portname2/proxy/: bar (200; 26.740428ms)
  Dec 16 17:33:44.757: INFO: (19) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname1/proxy/: tls baz (200; 27.751809ms)
  Dec 16 17:33:44.758: INFO: (19) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname2/proxy/: bar (200; 29.065025ms)
  E1216 17:33:44.770973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:33:44.771: INFO: (19) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:462/proxy/: tls qux (200; 42.888112ms)
  Dec 16 17:33:44.771: INFO: (19) /api/v1/namespaces/proxy-9736/services/https:proxy-service-9k5h7:tlsportname2/proxy/: tls qux (200; 41.152214ms)
  Dec 16 17:33:44.771: INFO: (19) /api/v1/namespaces/proxy-9736/pods/proxy-service-9k5h7-89njw:162/proxy/: bar (200; 42.658858ms)
  Dec 16 17:33:44.772: INFO: (19) /api/v1/namespaces/proxy-9736/pods/http:proxy-service-9k5h7-89njw:162/proxy/: bar (200; 42.448873ms)
  Dec 16 17:33:44.772: INFO: (19) /api/v1/namespaces/proxy-9736/pods/https:proxy-service-9k5h7-89njw:460/proxy/: tls baz (200; 42.631608ms)
  Dec 16 17:33:44.779: INFO: (19) /api/v1/namespaces/proxy-9736/services/http:proxy-service-9k5h7:portname1/proxy/: foo (200; 49.806257ms)
  Dec 16 17:33:44.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-9k5h7 in namespace proxy-9736, will wait for the garbage collector to delete the pods @ 12/16/23 17:33:44.792
  Dec 16 17:33:44.870: INFO: Deleting ReplicationController proxy-service-9k5h7 took: 16.919942ms
  Dec 16 17:33:44.971: INFO: Terminating ReplicationController proxy-service-9k5h7 pods took: 100.801418ms
  STEP: Destroying namespace "proxy-9736" for this suite. @ 12/16/23 17:33:45.672
• [13.172 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 12/16/23 17:33:45.696
  Dec 16 17:33:45.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-probe @ 12/16/23 17:33:45.7
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:33:45.731
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:33:45.738
  STEP: Creating pod busybox-a1d55b10-3d0b-405c-9d38-673681b59fa2 in namespace container-probe-2133 @ 12/16/23 17:33:45.744
  E1216 17:33:45.771489      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:46.771683      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:47.771925      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:33:47.786: INFO: Started pod busybox-a1d55b10-3d0b-405c-9d38-673681b59fa2 in namespace container-probe-2133
  STEP: checking the pod's current state and verifying that restartCount is present @ 12/16/23 17:33:47.787
  Dec 16 17:33:47.795: INFO: Initial restart count of pod busybox-a1d55b10-3d0b-405c-9d38-673681b59fa2 is 0
  E1216 17:33:48.773278      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:49.773823      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:50.773004      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:51.773267      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:52.774262      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:53.774375      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:54.774875      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:55.775121      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:56.775425      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:57.775606      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:58.790233      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:33:59.780759      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:00.780973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:01.781955      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:02.784285      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:03.783273      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:04.784099      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:05.784511      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:06.786379      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:07.785067      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:08.785303      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:09.787264      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:10.787557      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:11.787696      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:12.788479      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:13.788701      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:14.789149      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:15.789376      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:16.790254      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:17.790776      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:18.791316      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:19.791123      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:20.791525      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:21.791964      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:22.793159      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:23.793005      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:24.794094      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:25.794460      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:26.795056      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:27.795905      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:28.799190      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:29.796987      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:30.797103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:31.797793      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:32.797730      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:33.798022      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:34.798370      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:35.799191      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:36.800398      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:37.800578      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:38.801087      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:39.803045      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:40.807022      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:41.804152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:42.804724      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:43.807136      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:44.805790      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:45.813342      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:46.808342      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:47.809417      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:48.810369      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:49.811458      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:50.812102      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:51.815286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:52.815825      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:53.816604      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:54.816826      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:55.817255      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:56.818553      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:57.818878      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:58.820012      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:34:59.819286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:00.819637      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:01.819867      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:02.820876      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:03.821333      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:04.821643      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:05.821809      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:06.822964      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:07.823033      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:08.823852      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:09.823970      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:10.824096      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:11.824570      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:12.824878      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:13.825700      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:14.825685      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:15.826088      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:16.825912      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:17.827089      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:18.827533      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:19.827680      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:20.828292      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:21.829178      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:22.830511      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:23.830398      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:24.830480      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:25.831670      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:26.831987      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:27.832843      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:28.833263      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:29.833439      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:30.834532      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:31.834576      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:32.835607      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:33.835775      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:34.836963      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:35.837110      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:36.837507      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:37.838454      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:38.839378      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:39.840245      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:40.840674      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:41.841206      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:42.841633      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:43.841867      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:44.842704      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:45.842976      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:46.843829      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:47.845082      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:48.846105      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:49.846729      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:50.846920      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:51.847165      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:52.847768      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:53.848059      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:54.848292      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:55.849170      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:56.849560      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:57.849465      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:58.849574      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:35:59.853289      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:00.851326      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:01.851799      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:02.853169      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:03.853537      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:04.854009      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:05.871318      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:06.864890      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:07.863623      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:08.864515      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:09.865386      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:10.865685      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:11.866550      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:12.866983      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:13.867958      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:14.868694      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:15.869071      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:16.869750      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:17.870567      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:18.872381      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:19.871457      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:20.872058      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:21.872419      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:22.872810      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:23.873520      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:24.873775      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:25.873964      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:26.874220      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:27.874284      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:28.874897      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:29.875094      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:30.875198      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:31.875455      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:32.876113      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:33.876422      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:34.877122      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:35.877497      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:36.878259      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:37.880836      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:38.879547      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:39.880640      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:40.883389      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:41.882319      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:42.882769      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:43.886515      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:44.885157      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:45.885669      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:46.886269      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:47.886205      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:48.886660      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:49.886934      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:50.886782      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:51.887894      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:52.889236      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:53.889407      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:54.889851      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:55.890211      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:56.890753      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:57.891242      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:58.892396      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:36:59.892932      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:00.893840      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:01.894215      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:02.894866      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:03.895193      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:04.895416      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:05.895968      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:06.896067      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:07.896601      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:08.898400      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:09.898113      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:10.898654      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:11.898412      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:12.899181      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:13.900113      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:14.899813      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:15.900081      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:16.901003      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:17.901727      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:18.907574      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:19.903517      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:20.903907      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:21.903960      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:22.903857      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:23.904942      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:24.906507      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:25.905824      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:26.907205      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:27.907673      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:28.907693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:29.907839      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:30.908273      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:31.908528      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:32.909222      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:33.910338      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:34.909739      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:35.910170      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:36.910846      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:37.911637      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:38.912048      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:39.912785      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:40.913135      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:41.913546      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:42.914012      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:43.914157      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:44.914449      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:45.915144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:46.915221      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:47.915920      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:48.916533      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:37:49.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/16/23 17:37:49.142
  STEP: Destroying namespace "container-probe-2133" for this suite. @ 12/16/23 17:37:49.171
• [243.515 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 12/16/23 17:37:49.216
  Dec 16 17:37:49.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename emptydir @ 12/16/23 17:37:49.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:37:49.253
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:37:49.262
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 12/16/23 17:37:49.275
  E1216 17:37:49.916831      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:50.917481      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:51.918049      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:52.918444      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:37:53.328
  Dec 16 17:37:53.336: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-b76c8c19-6b5e-40a9-81c9-b6a7effa929a container test-container: <nil>
  STEP: delete the pod @ 12/16/23 17:37:53.377
  Dec 16 17:37:53.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-668" for this suite. @ 12/16/23 17:37:53.438
• [4.243 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 12/16/23 17:37:53.46
  Dec 16 17:37:53.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename services @ 12/16/23 17:37:53.464
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:37:53.518
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:37:53.526
  STEP: creating an Endpoint @ 12/16/23 17:37:53.541
  STEP: waiting for available Endpoint @ 12/16/23 17:37:53.552
  STEP: listing all Endpoints @ 12/16/23 17:37:53.56
  STEP: updating the Endpoint @ 12/16/23 17:37:53.57
  STEP: fetching the Endpoint @ 12/16/23 17:37:53.581
  STEP: patching the Endpoint @ 12/16/23 17:37:53.588
  STEP: fetching the Endpoint @ 12/16/23 17:37:53.616
  STEP: deleting the Endpoint by Collection @ 12/16/23 17:37:53.625
  STEP: waiting for Endpoint deletion @ 12/16/23 17:37:53.647
  STEP: fetching the Endpoint @ 12/16/23 17:37:53.652
  Dec 16 17:37:53.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7649" for this suite. @ 12/16/23 17:37:53.671
• [0.222 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 12/16/23 17:37:53.686
  Dec 16 17:37:53.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename job @ 12/16/23 17:37:53.691
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:37:53.724
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:37:53.729
  STEP: Creating Indexed job @ 12/16/23 17:37:53.735
  STEP: Ensuring job reaches completions @ 12/16/23 17:37:53.747
  E1216 17:37:53.919458      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:54.928303      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:55.925910      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:56.925202      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:57.926795      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:58.926326      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:37:59.926458      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:00.926797      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 12/16/23 17:38:01.756
  Dec 16 17:38:01.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1632" for this suite. @ 12/16/23 17:38:01.781
• [8.116 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 12/16/23 17:38:01.808
  Dec 16 17:38:01.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename job @ 12/16/23 17:38:01.811
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:38:01.848
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:38:01.855
  STEP: Creating a job @ 12/16/23 17:38:01.862
  STEP: Ensuring active pods == parallelism @ 12/16/23 17:38:01.882
  E1216 17:38:01.944207      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:02.928613      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 12/16/23 17:38:03.895
  STEP: deleting Job.batch foo in namespace job-3373, will wait for the garbage collector to delete the pods @ 12/16/23 17:38:03.895
  E1216 17:38:03.929646      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:38:03.967: INFO: Deleting Job.batch foo took: 12.906574ms
  Dec 16 17:38:04.069: INFO: Terminating Job.batch foo pods took: 101.709644ms
  E1216 17:38:04.930442      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:05.930940      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:06.931264      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:07.931852      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:08.932558      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:09.933081      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:10.933311      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:11.934304      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:12.935030      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:13.936148      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:14.936836      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:15.937386      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:16.937906      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:17.938252      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:18.938631      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:19.939160      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:20.939958      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:21.940428      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:22.940885      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:23.944310      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:24.941514      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:25.955503      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:26.947593      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:27.944889      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:28.947155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:29.947824      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:30.947768      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:31.954036      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:32.949737      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:33.950003      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:34.950238      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 12/16/23 17:38:35.77
  Dec 16 17:38:35.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3373" for this suite. @ 12/16/23 17:38:35.799
• [34.006 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 12/16/23 17:38:35.815
  Dec 16 17:38:35.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename disruption @ 12/16/23 17:38:35.82
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:38:35.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:38:35.9
  STEP: Waiting for the pdb to be processed @ 12/16/23 17:38:35.926
  E1216 17:38:35.951287      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:36.952442      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:37.952989      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 12/16/23 17:38:38.027
  Dec 16 17:38:38.043: INFO: running pods: 0 < 3
  E1216 17:38:38.953342      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:39.953414      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:38:40.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5314" for this suite. @ 12/16/23 17:38:40.081
• [4.290 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 12/16/23 17:38:40.107
  Dec 16 17:38:40.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename pods @ 12/16/23 17:38:40.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:38:40.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:38:40.152
  STEP: Create set of pods @ 12/16/23 17:38:40.157
  Dec 16 17:38:40.174: INFO: created test-pod-1
  Dec 16 17:38:40.195: INFO: created test-pod-2
  Dec 16 17:38:40.224: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 12/16/23 17:38:40.224
  E1216 17:38:40.956587      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:41.958485      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 12/16/23 17:38:42.328
  Dec 16 17:38:42.338: INFO: Pod quantity 3 is different from expected quantity 0
  E1216 17:38:42.957773      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:38:43.351: INFO: Pod quantity 3 is different from expected quantity 0
  E1216 17:38:43.958964      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:38:44.358: INFO: Pod quantity 1 is different from expected quantity 0
  E1216 17:38:44.961105      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:38:45.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2009" for this suite. @ 12/16/23 17:38:45.369
• [5.321 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 12/16/23 17:38:45.43
  Dec 16 17:38:45.430: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename resourcequota @ 12/16/23 17:38:45.434
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:38:45.476
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:38:45.483
  STEP: Creating a ResourceQuota with best effort scope @ 12/16/23 17:38:45.488
  STEP: Ensuring ResourceQuota status is calculated @ 12/16/23 17:38:45.497
  E1216 17:38:45.963544      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:46.965735      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not best effort scope @ 12/16/23 17:38:47.509
  STEP: Ensuring ResourceQuota status is calculated @ 12/16/23 17:38:47.519
  E1216 17:38:47.962497      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:48.962848      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a best-effort pod @ 12/16/23 17:38:49.531
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 12/16/23 17:38:49.559
  E1216 17:38:49.963587      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:50.965445      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 12/16/23 17:38:51.568
  E1216 17:38:51.965661      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:52.966732      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 12/16/23 17:38:53.578
  STEP: Ensuring resource quota status released the pod usage @ 12/16/23 17:38:53.608
  E1216 17:38:53.968024      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:54.970298      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a not best-effort pod @ 12/16/23 17:38:55.615
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 12/16/23 17:38:55.634
  E1216 17:38:55.969827      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:56.970233      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 12/16/23 17:38:57.643
  E1216 17:38:57.971183      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:38:58.972010      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 12/16/23 17:38:59.654
  STEP: Ensuring resource quota status released the pod usage @ 12/16/23 17:38:59.681
  E1216 17:38:59.972905      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:00.973795      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:39:01.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8172" for this suite. @ 12/16/23 17:39:01.701
• [16.283 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 12/16/23 17:39:01.715
  Dec 16 17:39:01.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename gc @ 12/16/23 17:39:01.72
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:39:01.752
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:39:01.758
  STEP: create the rc @ 12/16/23 17:39:01.767
  W1216 17:39:01.780874      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E1216 17:39:01.974394      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:02.974979      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:03.975142      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:04.975593      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:05.975696      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 12/16/23 17:39:06.804
  STEP: wait for all pods to be garbage collected @ 12/16/23 17:39:06.821
  E1216 17:39:06.976797      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:07.977405      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:08.979289      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:09.980839      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:10.994973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 12/16/23 17:39:11.843
  E1216 17:39:11.984817      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:39:12.083: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Dec 16 17:39:12.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5521" for this suite. @ 12/16/23 17:39:12.106
• [10.402 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 12/16/23 17:39:12.119
  Dec 16 17:39:12.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename discovery @ 12/16/23 17:39:12.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:39:12.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:39:12.162
  STEP: Setting up server cert @ 12/16/23 17:39:12.169
  E1216 17:39:12.990544      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:39:13.125: INFO: Checking APIGroup: apiregistration.k8s.io
  Dec 16 17:39:13.127: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Dec 16 17:39:13.127: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Dec 16 17:39:13.127: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Dec 16 17:39:13.127: INFO: Checking APIGroup: apps
  Dec 16 17:39:13.129: INFO: PreferredVersion.GroupVersion: apps/v1
  Dec 16 17:39:13.129: INFO: Versions found [{apps/v1 v1}]
  Dec 16 17:39:13.129: INFO: apps/v1 matches apps/v1
  Dec 16 17:39:13.130: INFO: Checking APIGroup: events.k8s.io
  Dec 16 17:39:13.131: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Dec 16 17:39:13.131: INFO: Versions found [{events.k8s.io/v1 v1}]
  Dec 16 17:39:13.131: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Dec 16 17:39:13.131: INFO: Checking APIGroup: authentication.k8s.io
  Dec 16 17:39:13.134: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Dec 16 17:39:13.134: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Dec 16 17:39:13.134: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Dec 16 17:39:13.134: INFO: Checking APIGroup: authorization.k8s.io
  Dec 16 17:39:13.136: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Dec 16 17:39:13.136: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Dec 16 17:39:13.137: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Dec 16 17:39:13.137: INFO: Checking APIGroup: autoscaling
  Dec 16 17:39:13.139: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Dec 16 17:39:13.140: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Dec 16 17:39:13.140: INFO: autoscaling/v2 matches autoscaling/v2
  Dec 16 17:39:13.140: INFO: Checking APIGroup: batch
  Dec 16 17:39:13.144: INFO: PreferredVersion.GroupVersion: batch/v1
  Dec 16 17:39:13.144: INFO: Versions found [{batch/v1 v1}]
  Dec 16 17:39:13.144: INFO: batch/v1 matches batch/v1
  Dec 16 17:39:13.144: INFO: Checking APIGroup: certificates.k8s.io
  Dec 16 17:39:13.146: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Dec 16 17:39:13.147: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Dec 16 17:39:13.147: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Dec 16 17:39:13.148: INFO: Checking APIGroup: networking.k8s.io
  Dec 16 17:39:13.151: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Dec 16 17:39:13.151: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Dec 16 17:39:13.151: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Dec 16 17:39:13.151: INFO: Checking APIGroup: policy
  Dec 16 17:39:13.153: INFO: PreferredVersion.GroupVersion: policy/v1
  Dec 16 17:39:13.153: INFO: Versions found [{policy/v1 v1}]
  Dec 16 17:39:13.153: INFO: policy/v1 matches policy/v1
  Dec 16 17:39:13.153: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Dec 16 17:39:13.155: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Dec 16 17:39:13.155: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Dec 16 17:39:13.156: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Dec 16 17:39:13.156: INFO: Checking APIGroup: storage.k8s.io
  Dec 16 17:39:13.158: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Dec 16 17:39:13.159: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Dec 16 17:39:13.159: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Dec 16 17:39:13.160: INFO: Checking APIGroup: admissionregistration.k8s.io
  Dec 16 17:39:13.162: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Dec 16 17:39:13.162: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Dec 16 17:39:13.162: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Dec 16 17:39:13.162: INFO: Checking APIGroup: apiextensions.k8s.io
  Dec 16 17:39:13.164: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Dec 16 17:39:13.165: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Dec 16 17:39:13.165: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Dec 16 17:39:13.165: INFO: Checking APIGroup: scheduling.k8s.io
  Dec 16 17:39:13.168: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Dec 16 17:39:13.168: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Dec 16 17:39:13.168: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Dec 16 17:39:13.168: INFO: Checking APIGroup: coordination.k8s.io
  Dec 16 17:39:13.170: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Dec 16 17:39:13.170: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Dec 16 17:39:13.170: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Dec 16 17:39:13.170: INFO: Checking APIGroup: node.k8s.io
  Dec 16 17:39:13.173: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Dec 16 17:39:13.173: INFO: Versions found [{node.k8s.io/v1 v1}]
  Dec 16 17:39:13.173: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Dec 16 17:39:13.174: INFO: Checking APIGroup: discovery.k8s.io
  Dec 16 17:39:13.205: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Dec 16 17:39:13.206: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Dec 16 17:39:13.206: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Dec 16 17:39:13.206: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Dec 16 17:39:13.210: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Dec 16 17:39:13.210: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Dec 16 17:39:13.210: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Dec 16 17:39:13.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-2953" for this suite. @ 12/16/23 17:39:13.231
• [1.139 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 12/16/23 17:39:13.264
  Dec 16 17:39:13.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename namespaces @ 12/16/23 17:39:13.271
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:39:13.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:39:13.312
  STEP: Updating Namespace "namespaces-6066" @ 12/16/23 17:39:13.317
  Dec 16 17:39:13.337: INFO: Namespace "namespaces-6066" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"e3ca193f-0902-4f7c-aa5c-6ee32ebbee92", "kubernetes.io/metadata.name":"namespaces-6066", "namespaces-6066":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Dec 16 17:39:13.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6066" for this suite. @ 12/16/23 17:39:13.356
• [0.114 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 12/16/23 17:39:13.378
  Dec 16 17:39:13.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename secrets @ 12/16/23 17:39:13.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:39:13.432
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:39:13.442
  STEP: Creating secret with name secret-test-6eeb1835-43fc-459f-8261-d401f741776e @ 12/16/23 17:39:13.453
  STEP: Creating a pod to test consume secrets @ 12/16/23 17:39:13.467
  E1216 17:39:13.986490      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:14.986724      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:15.988509      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:16.988891      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:39:17.537
  Dec 16 17:39:17.546: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-secrets-cf2fe96a-d8a9-4c19-87b4-9aeddf0614b5 container secret-env-test: <nil>
  STEP: delete the pod @ 12/16/23 17:39:17.575
  Dec 16 17:39:17.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3721" for this suite. @ 12/16/23 17:39:17.694
• [4.350 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 12/16/23 17:39:17.736
  Dec 16 17:39:17.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename downward-api @ 12/16/23 17:39:17.74
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:39:17.829
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:39:17.845
  STEP: Creating a pod to test downward api env vars @ 12/16/23 17:39:17.861
  E1216 17:39:17.989119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:19.004070      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:20.003957      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:21.004680      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:39:21.924
  Dec 16 17:39:21.933: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downward-api-7e6083fb-1e37-47ac-849b-cde1001fe47e container dapi-container: <nil>
  STEP: delete the pod @ 12/16/23 17:39:21.955
  E1216 17:39:22.004450      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:39:22.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9179" for this suite. @ 12/16/23 17:39:22.028
• [4.313 seconds]
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 12/16/23 17:39:22.052
  Dec 16 17:39:22.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename sched-pred @ 12/16/23 17:39:22.055
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:39:22.098
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:39:22.11
  Dec 16 17:39:22.134: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Dec 16 17:39:22.171: INFO: Waiting for terminating namespaces to be deleted...
  Dec 16 17:39:22.185: INFO: 
  Logging pods the apiserver thinks is on node phoh7xai9ouk-1 before test
  Dec 16 17:39:22.206: INFO: kube-flannel-ds-5sbtb from kube-flannel started at 2023-12-16 16:10:58 +0000 UTC (1 container statuses recorded)
  Dec 16 17:39:22.206: INFO: 	Container kube-flannel ready: true, restart count 1
  Dec 16 17:39:22.207: INFO: coredns-5d78c9869d-scnc5 from kube-system started at 2023-12-16 16:15:59 +0000 UTC (1 container statuses recorded)
  Dec 16 17:39:22.207: INFO: 	Container coredns ready: true, restart count 0
  Dec 16 17:39:22.207: INFO: kube-addon-manager-phoh7xai9ouk-1 from kube-system started at 2023-12-16 16:09:46 +0000 UTC (1 container statuses recorded)
  Dec 16 17:39:22.207: INFO: 	Container kube-addon-manager ready: true, restart count 1
  Dec 16 17:39:22.208: INFO: kube-apiserver-phoh7xai9ouk-1 from kube-system started at 2023-12-16 16:09:46 +0000 UTC (1 container statuses recorded)
  Dec 16 17:39:22.208: INFO: 	Container kube-apiserver ready: true, restart count 1
  Dec 16 17:39:22.208: INFO: kube-controller-manager-phoh7xai9ouk-1 from kube-system started at 2023-12-16 16:09:46 +0000 UTC (1 container statuses recorded)
  Dec 16 17:39:22.209: INFO: 	Container kube-controller-manager ready: true, restart count 1
  Dec 16 17:39:22.209: INFO: kube-proxy-7c5h8 from kube-system started at 2023-12-16 16:10:59 +0000 UTC (1 container statuses recorded)
  Dec 16 17:39:22.210: INFO: 	Container kube-proxy ready: true, restart count 0
  Dec 16 17:39:22.210: INFO: kube-scheduler-phoh7xai9ouk-1 from kube-system started at 2023-12-16 16:09:46 +0000 UTC (1 container statuses recorded)
  Dec 16 17:39:22.210: INFO: 	Container kube-scheduler ready: true, restart count 1
  Dec 16 17:39:22.210: INFO: sonobuoy-systemd-logs-daemon-set-0d7ed218a9fd4126-l6bt5 from sonobuoy started at 2023-12-16 16:12:30 +0000 UTC (2 container statuses recorded)
  Dec 16 17:39:22.211: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 16 17:39:22.211: INFO: 	Container systemd-logs ready: true, restart count 0
  Dec 16 17:39:22.211: INFO: 
  Logging pods the apiserver thinks is on node phoh7xai9ouk-2 before test
  Dec 16 17:39:22.235: INFO: kube-flannel-ds-p5tmh from kube-flannel started at 2023-12-16 16:11:26 +0000 UTC (1 container statuses recorded)
  Dec 16 17:39:22.235: INFO: 	Container kube-flannel ready: true, restart count 0
  Dec 16 17:39:22.235: INFO: coredns-5d78c9869d-848fd from kube-system started at 2023-12-16 16:11:25 +0000 UTC (1 container statuses recorded)
  Dec 16 17:39:22.235: INFO: 	Container coredns ready: true, restart count 0
  Dec 16 17:39:22.235: INFO: kube-addon-manager-phoh7xai9ouk-2 from kube-system started at 2023-12-16 16:09:29 +0000 UTC (1 container statuses recorded)
  Dec 16 17:39:22.235: INFO: 	Container kube-addon-manager ready: true, restart count 1
  Dec 16 17:39:22.235: INFO: kube-apiserver-phoh7xai9ouk-2 from kube-system started at 2023-12-16 16:09:29 +0000 UTC (1 container statuses recorded)
  Dec 16 17:39:22.235: INFO: 	Container kube-apiserver ready: true, restart count 1
  Dec 16 17:39:22.235: INFO: kube-controller-manager-phoh7xai9ouk-2 from kube-system started at 2023-12-16 16:09:29 +0000 UTC (1 container statuses recorded)
  Dec 16 17:39:22.235: INFO: 	Container kube-controller-manager ready: true, restart count 1
  Dec 16 17:39:22.235: INFO: kube-proxy-dzncl from kube-system started at 2023-12-16 16:11:26 +0000 UTC (1 container statuses recorded)
  Dec 16 17:39:22.235: INFO: 	Container kube-proxy ready: true, restart count 0
  Dec 16 17:39:22.235: INFO: kube-scheduler-phoh7xai9ouk-2 from kube-system started at 2023-12-16 16:09:29 +0000 UTC (1 container statuses recorded)
  Dec 16 17:39:22.235: INFO: 	Container kube-scheduler ready: true, restart count 1
  Dec 16 17:39:22.235: INFO: sonobuoy-systemd-logs-daemon-set-0d7ed218a9fd4126-2zwqp from sonobuoy started at 2023-12-16 16:12:30 +0000 UTC (2 container statuses recorded)
  Dec 16 17:39:22.235: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 16 17:39:22.235: INFO: 	Container systemd-logs ready: true, restart count 0
  Dec 16 17:39:22.235: INFO: 
  Logging pods the apiserver thinks is on node phoh7xai9ouk-3 before test
  Dec 16 17:39:22.256: INFO: kube-flannel-ds-dk7xw from kube-flannel started at 2023-12-16 16:39:50 +0000 UTC (1 container statuses recorded)
  Dec 16 17:39:22.256: INFO: 	Container kube-flannel ready: true, restart count 0
  Dec 16 17:39:22.257: INFO: kube-proxy-w4mds from kube-system started at 2023-12-16 16:11:29 +0000 UTC (1 container statuses recorded)
  Dec 16 17:39:22.257: INFO: 	Container kube-proxy ready: true, restart count 0
  Dec 16 17:39:22.257: INFO: sonobuoy from sonobuoy started at 2023-12-16 16:12:19 +0000 UTC (1 container statuses recorded)
  Dec 16 17:39:22.257: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Dec 16 17:39:22.258: INFO: sonobuoy-e2e-job-8b3ba51fd9314720 from sonobuoy started at 2023-12-16 16:12:30 +0000 UTC (2 container statuses recorded)
  Dec 16 17:39:22.258: INFO: 	Container e2e ready: true, restart count 0
  Dec 16 17:39:22.258: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 16 17:39:22.259: INFO: sonobuoy-systemd-logs-daemon-set-0d7ed218a9fd4126-zq8f6 from sonobuoy started at 2023-12-16 16:12:30 +0000 UTC (2 container statuses recorded)
  Dec 16 17:39:22.259: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 16 17:39:22.259: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 12/16/23 17:39:22.26
  E1216 17:39:23.005418      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:24.005454      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 12/16/23 17:39:24.34
  STEP: Trying to apply a random label on the found node. @ 12/16/23 17:39:24.391
  STEP: verifying the node has the label kubernetes.io/e2e-c8a36eaa-97fd-4300-a463-2e1bb4c5e651 95 @ 12/16/23 17:39:24.412
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 12/16/23 17:39:24.431
  E1216 17:39:25.006448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:26.006467      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.112 on the node which pod4 resides and expect not scheduled @ 12/16/23 17:39:26.488
  E1216 17:39:27.007575      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:28.008109      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:29.008451      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:30.009062      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:31.009526      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:32.010476      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:33.010711      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:34.011758      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:35.012899      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:36.012992      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:37.013501      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:38.014131      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:39.014739      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:40.016245      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:41.016179      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:42.016981      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:43.017682      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:44.017913      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:45.019216      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:46.019381      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:47.020852      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:48.021008      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:49.021520      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:50.022367      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:51.024472      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:52.023983      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:53.024206      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:54.024615      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:55.025510      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:56.025643      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:57.026051      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:58.026245      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:39:59.027002      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:00.027793      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:01.028040      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:02.028833      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:03.029409      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:04.030125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:05.030636      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:06.031260      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:07.031855      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:08.032866      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:09.033065      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:10.033321      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:11.033535      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:12.033867      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:13.034576      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:14.035520      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:15.035306      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:16.035420      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:17.036665      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:18.036567      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:19.037279      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:20.037598      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:21.037787      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:22.038862      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:23.039147      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:24.039417      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:25.039553      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:26.040965      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:27.042205      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:28.042805      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:29.043415      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:30.044214      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:31.045282      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:32.045423      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:33.045643      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:34.046103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:35.046904      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:36.047359      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:37.047920      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:38.048790      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:39.049370      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:40.050145      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:41.050441      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:42.050912      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:43.051233      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:44.051380      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:45.052354      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:46.052838      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:47.053583      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:48.053693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:49.054381      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:50.055119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:51.055403      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:52.055649      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:53.056665      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:54.057940      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:55.058609      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:56.059206      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:57.059129      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:58.059732      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:40:59.060113      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:00.061196      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:01.061798      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:02.062509      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:03.062957      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:04.066453      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:05.063670      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:06.064170      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:07.064334      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:08.064587      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:09.065170      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:10.066335      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:11.068716      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:12.068918      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:13.069041      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:14.069519      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:15.071331      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:16.071907      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:17.072092      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:18.072847      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:19.073204      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:20.073407      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:21.074371      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:22.075575      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:23.075632      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:24.075877      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:25.076294      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:26.077358      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:27.078693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:28.078599      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:29.078765      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:30.079041      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:31.079828      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:32.080463      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:33.081080      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:34.082216      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:35.086973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:36.085063      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:37.085890      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:38.088055      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:39.087455      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:40.088177      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:41.094714      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:42.093257      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:43.094420      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:44.096565      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:45.096377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:46.096728      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:47.097908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:48.098697      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:49.099595      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:50.101327      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:51.101127      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:52.101456      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:53.101743      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:54.101844      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:55.102303      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:56.102586      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:57.102721      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:58.103687      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:41:59.104437      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:00.104647      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:01.105589      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:02.105744      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:03.105949      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:04.106518      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:05.106564      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:06.106941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:07.108043      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:08.108355      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:09.108373      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:10.109535      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:11.112131      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:12.111608      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:13.111820      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:14.126598      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:15.114089      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:16.114259      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:17.118562      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:18.115483      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:19.115746      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:20.115957      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:21.117646      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:22.117363      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:23.143318      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:24.131346      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:25.131514      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:26.133997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:27.134055      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:28.135122      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:29.135909      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:30.137403      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:31.137142      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:32.136985      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:33.137931      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:34.138622      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:35.138941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:36.140140      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:37.142111      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:38.141173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:39.141942      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:40.141471      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:41.145400      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:42.141997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:43.142744      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:44.143595      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:45.143522      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:46.143962      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:47.143998      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:48.144468      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:49.145427      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:50.145634      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:51.146315      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:52.146468      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:53.148239      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:54.148085      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:55.148547      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:56.149124      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:57.159824      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:58.153535      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:42:59.153745      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:00.154247      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:01.154640      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:02.155446      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:03.156244      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:04.157498      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:05.157329      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:06.157900      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:07.157757      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:08.157875      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:09.158356      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:10.160494      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:11.159388      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:12.160365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:13.160572      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:14.161528      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:15.161915      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:16.162150      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:17.162183      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:18.162477      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:19.163416      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:20.164162      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:21.165870      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:22.166491      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:23.167476      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:24.168045      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:25.168517      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:26.169573      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:27.170425      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:28.171104      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:29.171908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:30.172008      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:31.172334      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:32.173237      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:33.173205      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:34.173927      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:35.174413      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:36.174803      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:37.175552      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:38.176721      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:39.178448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:40.178831      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:41.179291      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:42.179617      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:43.179915      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:44.182382      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:45.181222      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:46.182710      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:47.182835      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:48.183390      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:49.185372      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:50.185836      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:51.186193      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:52.186506      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:53.186799      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:54.187736      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:55.188260      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:56.189185      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:57.189531      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:58.189857      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:43:59.190853      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:00.190973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:01.191057      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:02.191478      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:03.192486      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:04.193235      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:05.193267      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:06.194125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:07.195055      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:08.196138      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:09.197117      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:10.197941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:11.198718      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:12.199180      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:13.199218      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:14.200420      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:15.201046      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:16.201639      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:17.202234      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:18.202904      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:19.203091      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:20.203822      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:21.206907      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:22.205901      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:23.206256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:24.207073      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:25.209119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:26.209143      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-c8a36eaa-97fd-4300-a463-2e1bb4c5e651 off the node phoh7xai9ouk-3 @ 12/16/23 17:44:26.518
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-c8a36eaa-97fd-4300-a463-2e1bb4c5e651 @ 12/16/23 17:44:26.552
  Dec 16 17:44:26.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8116" for this suite. @ 12/16/23 17:44:26.586
• [304.554 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 12/16/23 17:44:26.631
  Dec 16 17:44:26.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename deployment @ 12/16/23 17:44:26.636
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:44:26.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:44:26.715
  Dec 16 17:44:26.755: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E1216 17:44:27.209355      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:28.210287      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:29.210201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:30.214474      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:31.216983      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:44:31.766: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 12/16/23 17:44:31.767
  Dec 16 17:44:31.768: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 12/16/23 17:44:31.791
  Dec 16 17:44:31.824: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8940  68587a6d-130d-4b13-87c6-59ff641cc273 29144 1 2023-12-16 17:44:31 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-12-16 17:44:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038da608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Dec 16 17:44:31.849: INFO: New ReplicaSet "test-cleanup-deployment-68b75d69f8" of Deployment "test-cleanup-deployment":
  &ReplicaSet{ObjectMeta:{test-cleanup-deployment-68b75d69f8  deployment-8940  ac1d1eba-8a87-459a-9b0a-959ebe6e91f5 29149 1 2023-12-16 17:44:31 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 68587a6d-130d-4b13-87c6-59ff641cc273 0xc0038daae7 0xc0038daae8}] [] [{kube-controller-manager Update apps/v1 2023-12-16 17:44:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68587a6d-130d-4b13-87c6-59ff641cc273\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 68b75d69f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038dab78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Dec 16 17:44:31.849: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Dec 16 17:44:31.849: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8940  b6788927-4acd-4dae-bd46-e7465d52854a 29147 1 2023-12-16 17:44:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 68587a6d-130d-4b13-87c6-59ff641cc273 0xc0038da957 0xc0038da958}] [] [{e2e.test Update apps/v1 2023-12-16 17:44:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-16 17:44:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-12-16 17:44:31 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"68587a6d-130d-4b13-87c6-59ff641cc273\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0038daa78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Dec 16 17:44:31.873: INFO: Pod "test-cleanup-controller-8rfkk" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-8rfkk test-cleanup-controller- deployment-8940  3b779c89-b684-4727-b178-72642c8424ed 29131 0 2023-12-16 17:44:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller b6788927-4acd-4dae-bd46-e7465d52854a 0xc0038db317 0xc0038db318}] [] [{kube-controller-manager Update v1 2023-12-16 17:44:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6788927-4acd-4dae-bd46-e7465d52854a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:44:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7d2j7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7d2j7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:44:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:44:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:44:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:44:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.112,PodIP:10.233.66.94,StartTime:2023-12-16 17:44:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-16 17:44:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://2c3b9513cdd203122432fcc1ec3043d842737ef0190f38567cea080c33a69b5d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.94,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:44:31.873: INFO: Pod "test-cleanup-deployment-68b75d69f8-v6bgk" is not available:
  &Pod{ObjectMeta:{test-cleanup-deployment-68b75d69f8-v6bgk test-cleanup-deployment-68b75d69f8- deployment-8940  2cc2892a-85a2-4d33-b14d-de19c825ae18 29151 0 2023-12-16 17:44:31 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-68b75d69f8 ac1d1eba-8a87-459a-9b0a-959ebe6e91f5 0xc0038db517 0xc0038db518}] [] [{kube-controller-manager Update v1 2023-12-16 17:44:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac1d1eba-8a87-459a-9b0a-959ebe6e91f5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qnlfq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qnlfq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:44:31.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8940" for this suite. @ 12/16/23 17:44:31.887
• [5.303 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 12/16/23 17:44:31.936
  Dec 16 17:44:31.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename deployment @ 12/16/23 17:44:31.94
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:44:31.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:44:31.985
  Dec 16 17:44:32.008: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E1216 17:44:32.217021      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:33.217678      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:34.218461      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:35.219294      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:36.219935      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:44:37.026: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 12/16/23 17:44:37.026
  Dec 16 17:44:37.027: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E1216 17:44:37.220536      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:38.221536      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:44:39.036: INFO: Creating deployment "test-rollover-deployment"
  Dec 16 17:44:39.064: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E1216 17:44:39.221972      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:40.222164      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:44:41.081: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Dec 16 17:44:41.096: INFO: Ensure that both replica sets have 1 created replica
  Dec 16 17:44:41.109: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Dec 16 17:44:41.126: INFO: Updating deployment test-rollover-deployment
  Dec 16 17:44:41.127: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E1216 17:44:41.224330      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:42.226261      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:44:43.146: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Dec 16 17:44:43.171: INFO: Make sure deployment "test-rollover-deployment" is complete
  Dec 16 17:44:43.187: INFO: all replica sets need to contain the pod-template-hash label
  Dec 16 17:44:43.188: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 39, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:44:43.227213      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:44.227203      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:44:45.210: INFO: all replica sets need to contain the pod-template-hash label
  Dec 16 17:44:45.210: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 39, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:44:45.228380      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:46.228582      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:44:47.217: INFO: all replica sets need to contain the pod-template-hash label
  Dec 16 17:44:47.218: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 39, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:44:47.229273      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:48.229596      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:44:49.208: INFO: all replica sets need to contain the pod-template-hash label
  Dec 16 17:44:49.208: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 39, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:44:49.230473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:50.257067      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:44:51.206: INFO: all replica sets need to contain the pod-template-hash label
  Dec 16 17:44:51.206: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 39, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:44:51.236055      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:52.236691      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:44:53.207: INFO: 
  Dec 16 17:44:53.208: INFO: Ensure that both old replica sets have no replicas
  E1216 17:44:53.237417      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:44:53.239: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2796  e5015149-6601-45bb-a857-691d9c2a9cba 29304 2 2023-12-16 17:44:39 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-12-16 17:44:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-16 17:44:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005781bc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-12-16 17:44:39 +0000 UTC,LastTransitionTime:2023-12-16 17:44:39 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-12-16 17:44:52 +0000 UTC,LastTransitionTime:2023-12-16 17:44:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Dec 16 17:44:53.248: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-2796  783a14d1-3417-4025-aa98-d24e62e40a1b 29294 2 2023-12-16 17:44:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment e5015149-6601-45bb-a857-691d9c2a9cba 0xc003396147 0xc003396148}] [] [{kube-controller-manager Update apps/v1 2023-12-16 17:44:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e5015149-6601-45bb-a857-691d9c2a9cba\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-16 17:44:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033961f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Dec 16 17:44:53.248: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Dec 16 17:44:53.249: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2796  a1531a9e-0fb5-422f-8fff-175adf3b2a5d 29303 2 2023-12-16 17:44:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment e5015149-6601-45bb-a857-691d9c2a9cba 0xc003396017 0xc003396018}] [] [{e2e.test Update apps/v1 2023-12-16 17:44:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-16 17:44:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e5015149-6601-45bb-a857-691d9c2a9cba\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-12-16 17:44:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0033960d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Dec 16 17:44:53.249: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-2796  8af15102-2f00-4b5a-8dd0-610f4d7468fc 29263 2 2023-12-16 17:44:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment e5015149-6601-45bb-a857-691d9c2a9cba 0xc003396267 0xc003396268}] [] [{kube-controller-manager Update apps/v1 2023-12-16 17:44:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e5015149-6601-45bb-a857-691d9c2a9cba\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-16 17:44:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003396318 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Dec 16 17:44:53.259: INFO: Pod "test-rollover-deployment-57777854c9-wvp97" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-wvp97 test-rollover-deployment-57777854c9- deployment-2796  7a1ce4de-800b-4a49-9a12-335bc27dcc28 29273 0 2023-12-16 17:44:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 783a14d1-3417-4025-aa98-d24e62e40a1b 0xc003396877 0xc003396878}] [] [{kube-controller-manager Update v1 2023-12-16 17:44:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"783a14d1-3417-4025-aa98-d24e62e40a1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:44:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rv4dq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rv4dq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:44:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:44:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:44:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:44:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.112,PodIP:10.233.66.97,StartTime:2023-12-16 17:44:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-16 17:44:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://6149c3d2459d8d507a93aa94bf805719a3f7246858bcd754d4da3a73b217b439,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.97,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:44:53.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2796" for this suite. @ 12/16/23 17:44:53.272
• [21.351 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 12/16/23 17:44:53.291
  Dec 16 17:44:53.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename webhook @ 12/16/23 17:44:53.295
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:44:53.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:44:53.339
  STEP: Setting up server cert @ 12/16/23 17:44:53.399
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/16/23 17:44:54.058
  STEP: Deploying the webhook pod @ 12/16/23 17:44:54.085
  STEP: Wait for the deployment to be ready @ 12/16/23 17:44:54.123
  Dec 16 17:44:54.152: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E1216 17:44:54.238452      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:55.241000      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:44:56.178: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:44:56.241535      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:57.242395      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:44:58.192: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:44:58.242991      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:44:59.243808      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:45:00.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:45:00.244397      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:01.244726      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:45:02.199: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:45:02.244788      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:03.245111      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:45:04.188: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 44, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:45:04.245538      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:05.245589      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/16/23 17:45:06.186
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 17:45:06.21
  E1216 17:45:06.246561      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:45:07.212: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 12/16/23 17:45:07.222
  STEP: Registering slow webhook via the AdmissionRegistration API @ 12/16/23 17:45:07.222
  E1216 17:45:07.247134      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 12/16/23 17:45:07.263
  E1216 17:45:08.247327      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 12/16/23 17:45:08.285
  STEP: Registering slow webhook via the AdmissionRegistration API @ 12/16/23 17:45:08.285
  E1216 17:45:09.248144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 12/16/23 17:45:09.34
  STEP: Registering slow webhook via the AdmissionRegistration API @ 12/16/23 17:45:09.341
  E1216 17:45:10.248621      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:11.249794      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:12.250164      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:13.250510      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:14.250512      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 12/16/23 17:45:14.417
  STEP: Registering slow webhook via the AdmissionRegistration API @ 12/16/23 17:45:14.417
  E1216 17:45:15.252149      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:16.264952      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:17.256537      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:18.259709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:19.259424      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:45:19.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4631" for this suite. @ 12/16/23 17:45:19.742
  STEP: Destroying namespace "webhook-markers-5640" for this suite. @ 12/16/23 17:45:19.761
• [26.490 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 12/16/23 17:45:19.787
  Dec 16 17:45:19.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename webhook @ 12/16/23 17:45:19.792
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:45:19.843
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:45:19.857
  STEP: Setting up server cert @ 12/16/23 17:45:19.938
  E1216 17:45:20.260031      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/16/23 17:45:20.626
  STEP: Deploying the webhook pod @ 12/16/23 17:45:20.64
  STEP: Wait for the deployment to be ready @ 12/16/23 17:45:20.708
  Dec 16 17:45:20.726: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E1216 17:45:21.260626      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:22.261639      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:45:22.749: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:45:23.261709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:24.261927      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:45:24.757: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:45:25.262918      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:26.263152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:45:26.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:45:27.263398      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:28.263901      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:45:28.757: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:45:29.264787      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:30.265720      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:45:30.763: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 45, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:45:31.266433      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:32.266982      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/16/23 17:45:32.757
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 17:45:32.789
  E1216 17:45:33.267111      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:45:33.791: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 12/16/23 17:45:33.801
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 12/16/23 17:45:33.803
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 12/16/23 17:45:33.803
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 12/16/23 17:45:33.804
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 12/16/23 17:45:33.805
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 12/16/23 17:45:33.806
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 12/16/23 17:45:33.807
  Dec 16 17:45:33.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3318" for this suite. @ 12/16/23 17:45:33.934
  STEP: Destroying namespace "webhook-markers-2582" for this suite. @ 12/16/23 17:45:33.947
• [14.188 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 12/16/23 17:45:33.981
  Dec 16 17:45:33.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename proxy @ 12/16/23 17:45:33.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:45:34.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:45:34.039
  Dec 16 17:45:34.045: INFO: Creating pod...
  E1216 17:45:34.267786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:35.268532      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:45:36.083: INFO: Creating service...
  Dec 16 17:45:36.114: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8962/pods/agnhost/proxy/some/path/with/DELETE
  Dec 16 17:45:36.128: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Dec 16 17:45:36.128: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8962/pods/agnhost/proxy/some/path/with/GET
  Dec 16 17:45:36.139: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Dec 16 17:45:36.139: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8962/pods/agnhost/proxy/some/path/with/HEAD
  Dec 16 17:45:36.148: INFO: http.Client request:HEAD | StatusCode:200
  Dec 16 17:45:36.148: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8962/pods/agnhost/proxy/some/path/with/OPTIONS
  Dec 16 17:45:36.155: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Dec 16 17:45:36.156: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8962/pods/agnhost/proxy/some/path/with/PATCH
  Dec 16 17:45:36.164: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Dec 16 17:45:36.164: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8962/pods/agnhost/proxy/some/path/with/POST
  Dec 16 17:45:36.170: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Dec 16 17:45:36.170: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8962/pods/agnhost/proxy/some/path/with/PUT
  Dec 16 17:45:36.177: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Dec 16 17:45:36.177: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8962/services/test-service/proxy/some/path/with/DELETE
  Dec 16 17:45:36.189: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Dec 16 17:45:36.189: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8962/services/test-service/proxy/some/path/with/GET
  Dec 16 17:45:36.201: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Dec 16 17:45:36.201: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8962/services/test-service/proxy/some/path/with/HEAD
  Dec 16 17:45:36.213: INFO: http.Client request:HEAD | StatusCode:200
  Dec 16 17:45:36.214: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8962/services/test-service/proxy/some/path/with/OPTIONS
  Dec 16 17:45:36.224: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Dec 16 17:45:36.225: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8962/services/test-service/proxy/some/path/with/PATCH
  Dec 16 17:45:36.236: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Dec 16 17:45:36.236: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8962/services/test-service/proxy/some/path/with/POST
  Dec 16 17:45:36.246: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Dec 16 17:45:36.246: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8962/services/test-service/proxy/some/path/with/PUT
  Dec 16 17:45:36.257: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Dec 16 17:45:36.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E1216 17:45:36.269559      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-8962" for this suite. @ 12/16/23 17:45:36.273
• [2.307 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 12/16/23 17:45:36.3
  Dec 16 17:45:36.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 12/16/23 17:45:36.303
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:45:36.335
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:45:36.34
  STEP: create the container to handle the HTTPGet hook request. @ 12/16/23 17:45:36.361
  E1216 17:45:37.269460      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:38.269800      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 12/16/23 17:45:38.413
  E1216 17:45:39.270928      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:40.270896      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 12/16/23 17:45:40.457
  E1216 17:45:41.272379      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:42.273221      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 12/16/23 17:45:42.505
  Dec 16 17:45:42.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9972" for this suite. @ 12/16/23 17:45:42.565
• [6.283 seconds]
------------------------------
SSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 12/16/23 17:45:42.588
  Dec 16 17:45:42.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename dns @ 12/16/23 17:45:42.594
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:45:42.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:45:42.637
  STEP: Creating a test headless service @ 12/16/23 17:45:42.644
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-378.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-378.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-378.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-378.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-378.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-378.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-378.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-378.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-378.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-378.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 119.33.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.33.119_udp@PTR;check="$$(dig +tcp +noall +answer +search 119.33.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.33.119_tcp@PTR;sleep 1; done
   @ 12/16/23 17:45:42.691
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-378.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-378.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-378.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-378.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-378.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-378.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-378.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-378.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-378.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-378.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 119.33.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.33.119_udp@PTR;check="$$(dig +tcp +noall +answer +search 119.33.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.33.119_tcp@PTR;sleep 1; done
   @ 12/16/23 17:45:42.691
  STEP: creating a pod to probe DNS @ 12/16/23 17:45:42.691
  STEP: submitting the pod to kubernetes @ 12/16/23 17:45:42.691
  E1216 17:45:43.273195      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:44.272682      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:45.272661      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:46.276077      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 12/16/23 17:45:46.766
  STEP: looking for the results for each expected name from probers @ 12/16/23 17:45:46.777
  Dec 16 17:45:46.797: INFO: Unable to read wheezy_udp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:46.829: INFO: Unable to read wheezy_tcp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:46.839: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:46.850: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:46.934: INFO: Unable to read jessie_udp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:46.951: INFO: Unable to read jessie_tcp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:46.970: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:46.980: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:47.019: INFO: Lookups using dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821 failed for: [wheezy_udp@dns-test-service.dns-378.svc.cluster.local wheezy_tcp@dns-test-service.dns-378.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local jessie_udp@dns-test-service.dns-378.svc.cluster.local jessie_tcp@dns-test-service.dns-378.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local]

  E1216 17:45:47.274340      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:48.276383      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:49.276828      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:50.277134      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:51.277677      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:45:52.033: INFO: Unable to read wheezy_udp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:52.047: INFO: Unable to read wheezy_tcp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:52.061: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:52.074: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:52.132: INFO: Unable to read jessie_udp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:52.141: INFO: Unable to read jessie_tcp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:52.151: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:52.160: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:52.209: INFO: Lookups using dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821 failed for: [wheezy_udp@dns-test-service.dns-378.svc.cluster.local wheezy_tcp@dns-test-service.dns-378.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local jessie_udp@dns-test-service.dns-378.svc.cluster.local jessie_tcp@dns-test-service.dns-378.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local]

  E1216 17:45:52.277979      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:53.278506      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:54.278631      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:55.278810      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:56.279728      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:45:57.030: INFO: Unable to read wheezy_udp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:57.037: INFO: Unable to read wheezy_tcp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:57.043: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:57.051: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:57.085: INFO: Unable to read jessie_udp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:57.093: INFO: Unable to read jessie_tcp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:57.101: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:57.107: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:45:57.134: INFO: Lookups using dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821 failed for: [wheezy_udp@dns-test-service.dns-378.svc.cluster.local wheezy_tcp@dns-test-service.dns-378.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local jessie_udp@dns-test-service.dns-378.svc.cluster.local jessie_tcp@dns-test-service.dns-378.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local]

  E1216 17:45:57.281145      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:58.281618      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:45:59.281987      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:00.282140      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:01.282867      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:46:02.034: INFO: Unable to read wheezy_udp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:02.048: INFO: Unable to read wheezy_tcp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:02.059: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:02.071: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:02.133: INFO: Unable to read jessie_udp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:02.144: INFO: Unable to read jessie_tcp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:02.153: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:02.161: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:02.224: INFO: Lookups using dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821 failed for: [wheezy_udp@dns-test-service.dns-378.svc.cluster.local wheezy_tcp@dns-test-service.dns-378.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local jessie_udp@dns-test-service.dns-378.svc.cluster.local jessie_tcp@dns-test-service.dns-378.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local]

  E1216 17:46:02.284019      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:03.284318      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:04.286438      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:05.286348      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:06.306133      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:46:07.040: INFO: Unable to read wheezy_udp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:07.048: INFO: Unable to read wheezy_tcp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:07.059: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:07.068: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:07.121: INFO: Unable to read jessie_udp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:07.130: INFO: Unable to read jessie_tcp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:07.137: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:07.147: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:07.186: INFO: Lookups using dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821 failed for: [wheezy_udp@dns-test-service.dns-378.svc.cluster.local wheezy_tcp@dns-test-service.dns-378.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local jessie_udp@dns-test-service.dns-378.svc.cluster.local jessie_tcp@dns-test-service.dns-378.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local]

  E1216 17:46:07.298982      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:08.298908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:09.299624      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:10.300299      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:11.300269      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:46:12.036: INFO: Unable to read wheezy_udp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:12.047: INFO: Unable to read wheezy_tcp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:12.061: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:12.077: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:12.169: INFO: Unable to read jessie_udp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:12.187: INFO: Unable to read jessie_tcp@dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:12.218: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:12.239: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local from pod dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821: the server could not find the requested resource (get pods dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821)
  Dec 16 17:46:12.282: INFO: Lookups using dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821 failed for: [wheezy_udp@dns-test-service.dns-378.svc.cluster.local wheezy_tcp@dns-test-service.dns-378.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local jessie_udp@dns-test-service.dns-378.svc.cluster.local jessie_tcp@dns-test-service.dns-378.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-378.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-378.svc.cluster.local]

  E1216 17:46:12.301138      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:13.301250      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:14.302269      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:15.303256      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:16.303471      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:46:17.254: INFO: DNS probes using dns-378/dns-test-4084bb37-397a-48f1-9c5c-7ce80a42e821 succeeded

  Dec 16 17:46:17.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/16/23 17:46:17.27
  E1216 17:46:17.312559      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the test service @ 12/16/23 17:46:17.392
  STEP: deleting the test headless service @ 12/16/23 17:46:17.524
  STEP: Destroying namespace "dns-378" for this suite. @ 12/16/23 17:46:17.588
• [35.029 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:177
  STEP: Creating a kubernetes client @ 12/16/23 17:46:17.633
  Dec 16 17:46:17.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename daemonsets @ 12/16/23 17:46:17.657
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:46:17.695
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:46:17.702
  STEP: Creating simple DaemonSet "daemon-set" @ 12/16/23 17:46:17.823
  STEP: Check that daemon pods launch on every node of the cluster. @ 12/16/23 17:46:17.843
  Dec 16 17:46:17.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 17:46:17.884: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  E1216 17:46:18.312954      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:46:18.904: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 17:46:18.905: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  E1216 17:46:19.314619      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:46:19.936: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 16 17:46:19.937: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  E1216 17:46:20.313854      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:46:20.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 16 17:46:20.914: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  E1216 17:46:21.314560      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:46:21.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Dec 16 17:46:21.899: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 12/16/23 17:46:21.906
  Dec 16 17:46:21.972: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 16 17:46:21.972: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  E1216 17:46:22.317886      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:46:23.016: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 16 17:46:23.016: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  E1216 17:46:23.316943      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:46:24.013: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 16 17:46:24.013: INFO: Node phoh7xai9ouk-1 is running 0 daemon pod, expected 1
  E1216 17:46:24.318179      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:46:25.000: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Dec 16 17:46:25.000: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 12/16/23 17:46:25.008
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6089, will wait for the garbage collector to delete the pods @ 12/16/23 17:46:25.01
  Dec 16 17:46:25.101: INFO: Deleting DaemonSet.extensions daemon-set took: 23.932785ms
  Dec 16 17:46:25.202: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.478208ms
  E1216 17:46:25.317894      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:46:26.210: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 16 17:46:26.210: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Dec 16 17:46:26.226: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29876"},"items":null}

  Dec 16 17:46:26.232: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29876"},"items":null}

  Dec 16 17:46:26.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6089" for this suite. @ 12/16/23 17:46:26.291
• [8.680 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 12/16/23 17:46:26.306
  Dec 16 17:46:26.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename secrets @ 12/16/23 17:46:26.31
  E1216 17:46:26.318559      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:46:26.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:46:26.376
  STEP: Creating secret with name secret-test-68fa5ffa-9561-42a0-8eee-a66f0f72b03e @ 12/16/23 17:46:26.491
  STEP: Creating a pod to test consume secrets @ 12/16/23 17:46:26.505
  E1216 17:46:27.319791      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:28.321374      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:29.320937      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:30.321428      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:46:30.565
  Dec 16 17:46:30.574: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-secrets-9a268aaa-beba-40c9-b79c-1972b7dd617d container secret-volume-test: <nil>
  STEP: delete the pod @ 12/16/23 17:46:30.616
  Dec 16 17:46:30.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5065" for this suite. @ 12/16/23 17:46:30.668
  STEP: Destroying namespace "secret-namespace-5773" for this suite. @ 12/16/23 17:46:30.688
• [4.402 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 12/16/23 17:46:30.709
  Dec 16 17:46:30.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename configmap @ 12/16/23 17:46:30.711
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:46:30.773
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:46:30.78
  STEP: Creating configMap with name configmap-test-volume-afe57809-c32e-49ac-b861-6367a77dcda3 @ 12/16/23 17:46:30.788
  STEP: Creating a pod to test consume configMaps @ 12/16/23 17:46:30.802
  E1216 17:46:31.322415      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:32.323152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:33.323750      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:34.323628      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:46:34.872
  Dec 16 17:46:34.878: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-configmaps-40b364b1-1cc9-48ea-baff-e3e908f02907 container agnhost-container: <nil>
  STEP: delete the pod @ 12/16/23 17:46:34.896
  Dec 16 17:46:34.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5224" for this suite. @ 12/16/23 17:46:34.961
• [4.268 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 12/16/23 17:46:35.002
  Dec 16 17:46:35.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename dns @ 12/16/23 17:46:35.004
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:46:35.035
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:46:35.041
  STEP: Creating a test externalName service @ 12/16/23 17:46:35.049
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5101.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5101.svc.cluster.local; sleep 1; done
   @ 12/16/23 17:46:35.065
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5101.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5101.svc.cluster.local; sleep 1; done
   @ 12/16/23 17:46:35.065
  STEP: creating a pod to probe DNS @ 12/16/23 17:46:35.065
  STEP: submitting the pod to kubernetes @ 12/16/23 17:46:35.065
  E1216 17:46:35.324832      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:36.325609      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:37.325786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:38.326061      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 12/16/23 17:46:39.127
  STEP: looking for the results for each expected name from probers @ 12/16/23 17:46:39.132
  Dec 16 17:46:39.152: INFO: DNS probes using dns-test-00a81ad9-ee2d-4dc1-b9f6-cd37951f7c0a succeeded

  STEP: changing the externalName to bar.example.com @ 12/16/23 17:46:39.153
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5101.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5101.svc.cluster.local; sleep 1; done
   @ 12/16/23 17:46:39.168
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5101.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5101.svc.cluster.local; sleep 1; done
   @ 12/16/23 17:46:39.168
  STEP: creating a second pod to probe DNS @ 12/16/23 17:46:39.168
  STEP: submitting the pod to kubernetes @ 12/16/23 17:46:39.169
  E1216 17:46:39.327911      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:40.329540      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 12/16/23 17:46:41.236
  STEP: looking for the results for each expected name from probers @ 12/16/23 17:46:41.245
  Dec 16 17:46:41.268: INFO: File wheezy_udp@dns-test-service-3.dns-5101.svc.cluster.local from pod  dns-5101/dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 16 17:46:41.278: INFO: File jessie_udp@dns-test-service-3.dns-5101.svc.cluster.local from pod  dns-5101/dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 16 17:46:41.278: INFO: Lookups using dns-5101/dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 failed for: [wheezy_udp@dns-test-service-3.dns-5101.svc.cluster.local jessie_udp@dns-test-service-3.dns-5101.svc.cluster.local]

  E1216 17:46:41.330585      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:42.350108      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:43.350287      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:44.350530      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:45.350765      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:46:46.293: INFO: File wheezy_udp@dns-test-service-3.dns-5101.svc.cluster.local from pod  dns-5101/dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 16 17:46:46.302: INFO: File jessie_udp@dns-test-service-3.dns-5101.svc.cluster.local from pod  dns-5101/dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 16 17:46:46.302: INFO: Lookups using dns-5101/dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 failed for: [wheezy_udp@dns-test-service-3.dns-5101.svc.cluster.local jessie_udp@dns-test-service-3.dns-5101.svc.cluster.local]

  E1216 17:46:46.351342      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:47.352633      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:48.354819      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:49.353471      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:50.353161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:46:51.289: INFO: File wheezy_udp@dns-test-service-3.dns-5101.svc.cluster.local from pod  dns-5101/dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 16 17:46:51.299: INFO: File jessie_udp@dns-test-service-3.dns-5101.svc.cluster.local from pod  dns-5101/dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 16 17:46:51.299: INFO: Lookups using dns-5101/dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 failed for: [wheezy_udp@dns-test-service-3.dns-5101.svc.cluster.local jessie_udp@dns-test-service-3.dns-5101.svc.cluster.local]

  E1216 17:46:51.354350      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:52.354681      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:53.355337      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:54.355427      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:55.355633      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:46:56.289: INFO: File wheezy_udp@dns-test-service-3.dns-5101.svc.cluster.local from pod  dns-5101/dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 16 17:46:56.304: INFO: File jessie_udp@dns-test-service-3.dns-5101.svc.cluster.local from pod  dns-5101/dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 16 17:46:56.304: INFO: Lookups using dns-5101/dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 failed for: [wheezy_udp@dns-test-service-3.dns-5101.svc.cluster.local jessie_udp@dns-test-service-3.dns-5101.svc.cluster.local]

  E1216 17:46:56.356957      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:57.356993      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:58.357806      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:46:59.357836      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:00.359776      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:47:01.293: INFO: File wheezy_udp@dns-test-service-3.dns-5101.svc.cluster.local from pod  dns-5101/dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 16 17:47:01.304: INFO: File jessie_udp@dns-test-service-3.dns-5101.svc.cluster.local from pod  dns-5101/dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 16 17:47:01.304: INFO: Lookups using dns-5101/dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 failed for: [wheezy_udp@dns-test-service-3.dns-5101.svc.cluster.local jessie_udp@dns-test-service-3.dns-5101.svc.cluster.local]

  E1216 17:47:01.360042      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:02.360186      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:03.361314      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:04.360528      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:05.368477      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:47:06.291: INFO: File wheezy_udp@dns-test-service-3.dns-5101.svc.cluster.local from pod  dns-5101/dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 contains '' instead of 'bar.example.com.'
  Dec 16 17:47:06.297: INFO: File jessie_udp@dns-test-service-3.dns-5101.svc.cluster.local from pod  dns-5101/dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 16 17:47:06.297: INFO: Lookups using dns-5101/dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 failed for: [wheezy_udp@dns-test-service-3.dns-5101.svc.cluster.local jessie_udp@dns-test-service-3.dns-5101.svc.cluster.local]

  E1216 17:47:06.363242      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:07.367457      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:08.366899      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:09.367818      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:10.367540      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:47:11.302: INFO: DNS probes using dns-test-0ad90879-a80d-4bb9-93e0-4b6118e53c03 succeeded

  STEP: changing the service to type=ClusterIP @ 12/16/23 17:47:11.302
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5101.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5101.svc.cluster.local; sleep 1; done
   @ 12/16/23 17:47:11.353
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5101.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5101.svc.cluster.local; sleep 1; done
   @ 12/16/23 17:47:11.355
  STEP: creating a third pod to probe DNS @ 12/16/23 17:47:11.356
  E1216 17:47:11.368356      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: submitting the pod to kubernetes @ 12/16/23 17:47:11.368
  E1216 17:47:12.370234      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:13.370223      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:14.372875      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:15.372997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:16.373713      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:17.373638      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:18.373556      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:19.373850      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:20.374103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:21.375386      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:22.375632      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:23.376131      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:24.376597      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:25.377626      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:26.378567      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:27.378858      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:28.379782      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:29.380051      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:30.379874      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:31.382485      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:32.381435      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:33.381706      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:34.381971      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:35.382580      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:36.384661      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:37.385292      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:38.387072      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:39.386802      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 12/16/23 17:47:39.569
  STEP: looking for the results for each expected name from probers @ 12/16/23 17:47:39.576
  Dec 16 17:47:39.602: INFO: DNS probes using dns-test-24fbc66f-70a3-472f-bb26-692f84831d5b succeeded

  Dec 16 17:47:39.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/16/23 17:47:39.614
  STEP: deleting the pod @ 12/16/23 17:47:39.638
  STEP: deleting the pod @ 12/16/23 17:47:39.662
  STEP: deleting the test externalName service @ 12/16/23 17:47:39.692
  STEP: Destroying namespace "dns-5101" for this suite. @ 12/16/23 17:47:39.743
• [64.773 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 12/16/23 17:47:39.782
  Dec 16 17:47:39.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename deployment @ 12/16/23 17:47:39.796
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:47:39.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:47:39.837
  Dec 16 17:47:39.847: INFO: Creating deployment "webserver-deployment"
  Dec 16 17:47:39.864: INFO: Waiting for observed generation 1
  E1216 17:47:40.388294      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:41.389594      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:47:41.880: INFO: Waiting for all required pods to come up
  Dec 16 17:47:41.894: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 12/16/23 17:47:41.894
  E1216 17:47:42.390410      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:43.391335      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:47:43.942: INFO: Waiting for deployment "webserver-deployment" to complete
  Dec 16 17:47:43.954: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Dec 16 17:47:43.973: INFO: Updating deployment webserver-deployment
  Dec 16 17:47:43.973: INFO: Waiting for observed generation 2
  E1216 17:47:44.391891      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:45.392269      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:47:45.994: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Dec 16 17:47:46.000: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Dec 16 17:47:46.005: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Dec 16 17:47:46.025: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Dec 16 17:47:46.025: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Dec 16 17:47:46.033: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Dec 16 17:47:46.045: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Dec 16 17:47:46.045: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Dec 16 17:47:46.065: INFO: Updating deployment webserver-deployment
  Dec 16 17:47:46.065: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Dec 16 17:47:46.088: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Dec 16 17:47:46.122: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Dec 16 17:47:46.253: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-6782  7b56d983-d924-40d4-b8e1-22c5a43b99ef 30420 3 2023-12-16 17:47:39 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0054254a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-12-16 17:47:44 +0000 UTC,LastTransitionTime:2023-12-16 17:47:39 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-12-16 17:47:46 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Dec 16 17:47:46.320: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-6782  aa0157cf-3f44-453f-8eab-8f58733ca7ab 30406 3 2023-12-16 17:47:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 7b56d983-d924-40d4-b8e1-22c5a43b99ef 0xc0054259a7 0xc0054259a8}] [] [{kube-controller-manager Update apps/v1 2023-12-16 17:47:44 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b56d983-d924-40d4-b8e1-22c5a43b99ef\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005425a48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Dec 16 17:47:46.321: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Dec 16 17:47:46.321: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-6782  fa666490-99f6-4c25-a10d-5e3040537d1f 30405 3 2023-12-16 17:47:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 7b56d983-d924-40d4-b8e1-22c5a43b99ef 0xc0054258b7 0xc0054258b8}] [] [{kube-controller-manager Update apps/v1 2023-12-16 17:47:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b56d983-d924-40d4-b8e1-22c5a43b99ef\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005425948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  E1216 17:47:46.393002      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:47:46.499: INFO: Pod "webserver-deployment-67bd4bf6dc-2skpp" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2skpp webserver-deployment-67bd4bf6dc- deployment-6782  a6451219-e3b3-4cf6-8d21-e461ebb48bd5 30437 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc001bf8b27 0xc001bf8b28}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vvfrs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vvfrs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.500: INFO: Pod "webserver-deployment-67bd4bf6dc-2w48z" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2w48z webserver-deployment-67bd4bf6dc- deployment-6782  e187f37c-bc60-4620-8bf0-04f681748a34 30444 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc001bf8e70 0xc001bf8e71}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v9nbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v9nbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.500: INFO: Pod "webserver-deployment-67bd4bf6dc-5czcz" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5czcz webserver-deployment-67bd4bf6dc- deployment-6782  77712f2a-8391-4621-a77a-a12bf2c72b88 30280 0 2023-12-16 17:47:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc001bf9177 0xc001bf9178}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:47:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xf9sm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xf9sm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.172,PodIP:10.233.64.115,StartTime:2023-12-16 17:47:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-16 17:47:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://b92c0b3e3a40d6aca888373a7ec457b5f830e0cd6db66e08c4f1f0c365cae70d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.115,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.501: INFO: Pod "webserver-deployment-67bd4bf6dc-6t2qr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6t2qr webserver-deployment-67bd4bf6dc- deployment-6782  0855638a-36fb-4d96-a2eb-9e3f7745b057 30438 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc001bf9387 0xc001bf9388}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sb47j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sb47j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.501: INFO: Pod "webserver-deployment-67bd4bf6dc-9z9rm" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9z9rm webserver-deployment-67bd4bf6dc- deployment-6782  22fac35f-7c8d-4afe-93ce-4db6c651ea2d 30441 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc001bf9607 0xc001bf9608}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lcxc9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lcxc9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.49,PodIP:,StartTime:2023-12-16 17:47:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.501: INFO: Pod "webserver-deployment-67bd4bf6dc-cq9fh" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-cq9fh webserver-deployment-67bd4bf6dc- deployment-6782  55bbe765-7692-49c0-a6e6-2dd1e69fdcca 30271 0 2023-12-16 17:47:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc001bf9827 0xc001bf9828}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:47:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5r9s9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5r9s9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.49,PodIP:10.233.65.116,StartTime:2023-12-16 17:47:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-16 17:47:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://bfd74682c6d27db82cb5c1cd2dac5fe0b6afe53bdb35d3937bd360b05a309a36,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.116,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.502: INFO: Pod "webserver-deployment-67bd4bf6dc-dlwq7" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dlwq7 webserver-deployment-67bd4bf6dc- deployment-6782  d94eaebc-b178-48a8-af7c-5415c2145345 30276 0 2023-12-16 17:47:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc001bf9a47 0xc001bf9a48}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:47:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.114\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gmc64,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gmc64,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.49,PodIP:10.233.65.114,StartTime:2023-12-16 17:47:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-16 17:47:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://77e1b22bdf4a8dc7eacfa25ceac6a09fbe63e57780a83597f5d0fb36e7d4505f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.114,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.502: INFO: Pod "webserver-deployment-67bd4bf6dc-fsbzt" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-fsbzt webserver-deployment-67bd4bf6dc- deployment-6782  c78a637e-720a-4fc1-86e7-a7204ec170ff 30297 0 2023-12-16 17:47:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc001bf9c37 0xc001bf9c38}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:47:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9pb6z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9pb6z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.112,PodIP:10.233.66.109,StartTime:2023-12-16 17:47:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-16 17:47:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://5b17d8c36369a3299aa55d779a40d181201b692e0a2595ec9d5f092f3639ccfb,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.109,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.503: INFO: Pod "webserver-deployment-67bd4bf6dc-gdgt7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gdgt7 webserver-deployment-67bd4bf6dc- deployment-6782  843188aa-e3e9-467f-8852-8180c17ef249 30451 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc001bf9e27 0xc001bf9e28}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z9f64,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z9f64,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.503: INFO: Pod "webserver-deployment-67bd4bf6dc-gkm2q" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gkm2q webserver-deployment-67bd4bf6dc- deployment-6782  4f7e85a6-4d18-4259-b00f-13f8b9bc2786 30443 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc001bf9f90 0xc001bf9f91}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d8ws8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d8ws8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.503: INFO: Pod "webserver-deployment-67bd4bf6dc-hdqb8" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hdqb8 webserver-deployment-67bd4bf6dc- deployment-6782  a48aba40-392d-401e-8cb8-98f39f4dc6e1 30269 0 2023-12-16 17:47:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc001b8e307 0xc001b8e308}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:47:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-llmcj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-llmcj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.49,PodIP:10.233.65.115,StartTime:2023-12-16 17:47:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-16 17:47:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://c65c7c683543c67d2a13a4758a1778c93d3c826e474541d7a570f10882775777,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.115,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.504: INFO: Pod "webserver-deployment-67bd4bf6dc-jgt87" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jgt87 webserver-deployment-67bd4bf6dc- deployment-6782  9014765c-b02d-4469-8377-065810b080c1 30446 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc001b8eab7 0xc001b8eab8}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9tbmg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9tbmg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.112,PodIP:,StartTime:2023-12-16 17:47:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.504: INFO: Pod "webserver-deployment-67bd4bf6dc-jnbhm" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jnbhm webserver-deployment-67bd4bf6dc- deployment-6782  82b91b94-9655-4f0d-92e6-b98383079eef 30295 0 2023-12-16 17:47:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc001b8eeb7 0xc001b8eeb8}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:47:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8kp5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8kp5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.112,PodIP:10.233.66.108,StartTime:2023-12-16 17:47:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-16 17:47:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://d79d6dd5ca69dd9cc088fe457c406ab098633e0da51ad791f9de3620058d3c25,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.108,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.506: INFO: Pod "webserver-deployment-67bd4bf6dc-m6skk" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-m6skk webserver-deployment-67bd4bf6dc- deployment-6782  4bc66600-a301-4a3d-925e-320bdd74094b 30282 0 2023-12-16 17:47:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc001b8f3f7 0xc001b8f3f8}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:47:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8xlml,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8xlml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.172,PodIP:10.233.64.116,StartTime:2023-12-16 17:47:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-16 17:47:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://1f86354035c419f4cd3e4ff3bbdb8ceb2dfe3efdb7c203be5cf21c8694f87d9d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.116,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.507: INFO: Pod "webserver-deployment-67bd4bf6dc-p64xj" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-p64xj webserver-deployment-67bd4bf6dc- deployment-6782  8cf41a32-9e20-4211-98a2-41d2cceab098 30445 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc001b8fa77 0xc001b8fa78}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6zgqm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6zgqm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.507: INFO: Pod "webserver-deployment-67bd4bf6dc-qn746" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-qn746 webserver-deployment-67bd4bf6dc- deployment-6782  6e9ecca3-c137-478a-8e0a-e9f836be8a4d 30447 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc001b8fd17 0xc001b8fd18}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gjxd7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gjxd7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.507: INFO: Pod "webserver-deployment-67bd4bf6dc-tlqlz" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-tlqlz webserver-deployment-67bd4bf6dc- deployment-6782  09e751e8-7fbc-4c6b-a87e-e9b9a2f342e1 30442 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc0032a20c0 0xc0032a20c1}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v2ncq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v2ncq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.508: INFO: Pod "webserver-deployment-67bd4bf6dc-vjjvd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vjjvd webserver-deployment-67bd4bf6dc- deployment-6782  f899cc6c-96e1-4e05-ba94-5296d55590e5 30422 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc0032a2400 0xc0032a2401}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8ttjm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8ttjm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.112,PodIP:,StartTime:2023-12-16 17:47:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.508: INFO: Pod "webserver-deployment-67bd4bf6dc-vxzgw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vxzgw webserver-deployment-67bd4bf6dc- deployment-6782  3e494873-6e41-455d-a018-288ce933d148 30448 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc0032a27b7 0xc0032a27b8}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vq5lg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vq5lg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.509: INFO: Pod "webserver-deployment-67bd4bf6dc-xsm94" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xsm94 webserver-deployment-67bd4bf6dc- deployment-6782  106fb221-27c6-490a-94df-f0f0ff4ac807 30285 0 2023-12-16 17:47:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc fa666490-99f6-4c25-a10d-5e3040537d1f 0xc0032a28f7 0xc0032a28f8}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa666490-99f6-4c25-a10d-5e3040537d1f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:47:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.114\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-khp55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-khp55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.172,PodIP:10.233.64.114,StartTime:2023-12-16 17:47:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-16 17:47:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://328e13db1606ee5be14b4bf18aa8b3dfedd9bf5c94558ac9e417262e2626daf5,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.114,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.509: INFO: Pod "webserver-deployment-7b75d79cf5-4x4r7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-4x4r7 webserver-deployment-7b75d79cf5- deployment-6782  b83d7d59-43a2-412e-aed3-3d5ce02f4bb8 30435 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa0157cf-3f44-453f-8eab-8f58733ca7ab 0xc0032a2ae7 0xc0032a2ae8}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa0157cf-3f44-453f-8eab-8f58733ca7ab\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n5ldk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n5ldk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.509: INFO: Pod "webserver-deployment-7b75d79cf5-7854v" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-7854v webserver-deployment-7b75d79cf5- deployment-6782  8bfc81b7-522e-4cce-bb28-2cb9a648f182 30318 0 2023-12-16 17:47:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa0157cf-3f44-453f-8eab-8f58733ca7ab 0xc0032a2c37 0xc0032a2c38}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa0157cf-3f44-453f-8eab-8f58733ca7ab\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:47:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t568f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t568f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.112,PodIP:,StartTime:2023-12-16 17:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.510: INFO: Pod "webserver-deployment-7b75d79cf5-8vzvq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-8vzvq webserver-deployment-7b75d79cf5- deployment-6782  066730e4-212d-41f6-8f88-057bc0422eed 30436 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa0157cf-3f44-453f-8eab-8f58733ca7ab 0xc0032a2e27 0xc0032a2e28}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa0157cf-3f44-453f-8eab-8f58733ca7ab\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dqbwp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqbwp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.510: INFO: Pod "webserver-deployment-7b75d79cf5-gb7zj" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-gb7zj webserver-deployment-7b75d79cf5- deployment-6782  e95e05d8-bbce-4245-b984-fdf47262abbb 30327 0 2023-12-16 17:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa0157cf-3f44-453f-8eab-8f58733ca7ab 0xc0032a2f77 0xc0032a2f78}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa0157cf-3f44-453f-8eab-8f58733ca7ab\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:47:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h97xn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h97xn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.172,PodIP:,StartTime:2023-12-16 17:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.510: INFO: Pod "webserver-deployment-7b75d79cf5-l64kn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-l64kn webserver-deployment-7b75d79cf5- deployment-6782  9961c89a-07df-4567-aee8-8fc030142681 30347 0 2023-12-16 17:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa0157cf-3f44-453f-8eab-8f58733ca7ab 0xc0032a3167 0xc0032a3168}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa0157cf-3f44-453f-8eab-8f58733ca7ab\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:47:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2kkgj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2kkgj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.112,PodIP:,StartTime:2023-12-16 17:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.511: INFO: Pod "webserver-deployment-7b75d79cf5-l74xc" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-l74xc webserver-deployment-7b75d79cf5- deployment-6782  5820cd8d-5c00-47b8-bf48-bbd1a90c00f0 30352 0 2023-12-16 17:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa0157cf-3f44-453f-8eab-8f58733ca7ab 0xc0032a3377 0xc0032a3378}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa0157cf-3f44-453f-8eab-8f58733ca7ab\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:47:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jb9jl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jb9jl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.172,PodIP:,StartTime:2023-12-16 17:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.511: INFO: Pod "webserver-deployment-7b75d79cf5-lb897" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lb897 webserver-deployment-7b75d79cf5- deployment-6782  88aea673-833d-4233-8534-fa377326007a 30424 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa0157cf-3f44-453f-8eab-8f58733ca7ab 0xc0032a3577 0xc0032a3578}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa0157cf-3f44-453f-8eab-8f58733ca7ab\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2v6hf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2v6hf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.49,PodIP:,StartTime:2023-12-16 17:47:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.511: INFO: Pod "webserver-deployment-7b75d79cf5-mzd4h" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-mzd4h webserver-deployment-7b75d79cf5- deployment-6782  9db0e23a-1840-47ae-bc69-f3dce2efe312 30434 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa0157cf-3f44-453f-8eab-8f58733ca7ab 0xc0032a3797 0xc0032a3798}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa0157cf-3f44-453f-8eab-8f58733ca7ab\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f57gz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f57gz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.512: INFO: Pod "webserver-deployment-7b75d79cf5-qxgn6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-qxgn6 webserver-deployment-7b75d79cf5- deployment-6782  e7e7c99b-fcb8-447e-a851-100a37655252 30323 0 2023-12-16 17:47:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa0157cf-3f44-453f-8eab-8f58733ca7ab 0xc0032a38e7 0xc0032a38e8}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa0157cf-3f44-453f-8eab-8f58733ca7ab\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-16 17:47:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5pnt9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5pnt9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.49,PodIP:,StartTime:2023-12-16 17:47:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.512: INFO: Pod "webserver-deployment-7b75d79cf5-s5tgx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-s5tgx webserver-deployment-7b75d79cf5- deployment-6782  1b059801-0af9-412d-ae07-f42d124426a1 30430 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa0157cf-3f44-453f-8eab-8f58733ca7ab 0xc0032a3ad7 0xc0032a3ad8}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa0157cf-3f44-453f-8eab-8f58733ca7ab\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9rtv5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9rtv5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.516: INFO: Pod "webserver-deployment-7b75d79cf5-sbdmb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-sbdmb webserver-deployment-7b75d79cf5- deployment-6782  7352151c-2ba3-44d0-8eaa-58af3838386d 30431 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa0157cf-3f44-453f-8eab-8f58733ca7ab 0xc0032a3c50 0xc0032a3c51}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa0157cf-3f44-453f-8eab-8f58733ca7ab\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-btnmv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-btnmv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:phoh7xai9ouk-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-16 17:47:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.516: INFO: Pod "webserver-deployment-7b75d79cf5-v8nh6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-v8nh6 webserver-deployment-7b75d79cf5- deployment-6782  b9ce2992-4de2-41b2-b854-f3292cb96530 30433 0 2023-12-16 17:47:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa0157cf-3f44-453f-8eab-8f58733ca7ab 0xc0032a3dc0 0xc0032a3dc1}] [] [{kube-controller-manager Update v1 2023-12-16 17:47:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa0157cf-3f44-453f-8eab-8f58733ca7ab\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8pl5q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8pl5q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 16 17:47:46.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6782" for this suite. @ 12/16/23 17:47:46.767
• [7.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 12/16/23 17:47:46.895
  Dec 16 17:47:46.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename secrets @ 12/16/23 17:47:46.897
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:47:47.102
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:47:47.122
  STEP: Creating secret with name secret-test-d0c94373-6e02-4917-9766-c1866ee00f49 @ 12/16/23 17:47:47.128
  STEP: Creating a pod to test consume secrets @ 12/16/23 17:47:47.143
  E1216 17:47:47.394581      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:48.394991      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:49.395315      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:50.396116      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:47:51.293
  Dec 16 17:47:51.300: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-secrets-2abd7ca2-6b5b-4d81-b592-dd4de66410b2 container secret-volume-test: <nil>
  STEP: delete the pod @ 12/16/23 17:47:51.322
  Dec 16 17:47:51.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3748" for this suite. @ 12/16/23 17:47:51.379
• [4.500 seconds]
------------------------------
SSSS
------------------------------
  E1216 17:47:51.396070      13 retrywatcher.go:130] "Watch failed" err="context canceled"
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 12/16/23 17:47:51.397
  Dec 16 17:47:51.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename downward-api @ 12/16/23 17:47:51.401
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:47:51.442
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:47:51.447
  STEP: Creating a pod to test downward API volume plugin @ 12/16/23 17:47:51.453
  E1216 17:47:52.397313      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:53.397648      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:54.423210      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:55.406978      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:47:55.594
  Dec 16 17:47:55.613: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downwardapi-volume-b9ffd87b-75f2-4a76-9429-daeb0bdae115 container client-container: <nil>
  STEP: delete the pod @ 12/16/23 17:47:55.665
  Dec 16 17:47:55.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4402" for this suite. @ 12/16/23 17:47:55.846
• [4.469 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 12/16/23 17:47:55.868
  Dec 16 17:47:55.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename secrets @ 12/16/23 17:47:55.893
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:47:56.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:47:56.013
  STEP: Creating secret with name secret-test-ebcd64d9-ce04-4b27-bfd3-5f52543e3c09 @ 12/16/23 17:47:56.02
  STEP: Creating a pod to test consume secrets @ 12/16/23 17:47:56.036
  E1216 17:47:56.407712      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:57.408175      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:58.408312      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:47:59.409462      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:48:00.09
  Dec 16 17:48:00.097: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-secrets-0ba4ba01-bfa8-4c05-948a-bfa14670f4a0 container secret-volume-test: <nil>
  STEP: delete the pod @ 12/16/23 17:48:00.111
  Dec 16 17:48:00.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3497" for this suite. @ 12/16/23 17:48:00.156
• [4.302 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 12/16/23 17:48:00.172
  Dec 16 17:48:00.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 17:48:00.174
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:48:00.206
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:48:00.212
  STEP: Creating a pod to test downward API volume plugin @ 12/16/23 17:48:00.221
  E1216 17:48:00.409207      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:01.409528      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:02.410232      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:03.410808      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:48:04.267
  Dec 16 17:48:04.274: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downwardapi-volume-0235b3c8-50b9-4876-8833-637c5aa014af container client-container: <nil>
  STEP: delete the pod @ 12/16/23 17:48:04.302
  Dec 16 17:48:04.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2999" for this suite. @ 12/16/23 17:48:04.353
• [4.200 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 12/16/23 17:48:04.373
  Dec 16 17:48:04.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename configmap @ 12/16/23 17:48:04.376
  E1216 17:48:04.411151      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:48:04.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:48:04.419
  Dec 16 17:48:04.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2968" for this suite. @ 12/16/23 17:48:04.561
• [0.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 12/16/23 17:48:04.585
  Dec 16 17:48:04.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename subpath @ 12/16/23 17:48:04.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:48:04.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:48:04.712
  STEP: Setting up data @ 12/16/23 17:48:04.719
  STEP: Creating pod pod-subpath-test-configmap-77r2 @ 12/16/23 17:48:04.743
  STEP: Creating a pod to test atomic-volume-subpath @ 12/16/23 17:48:04.744
  E1216 17:48:05.411652      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:06.411778      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:07.411985      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:08.412970      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:09.413796      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:10.414156      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:11.415030      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:12.415490      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:13.415481      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:14.416026      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:15.416676      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:16.417308      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:17.417587      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:18.421940      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:19.419957      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:20.419895      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:21.420552      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:22.420569      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:23.420893      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:24.421656      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:25.422196      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:26.423058      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:27.423903      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:28.425368      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:48:28.946
  Dec 16 17:48:28.953: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-subpath-test-configmap-77r2 container test-container-subpath-configmap-77r2: <nil>
  STEP: delete the pod @ 12/16/23 17:48:28.973
  STEP: Deleting pod pod-subpath-test-configmap-77r2 @ 12/16/23 17:48:29.003
  Dec 16 17:48:29.004: INFO: Deleting pod "pod-subpath-test-configmap-77r2" in namespace "subpath-3430"
  Dec 16 17:48:29.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-3430" for this suite. @ 12/16/23 17:48:29.022
• [24.451 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 12/16/23 17:48:29.048
  Dec 16 17:48:29.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename podtemplate @ 12/16/23 17:48:29.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:48:29.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:48:29.09
  STEP: Create set of pod templates @ 12/16/23 17:48:29.098
  Dec 16 17:48:29.111: INFO: created test-podtemplate-1
  Dec 16 17:48:29.119: INFO: created test-podtemplate-2
  Dec 16 17:48:29.128: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 12/16/23 17:48:29.128
  STEP: delete collection of pod templates @ 12/16/23 17:48:29.135
  Dec 16 17:48:29.135: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 12/16/23 17:48:29.174
  Dec 16 17:48:29.174: INFO: requesting list of pod templates to confirm quantity
  Dec 16 17:48:29.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-6953" for this suite. @ 12/16/23 17:48:29.221
• [0.191 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 12/16/23 17:48:29.241
  Dec 16 17:48:29.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename containers @ 12/16/23 17:48:29.243
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:48:29.269
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:48:29.275
  STEP: Creating a pod to test override all @ 12/16/23 17:48:29.283
  E1216 17:48:29.425741      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:30.427771      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:31.428055      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:32.428365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:48:33.318
  Dec 16 17:48:33.325: INFO: Trying to get logs from node phoh7xai9ouk-3 pod client-containers-3cb8f6f6-a46f-44fd-aab8-3f9af59ef9be container agnhost-container: <nil>
  STEP: delete the pod @ 12/16/23 17:48:33.342
  Dec 16 17:48:33.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-1059" for this suite. @ 12/16/23 17:48:33.381
• [4.157 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 12/16/23 17:48:33.403
  Dec 16 17:48:33.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename services @ 12/16/23 17:48:33.409
  E1216 17:48:33.428697      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:48:33.437
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:48:33.443
  STEP: creating service in namespace services-2309 @ 12/16/23 17:48:33.448
  STEP: creating service affinity-nodeport-transition in namespace services-2309 @ 12/16/23 17:48:33.448
  STEP: creating replication controller affinity-nodeport-transition in namespace services-2309 @ 12/16/23 17:48:33.482
  I1216 17:48:33.506245      13 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-2309, replica count: 3
  E1216 17:48:34.429633      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:35.430150      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:36.430262      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:48:36.562087      13 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Dec 16 17:48:36.594: INFO: Creating new exec pod
  E1216 17:48:37.430889      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:38.432185      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:39.432412      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:48:39.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-2309 exec execpod-affinity4rxcp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Dec 16 17:48:40.095: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Dec 16 17:48:40.095: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 16 17:48:40.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-2309 exec execpod-affinity4rxcp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.40.178 80'
  Dec 16 17:48:40.381: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.40.178 80\nConnection to 10.233.40.178 80 port [tcp/http] succeeded!\n"
  Dec 16 17:48:40.381: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 16 17:48:40.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-2309 exec execpod-affinity4rxcp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.112 32725'
  E1216 17:48:40.433022      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:48:40.724: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.112 32725\nConnection to 192.168.121.112 32725 port [tcp/*] succeeded!\n"
  Dec 16 17:48:40.724: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 16 17:48:40.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-2309 exec execpod-affinity4rxcp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.172 32725'
  Dec 16 17:48:40.990: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.172 32725\nConnection to 192.168.121.172 32725 port [tcp/*] succeeded!\n"
  Dec 16 17:48:40.990: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 16 17:48:41.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-2309 exec execpod-affinity4rxcp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.172:32725/ ; done'
  E1216 17:48:41.434098      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:48:41.505: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n"
  Dec 16 17:48:41.505: INFO: stdout: "\naffinity-nodeport-transition-wtwph\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-bz5k8\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-wtwph\naffinity-nodeport-transition-wtwph\naffinity-nodeport-transition-wtwph\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-wtwph\naffinity-nodeport-transition-bz5k8\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-bz5k8\naffinity-nodeport-transition-bz5k8\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-bz5k8"
  Dec 16 17:48:41.505: INFO: Received response from host: affinity-nodeport-transition-wtwph
  Dec 16 17:48:41.505: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:41.505: INFO: Received response from host: affinity-nodeport-transition-bz5k8
  Dec 16 17:48:41.505: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:41.505: INFO: Received response from host: affinity-nodeport-transition-wtwph
  Dec 16 17:48:41.505: INFO: Received response from host: affinity-nodeport-transition-wtwph
  Dec 16 17:48:41.505: INFO: Received response from host: affinity-nodeport-transition-wtwph
  Dec 16 17:48:41.505: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:41.505: INFO: Received response from host: affinity-nodeport-transition-wtwph
  Dec 16 17:48:41.505: INFO: Received response from host: affinity-nodeport-transition-bz5k8
  Dec 16 17:48:41.505: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:41.505: INFO: Received response from host: affinity-nodeport-transition-bz5k8
  Dec 16 17:48:41.505: INFO: Received response from host: affinity-nodeport-transition-bz5k8
  Dec 16 17:48:41.505: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:41.505: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:41.505: INFO: Received response from host: affinity-nodeport-transition-bz5k8
  Dec 16 17:48:41.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-2309 exec execpod-affinity4rxcp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.172:32725/ ; done'
  Dec 16 17:48:42.063: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.172:32725/\n"
  Dec 16 17:48:42.063: INFO: stdout: "\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-2nmqr\naffinity-nodeport-transition-2nmqr"
  Dec 16 17:48:42.063: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:42.063: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:42.063: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:42.063: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:42.063: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:42.063: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:42.063: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:42.063: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:42.063: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:42.063: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:42.063: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:42.063: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:42.063: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:42.063: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:42.063: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:42.063: INFO: Received response from host: affinity-nodeport-transition-2nmqr
  Dec 16 17:48:42.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 16 17:48:42.073: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2309, will wait for the garbage collector to delete the pods @ 12/16/23 17:48:42.097
  Dec 16 17:48:42.179: INFO: Deleting ReplicationController affinity-nodeport-transition took: 17.574006ms
  Dec 16 17:48:42.380: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 201.591635ms
  E1216 17:48:42.435781      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:43.434601      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:44.435711      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-2309" for this suite. @ 12/16/23 17:48:44.436
• [11.049 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 12/16/23 17:48:44.454
  Dec 16 17:48:44.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename downward-api @ 12/16/23 17:48:44.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:48:44.49
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:48:44.496
  STEP: Creating the pod @ 12/16/23 17:48:44.503
  E1216 17:48:45.435913      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:46.436727      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:48:47.093: INFO: Successfully updated pod "labelsupdate73dbc429-1ac7-411a-9c18-b85a141ad121"
  E1216 17:48:47.437705      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:48.437907      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:48:49.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8248" for this suite. @ 12/16/23 17:48:49.142
• [4.706 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 12/16/23 17:48:49.162
  Dec 16 17:48:49.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 17:48:49.165
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:48:49.2
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:48:49.211
  STEP: Creating configMap with name projected-configmap-test-volume-20cd7c35-f2b5-4a3c-992a-dc1f4f0941a9 @ 12/16/23 17:48:49.217
  STEP: Creating a pod to test consume configMaps @ 12/16/23 17:48:49.228
  E1216 17:48:49.438742      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:50.439762      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:51.440099      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:52.440819      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:48:53.293
  Dec 16 17:48:53.303: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-projected-configmaps-61f2022c-251b-4cf8-9827-545a731ad706 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 12/16/23 17:48:53.32
  Dec 16 17:48:53.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2889" for this suite. @ 12/16/23 17:48:53.373
• [4.230 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 12/16/23 17:48:53.395
  Dec 16 17:48:53.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename replicaset @ 12/16/23 17:48:53.401
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:48:53.439
  E1216 17:48:53.440796      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:48:53.445
  Dec 16 17:48:53.500: INFO: Pod name sample-pod: Found 0 pods out of 1
  E1216 17:48:54.448604      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:55.442285      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:56.442803      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:57.442885      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:48:58.443110      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:48:58.515: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 12/16/23 17:48:58.515
  STEP: Scaling up "test-rs" replicaset  @ 12/16/23 17:48:58.515
  Dec 16 17:48:58.540: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 12/16/23 17:48:58.54
  W1216 17:48:58.556186      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Dec 16 17:48:58.562: INFO: observed ReplicaSet test-rs in namespace replicaset-4934 with ReadyReplicas 1, AvailableReplicas 1
  Dec 16 17:48:58.636: INFO: observed ReplicaSet test-rs in namespace replicaset-4934 with ReadyReplicas 1, AvailableReplicas 1
  Dec 16 17:48:58.669: INFO: observed ReplicaSet test-rs in namespace replicaset-4934 with ReadyReplicas 1, AvailableReplicas 1
  Dec 16 17:48:58.732: INFO: observed ReplicaSet test-rs in namespace replicaset-4934 with ReadyReplicas 1, AvailableReplicas 1
  E1216 17:48:59.444351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:48:59.766: INFO: observed ReplicaSet test-rs in namespace replicaset-4934 with ReadyReplicas 2, AvailableReplicas 2
  Dec 16 17:49:00.326: INFO: observed Replicaset test-rs in namespace replicaset-4934 with ReadyReplicas 3 found true
  Dec 16 17:49:00.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4934" for this suite. @ 12/16/23 17:49:00.337
• [6.963 seconds]
------------------------------
SSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 12/16/23 17:49:00.359
  Dec 16 17:49:00.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename events @ 12/16/23 17:49:00.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:49:00.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:49:00.409
  STEP: creating a test event @ 12/16/23 17:49:00.416
  STEP: listing events in all namespaces @ 12/16/23 17:49:00.426
  STEP: listing events in test namespace @ 12/16/23 17:49:00.44
  E1216 17:49:00.445221      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing events with field selection filtering on source @ 12/16/23 17:49:00.451
  STEP: listing events with field selection filtering on reportingController @ 12/16/23 17:49:00.462
  STEP: getting the test event @ 12/16/23 17:49:00.474
  STEP: patching the test event @ 12/16/23 17:49:00.483
  STEP: getting the test event @ 12/16/23 17:49:00.502
  STEP: updating the test event @ 12/16/23 17:49:00.515
  STEP: getting the test event @ 12/16/23 17:49:00.532
  STEP: deleting the test event @ 12/16/23 17:49:00.539
  STEP: listing events in all namespaces @ 12/16/23 17:49:00.561
  STEP: listing events in test namespace @ 12/16/23 17:49:00.568
  Dec 16 17:49:00.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-1118" for this suite. @ 12/16/23 17:49:00.586
• [0.242 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 12/16/23 17:49:00.602
  Dec 16 17:49:00.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename crd-publish-openapi @ 12/16/23 17:49:00.605
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:49:00.729
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:49:00.741
  Dec 16 17:49:00.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:49:01.445680      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:02.445735      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 12/16/23 17:49:02.702
  Dec 16 17:49:02.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-9921 --namespace=crd-publish-openapi-9921 create -f -'
  E1216 17:49:03.446055      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:04.437: INFO: stderr: ""
  Dec 16 17:49:04.437: INFO: stdout: "e2e-test-crd-publish-openapi-356-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Dec 16 17:49:04.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-9921 --namespace=crd-publish-openapi-9921 delete e2e-test-crd-publish-openapi-356-crds test-cr'
  E1216 17:49:04.453494      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:04.641: INFO: stderr: ""
  Dec 16 17:49:04.641: INFO: stdout: "e2e-test-crd-publish-openapi-356-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Dec 16 17:49:04.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-9921 --namespace=crd-publish-openapi-9921 apply -f -'
  E1216 17:49:05.454173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:06.069: INFO: stderr: ""
  Dec 16 17:49:06.069: INFO: stdout: "e2e-test-crd-publish-openapi-356-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Dec 16 17:49:06.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-9921 --namespace=crd-publish-openapi-9921 delete e2e-test-crd-publish-openapi-356-crds test-cr'
  E1216 17:49:06.454996      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:06.514: INFO: stderr: ""
  Dec 16 17:49:06.514: INFO: stdout: "e2e-test-crd-publish-openapi-356-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 12/16/23 17:49:06.514
  Dec 16 17:49:06.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-9921 explain e2e-test-crd-publish-openapi-356-crds'
  Dec 16 17:49:07.132: INFO: stderr: ""
  Dec 16 17:49:07.132: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-356-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E1216 17:49:07.455750      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:08.455977      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:08.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9921" for this suite. @ 12/16/23 17:49:08.956
• [8.369 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 12/16/23 17:49:08.98
  Dec 16 17:49:08.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 17:49:08.982
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:49:09.015
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:49:09.02
  STEP: Creating configMap with name projected-configmap-test-volume-f9e0d1bf-dece-413a-acf2-5fc6ddc65bf9 @ 12/16/23 17:49:09.026
  STEP: Creating a pod to test consume configMaps @ 12/16/23 17:49:09.036
  E1216 17:49:09.456085      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:10.457102      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:11.457320      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:12.457725      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:49:13.072
  Dec 16 17:49:13.078: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-projected-configmaps-892ae781-a0f1-4af0-bc8f-7a493fe10f20 container agnhost-container: <nil>
  STEP: delete the pod @ 12/16/23 17:49:13.111
  Dec 16 17:49:13.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9071" for this suite. @ 12/16/23 17:49:13.148
• [4.180 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 12/16/23 17:49:13.163
  Dec 16 17:49:13.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename pods @ 12/16/23 17:49:13.165
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:49:13.232
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:49:13.24
  Dec 16 17:49:13.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: creating the pod @ 12/16/23 17:49:13.25
  STEP: submitting the pod to kubernetes @ 12/16/23 17:49:13.251
  E1216 17:49:13.458597      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:14.459351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:15.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2058" for this suite. @ 12/16/23 17:49:15.416
• [2.266 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 12/16/23 17:49:15.432
  Dec 16 17:49:15.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename pods @ 12/16/23 17:49:15.435
  E1216 17:49:15.459861      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:49:15.469
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:49:15.478
  STEP: creating the pod @ 12/16/23 17:49:15.483
  STEP: submitting the pod to kubernetes @ 12/16/23 17:49:15.483
  STEP: verifying QOS class is set on the pod @ 12/16/23 17:49:15.501
  Dec 16 17:49:15.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9068" for this suite. @ 12/16/23 17:49:15.533
• [0.120 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 12/16/23 17:49:15.555
  Dec 16 17:49:15.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubelet-test @ 12/16/23 17:49:15.558
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:49:15.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:49:15.603
  E1216 17:49:16.461122      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:17.461350      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:17.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-400" for this suite. @ 12/16/23 17:49:17.682
• [2.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 12/16/23 17:49:17.711
  Dec 16 17:49:17.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename security-context-test @ 12/16/23 17:49:17.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:49:17.77
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:49:17.776
  E1216 17:49:18.462458      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:19.463581      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:20.463261      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:21.466445      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:21.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-2119" for this suite. @ 12/16/23 17:49:21.853
• [4.155 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 12/16/23 17:49:21.869
  Dec 16 17:49:21.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename webhook @ 12/16/23 17:49:21.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:49:21.9
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:49:21.906
  STEP: Setting up server cert @ 12/16/23 17:49:21.965
  E1216 17:49:22.468240      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/16/23 17:49:23.396
  STEP: Deploying the webhook pod @ 12/16/23 17:49:23.419
  STEP: Wait for the deployment to be ready @ 12/16/23 17:49:23.444
  E1216 17:49:23.468559      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:23.468: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E1216 17:49:24.468813      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:25.469901      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:25.489: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:49:26.470348      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:27.470817      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:27.495: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:49:28.471640      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:29.472484      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:29.500: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:49:30.472393      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:31.472604      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:31.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:49:32.472943      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:33.473018      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:33.498: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 16, 17, 49, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E1216 17:49:34.473249      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:35.474020      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/16/23 17:49:35.497
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 17:49:35.521
  E1216 17:49:36.474440      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:36.522: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Dec 16 17:49:36.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1485-crds.webhook.example.com via the AdmissionRegistration API @ 12/16/23 17:49:37.058
  STEP: Creating a custom resource that should be mutated by the webhook @ 12/16/23 17:49:37.102
  E1216 17:49:37.475246      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:38.475616      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:39.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E1216 17:49:39.480670      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-3342" for this suite. @ 12/16/23 17:49:40.152
  STEP: Destroying namespace "webhook-markers-6204" for this suite. @ 12/16/23 17:49:40.17
• [18.342 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 12/16/23 17:49:40.217
  Dec 16 17:49:40.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename disruption @ 12/16/23 17:49:40.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:49:40.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:49:40.262
  STEP: creating the pdb @ 12/16/23 17:49:40.27
  STEP: Waiting for the pdb to be processed @ 12/16/23 17:49:40.284
  E1216 17:49:40.481570      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:41.482408      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 12/16/23 17:49:42.302
  STEP: Waiting for the pdb to be processed @ 12/16/23 17:49:42.32
  STEP: patching the pdb @ 12/16/23 17:49:42.342
  STEP: Waiting for the pdb to be processed @ 12/16/23 17:49:42.368
  E1216 17:49:42.483495      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:43.483731      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be deleted @ 12/16/23 17:49:44.399
  Dec 16 17:49:44.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9298" for this suite. @ 12/16/23 17:49:44.414
• [4.210 seconds]
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 12/16/23 17:49:44.428
  Dec 16 17:49:44.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename security-context-test @ 12/16/23 17:49:44.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:49:44.464
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:49:44.47
  E1216 17:49:44.484500      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:45.485025      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:46.485094      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:47.486196      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:48.487519      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:49.487755      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:50.489302      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:50.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9810" for this suite. @ 12/16/23 17:49:50.56
• [6.146 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 12/16/23 17:49:50.576
  Dec 16 17:49:50.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubectl @ 12/16/23 17:49:50.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:49:50.62
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:49:50.628
  Dec 16 17:49:50.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-8696 create -f -'
  E1216 17:49:51.489509      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:51.637: INFO: stderr: ""
  Dec 16 17:49:51.637: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Dec 16 17:49:51.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-8696 create -f -'
  E1216 17:49:52.490091      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:53.389: INFO: stderr: ""
  Dec 16 17:49:53.389: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 12/16/23 17:49:53.389
  E1216 17:49:53.490331      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:54.399: INFO: Selector matched 1 pods for map[app:agnhost]
  Dec 16 17:49:54.399: INFO: Found 1 / 1
  Dec 16 17:49:54.399: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Dec 16 17:49:54.408: INFO: Selector matched 1 pods for map[app:agnhost]
  Dec 16 17:49:54.408: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Dec 16 17:49:54.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-8696 describe pod agnhost-primary-d5qsb'
  E1216 17:49:54.491499      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:54.641: INFO: stderr: ""
  Dec 16 17:49:54.641: INFO: stdout: "Name:             agnhost-primary-d5qsb\nNamespace:        kubectl-8696\nPriority:         0\nService Account:  default\nNode:             phoh7xai9ouk-3/192.168.121.112\nStart Time:       Sat, 16 Dec 2023 17:49:51 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.66.137\nIPs:\n  IP:           10.233.66.137\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://6c075d76e0ad2264f33c8bf0b2af9199d5a07989f2bc3522eb592789b737f8f9\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 16 Dec 2023 17:49:52 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qlwq4 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-qlwq4:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-8696/agnhost-primary-d5qsb to phoh7xai9ouk-3\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
  Dec 16 17:49:54.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-8696 describe rc agnhost-primary'
  Dec 16 17:49:54.919: INFO: stderr: ""
  Dec 16 17:49:54.919: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8696\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-d5qsb\n"
  Dec 16 17:49:54.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-8696 describe service agnhost-primary'
  Dec 16 17:49:55.092: INFO: stderr: ""
  Dec 16 17:49:55.092: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8696\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.2.250\nIPs:               10.233.2.250\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.66.137:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Dec 16 17:49:55.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-8696 describe node phoh7xai9ouk-1'
  Dec 16 17:49:55.346: INFO: stderr: ""
  Dec 16 17:49:55.346: INFO: stdout: "Name:               phoh7xai9ouk-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=phoh7xai9ouk-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"56:4e:79:f5:8a:f6\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.121.172\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 16 Dec 2023 16:02:23 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  phoh7xai9ouk-1\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 16 Dec 2023 17:49:52 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sat, 16 Dec 2023 16:11:33 +0000   Sat, 16 Dec 2023 16:11:33 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Sat, 16 Dec 2023 17:46:33 +0000   Sat, 16 Dec 2023 16:02:15 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 16 Dec 2023 17:46:33 +0000   Sat, 16 Dec 2023 16:02:15 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 16 Dec 2023 17:46:33 +0000   Sat, 16 Dec 2023 16:02:15 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 16 Dec 2023 17:46:33 +0000   Sat, 16 Dec 2023 16:04:19 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.172\n  Hostname:    phoh7xai9ouk-1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  115008636Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8123980Ki\n  pods:               110\nAllocatable:\n  cpu:                1600m\n  ephemeral-storage:  111880401014\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3274316Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 c30a8c0071df4e97abd5e410883fb19e\n  System UUID:                c30a8c00-71df-4e97-abd5-e410883fb19e\n  Boot ID:                    185800a4-254b-439b-98cb-b36fc6094e9e\n  Kernel Version:             6.2.0-39-generic\n  OS Image:                   Ubuntu 22.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.27.1\n  Kubelet Version:            v1.27.8\n  Kube-Proxy Version:         v1.27.8\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-flannel                kube-flannel-ds-5sbtb                                      100m (6%)     0 (0%)      50Mi (1%)        0 (0%)         98m\n  kube-system                 coredns-5d78c9869d-scnc5                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (5%)     93m\n  kube-system                 kube-addon-manager-phoh7xai9ouk-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         98m\n  kube-system                 kube-apiserver-phoh7xai9ouk-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         98m\n  kube-system                 kube-controller-manager-phoh7xai9ouk-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         98m\n  kube-system                 kube-proxy-7c5h8                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\n  kube-system                 kube-scheduler-phoh7xai9ouk-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         98m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0d7ed218a9fd4126-l6bt5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         97m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                755m (47%)  0 (0%)\n  memory             170Mi (5%)  170Mi (5%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason         Age                 From     Message\n  ----     ------         ----                ----     -------\n  Warning  ImageGCFailed  10s (x20 over 95m)  kubelet  failed to get imageFs info: non-existent label \"crio-images\"\n"
  Dec 16 17:49:55.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-8696 describe namespace kubectl-8696'
  E1216 17:49:55.492523      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:49:55.576: INFO: stderr: ""
  Dec 16 17:49:55.577: INFO: stdout: "Name:         kubectl-8696\nLabels:       e2e-framework=kubectl\n              e2e-run=e3ca193f-0902-4f7c-aa5c-6ee32ebbee92\n              kubernetes.io/metadata.name=kubectl-8696\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Dec 16 17:49:55.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8696" for this suite. @ 12/16/23 17:49:55.589
• [5.029 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 12/16/23 17:49:55.607
  Dec 16 17:49:55.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename sched-preemption @ 12/16/23 17:49:55.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:49:55.657
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:49:55.663
  Dec 16 17:49:55.713: INFO: Waiting up to 1m0s for all nodes to be ready
  E1216 17:49:56.492929      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:57.493009      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:58.493161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:49:59.493393      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:00.494116      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:01.494377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:02.494597      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:03.495190      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:04.495730      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:05.496490      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:06.497334      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:07.497976      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:08.497401      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:09.497983      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:10.498316      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:11.499001      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:12.499150      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:13.499495      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:14.500494      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:15.501861      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:16.502032      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:17.502934      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:18.503708      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:19.504209      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:20.504786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:21.505562      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:22.506336      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:23.506403      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:24.507410      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:25.507923      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:26.509171      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:27.509306      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:28.509806      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:29.509999      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:30.510978      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:31.511529      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:32.512018      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:33.512316      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:34.512873      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:35.513129      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:36.513811      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:37.514517      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:38.515040      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:39.515631      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:40.516122      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:41.516246      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:42.516506      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:43.516830      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:44.516995      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:45.517142      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:46.518156      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:47.518724      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:48.519370      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:49.519712      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:50.519823      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:51.520136      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:52.520867      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:53.521052      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:54.521634      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:55.522657      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:50:55.792: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 12/16/23 17:50:55.802
  Dec 16 17:50:55.871: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Dec 16 17:50:55.908: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Dec 16 17:50:55.998: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Dec 16 17:50:56.009: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Dec 16 17:50:56.118: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Dec 16 17:50:56.136: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 12/16/23 17:50:56.136
  E1216 17:50:56.523173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:57.523396      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 12/16/23 17:50:58.231
  E1216 17:50:58.528125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:50:59.524829      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:00.526115      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:01.526411      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:02.526350      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:03.529863      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:51:04.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-1154" for this suite. @ 12/16/23 17:51:04.486
• [68.892 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 12/16/23 17:51:04.503
  Dec 16 17:51:04.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-runtime @ 12/16/23 17:51:04.506
  E1216 17:51:04.527319      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:51:04.545
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:51:04.55
  STEP: create the container @ 12/16/23 17:51:04.558
  W1216 17:51:04.585748      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 12/16/23 17:51:04.586
  E1216 17:51:05.528119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:06.527945      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 12/16/23 17:51:06.624
  STEP: the container should be terminated @ 12/16/23 17:51:06.632
  STEP: the termination message should be set @ 12/16/23 17:51:06.632
  Dec 16 17:51:06.632: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 12/16/23 17:51:06.633
  Dec 16 17:51:06.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2415" for this suite. @ 12/16/23 17:51:06.67
• [2.181 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 12/16/23 17:51:06.687
  Dec 16 17:51:06.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename emptydir-wrapper @ 12/16/23 17:51:06.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:51:06.728
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:51:06.734
  STEP: Creating 50 configmaps @ 12/16/23 17:51:06.742
  STEP: Creating RC which spawns configmap-volume pods @ 12/16/23 17:51:07.283
  Dec 16 17:51:07.313: INFO: Pod name wrapped-volume-race-75391038-17e1-42de-be28-8b58abf6ed0d: Found 0 pods out of 5
  E1216 17:51:07.528941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:08.531047      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:09.531112      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:10.531449      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:11.531319      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:51:12.341: INFO: Pod name wrapped-volume-race-75391038-17e1-42de-be28-8b58abf6ed0d: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 12/16/23 17:51:12.341
  STEP: Creating RC which spawns configmap-volume pods @ 12/16/23 17:51:12.435
  Dec 16 17:51:12.495: INFO: Pod name wrapped-volume-race-90c20c4a-3130-4286-8ccd-d968af04b99c: Found 0 pods out of 5
  E1216 17:51:12.532341      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:13.532942      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:14.533725      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:15.533796      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:16.534591      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:51:17.515: INFO: Pod name wrapped-volume-race-90c20c4a-3130-4286-8ccd-d968af04b99c: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 12/16/23 17:51:17.515
  E1216 17:51:17.539227      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 12/16/23 17:51:17.581
  Dec 16 17:51:17.615: INFO: Pod name wrapped-volume-race-aea51554-bc6b-4188-b2c5-5148bac1a0a0: Found 0 pods out of 5
  E1216 17:51:18.539574      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:19.540928      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:20.543007      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:21.542129      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:22.541744      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:51:22.639: INFO: Pod name wrapped-volume-race-aea51554-bc6b-4188-b2c5-5148bac1a0a0: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 12/16/23 17:51:22.639
  Dec 16 17:51:22.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-aea51554-bc6b-4188-b2c5-5148bac1a0a0 in namespace emptydir-wrapper-6928, will wait for the garbage collector to delete the pods @ 12/16/23 17:51:22.694
  Dec 16 17:51:22.770: INFO: Deleting ReplicationController wrapped-volume-race-aea51554-bc6b-4188-b2c5-5148bac1a0a0 took: 14.680612ms
  Dec 16 17:51:22.972: INFO: Terminating ReplicationController wrapped-volume-race-aea51554-bc6b-4188-b2c5-5148bac1a0a0 pods took: 201.765294ms
  E1216 17:51:23.542918      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:24.543216      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-90c20c4a-3130-4286-8ccd-d968af04b99c in namespace emptydir-wrapper-6928, will wait for the garbage collector to delete the pods @ 12/16/23 17:51:25.175
  Dec 16 17:51:25.262: INFO: Deleting ReplicationController wrapped-volume-race-90c20c4a-3130-4286-8ccd-d968af04b99c took: 17.742551ms
  Dec 16 17:51:25.364: INFO: Terminating ReplicationController wrapped-volume-race-90c20c4a-3130-4286-8ccd-d968af04b99c pods took: 101.440401ms
  E1216 17:51:25.544187      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:26.544630      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-75391038-17e1-42de-be28-8b58abf6ed0d in namespace emptydir-wrapper-6928, will wait for the garbage collector to delete the pods @ 12/16/23 17:51:27.365
  Dec 16 17:51:27.447: INFO: Deleting ReplicationController wrapped-volume-race-75391038-17e1-42de-be28-8b58abf6ed0d took: 18.164392ms
  E1216 17:51:27.546364      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:51:27.547: INFO: Terminating ReplicationController wrapped-volume-race-75391038-17e1-42de-be28-8b58abf6ed0d pods took: 100.600491ms
  E1216 17:51:28.547443      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:29.548634      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 12/16/23 17:51:29.549
  STEP: Destroying namespace "emptydir-wrapper-6928" for this suite. @ 12/16/23 17:51:30.175
• [23.507 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 12/16/23 17:51:30.201
  Dec 16 17:51:30.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename emptydir @ 12/16/23 17:51:30.205
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:51:30.248
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:51:30.253
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 12/16/23 17:51:30.26
  E1216 17:51:30.549676      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:31.550443      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:32.551282      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:33.552063      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:51:34.355
  Dec 16 17:51:34.363: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-e87cf1e9-e3bd-485d-9193-ad8b91641cc0 container test-container: <nil>
  STEP: delete the pod @ 12/16/23 17:51:34.405
  Dec 16 17:51:34.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7047" for this suite. @ 12/16/23 17:51:34.457
• [4.272 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 12/16/23 17:51:34.484
  Dec 16 17:51:34.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename webhook @ 12/16/23 17:51:34.487
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:51:34.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:51:34.535
  E1216 17:51:34.552567      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 12/16/23 17:51:34.593
  E1216 17:51:35.552135      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/16/23 17:51:35.607
  STEP: Deploying the webhook pod @ 12/16/23 17:51:35.622
  STEP: Wait for the deployment to be ready @ 12/16/23 17:51:35.649
  Dec 16 17:51:35.682: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E1216 17:51:36.552847      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:37.552899      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/16/23 17:51:37.704
  STEP: Verifying the service has paired with the endpoint @ 12/16/23 17:51:37.725
  E1216 17:51:38.553247      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:51:38.727: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Dec 16 17:51:38.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1706-crds.webhook.example.com via the AdmissionRegistration API @ 12/16/23 17:51:39.273
  STEP: Creating a custom resource that should be mutated by the webhook @ 12/16/23 17:51:39.339
  E1216 17:51:39.553911      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:40.555018      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:51:41.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E1216 17:51:41.555041      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-2636" for this suite. @ 12/16/23 17:51:42.197
  STEP: Destroying namespace "webhook-markers-4231" for this suite. @ 12/16/23 17:51:42.218
• [7.747 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 12/16/23 17:51:42.234
  Dec 16 17:51:42.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-runtime @ 12/16/23 17:51:42.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:51:42.326
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:51:42.331
  STEP: create the container @ 12/16/23 17:51:42.336
  W1216 17:51:42.351245      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 12/16/23 17:51:42.351
  E1216 17:51:42.556203      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:43.556570      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 12/16/23 17:51:44.372
  STEP: the container should be terminated @ 12/16/23 17:51:44.378
  STEP: the termination message should be set @ 12/16/23 17:51:44.378
  Dec 16 17:51:44.378: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 12/16/23 17:51:44.378
  Dec 16 17:51:44.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-975" for this suite. @ 12/16/23 17:51:44.413
• [2.192 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 12/16/23 17:51:44.428
  Dec 16 17:51:44.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename crd-publish-openapi @ 12/16/23 17:51:44.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:51:44.46
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:51:44.465
  Dec 16 17:51:44.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:51:44.557314      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:45.557896      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 12/16/23 17:51:46.291
  Dec 16 17:51:46.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2405 --namespace=crd-publish-openapi-2405 create -f -'
  E1216 17:51:46.558537      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:47.559094      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:51:47.929: INFO: stderr: ""
  Dec 16 17:51:47.929: INFO: stdout: "e2e-test-crd-publish-openapi-4063-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Dec 16 17:51:47.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2405 --namespace=crd-publish-openapi-2405 delete e2e-test-crd-publish-openapi-4063-crds test-foo'
  Dec 16 17:51:48.103: INFO: stderr: ""
  Dec 16 17:51:48.103: INFO: stdout: "e2e-test-crd-publish-openapi-4063-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Dec 16 17:51:48.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2405 --namespace=crd-publish-openapi-2405 apply -f -'
  E1216 17:51:48.559087      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:51:49.485: INFO: stderr: ""
  Dec 16 17:51:49.486: INFO: stdout: "e2e-test-crd-publish-openapi-4063-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Dec 16 17:51:49.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2405 --namespace=crd-publish-openapi-2405 delete e2e-test-crd-publish-openapi-4063-crds test-foo'
  E1216 17:51:49.560089      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:51:49.727: INFO: stderr: ""
  Dec 16 17:51:49.727: INFO: stdout: "e2e-test-crd-publish-openapi-4063-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 12/16/23 17:51:49.727
  Dec 16 17:51:49.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2405 --namespace=crd-publish-openapi-2405 create -f -'
  Dec 16 17:51:50.219: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 12/16/23 17:51:50.219
  Dec 16 17:51:50.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2405 --namespace=crd-publish-openapi-2405 create -f -'
  E1216 17:51:50.561071      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:51:50.730: INFO: rc: 1
  Dec 16 17:51:50.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2405 --namespace=crd-publish-openapi-2405 apply -f -'
  Dec 16 17:51:51.246: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 12/16/23 17:51:51.247
  Dec 16 17:51:51.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2405 --namespace=crd-publish-openapi-2405 create -f -'
  E1216 17:51:51.561795      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:51:51.711: INFO: rc: 1
  Dec 16 17:51:51.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2405 --namespace=crd-publish-openapi-2405 apply -f -'
  Dec 16 17:51:52.188: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 12/16/23 17:51:52.188
  Dec 16 17:51:52.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2405 explain e2e-test-crd-publish-openapi-4063-crds'
  E1216 17:51:52.562023      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:51:52.637: INFO: stderr: ""
  Dec 16 17:51:52.637: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4063-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 12/16/23 17:51:52.645
  Dec 16 17:51:52.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2405 explain e2e-test-crd-publish-openapi-4063-crds.metadata'
  Dec 16 17:51:53.143: INFO: stderr: ""
  Dec 16 17:51:53.143: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4063-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Dec 16 17:51:53.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2405 explain e2e-test-crd-publish-openapi-4063-crds.spec'
  E1216 17:51:53.562087      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:51:53.601: INFO: stderr: ""
  Dec 16 17:51:53.601: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4063-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Dec 16 17:51:53.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2405 explain e2e-test-crd-publish-openapi-4063-crds.spec.bars'
  Dec 16 17:51:54.130: INFO: stderr: ""
  Dec 16 17:51:54.130: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4063-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 12/16/23 17:51:54.13
  Dec 16 17:51:54.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=crd-publish-openapi-2405 explain e2e-test-crd-publish-openapi-4063-crds.spec.bars2'
  E1216 17:51:54.563178      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:51:54.606: INFO: rc: 1
  E1216 17:51:55.563330      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:51:56.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2405" for this suite. @ 12/16/23 17:51:56.513
• [12.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 12/16/23 17:51:56.535
  Dec 16 17:51:56.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename configmap @ 12/16/23 17:51:56.538
  E1216 17:51:56.563771      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:51:56.579
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:51:56.586
  STEP: Creating configMap with name configmap-test-volume-map-a5b44ae0-6540-4c21-8c82-a98e047bcb63 @ 12/16/23 17:51:56.594
  STEP: Creating a pod to test consume configMaps @ 12/16/23 17:51:56.626
  E1216 17:51:57.564101      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:58.564156      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:51:59.564175      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:00.564824      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:52:00.769
  Dec 16 17:52:00.776: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-configmaps-72e5e5c9-a183-4b32-9405-943dc69503f1 container agnhost-container: <nil>
  STEP: delete the pod @ 12/16/23 17:52:00.822
  Dec 16 17:52:00.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-380" for this suite. @ 12/16/23 17:52:00.862
• [4.338 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 12/16/23 17:52:00.884
  Dec 16 17:52:00.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 17:52:00.887
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:52:00.949
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:52:00.957
  STEP: Creating a pod to test downward API volume plugin @ 12/16/23 17:52:00.968
  E1216 17:52:01.565411      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:02.565704      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:03.565966      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:04.566312      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:52:05.035
  Dec 16 17:52:05.043: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downwardapi-volume-22f7d172-019e-442d-94fb-3008eb00587e container client-container: <nil>
  STEP: delete the pod @ 12/16/23 17:52:05.057
  Dec 16 17:52:05.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8128" for this suite. @ 12/16/23 17:52:05.1
• [4.229 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 12/16/23 17:52:05.115
  Dec 16 17:52:05.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename crd-publish-openapi @ 12/16/23 17:52:05.118
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:52:05.152
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:52:05.157
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 12/16/23 17:52:05.163
  Dec 16 17:52:05.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:52:05.567216      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:06.567775      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:07.568075      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:08.569811      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:09.570507      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:10.570675      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:11.571523      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:12.571723      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 12/16/23 17:52:12.671
  Dec 16 17:52:12.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:52:13.572132      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:52:14.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:52:14.572497      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:15.573180      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:16.573740      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:17.573682      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:18.575412      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:19.575275      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:20.577270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:21.575793      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:22.576017      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:23.579953      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:52:23.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3476" for this suite. @ 12/16/23 17:52:23.645
• [18.546 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 12/16/23 17:52:23.674
  Dec 16 17:52:23.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename configmap @ 12/16/23 17:52:23.678
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:52:23.714
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:52:23.723
  STEP: Creating configMap with name configmap-test-volume-76612b4c-0482-4a45-bd76-3eac5a5ed6ab @ 12/16/23 17:52:23.73
  STEP: Creating a pod to test consume configMaps @ 12/16/23 17:52:23.741
  E1216 17:52:24.579435      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:25.579563      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:26.580507      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:27.580919      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:52:27.79
  Dec 16 17:52:27.797: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-configmaps-1f570e48-25ab-4007-a68b-39e0c5ab5201 container configmap-volume-test: <nil>
  STEP: delete the pod @ 12/16/23 17:52:27.809
  Dec 16 17:52:27.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2132" for this suite. @ 12/16/23 17:52:27.844
• [4.182 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 12/16/23 17:52:27.859
  Dec 16 17:52:27.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename pods @ 12/16/23 17:52:27.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:52:27.897
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:52:27.901
  STEP: creating a Pod with a static label @ 12/16/23 17:52:27.923
  STEP: watching for Pod to be ready @ 12/16/23 17:52:27.946
  Dec 16 17:52:27.952: INFO: observed Pod pod-test in namespace pods-8363 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Dec 16 17:52:27.953: INFO: observed Pod pod-test in namespace pods-8363 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:52:27 +0000 UTC  }]
  Dec 16 17:52:27.982: INFO: observed Pod pod-test in namespace pods-8363 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:52:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:52:27 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:52:27 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:52:27 +0000 UTC  }]
  E1216 17:52:28.581881      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:52:29.382: INFO: Found Pod pod-test in namespace pods-8363 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:52:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:52:29 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:52:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-12-16 17:52:27 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 12/16/23 17:52:29.389
  STEP: getting the Pod and ensuring that it's patched @ 12/16/23 17:52:29.405
  STEP: replacing the Pod's status Ready condition to False @ 12/16/23 17:52:29.411
  STEP: check the Pod again to ensure its Ready conditions are False @ 12/16/23 17:52:29.433
  STEP: deleting the Pod via a Collection with a LabelSelector @ 12/16/23 17:52:29.433
  STEP: watching for the Pod to be deleted @ 12/16/23 17:52:29.453
  Dec 16 17:52:29.457: INFO: observed event type MODIFIED
  E1216 17:52:29.582338      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:30.583440      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:52:31.401: INFO: observed event type MODIFIED
  Dec 16 17:52:31.548: INFO: observed event type MODIFIED
  E1216 17:52:31.584069      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:52:32.419: INFO: observed event type MODIFIED
  Dec 16 17:52:32.447: INFO: observed event type MODIFIED
  Dec 16 17:52:32.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8363" for this suite. @ 12/16/23 17:52:32.474
• [4.626 seconds]
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 12/16/23 17:52:32.486
  Dec 16 17:52:32.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename services @ 12/16/23 17:52:32.489
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:52:32.523
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:52:32.53
  STEP: creating service nodeport-test with type=NodePort in namespace services-1469 @ 12/16/23 17:52:32.539
  STEP: creating replication controller nodeport-test in namespace services-1469 @ 12/16/23 17:52:32.575
  E1216 17:52:32.584590      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:52:32.628343      13 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-1469, replica count: 2
  E1216 17:52:33.585300      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:34.585805      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:35.586004      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:52:35.680500      13 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Dec 16 17:52:35.680: INFO: Creating new exec pod
  E1216 17:52:36.586286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:37.586547      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:38.586966      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:52:38.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-1469 exec execpod5vj6v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Dec 16 17:52:39.085: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Dec 16 17:52:39.085: INFO: stdout: "nodeport-test-nbjjq"
  Dec 16 17:52:39.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-1469 exec execpod5vj6v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.7.132 80'
  Dec 16 17:52:39.357: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.7.132 80\nConnection to 10.233.7.132 80 port [tcp/http] succeeded!\n"
  Dec 16 17:52:39.357: INFO: stdout: "nodeport-test-nbjjq"
  Dec 16 17:52:39.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-1469 exec execpod5vj6v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.172 30722'
  E1216 17:52:39.587993      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:52:39.642: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.172 30722\nConnection to 192.168.121.172 30722 port [tcp/*] succeeded!\n"
  Dec 16 17:52:39.642: INFO: stdout: "nodeport-test-nbjjq"
  Dec 16 17:52:39.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-1469 exec execpod5vj6v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.49 30722'
  Dec 16 17:52:39.931: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.49 30722\nConnection to 192.168.121.49 30722 port [tcp/*] succeeded!\n"
  Dec 16 17:52:39.931: INFO: stdout: "nodeport-test-gjglg"
  Dec 16 17:52:39.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1469" for this suite. @ 12/16/23 17:52:39.942
• [7.472 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 12/16/23 17:52:39.959
  Dec 16 17:52:39.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename emptydir @ 12/16/23 17:52:39.962
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:52:40.011
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:52:40.019
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 12/16/23 17:52:40.027
  E1216 17:52:40.588398      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:41.588693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:42.589141      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:43.589474      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:52:44.076
  Dec 16 17:52:44.084: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-6b57bc70-15fa-4c6c-b523-cfb602497796 container test-container: <nil>
  STEP: delete the pod @ 12/16/23 17:52:44.096
  Dec 16 17:52:44.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3855" for this suite. @ 12/16/23 17:52:44.13
• [4.184 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 12/16/23 17:52:44.15
  Dec 16 17:52:44.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 17:52:44.153
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:52:44.184
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:52:44.19
  STEP: Creating secret with name s-test-opt-del-ecf3c00e-50e7-4ccf-8b35-09efc7e24932 @ 12/16/23 17:52:44.206
  STEP: Creating secret with name s-test-opt-upd-1c865440-c838-42a6-9ae9-b8941f821f4f @ 12/16/23 17:52:44.215
  STEP: Creating the pod @ 12/16/23 17:52:44.222
  E1216 17:52:44.589556      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:45.589940      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-ecf3c00e-50e7-4ccf-8b35-09efc7e24932 @ 12/16/23 17:52:46.316
  STEP: Updating secret s-test-opt-upd-1c865440-c838-42a6-9ae9-b8941f821f4f @ 12/16/23 17:52:46.33
  STEP: Creating secret with name s-test-opt-create-d36fb0aa-73c8-400c-a01f-d06bf5c826bb @ 12/16/23 17:52:46.341
  STEP: waiting to observe update in volume @ 12/16/23 17:52:46.357
  E1216 17:52:46.590114      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:47.591194      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:48.592115      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:49.592289      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:50.592372      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:51.592775      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:52.593213      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:53.593577      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:54.594461      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:55.594837      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:56.595174      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:57.595223      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:58.595484      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:52:59.595811      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:00.596387      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:01.596628      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:02.596693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:03.597164      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:04.597624      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:05.598240      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:06.598906      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:07.599179      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:08.600434      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:09.600282      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:10.600908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:11.601684      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:12.602420      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:13.602753      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:14.603163      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:15.603252      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:16.603406      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:17.603989      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:18.603915      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:19.604121      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:20.604980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:21.605148      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:22.606147      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:23.606266      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:24.606544      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:25.607299      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:26.607121      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:27.607363      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:28.607630      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:29.607764      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:30.608118      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:31.608182      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:32.609522      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:33.609296      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:34.610178      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:35.610367      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:36.610859      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:37.611102      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:38.612152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:39.612201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:40.612389      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:41.613222      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:42.613725      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:43.614084      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:44.614873      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:45.615912      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:46.616312      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:47.616363      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:48.617078      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:49.618233      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:50.618955      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:51.619624      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:52.620308      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:53.621091      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:54.621462      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:55.621752      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:56.622113      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:57.622597      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:58.622900      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:53:59.622946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:00.622991      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:01.624135      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:02.624233      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:03.625155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:04.625448      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:05.626632      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:06.627614      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:07.628205      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:08.629081      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:09.629018      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:10.629345      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:11.629489      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:12.629787      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:13.629915      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:14.630213      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:15.631394      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:16.631539      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:54:17.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-507" for this suite. @ 12/16/23 17:54:17.423
• [93.288 seconds]
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 12/16/23 17:54:17.44
  Dec 16 17:54:17.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename prestop @ 12/16/23 17:54:17.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:54:17.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:54:17.494
  STEP: Creating server pod server in namespace prestop-8700 @ 12/16/23 17:54:17.502
  STEP: Waiting for pods to come up. @ 12/16/23 17:54:17.523
  E1216 17:54:17.631943      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:18.632672      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-8700 @ 12/16/23 17:54:19.546
  E1216 17:54:19.633242      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:20.634137      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 12/16/23 17:54:21.575
  E1216 17:54:21.634826      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:22.635883      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:23.638068      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:24.636540      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:25.637013      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:54:26.597: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Dec 16 17:54:26.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 12/16/23 17:54:26.607
  STEP: Destroying namespace "prestop-8700" for this suite. @ 12/16/23 17:54:26.628
  E1216 17:54:26.637603      13 retrywatcher.go:130] "Watch failed" err="context canceled"
• [9.234 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 12/16/23 17:54:26.699
  Dec 16 17:54:26.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename services @ 12/16/23 17:54:26.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:54:26.757
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:54:26.763
  STEP: creating a collection of services @ 12/16/23 17:54:26.768
  Dec 16 17:54:26.769: INFO: Creating e2e-svc-a-wg296
  Dec 16 17:54:26.812: INFO: Creating e2e-svc-b-hzrgn
  Dec 16 17:54:26.867: INFO: Creating e2e-svc-c-zhll8
  STEP: deleting service collection @ 12/16/23 17:54:26.9
  Dec 16 17:54:27.006: INFO: Collection of services has been deleted
  Dec 16 17:54:27.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2713" for this suite. @ 12/16/23 17:54:27.016
• [0.334 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 12/16/23 17:54:27.035
  Dec 16 17:54:27.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename services @ 12/16/23 17:54:27.038
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:54:27.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:54:27.075
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-6544 @ 12/16/23 17:54:27.081
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 12/16/23 17:54:27.111
  STEP: creating service externalsvc in namespace services-6544 @ 12/16/23 17:54:27.111
  STEP: creating replication controller externalsvc in namespace services-6544 @ 12/16/23 17:54:27.149
  I1216 17:54:27.166121      13 runners.go:194] Created replication controller with name: externalsvc, namespace: services-6544, replica count: 2
  E1216 17:54:27.637857      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:28.638794      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:29.638528      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 17:54:30.223229      13 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 12/16/23 17:54:30.233
  Dec 16 17:54:30.273: INFO: Creating new exec pod
  E1216 17:54:30.639246      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:31.640087      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:54:32.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-6544 exec execpod9p9mx -- /bin/sh -x -c nslookup nodeport-service.services-6544.svc.cluster.local'
  E1216 17:54:32.640781      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:54:32.788: INFO: stderr: "+ nslookup nodeport-service.services-6544.svc.cluster.local\n"
  Dec 16 17:54:32.788: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-6544.svc.cluster.local\tcanonical name = externalsvc.services-6544.svc.cluster.local.\nName:\texternalsvc.services-6544.svc.cluster.local\nAddress: 10.233.31.159\n\n"
  Dec 16 17:54:32.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-6544, will wait for the garbage collector to delete the pods @ 12/16/23 17:54:32.802
  Dec 16 17:54:32.875: INFO: Deleting ReplicationController externalsvc took: 13.932071ms
  Dec 16 17:54:32.977: INFO: Terminating ReplicationController externalsvc pods took: 101.3349ms
  E1216 17:54:33.641196      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:34.641716      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:54:35.219: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-6544" for this suite. @ 12/16/23 17:54:35.247
• [8.224 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 12/16/23 17:54:35.265
  Dec 16 17:54:35.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename svcaccounts @ 12/16/23 17:54:35.27
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:54:35.315
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:54:35.322
  STEP: creating a ServiceAccount @ 12/16/23 17:54:35.328
  STEP: watching for the ServiceAccount to be added @ 12/16/23 17:54:35.347
  STEP: patching the ServiceAccount @ 12/16/23 17:54:35.355
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 12/16/23 17:54:35.369
  STEP: deleting the ServiceAccount @ 12/16/23 17:54:35.377
  Dec 16 17:54:35.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4231" for this suite. @ 12/16/23 17:54:35.417
• [0.164 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 12/16/23 17:54:35.441
  Dec 16 17:54:35.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 17:54:35.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:54:35.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:54:35.487
  STEP: Creating configMap with name cm-test-opt-del-795d3ba4-50dd-4d3f-a5db-97a16350720a @ 12/16/23 17:54:35.502
  STEP: Creating configMap with name cm-test-opt-upd-96c01e35-24d6-4394-b056-d8f24c2e8e46 @ 12/16/23 17:54:35.511
  STEP: Creating the pod @ 12/16/23 17:54:35.52
  E1216 17:54:35.641785      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:36.642135      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-795d3ba4-50dd-4d3f-a5db-97a16350720a @ 12/16/23 17:54:37.604
  STEP: Updating configmap cm-test-opt-upd-96c01e35-24d6-4394-b056-d8f24c2e8e46 @ 12/16/23 17:54:37.618
  STEP: Creating configMap with name cm-test-opt-create-945e661b-e246-4e70-804d-005b6a1a1ca0 @ 12/16/23 17:54:37.626
  STEP: waiting to observe update in volume @ 12/16/23 17:54:37.636
  E1216 17:54:37.642649      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:38.643595      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:39.643661      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:40.644409      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:41.645042      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:42.645775      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:43.646286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:44.646925      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:45.647605      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:46.648103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:47.648862      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:48.649033      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:49.649365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:50.649962      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:51.650146      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:52.650184      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:53.650765      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:54.651305      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:55.651374      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:56.651885      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:57.652074      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:58.652958      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:54:59.652861      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:00.652957      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:01.653114      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:02.653209      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:03.653944      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:04.654069      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:05.654396      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:06.655419      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:07.656155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:08.656285      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:09.657088      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:10.657331      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:11.658034      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:12.659147      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:13.659478      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:14.660401      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:15.661132      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:16.661334      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:17.661692      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:18.661729      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:19.661972      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:20.662136      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:21.663016      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:22.663122      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:23.663415      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:24.663464      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:25.663711      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:26.664519      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:27.664683      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:28.664847      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:29.665865      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:30.666790      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:31.667190      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:32.677144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:33.670130      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:34.670216      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:35.671136      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:36.689231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:37.673746      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:38.673968      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:39.674278      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:40.674612      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:41.674808      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:42.674948      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:43.675176      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:44.675336      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:45.675542      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:46.675864      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:47.676584      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:48.677429      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:49.677620      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:50.677821      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:51.678559      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:52.679658      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:53.679822      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:54.680207      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:55.680470      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:56.681155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:57.681436      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:58.681723      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:55:59.682535      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:00.683837      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:01.683823      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:56:02.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6575" for this suite. @ 12/16/23 17:56:02.549
• [87.122 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 12/16/23 17:56:02.565
  Dec 16 17:56:02.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename init-container @ 12/16/23 17:56:02.57
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:56:02.62
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:56:02.625
  STEP: creating the pod @ 12/16/23 17:56:02.63
  Dec 16 17:56:02.630: INFO: PodSpec: initContainers in spec.initContainers
  E1216 17:56:02.684227      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:03.684900      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:04.685741      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:05.686027      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:56:06.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-5673" for this suite. @ 12/16/23 17:56:06.521
• [3.970 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 12/16/23 17:56:06.537
  Dec 16 17:56:06.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename tables @ 12/16/23 17:56:06.539
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:56:06.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:56:06.581
  Dec 16 17:56:06.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-851" for this suite. @ 12/16/23 17:56:06.599
• [0.077 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 12/16/23 17:56:06.617
  Dec 16 17:56:06.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename gc @ 12/16/23 17:56:06.62
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:56:06.651
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:56:06.658
  STEP: create the rc @ 12/16/23 17:56:06.672
  W1216 17:56:06.682692      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E1216 17:56:06.686928      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:07.687304      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:08.687868      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:09.688217      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:10.688391      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:11.689110      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:12.689631      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 12/16/23 17:56:12.76
  STEP: wait for the rc to be deleted @ 12/16/23 17:56:13.02
  E1216 17:56:13.690397      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:56:14.064: INFO: 80 pods remaining
  Dec 16 17:56:14.066: INFO: 80 pods has nil DeletionTimestamp
  Dec 16 17:56:14.067: INFO: 
  E1216 17:56:14.699400      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:56:15.276: INFO: 72 pods remaining
  Dec 16 17:56:15.276: INFO: 71 pods has nil DeletionTimestamp
  Dec 16 17:56:15.276: INFO: 
  E1216 17:56:15.704148      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:56:16.056: INFO: 59 pods remaining
  Dec 16 17:56:16.056: INFO: 59 pods has nil DeletionTimestamp
  Dec 16 17:56:16.056: INFO: 
  E1216 17:56:16.696199      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:56:17.250: INFO: 45 pods remaining
  Dec 16 17:56:17.250: INFO: 42 pods has nil DeletionTimestamp
  Dec 16 17:56:17.250: INFO: 
  E1216 17:56:17.697008      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:56:18.113: INFO: 29 pods remaining
  Dec 16 17:56:18.117: INFO: 28 pods has nil DeletionTimestamp
  Dec 16 17:56:18.117: INFO: 
  E1216 17:56:18.697895      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:56:19.052: INFO: 18 pods remaining
  Dec 16 17:56:19.052: INFO: 18 pods has nil DeletionTimestamp
  Dec 16 17:56:19.052: INFO: 
  E1216 17:56:19.698187      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 12/16/23 17:56:20.029
  Dec 16 17:56:20.514: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Dec 16 17:56:20.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7021" for this suite. @ 12/16/23 17:56:20.551
• [13.981 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 12/16/23 17:56:20.601
  Dec 16 17:56:20.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename conformance-tests @ 12/16/23 17:56:20.608
  E1216 17:56:20.699205      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:56:20.906
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:56:20.918
  STEP: Getting node addresses @ 12/16/23 17:56:20.924
  Dec 16 17:56:20.924: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Dec 16 17:56:20.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-3435" for this suite. @ 12/16/23 17:56:20.957
• [0.412 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 12/16/23 17:56:21.014
  Dec 16 17:56:21.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename configmap @ 12/16/23 17:56:21.016
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:56:21.086
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:56:21.091
  STEP: Creating configMap with name cm-test-opt-del-038dcb53-34f9-4f80-a47c-697f2b9d0397 @ 12/16/23 17:56:21.114
  STEP: Creating configMap with name cm-test-opt-upd-25fb8bdd-de26-4558-b85d-5ee6f5043ff3 @ 12/16/23 17:56:21.13
  STEP: Creating the pod @ 12/16/23 17:56:21.14
  E1216 17:56:21.699696      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:22.700528      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:23.703907      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:24.704071      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:25.704231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:26.705155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:27.705284      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:28.706323      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-038dcb53-34f9-4f80-a47c-697f2b9d0397 @ 12/16/23 17:56:29.273
  STEP: Updating configmap cm-test-opt-upd-25fb8bdd-de26-4558-b85d-5ee6f5043ff3 @ 12/16/23 17:56:29.292
  STEP: Creating configMap with name cm-test-opt-create-02e8f823-8b09-4fc7-8037-3e5dd3844f02 @ 12/16/23 17:56:29.304
  STEP: waiting to observe update in volume @ 12/16/23 17:56:29.321
  E1216 17:56:29.706841      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:30.706749      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:31.707041      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:32.707280      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:33.707946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:34.711946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:35.710392      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:36.716818      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:37.710890      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:38.711155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:39.711587      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:40.711906      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:41.712961      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:42.712695      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:43.713550      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:44.713824      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:45.714489      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:46.715711      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:47.716978      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:48.717385      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:49.718199      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:50.717802      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:51.718219      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:52.719480      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:53.718511      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:54.719040      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:55.719860      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:56.720768      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:57.721290      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:58.721706      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:56:59.721696      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:00.722805      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:01.723623      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:02.724029      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:03.724354      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:04.724777      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:05.725612      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:06.726573      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:07.727032      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:08.727130      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:09.728441      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:10.728335      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:11.728854      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:12.728817      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:13.729185      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:14.729481      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:15.730352      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:16.730508      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:17.731472      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:18.732060      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:19.732939      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:20.733386      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:21.734489      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:22.734617      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:23.735174      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:24.735693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:25.735735      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:26.735842      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:27.736850      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:28.736706      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:29.744187      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:30.740806      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:31.741775      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:32.742002      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:33.742358      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:34.743260      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:35.745300      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:36.746161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:37.746329      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:38.747042      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:39.747287      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:40.747563      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:41.747871      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:42.748468      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:43.749361      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:44.749549      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:45.749747      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:46.750533      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:47.751003      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:48.751861      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:49.752715      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:50.753559      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:51.754375      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:52.755166      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:53.755560      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:54.755848      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:55.756002      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:56.757858      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:57.757853      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:57:58.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7700" for this suite. @ 12/16/23 17:57:58.341
• [97.343 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 12/16/23 17:57:58.359
  Dec 16 17:57:58.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename crd-watch @ 12/16/23 17:57:58.364
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:57:58.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:57:58.42
  Dec 16 17:57:58.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:57:58.757945      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:57:59.759046      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:00.759607      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 12/16/23 17:58:01.103
  Dec 16 17:58:01.120: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-12-16T17:58:01Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-12-16T17:58:01Z]] name:name1 resourceVersion:35298 uid:e564e449-c2c8-4265-9414-75efdaabe5c5] num:map[num1:9223372036854775807 num2:1000000]]}
  E1216 17:58:01.760550      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:02.760563      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:03.761662      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:04.761738      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:05.762089      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:06.763152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:07.763381      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:08.763552      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:09.771814      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:10.782938      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 12/16/23 17:58:11.122
  Dec 16 17:58:11.137: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-12-16T17:58:11Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-12-16T17:58:11Z]] name:name2 resourceVersion:35338 uid:86f9ac65-bf09-4a17-b0b5-34d7785952e9] num:map[num1:9223372036854775807 num2:1000000]]}
  E1216 17:58:11.770560      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:12.772192      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:13.772020      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:14.773628      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:15.772491      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:16.773267      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:17.773846      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:18.774417      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:19.774941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:20.775374      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 12/16/23 17:58:21.14
  Dec 16 17:58:21.160: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-12-16T17:58:01Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-12-16T17:58:21Z]] name:name1 resourceVersion:35356 uid:e564e449-c2c8-4265-9414-75efdaabe5c5] num:map[num1:9223372036854775807 num2:1000000]]}
  E1216 17:58:21.775557      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:22.776962      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:23.777155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:24.777368      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:25.777934      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:26.778550      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:27.779402      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:28.780270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:29.781592      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:30.781996      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 12/16/23 17:58:31.162
  Dec 16 17:58:31.182: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-12-16T17:58:11Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-12-16T17:58:31Z]] name:name2 resourceVersion:35375 uid:86f9ac65-bf09-4a17-b0b5-34d7785952e9] num:map[num1:9223372036854775807 num2:1000000]]}
  E1216 17:58:31.782746      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:32.783720      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:33.783973      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:34.784039      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:35.784929      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:36.785099      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:37.785648      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:38.786749      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:39.787243      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:40.787605      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 12/16/23 17:58:41.184
  Dec 16 17:58:41.215: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-12-16T17:58:01Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-12-16T17:58:21Z]] name:name1 resourceVersion:35393 uid:e564e449-c2c8-4265-9414-75efdaabe5c5] num:map[num1:9223372036854775807 num2:1000000]]}
  E1216 17:58:41.788253      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:42.788520      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:43.789547      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:44.790209      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:45.790467      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:46.790643      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:47.790945      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:48.791286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:49.791451      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:50.791692      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 12/16/23 17:58:51.215
  Dec 16 17:58:51.236: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-12-16T17:58:11Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-12-16T17:58:31Z]] name:name2 resourceVersion:35410 uid:86f9ac65-bf09-4a17-b0b5-34d7785952e9] num:map[num1:9223372036854775807 num2:1000000]]}
  E1216 17:58:51.793161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:52.793523      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:53.794236      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:54.794519      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:55.794623      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:56.794993      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:57.795364      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:58.795468      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:58:59.795991      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:00.796826      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:59:01.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-8494" for this suite. @ 12/16/23 17:59:01.771
• [63.431 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 12/16/23 17:59:01.795
  Dec 16 17:59:01.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  E1216 17:59:01.797125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Building a namespace api object, basename configmap @ 12/16/23 17:59:01.798
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:59:01.85
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:59:01.854
  STEP: Creating configMap with name configmap-test-volume-c61cae63-dabe-47e8-b961-5bf8225ce383 @ 12/16/23 17:59:01.859
  STEP: Creating a pod to test consume configMaps @ 12/16/23 17:59:01.871
  E1216 17:59:02.797521      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:03.797714      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:04.799022      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:05.799002      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:59:05.917
  Dec 16 17:59:05.930: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-configmaps-ae8251d6-8de9-4e7e-b30d-f150c3305158 container agnhost-container: <nil>
  STEP: delete the pod @ 12/16/23 17:59:05.947
  Dec 16 17:59:05.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8861" for this suite. @ 12/16/23 17:59:05.982
• [4.198 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 12/16/23 17:59:05.996
  Dec 16 17:59:05.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename downward-api @ 12/16/23 17:59:06
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:59:06.03
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:59:06.036
  STEP: Creating a pod to test downward API volume plugin @ 12/16/23 17:59:06.042
  E1216 17:59:06.800921      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:07.800783      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:08.801286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:09.801509      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 17:59:10.084
  Dec 16 17:59:10.090: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downwardapi-volume-85b143ac-b368-4503-9e31-1480313c96cd container client-container: <nil>
  STEP: delete the pod @ 12/16/23 17:59:10.106
  Dec 16 17:59:10.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4238" for this suite. @ 12/16/23 17:59:10.149
• [4.166 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 12/16/23 17:59:10.165
  Dec 16 17:59:10.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 12/16/23 17:59:10.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:59:10.206
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:59:10.211
  STEP: create the container to handle the HTTPGet hook request. @ 12/16/23 17:59:10.228
  E1216 17:59:10.801728      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:11.801756      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 12/16/23 17:59:12.289
  E1216 17:59:12.801897      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:13.802916      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:14.803483      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:15.804088      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:16.804362      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:17.805174      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:18.805802      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:19.806418      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:20.807144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:21.807456      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:22.807732      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:23.807933      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:24.808953      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:25.809131      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:26.809853      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:27.811053      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:28.811823      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:29.811991      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 12/16/23 17:59:30.405
  STEP: delete the pod with lifecycle hook @ 12/16/23 17:59:30.429
  E1216 17:59:30.812877      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:31.813747      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 17:59:32.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-6971" for this suite. @ 12/16/23 17:59:32.475
• [22.322 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 12/16/23 17:59:32.499
  Dec 16 17:59:32.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 17:59:32.502
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 17:59:32.536
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 17:59:32.541
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-ad81318d-0abc-4ef9-af51-002b18024d6d @ 12/16/23 17:59:32.564
  STEP: Creating the pod @ 12/16/23 17:59:32.575
  E1216 17:59:32.814842      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:33.814298      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-ad81318d-0abc-4ef9-af51-002b18024d6d @ 12/16/23 17:59:34.684
  STEP: waiting to observe update in volume @ 12/16/23 17:59:34.7
  E1216 17:59:34.814643      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:35.816221      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:36.816294      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:37.817087      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:38.818091      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:39.819397      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:40.818501      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:41.819517      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:42.820368      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:43.821039      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:44.821119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:45.822295      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:46.823197      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:47.823812      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:48.823893      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:49.824276      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:50.824421      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:51.824632      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:52.824655      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:53.825105      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:54.825046      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:55.826163      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:56.827046      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:57.827567      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:58.827876      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 17:59:59.828157      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:00.828332      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:01.828918      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:02.829608      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:03.830166      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:04.830322      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:05.831471      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:06.831894      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:07.832166      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:08.832988      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:09.833337      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:10.833720      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:11.834572      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:12.835735      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:13.835592      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:14.836011      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:15.836821      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:16.837179      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:17.837693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:18.837927      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:19.839126      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:20.839191      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:21.840042      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:22.840923      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:23.842071      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:24.842098      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:25.842246      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:26.843396      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:27.844642      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:28.844842      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:29.844881      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:30.845537      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:31.846163      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:32.846770      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:33.847746      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:34.848531      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:35.849697      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:36.849949      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:37.850108      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:38.850592      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:39.850673      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:40.852643      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:41.853409      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:42.853931      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:43.855101      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:44.855162      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:45.855152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:46.855762      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:00:47.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3816" for this suite. @ 12/16/23 18:00:47.486
• [75.005 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 12/16/23 18:00:47.512
  Dec 16 18:00:47.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename server-version @ 12/16/23 18:00:47.517
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 18:00:47.557
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 18:00:47.564
  STEP: Request ServerVersion @ 12/16/23 18:00:47.571
  STEP: Confirm major version @ 12/16/23 18:00:47.573
  Dec 16 18:00:47.574: INFO: Major version: 1
  STEP: Confirm minor version @ 12/16/23 18:00:47.574
  Dec 16 18:00:47.574: INFO: cleanMinorVersion: 27
  Dec 16 18:00:47.574: INFO: Minor version: 27
  Dec 16 18:00:47.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-2890" for this suite. @ 12/16/23 18:00:47.584
• [0.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 12/16/23 18:00:47.621
  Dec 16 18:00:47.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubectl-logs @ 12/16/23 18:00:47.623
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 18:00:47.662
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 18:00:47.673
  STEP: creating an pod @ 12/16/23 18:00:47.683
  Dec 16 18:00:47.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-logs-3630 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  E1216 18:00:47.856193      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:00:47.890: INFO: stderr: ""
  Dec 16 18:00:47.890: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 12/16/23 18:00:47.89
  Dec 16 18:00:47.890: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E1216 18:00:48.857137      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:49.857416      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:00:49.915: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 12/16/23 18:00:49.915
  Dec 16 18:00:49.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-logs-3630 logs logs-generator logs-generator'
  Dec 16 18:00:50.148: INFO: stderr: ""
  Dec 16 18:00:50.149: INFO: stdout: "I1216 18:00:48.684337       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/r9k 488\nI1216 18:00:48.884283       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/zkf5 238\nI1216 18:00:49.085055       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/mc5 222\nI1216 18:00:49.284355       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/h5vp 537\nI1216 18:00:49.484795       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/288j 386\nI1216 18:00:49.684261       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/nmw 240\nI1216 18:00:49.884782       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/bhf6 545\nI1216 18:00:50.084341       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/8z4f 487\n"
  STEP: limiting log lines @ 12/16/23 18:00:50.149
  Dec 16 18:00:50.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-logs-3630 logs logs-generator logs-generator --tail=1'
  Dec 16 18:00:50.361: INFO: stderr: ""
  Dec 16 18:00:50.361: INFO: stdout: "I1216 18:00:50.285239       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/7kd 553\n"
  Dec 16 18:00:50.361: INFO: got output "I1216 18:00:50.285239       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/7kd 553\n"
  STEP: limiting log bytes @ 12/16/23 18:00:50.361
  Dec 16 18:00:50.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-logs-3630 logs logs-generator logs-generator --limit-bytes=1'
  Dec 16 18:00:50.551: INFO: stderr: ""
  Dec 16 18:00:50.551: INFO: stdout: "I"
  Dec 16 18:00:50.551: INFO: got output "I"
  STEP: exposing timestamps @ 12/16/23 18:00:50.551
  Dec 16 18:00:50.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-logs-3630 logs logs-generator logs-generator --tail=1 --timestamps'
  Dec 16 18:00:50.731: INFO: stderr: ""
  Dec 16 18:00:50.731: INFO: stdout: "2023-12-16T18:00:50.684382429Z I1216 18:00:50.684315       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/2spb 481\n"
  Dec 16 18:00:50.731: INFO: got output "2023-12-16T18:00:50.684382429Z I1216 18:00:50.684315       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/2spb 481\n"
  STEP: restricting to a time range @ 12/16/23 18:00:50.732
  E1216 18:00:50.857554      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:51.858018      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:52.858554      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:00:53.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-logs-3630 logs logs-generator logs-generator --since=1s'
  Dec 16 18:00:53.432: INFO: stderr: ""
  Dec 16 18:00:53.432: INFO: stdout: "I1216 18:00:52.484936       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/4h6q 479\nI1216 18:00:52.684288       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/z5j 464\nI1216 18:00:52.884820       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/slg5 231\nI1216 18:00:53.084254       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/546q 536\nI1216 18:00:53.285656       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/default/pods/ngw 446\n"
  Dec 16 18:00:53.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-logs-3630 logs logs-generator logs-generator --since=24h'
  Dec 16 18:00:53.615: INFO: stderr: ""
  Dec 16 18:00:53.616: INFO: stdout: "I1216 18:00:48.684337       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/r9k 488\nI1216 18:00:48.884283       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/zkf5 238\nI1216 18:00:49.085055       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/mc5 222\nI1216 18:00:49.284355       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/h5vp 537\nI1216 18:00:49.484795       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/288j 386\nI1216 18:00:49.684261       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/nmw 240\nI1216 18:00:49.884782       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/bhf6 545\nI1216 18:00:50.084341       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/8z4f 487\nI1216 18:00:50.285239       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/7kd 553\nI1216 18:00:50.484563       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/4vrn 460\nI1216 18:00:50.684315       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/2spb 481\nI1216 18:00:50.884716       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/qknd 308\nI1216 18:00:51.084220       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/k4rp 545\nI1216 18:00:51.284869       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/5b89 234\nI1216 18:00:51.484280       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/7pm9 500\nI1216 18:00:51.684870       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/xs56 594\nI1216 18:00:51.884411       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/94dt 456\nI1216 18:00:52.084876       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/5ff 596\nI1216 18:00:52.284397       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/mjq 568\nI1216 18:00:52.484936       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/4h6q 479\nI1216 18:00:52.684288       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/z5j 464\nI1216 18:00:52.884820       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/slg5 231\nI1216 18:00:53.084254       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/546q 536\nI1216 18:00:53.285656       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/default/pods/ngw 446\nI1216 18:00:53.484019       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/4z4g 458\n"
  Dec 16 18:00:53.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-logs-3630 delete pod logs-generator'
  E1216 18:00:53.859050      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:00:54.455: INFO: stderr: ""
  Dec 16 18:00:54.455: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Dec 16 18:00:54.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-3630" for this suite. @ 12/16/23 18:00:54.466
• [6.862 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 12/16/23 18:00:54.484
  Dec 16 18:00:54.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename services @ 12/16/23 18:00:54.486
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 18:00:54.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 18:00:54.532
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-7728 @ 12/16/23 18:00:54.54
  STEP: changing the ExternalName service to type=ClusterIP @ 12/16/23 18:00:54.551
  STEP: creating replication controller externalname-service in namespace services-7728 @ 12/16/23 18:00:54.598
  I1216 18:00:54.626808      13 runners.go:194] Created replication controller with name: externalname-service, namespace: services-7728, replica count: 2
  E1216 18:00:54.859878      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:55.860601      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:56.861022      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I1216 18:00:57.679234      13 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Dec 16 18:00:57.679: INFO: Creating new exec pod
  E1216 18:00:57.861293      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:58.862296      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:00:59.863137      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:01:00.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-7728 exec execpodnr65r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  E1216 18:01:00.865343      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:01:01.070: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Dec 16 18:01:01.070: INFO: stdout: "externalname-service-wsf4t"
  Dec 16 18:01:01.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-7728 exec execpodnr65r -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.17.115 80'
  Dec 16 18:01:01.370: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.17.115 80\nConnection to 10.233.17.115 80 port [tcp/http] succeeded!\n"
  Dec 16 18:01:01.370: INFO: stdout: "externalname-service-wsf4t"
  Dec 16 18:01:01.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 16 18:01:01.382: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-7728" for this suite. @ 12/16/23 18:01:01.42
• [6.965 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 12/16/23 18:01:01.465
  Dec 16 18:01:01.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename services @ 12/16/23 18:01:01.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 18:01:01.513
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 18:01:01.521
  STEP: creating service endpoint-test2 in namespace services-1668 @ 12/16/23 18:01:01.53
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1668 to expose endpoints map[] @ 12/16/23 18:01:01.558
  Dec 16 18:01:01.564: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  E1216 18:01:01.864496      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:01:02.586: INFO: successfully validated that service endpoint-test2 in namespace services-1668 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-1668 @ 12/16/23 18:01:02.586
  E1216 18:01:02.865533      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:03.866997      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1668 to expose endpoints map[pod1:[80]] @ 12/16/23 18:01:04.662
  Dec 16 18:01:04.694: INFO: successfully validated that service endpoint-test2 in namespace services-1668 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 12/16/23 18:01:04.695
  Dec 16 18:01:04.695: INFO: Creating new exec pod
  E1216 18:01:04.866772      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:05.868689      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:06.871363      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:01:07.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-1668 exec execpodx56bv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E1216 18:01:07.868886      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:01:08.177: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Dec 16 18:01:08.177: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 16 18:01:08.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-1668 exec execpodx56bv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.27.169 80'
  Dec 16 18:01:08.507: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.27.169 80\nConnection to 10.233.27.169 80 port [tcp/http] succeeded!\n"
  Dec 16 18:01:08.507: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-1668 @ 12/16/23 18:01:08.507
  E1216 18:01:08.869258      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:09.869495      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1668 to expose endpoints map[pod1:[80] pod2:[80]] @ 12/16/23 18:01:10.548
  Dec 16 18:01:10.608: INFO: successfully validated that service endpoint-test2 in namespace services-1668 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 12/16/23 18:01:10.608
  E1216 18:01:10.871528      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:01:11.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-1668 exec execpodx56bv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E1216 18:01:11.872271      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:01:11.939: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Dec 16 18:01:11.940: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 16 18:01:11.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-1668 exec execpodx56bv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.27.169 80'
  Dec 16 18:01:12.246: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.27.169 80\nConnection to 10.233.27.169 80 port [tcp/http] succeeded!\n"
  Dec 16 18:01:12.246: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-1668 @ 12/16/23 18:01:12.246
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1668 to expose endpoints map[pod2:[80]] @ 12/16/23 18:01:12.28
  Dec 16 18:01:12.352: INFO: successfully validated that service endpoint-test2 in namespace services-1668 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 12/16/23 18:01:12.353
  E1216 18:01:12.872959      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:01:13.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-1668 exec execpodx56bv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Dec 16 18:01:13.668: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Dec 16 18:01:13.668: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 16 18:01:13.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=services-1668 exec execpodx56bv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.27.169 80'
  E1216 18:01:13.873841      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:01:14.034: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.27.169 80\nConnection to 10.233.27.169 80 port [tcp/http] succeeded!\n"
  Dec 16 18:01:14.034: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-1668 @ 12/16/23 18:01:14.034
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1668 to expose endpoints map[] @ 12/16/23 18:01:14.09
  E1216 18:01:14.874020      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:01:15.193: INFO: successfully validated that service endpoint-test2 in namespace services-1668 exposes endpoints map[]
  Dec 16 18:01:15.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1668" for this suite. @ 12/16/23 18:01:15.257
• [13.810 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 12/16/23 18:01:15.28
  Dec 16 18:01:15.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename kubectl @ 12/16/23 18:01:15.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 18:01:15.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 18:01:15.33
  STEP: validating api versions @ 12/16/23 18:01:15.336
  Dec 16 18:01:15.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3827340843 --namespace=kubectl-8158 api-versions'
  Dec 16 18:01:15.518: INFO: stderr: ""
  Dec 16 18:01:15.518: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Dec 16 18:01:15.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8158" for this suite. @ 12/16/23 18:01:15.53
• [0.264 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 12/16/23 18:01:15.546
  Dec 16 18:01:15.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename downward-api @ 12/16/23 18:01:15.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 18:01:15.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 18:01:15.589
  STEP: Creating the pod @ 12/16/23 18:01:15.597
  E1216 18:01:15.875221      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:16.875459      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:17.876182      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:01:18.183: INFO: Successfully updated pod "annotationupdate53d62248-ce0e-4b62-9fce-27134f00f0ea"
  E1216 18:01:18.876481      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:19.876911      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:20.877208      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:21.878260      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:01:22.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5207" for this suite. @ 12/16/23 18:01:22.252
• [6.723 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 12/16/23 18:01:22.272
  Dec 16 18:01:22.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename podtemplate @ 12/16/23 18:01:22.274
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 18:01:22.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 18:01:22.33
  STEP: Create a pod template @ 12/16/23 18:01:22.338
  STEP: Replace a pod template @ 12/16/23 18:01:22.349
  Dec 16 18:01:22.367: INFO: Found updated podtemplate annotation: "true"

  Dec 16 18:01:22.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-1255" for this suite. @ 12/16/23 18:01:22.383
• [0.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 12/16/23 18:01:22.41
  Dec 16 18:01:22.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 18:01:22.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 18:01:22.448
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 18:01:22.455
  STEP: Creating projection with secret that has name projected-secret-test-14943f46-c5fd-4bf0-b85e-e9f93008c40b @ 12/16/23 18:01:22.461
  STEP: Creating a pod to test consume secrets @ 12/16/23 18:01:22.474
  E1216 18:01:22.879007      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:23.879225      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 18:01:24.513
  Dec 16 18:01:24.521: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-projected-secrets-388a07a0-9743-4ae3-8e5b-46509c41a849 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 12/16/23 18:01:24.534
  Dec 16 18:01:24.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1276" for this suite. @ 12/16/23 18:01:24.571
• [2.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 12/16/23 18:01:24.586
  Dec 16 18:01:24.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename job @ 12/16/23 18:01:24.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 18:01:24.652
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 18:01:24.657
  STEP: Creating a job @ 12/16/23 18:01:24.662
  STEP: Ensuring job reaches completions @ 12/16/23 18:01:24.674
  E1216 18:01:24.880151      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:25.880449      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:26.880883      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:27.881552      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:28.882520      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:29.882967      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:30.883905      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:31.887081      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:32.885969      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:33.887737      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:01:34.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-114" for this suite. @ 12/16/23 18:01:34.703
• [10.131 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 12/16/23 18:01:34.718
  Dec 16 18:01:34.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename subpath @ 12/16/23 18:01:34.724
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 18:01:34.757
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 18:01:34.765
  STEP: Setting up data @ 12/16/23 18:01:34.773
  STEP: Creating pod pod-subpath-test-secret-4grq @ 12/16/23 18:01:34.8
  STEP: Creating a pod to test atomic-volume-subpath @ 12/16/23 18:01:34.8
  E1216 18:01:34.886173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:35.886596      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:36.887144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:37.888327      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:38.887947      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:39.887998      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:40.890295      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:41.890208      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:42.891281      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:43.892035      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:44.892685      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:45.892899      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:46.893554      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:47.894401      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:48.894957      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:49.895665      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:50.895792      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:51.896476      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:52.897200      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:53.897560      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:54.897853      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:55.898269      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:56.898939      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 18:01:56.968
  Dec 16 18:01:56.979: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-subpath-test-secret-4grq container test-container-subpath-secret-4grq: <nil>
  STEP: delete the pod @ 12/16/23 18:01:57.006
  STEP: Deleting pod pod-subpath-test-secret-4grq @ 12/16/23 18:01:57.042
  Dec 16 18:01:57.042: INFO: Deleting pod "pod-subpath-test-secret-4grq" in namespace "subpath-1776"
  Dec 16 18:01:57.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1776" for this suite. @ 12/16/23 18:01:57.066
• [22.365 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 12/16/23 18:01:57.085
  Dec 16 18:01:57.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename downward-api @ 12/16/23 18:01:57.089
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 18:01:57.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 18:01:57.139
  STEP: Creating a pod to test downward API volume plugin @ 12/16/23 18:01:57.147
  E1216 18:01:57.899461      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:58.899805      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:01:59.899899      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:00.900176      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 18:02:01.205
  Dec 16 18:02:01.214: INFO: Trying to get logs from node phoh7xai9ouk-3 pod downwardapi-volume-ead39f2b-c6af-49b5-bdc0-3e43b3750a86 container client-container: <nil>
  STEP: delete the pod @ 12/16/23 18:02:01.231
  Dec 16 18:02:01.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3703" for this suite. @ 12/16/23 18:02:01.275
• [4.208 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 12/16/23 18:02:01.297
  Dec 16 18:02:01.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename csistoragecapacity @ 12/16/23 18:02:01.3
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 18:02:01.334
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 18:02:01.339
  STEP: getting /apis @ 12/16/23 18:02:01.347
  STEP: getting /apis/storage.k8s.io @ 12/16/23 18:02:01.359
  STEP: getting /apis/storage.k8s.io/v1 @ 12/16/23 18:02:01.362
  STEP: creating @ 12/16/23 18:02:01.365
  STEP: watching @ 12/16/23 18:02:01.409
  Dec 16 18:02:01.409: INFO: starting watch
  STEP: getting @ 12/16/23 18:02:01.429
  STEP: listing in namespace @ 12/16/23 18:02:01.436
  STEP: listing across namespaces @ 12/16/23 18:02:01.444
  STEP: patching @ 12/16/23 18:02:01.451
  STEP: updating @ 12/16/23 18:02:01.468
  Dec 16 18:02:01.480: INFO: waiting for watch events with expected annotations in namespace
  Dec 16 18:02:01.481: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 12/16/23 18:02:01.483
  STEP: deleting a collection @ 12/16/23 18:02:01.515
  Dec 16 18:02:01.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-2535" for this suite. @ 12/16/23 18:02:01.56
• [0.277 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 12/16/23 18:02:01.576
  Dec 16 18:02:01.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename sysctl @ 12/16/23 18:02:01.578
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 18:02:01.614
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 18:02:01.621
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 12/16/23 18:02:01.627
  STEP: Watching for error events or started pod @ 12/16/23 18:02:01.645
  E1216 18:02:01.900125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:02.901232      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 12/16/23 18:02:03.653
  E1216 18:02:03.901125      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:04.901129      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 12/16/23 18:02:05.672
  STEP: Getting logs from the pod @ 12/16/23 18:02:05.673
  STEP: Checking that the sysctl is actually updated @ 12/16/23 18:02:05.704
  Dec 16 18:02:05.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-6385" for this suite. @ 12/16/23 18:02:05.713
• [4.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 12/16/23 18:02:05.734
  Dec 16 18:02:05.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename csiinlinevolumes @ 12/16/23 18:02:05.737
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 18:02:05.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 18:02:05.778
  STEP: creating @ 12/16/23 18:02:05.784
  STEP: getting @ 12/16/23 18:02:05.83
  STEP: listing in namespace @ 12/16/23 18:02:05.839
  STEP: patching @ 12/16/23 18:02:05.853
  STEP: deleting @ 12/16/23 18:02:05.87
  Dec 16 18:02:05.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E1216 18:02:05.904388      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "csiinlinevolumes-650" for this suite. @ 12/16/23 18:02:05.912
• [0.195 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:1028
  STEP: Creating a kubernetes client @ 12/16/23 18:02:05.931
  Dec 16 18:02:05.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename statefulset @ 12/16/23 18:02:05.934
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 18:02:05.964
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 18:02:05.972
  STEP: Creating service test in namespace statefulset-4350 @ 12/16/23 18:02:05.978
  STEP: Creating statefulset ss in namespace statefulset-4350 @ 12/16/23 18:02:05.992
  Dec 16 18:02:06.023: INFO: Found 0 stateful pods, waiting for 1
  E1216 18:02:06.901720      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:07.903425      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:08.902307      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:09.907666      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:10.906601      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:11.908207      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:12.909202      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:13.908019      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:14.913536      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:15.924993      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:02:16.051: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 12/16/23 18:02:16.077
  STEP: Getting /status @ 12/16/23 18:02:16.109
  Dec 16 18:02:16.126: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 12/16/23 18:02:16.126
  Dec 16 18:02:16.155: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 12/16/23 18:02:16.155
  Dec 16 18:02:16.166: INFO: Observed &StatefulSet event: ADDED
  Dec 16 18:02:16.166: INFO: Found Statefulset ss in namespace statefulset-4350 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Dec 16 18:02:16.167: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 12/16/23 18:02:16.168
  Dec 16 18:02:16.168: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Dec 16 18:02:16.183: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 12/16/23 18:02:16.183
  Dec 16 18:02:16.189: INFO: Observed &StatefulSet event: ADDED
  Dec 16 18:02:16.189: INFO: Deleting all statefulset in ns statefulset-4350
  Dec 16 18:02:16.200: INFO: Scaling statefulset ss to 0
  E1216 18:02:16.917288      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:17.917952      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:18.918561      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:19.918664      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:20.918825      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:21.918916      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:22.920007      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:23.920721      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:24.921382      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:25.922391      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 16 18:02:26.243: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 16 18:02:26.267: INFO: Deleting statefulset ss
  Dec 16 18:02:26.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4350" for this suite. @ 12/16/23 18:02:26.321
• [20.409 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 12/16/23 18:02:26.343
  Dec 16 18:02:26.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3827340843
  STEP: Building a namespace api object, basename projected @ 12/16/23 18:02:26.346
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/16/23 18:02:26.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/16/23 18:02:26.381
  STEP: Creating configMap with name projected-configmap-test-volume-4ce2de50-825e-452d-800f-8d3581d2bfc4 @ 12/16/23 18:02:26.386
  STEP: Creating a pod to test consume configMaps @ 12/16/23 18:02:26.396
  E1216 18:02:26.923107      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:27.924516      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:28.925706      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1216 18:02:29.925552      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/16/23 18:02:30.441
  Dec 16 18:02:30.447: INFO: Trying to get logs from node phoh7xai9ouk-3 pod pod-projected-configmaps-e3beefc5-f579-4077-97ba-db065003a36c container agnhost-container: <nil>
  STEP: delete the pod @ 12/16/23 18:02:30.461
  Dec 16 18:02:30.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2494" for this suite. @ 12/16/23 18:02:30.511
• [4.187 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Dec 16 18:02:30.544: INFO: Running AfterSuite actions on node 1
  Dec 16 18:02:30.545: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.001 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.153 seconds]
------------------------------

Ran 378 of 7209 Specs in 6537.299 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6831 Skipped
PASS

Ginkgo ran 1 suite in 1h48m58.35437461s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

