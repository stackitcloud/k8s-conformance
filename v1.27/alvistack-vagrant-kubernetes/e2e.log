  I0415 08:22:17.220219      14 e2e.go:117] Starting e2e run "b296a868-5d29-45ef-ab0f-9de0eeb55529" on Ginkgo node 1
  Apr 15 08:22:17.302: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1713169336 - will randomize all specs

Will run 378 of 7209 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Apr 15 08:22:17.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 08:22:17.892: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Apr 15 08:22:18.007: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Apr 15 08:22:18.022: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
  Apr 15 08:22:18.022: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  Apr 15 08:22:18.023: INFO: e2e test version: v1.27.12
  Apr 15 08:22:18.025: INFO: kube-apiserver version: v1.27.12
  Apr 15 08:22:18.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 08:22:18.038: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.155 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 04/15/24 08:22:18.64
  Apr 15 08:22:18.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename proxy @ 04/15/24 08:22:18.642
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:22:18.673
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:22:18.678
  Apr 15 08:22:18.685: INFO: Creating pod...
  Apr 15 08:22:20.722: INFO: Creating service...
  Apr 15 08:22:20.770: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1059/pods/agnhost/proxy/some/path/with/DELETE
  Apr 15 08:22:20.798: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 15 08:22:20.798: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1059/pods/agnhost/proxy/some/path/with/GET
  Apr 15 08:22:20.808: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Apr 15 08:22:20.808: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1059/pods/agnhost/proxy/some/path/with/HEAD
  Apr 15 08:22:20.816: INFO: http.Client request:HEAD | StatusCode:200
  Apr 15 08:22:20.817: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1059/pods/agnhost/proxy/some/path/with/OPTIONS
  Apr 15 08:22:20.828: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 15 08:22:20.828: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1059/pods/agnhost/proxy/some/path/with/PATCH
  Apr 15 08:22:20.837: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 15 08:22:20.837: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1059/pods/agnhost/proxy/some/path/with/POST
  Apr 15 08:22:20.846: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 15 08:22:20.846: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1059/pods/agnhost/proxy/some/path/with/PUT
  Apr 15 08:22:20.862: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 15 08:22:20.862: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1059/services/test-service/proxy/some/path/with/DELETE
  Apr 15 08:22:20.877: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 15 08:22:20.877: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1059/services/test-service/proxy/some/path/with/GET
  Apr 15 08:22:20.896: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Apr 15 08:22:20.897: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1059/services/test-service/proxy/some/path/with/HEAD
  Apr 15 08:22:20.910: INFO: http.Client request:HEAD | StatusCode:200
  Apr 15 08:22:20.910: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1059/services/test-service/proxy/some/path/with/OPTIONS
  Apr 15 08:22:20.935: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 15 08:22:20.936: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1059/services/test-service/proxy/some/path/with/PATCH
  Apr 15 08:22:20.957: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 15 08:22:20.957: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1059/services/test-service/proxy/some/path/with/POST
  Apr 15 08:22:20.977: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 15 08:22:20.978: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-1059/services/test-service/proxy/some/path/with/PUT
  Apr 15 08:22:20.992: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 15 08:22:20.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-1059" for this suite. @ 04/15/24 08:22:21.006
• [2.399 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 04/15/24 08:22:21.049
  Apr 15 08:22:21.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 08:22:21.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:22:21.102
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:22:21.109
  STEP: Creating the pod @ 04/15/24 08:22:21.117
  Apr 15 08:22:23.752: INFO: Successfully updated pod "annotationupdateb60a4020-b768-4677-a131-95e414e1e1f1"
  Apr 15 08:22:25.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4071" for this suite. @ 04/15/24 08:22:25.803
• [4.770 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 04/15/24 08:22:25.821
  Apr 15 08:22:25.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename tables @ 04/15/24 08:22:25.824
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:22:25.912
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:22:25.92
  Apr 15 08:22:25.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-1009" for this suite. @ 04/15/24 08:22:25.955
• [0.152 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 04/15/24 08:22:25.983
  Apr 15 08:22:25.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 08:22:25.986
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:22:26.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:22:26.033
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 04/15/24 08:22:26.042
  STEP: Saw pod success @ 04/15/24 08:22:30.097
  Apr 15 08:22:30.107: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-ffd6a273-2aa5-44c1-a591-2dc6c5732ef6 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 08:22:30.124
  Apr 15 08:22:30.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1134" for this suite. @ 04/15/24 08:22:30.175
• [4.207 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 04/15/24 08:22:30.196
  Apr 15 08:22:30.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename podtemplate @ 04/15/24 08:22:30.198
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:22:30.244
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:22:30.251
  STEP: Create set of pod templates @ 04/15/24 08:22:30.257
  Apr 15 08:22:30.269: INFO: created test-podtemplate-1
  Apr 15 08:22:30.279: INFO: created test-podtemplate-2
  Apr 15 08:22:30.291: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 04/15/24 08:22:30.291
  STEP: delete collection of pod templates @ 04/15/24 08:22:30.298
  Apr 15 08:22:30.298: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 04/15/24 08:22:30.331
  Apr 15 08:22:30.331: INFO: requesting list of pod templates to confirm quantity
  Apr 15 08:22:30.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-2283" for this suite. @ 04/15/24 08:22:30.35
• [0.169 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 04/15/24 08:22:30.37
  Apr 15 08:22:30.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-runtime @ 04/15/24 08:22:30.372
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:22:30.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:22:30.423
  STEP: create the container @ 04/15/24 08:22:30.431
  W0415 08:22:30.450393      14 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/15/24 08:22:30.451
  STEP: get the container status @ 04/15/24 08:22:33.499
  STEP: the container should be terminated @ 04/15/24 08:22:33.506
  STEP: the termination message should be set @ 04/15/24 08:22:33.506
  Apr 15 08:22:33.506: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 04/15/24 08:22:33.506
  Apr 15 08:22:33.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5478" for this suite. @ 04/15/24 08:22:33.548
• [3.191 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 04/15/24 08:22:33.563
  Apr 15 08:22:33.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 08:22:33.566
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:22:33.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:22:33.608
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 08:22:33.614
  STEP: Saw pod success @ 04/15/24 08:22:37.673
  Apr 15 08:22:37.682: INFO: Trying to get logs from node ahz3daisheng-3 pod downwardapi-volume-70053c33-98d5-4c9a-9cef-8a1cadf4a519 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 08:22:37.693
  Apr 15 08:22:37.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5533" for this suite. @ 04/15/24 08:22:37.739
• [4.188 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 04/15/24 08:22:37.752
  Apr 15 08:22:37.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename csiinlinevolumes @ 04/15/24 08:22:37.754
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:22:37.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:22:37.79
  STEP: creating @ 04/15/24 08:22:37.796
  STEP: getting @ 04/15/24 08:22:37.834
  STEP: listing in namespace @ 04/15/24 08:22:37.842
  STEP: patching @ 04/15/24 08:22:37.857
  STEP: deleting @ 04/15/24 08:22:37.881
  Apr 15 08:22:37.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-6549" for this suite. @ 04/15/24 08:22:37.913
• [0.174 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 04/15/24 08:22:37.928
  Apr 15 08:22:37.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename pods @ 04/15/24 08:22:37.93
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:22:37.965
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:22:37.972
  STEP: creating the pod @ 04/15/24 08:22:37.978
  STEP: submitting the pod to kubernetes @ 04/15/24 08:22:37.978
  W0415 08:22:38.005833      14 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: verifying the pod is in kubernetes @ 04/15/24 08:22:40.042
  STEP: updating the pod @ 04/15/24 08:22:40.051
  Apr 15 08:22:40.582: INFO: Successfully updated pod "pod-update-activedeadlineseconds-37cec85f-5638-48b2-a076-812aafe4a393"
  Apr 15 08:22:46.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3182" for this suite. @ 04/15/24 08:22:46.644
• [8.737 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 04/15/24 08:22:46.666
  Apr 15 08:22:46.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 08:22:46.67
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:22:46.715
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:22:46.721
  STEP: Discovering how many secrets are in namespace by default @ 04/15/24 08:22:46.727
  STEP: Counting existing ResourceQuota @ 04/15/24 08:22:51.738
  STEP: Creating a ResourceQuota @ 04/15/24 08:22:56.748
  STEP: Ensuring resource quota status is calculated @ 04/15/24 08:22:56.763
  STEP: Creating a Secret @ 04/15/24 08:22:58.778
  STEP: Ensuring resource quota status captures secret creation @ 04/15/24 08:22:58.851
  STEP: Deleting a secret @ 04/15/24 08:23:00.862
  STEP: Ensuring resource quota status released usage @ 04/15/24 08:23:00.876
  Apr 15 08:23:02.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6233" for this suite. @ 04/15/24 08:23:02.9
• [16.250 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 04/15/24 08:23:02.923
  Apr 15 08:23:02.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename limitrange @ 04/15/24 08:23:02.925
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:23:02.962
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:23:02.968
  STEP: Creating a LimitRange @ 04/15/24 08:23:02.974
  STEP: Setting up watch @ 04/15/24 08:23:02.975
  STEP: Submitting a LimitRange @ 04/15/24 08:23:03.084
  STEP: Verifying LimitRange creation was observed @ 04/15/24 08:23:03.096
  STEP: Fetching the LimitRange to ensure it has proper values @ 04/15/24 08:23:03.096
  Apr 15 08:23:03.105: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Apr 15 08:23:03.106: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 04/15/24 08:23:03.106
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 04/15/24 08:23:03.117
  Apr 15 08:23:03.129: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Apr 15 08:23:03.129: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 04/15/24 08:23:03.129
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 04/15/24 08:23:03.144
  Apr 15 08:23:03.151: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Apr 15 08:23:03.151: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 04/15/24 08:23:03.152
  STEP: Failing to create a Pod with more than max resources @ 04/15/24 08:23:03.158
  STEP: Updating a LimitRange @ 04/15/24 08:23:03.163
  STEP: Verifying LimitRange updating is effective @ 04/15/24 08:23:03.178
  STEP: Creating a Pod with less than former min resources @ 04/15/24 08:23:05.198
  STEP: Failing to create a Pod with more than max resources @ 04/15/24 08:23:05.218
  STEP: Deleting a LimitRange @ 04/15/24 08:23:05.23
  STEP: Verifying the LimitRange was deleted @ 04/15/24 08:23:05.272
  Apr 15 08:23:10.281: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 04/15/24 08:23:10.281
  Apr 15 08:23:10.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-9918" for this suite. @ 04/15/24 08:23:10.323
• [7.414 seconds]
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 04/15/24 08:23:10.338
  Apr 15 08:23:10.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename subpath @ 04/15/24 08:23:10.342
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:23:10.39
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:23:10.397
  STEP: Setting up data @ 04/15/24 08:23:10.405
  STEP: Creating pod pod-subpath-test-configmap-mrp5 @ 04/15/24 08:23:10.428
  STEP: Creating a pod to test atomic-volume-subpath @ 04/15/24 08:23:10.429
  STEP: Saw pod success @ 04/15/24 08:23:34.581
  Apr 15 08:23:34.592: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-subpath-test-configmap-mrp5 container test-container-subpath-configmap-mrp5: <nil>
  STEP: delete the pod @ 04/15/24 08:23:34.619
  STEP: Deleting pod pod-subpath-test-configmap-mrp5 @ 04/15/24 08:23:34.651
  Apr 15 08:23:34.651: INFO: Deleting pod "pod-subpath-test-configmap-mrp5" in namespace "subpath-903"
  Apr 15 08:23:34.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-903" for this suite. @ 04/15/24 08:23:34.678
• [24.356 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 04/15/24 08:23:34.704
  Apr 15 08:23:34.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename webhook @ 04/15/24 08:23:34.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:23:34.752
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:23:34.759
  STEP: Setting up server cert @ 04/15/24 08:23:34.826
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 08:23:36.249
  STEP: Deploying the webhook pod @ 04/15/24 08:23:36.265
  STEP: Wait for the deployment to be ready @ 04/15/24 08:23:36.291
  Apr 15 08:23:36.321: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/15/24 08:23:38.352
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 08:23:38.383
  Apr 15 08:23:39.384: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 04/15/24 08:23:39.53
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/15/24 08:23:39.622
  STEP: Deleting the collection of validation webhooks @ 04/15/24 08:23:39.704
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/15/24 08:23:39.912
  Apr 15 08:23:39.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6277" for this suite. @ 04/15/24 08:23:40.074
  STEP: Destroying namespace "webhook-markers-2757" for this suite. @ 04/15/24 08:23:40.096
• [5.421 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 04/15/24 08:23:40.131
  Apr 15 08:23:40.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename replicaset @ 04/15/24 08:23:40.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:23:40.179
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:23:40.185
  STEP: Create a Replicaset @ 04/15/24 08:23:40.2
  STEP: Verify that the required pods have come up. @ 04/15/24 08:23:40.218
  Apr 15 08:23:40.224: INFO: Pod name sample-pod: Found 0 pods out of 1
  Apr 15 08:23:45.235: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/15/24 08:23:45.235
  STEP: Getting /status @ 04/15/24 08:23:45.235
  Apr 15 08:23:45.248: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 04/15/24 08:23:45.249
  Apr 15 08:23:45.275: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 04/15/24 08:23:45.275
  Apr 15 08:23:45.283: INFO: Observed &ReplicaSet event: ADDED
  Apr 15 08:23:45.283: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 15 08:23:45.283: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 15 08:23:45.284: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 15 08:23:45.284: INFO: Found replicaset test-rs in namespace replicaset-2096 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 15 08:23:45.284: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 04/15/24 08:23:45.284
  Apr 15 08:23:45.284: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 15 08:23:45.303: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 04/15/24 08:23:45.303
  Apr 15 08:23:45.310: INFO: Observed &ReplicaSet event: ADDED
  Apr 15 08:23:45.311: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 15 08:23:45.312: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 15 08:23:45.312: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 15 08:23:45.312: INFO: Observed replicaset test-rs in namespace replicaset-2096 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 15 08:23:45.313: INFO: Observed &ReplicaSet event: MODIFIED
  Apr 15 08:23:45.313: INFO: Found replicaset test-rs in namespace replicaset-2096 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Apr 15 08:23:45.313: INFO: Replicaset test-rs has a patched status
  Apr 15 08:23:45.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2096" for this suite. @ 04/15/24 08:23:45.33
• [5.217 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 04/15/24 08:23:45.352
  Apr 15 08:23:45.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename pods @ 04/15/24 08:23:45.354
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:23:45.407
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:23:45.412
  STEP: Saw pod success @ 04/15/24 08:23:51.55
  Apr 15 08:23:51.560: INFO: Trying to get logs from node ahz3daisheng-2 pod client-envvars-b2391ac9-0b7e-41fe-bf02-2de2f1dd71b4 container env3cont: <nil>
  STEP: delete the pod @ 04/15/24 08:23:51.606
  Apr 15 08:23:51.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5988" for this suite. @ 04/15/24 08:23:51.66
• [6.326 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 04/15/24 08:23:51.681
  Apr 15 08:23:51.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 08:23:51.685
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:23:51.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:23:51.752
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 08:23:51.778
  STEP: Saw pod success @ 04/15/24 08:23:55.842
  Apr 15 08:23:55.850: INFO: Trying to get logs from node ahz3daisheng-3 pod downwardapi-volume-68fd99ba-fb7b-4cb2-ae9c-de1072ce0f3c container client-container: <nil>
  STEP: delete the pod @ 04/15/24 08:23:55.866
  Apr 15 08:23:55.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5160" for this suite. @ 04/15/24 08:23:55.915
• [4.249 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 04/15/24 08:23:55.935
  Apr 15 08:23:55.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename deployment @ 04/15/24 08:23:55.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:23:55.973
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:23:55.98
  Apr 15 08:23:55.988: INFO: Creating deployment "webserver-deployment"
  Apr 15 08:23:56.001: INFO: Waiting for observed generation 1
  Apr 15 08:23:58.044: INFO: Waiting for all required pods to come up
  Apr 15 08:23:58.073: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 04/15/24 08:23:58.074
  Apr 15 08:24:00.142: INFO: Waiting for deployment "webserver-deployment" to complete
  Apr 15 08:24:00.157: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Apr 15 08:24:00.187: INFO: Updating deployment webserver-deployment
  Apr 15 08:24:00.187: INFO: Waiting for observed generation 2
  Apr 15 08:24:02.213: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Apr 15 08:24:02.222: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Apr 15 08:24:02.230: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Apr 15 08:24:02.250: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Apr 15 08:24:02.250: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Apr 15 08:24:02.258: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Apr 15 08:24:02.273: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Apr 15 08:24:02.273: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Apr 15 08:24:02.301: INFO: Updating deployment webserver-deployment
  Apr 15 08:24:02.302: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Apr 15 08:24:02.331: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Apr 15 08:24:02.352: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Apr 15 08:24:02.398: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-7230  0782fc1e-132d-40bf-900c-7079d02d6eac 182445 3 2024-04-15 08:23:55 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-04-15 08:24:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 08:24:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cb36a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2024-04-15 08:24:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:56 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-04-15 08:24:02 +0000 UTC,LastTransitionTime:2024-04-15 08:24:02 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Apr 15 08:24:02.435: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-7230  658675f0-4b46-448d-9444-3927c4331cfa 182440 3 2024-04-15 08:24:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 0782fc1e-132d-40bf-900c-7079d02d6eac 0xc003bcbfa7 0xc003bcbfa8}] [] [{kube-controller-manager Update apps/v1 2024-04-15 08:24:00 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2024-04-15 08:24:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0782fc1e-132d-40bf-900c-7079d02d6eac\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d42048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 08:24:02.435: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Apr 15 08:24:02.435: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-7230  6a2a2398-4405-48b6-bd55-bf1a65d7e278 182437 3 2024-04-15 08:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 0782fc1e-132d-40bf-900c-7079d02d6eac 0xc003bcbeb7 0xc003bcbeb8}] [] [{kube-controller-manager Update apps/v1 2024-04-15 08:24:00 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2024-04-15 08:24:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0782fc1e-132d-40bf-900c-7079d02d6eac\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bcbf48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 08:24:02.479: INFO: Pod "webserver-deployment-67bd4bf6dc-72546" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-72546 webserver-deployment-67bd4bf6dc- deployment-7230  d92cc2d5-4b03-4871-9638-e390942edcec 182337 0 2024-04-15 08:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6a2a2398-4405-48b6-bd55-bf1a65d7e278 0xc003cb3ae7 0xc003cb3ae8}] [] [{kube-controller-manager Update v1 2024-04-15 08:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2a2398-4405-48b6-bd55-bf1a65d7e278\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wq9j9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wq9j9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.199,PodIP:10.233.66.162,StartTime:2024-04-15 08:23:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 08:23:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://c89ed00b39ca2df6820460f8c6a1452c0e12b5bee0e2a2c2d8f34c933a316d93,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.162,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.481: INFO: Pod "webserver-deployment-67bd4bf6dc-7zjd8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-7zjd8 webserver-deployment-67bd4bf6dc- deployment-7230  3fa40d87-1d26-41d4-8ede-7772b9adcd2d 182454 0 2024-04-15 08:24:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6a2a2398-4405-48b6-bd55-bf1a65d7e278 0xc003cb3cd7 0xc003cb3cd8}] [] [{kube-controller-manager Update v1 2024-04-15 08:24:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2a2398-4405-48b6-bd55-bf1a65d7e278\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lcfxs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lcfxs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.483: INFO: Pod "webserver-deployment-67bd4bf6dc-8vfpg" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8vfpg webserver-deployment-67bd4bf6dc- deployment-7230  7de811f6-f975-48f2-b2df-097377931d5b 182342 0 2024-04-15 08:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6a2a2398-4405-48b6-bd55-bf1a65d7e278 0xc003cb3e20 0xc003cb3e21}] [] [{kube-controller-manager Update v1 2024-04-15 08:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2a2398-4405-48b6-bd55-bf1a65d7e278\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.164\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tqn74,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tqn74,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.199,PodIP:10.233.66.164,StartTime:2024-04-15 08:23:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 08:23:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://77bc43094c91afe0855c2e7ef631ec2b4077616332f530683d82653c783b2203,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.164,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.484: INFO: Pod "webserver-deployment-67bd4bf6dc-czd9c" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-czd9c webserver-deployment-67bd4bf6dc- deployment-7230  06bf5a61-906b-4b13-9ab6-b99a4220c8cb 182459 0 2024-04-15 08:24:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6a2a2398-4405-48b6-bd55-bf1a65d7e278 0xc003dfe007 0xc003dfe008}] [] [{kube-controller-manager Update v1 2024-04-15 08:24:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2a2398-4405-48b6-bd55-bf1a65d7e278\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:24:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b6pj8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b6pj8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.199,PodIP:,StartTime:2024-04-15 08:24:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.485: INFO: Pod "webserver-deployment-67bd4bf6dc-hrb96" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hrb96 webserver-deployment-67bd4bf6dc- deployment-7230  89a46ca3-d289-41a8-a292-2ead171c8bec 182353 0 2024-04-15 08:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6a2a2398-4405-48b6-bd55-bf1a65d7e278 0xc003dfe1d7 0xc003dfe1d8}] [] [{kube-controller-manager Update v1 2024-04-15 08:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2a2398-4405-48b6-bd55-bf1a65d7e278\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.243\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6bmvx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6bmvx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:10.233.64.243,StartTime:2024-04-15 08:23:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 08:23:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://dba6ccf96bd43d942abbdeb1004f67b83adc7837f96d3905ac5a2cfbdf38ccc3,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.243,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.486: INFO: Pod "webserver-deployment-67bd4bf6dc-k7rhn" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-k7rhn webserver-deployment-67bd4bf6dc- deployment-7230  c113aacf-b017-46f6-b4df-5df4c25e7d68 182321 0 2024-04-15 08:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6a2a2398-4405-48b6-bd55-bf1a65d7e278 0xc003dfe3c7 0xc003dfe3c8}] [] [{kube-controller-manager Update v1 2024-04-15 08:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2a2398-4405-48b6-bd55-bf1a65d7e278\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:23:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wp478,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wp478,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.120,PodIP:10.233.65.10,StartTime:2024-04-15 08:23:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 08:23:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://5e7b34baeb3608cc7e311e96209a48a928a5d9cfa68c61b6141d6227a9b0fa19,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.10,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.486: INFO: Pod "webserver-deployment-67bd4bf6dc-mspck" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-mspck webserver-deployment-67bd4bf6dc- deployment-7230  354e738e-ff60-45cd-a0f5-6471164da4ad 182456 0 2024-04-15 08:24:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6a2a2398-4405-48b6-bd55-bf1a65d7e278 0xc003dfe5b7 0xc003dfe5b8}] [] [{kube-controller-manager Update v1 2024-04-15 08:24:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2a2398-4405-48b6-bd55-bf1a65d7e278\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w6l69,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w6l69,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.488: INFO: Pod "webserver-deployment-67bd4bf6dc-mwxxp" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-mwxxp webserver-deployment-67bd4bf6dc- deployment-7230  c13b9f21-dde7-4a7b-8038-f76f279bde76 182355 0 2024-04-15 08:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6a2a2398-4405-48b6-bd55-bf1a65d7e278 0xc003dfe700 0xc003dfe701}] [] [{kube-controller-manager Update v1 2024-04-15 08:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2a2398-4405-48b6-bd55-bf1a65d7e278\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.241\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kglx8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kglx8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:10.233.64.241,StartTime:2024-04-15 08:23:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 08:23:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://a01f39cd6f14c6f3c9d8aa3643ea8a89ccf333f23a0c4349dfd8b01bd5ff7d81,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.241,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.489: INFO: Pod "webserver-deployment-67bd4bf6dc-q4mfp" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-q4mfp webserver-deployment-67bd4bf6dc- deployment-7230  cf8d79f3-00d7-4135-93a3-7f231b8c33c3 182330 0 2024-04-15 08:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6a2a2398-4405-48b6-bd55-bf1a65d7e278 0xc003dfe8e7 0xc003dfe8e8}] [] [{kube-controller-manager Update v1 2024-04-15 08:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2a2398-4405-48b6-bd55-bf1a65d7e278\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:23:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b77r4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b77r4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.120,PodIP:10.233.65.12,StartTime:2024-04-15 08:23:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 08:23:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://e0c9d4322cf062621538876e76ae758352c8f986da80290f733217e1c592ec27,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.12,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.491: INFO: Pod "webserver-deployment-67bd4bf6dc-q82dv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-q82dv webserver-deployment-67bd4bf6dc- deployment-7230  613d221f-0c54-40cf-b5ff-ac2d06f356ef 182448 0 2024-04-15 08:24:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6a2a2398-4405-48b6-bd55-bf1a65d7e278 0xc003dfead7 0xc003dfead8}] [] [{kube-controller-manager Update v1 2024-04-15 08:24:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2a2398-4405-48b6-bd55-bf1a65d7e278\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w4slq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w4slq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.492: INFO: Pod "webserver-deployment-67bd4bf6dc-qcv8f" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-qcv8f webserver-deployment-67bd4bf6dc- deployment-7230  e614d668-43f2-4f3b-a222-3cc46391bf24 182457 0 2024-04-15 08:24:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6a2a2398-4405-48b6-bd55-bf1a65d7e278 0xc003dfec40 0xc003dfec41}] [] [{kube-controller-manager Update v1 2024-04-15 08:24:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2a2398-4405-48b6-bd55-bf1a65d7e278\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d95hb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d95hb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.493: INFO: Pod "webserver-deployment-67bd4bf6dc-r6zzw" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-r6zzw webserver-deployment-67bd4bf6dc- deployment-7230  b6f4b860-e53e-46c4-b0f7-035e13de0080 182358 0 2024-04-15 08:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6a2a2398-4405-48b6-bd55-bf1a65d7e278 0xc003dfeda0 0xc003dfeda1}] [] [{kube-controller-manager Update v1 2024-04-15 08:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2a2398-4405-48b6-bd55-bf1a65d7e278\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:23:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wwv4s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wwv4s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:10.233.64.242,StartTime:2024-04-15 08:23:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 08:23:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://4f0b1404c88b5dbc1a00d7bd359a42e9394493f393914e8df6efb638ae73cbdc,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.242,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.495: INFO: Pod "webserver-deployment-67bd4bf6dc-vphkf" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vphkf webserver-deployment-67bd4bf6dc- deployment-7230  10373df7-a52d-45e4-aa30-a61ebce18557 182325 0 2024-04-15 08:23:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6a2a2398-4405-48b6-bd55-bf1a65d7e278 0xc003dfef87 0xc003dfef88}] [] [{kube-controller-manager Update v1 2024-04-15 08:23:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2a2398-4405-48b6-bd55-bf1a65d7e278\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:23:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xhsvc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xhsvc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:23:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.120,PodIP:10.233.65.11,StartTime:2024-04-15 08:23:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 08:23:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://c9a7105950b859f9546f2ce08738586c3bf296b9d6556a8dab9068b895900fc6,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.11,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.495: INFO: Pod "webserver-deployment-67bd4bf6dc-xr6ls" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xr6ls webserver-deployment-67bd4bf6dc- deployment-7230  1cea732d-fe80-4f80-8e47-fbdc2971b860 182452 0 2024-04-15 08:24:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6a2a2398-4405-48b6-bd55-bf1a65d7e278 0xc003dff177 0xc003dff178}] [] [{kube-controller-manager Update v1 2024-04-15 08:24:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2a2398-4405-48b6-bd55-bf1a65d7e278\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8p4dh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8p4dh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.496: INFO: Pod "webserver-deployment-67bd4bf6dc-z2wfn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-z2wfn webserver-deployment-67bd4bf6dc- deployment-7230  1a82c7c9-61e3-4a85-8263-55010de73a4a 182458 0 2024-04-15 08:24:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 6a2a2398-4405-48b6-bd55-bf1a65d7e278 0xc003dff2c0 0xc003dff2c1}] [] [{kube-controller-manager Update v1 2024-04-15 08:24:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a2a2398-4405-48b6-bd55-bf1a65d7e278\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vp8p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vp8p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.496: INFO: Pod "webserver-deployment-7b75d79cf5-6dlw5" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-6dlw5 webserver-deployment-7b75d79cf5- deployment-7230  2fc23ef4-dfc3-43b8-a847-f74ce96c89ee 182395 0 2024-04-15 08:24:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 658675f0-4b46-448d-9444-3927c4331cfa 0xc003dff400 0xc003dff401}] [] [{kube-controller-manager Update v1 2024-04-15 08:24:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"658675f0-4b46-448d-9444-3927c4331cfa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:24:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drkvh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drkvh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.120,PodIP:,StartTime:2024-04-15 08:24:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.496: INFO: Pod "webserver-deployment-7b75d79cf5-99jhs" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-99jhs webserver-deployment-7b75d79cf5- deployment-7230  c0e62b2d-bc4d-4d78-8646-c61cc5a70f3d 182450 0 2024-04-15 08:24:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 658675f0-4b46-448d-9444-3927c4331cfa 0xc003dff5e7 0xc003dff5e8}] [] [{kube-controller-manager Update v1 2024-04-15 08:24:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"658675f0-4b46-448d-9444-3927c4331cfa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rn545,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rn545,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.497: INFO: Pod "webserver-deployment-7b75d79cf5-9l96r" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-9l96r webserver-deployment-7b75d79cf5- deployment-7230  a742506a-2c54-4975-a3c0-b9af53e81855 182449 0 2024-04-15 08:24:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 658675f0-4b46-448d-9444-3927c4331cfa 0xc003dff740 0xc003dff741}] [] [{kube-controller-manager Update v1 2024-04-15 08:24:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"658675f0-4b46-448d-9444-3927c4331cfa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8m8g6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8m8g6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.497: INFO: Pod "webserver-deployment-7b75d79cf5-hknz4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-hknz4 webserver-deployment-7b75d79cf5- deployment-7230  977790e3-fd93-47a4-8f96-0118940f55e7 182418 0 2024-04-15 08:24:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 658675f0-4b46-448d-9444-3927c4331cfa 0xc003dff890 0xc003dff891}] [] [{kube-controller-manager Update v1 2024-04-15 08:24:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"658675f0-4b46-448d-9444-3927c4331cfa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:24:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hf24v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hf24v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.199,PodIP:,StartTime:2024-04-15 08:24:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.498: INFO: Pod "webserver-deployment-7b75d79cf5-knxmq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-knxmq webserver-deployment-7b75d79cf5- deployment-7230  146bafc7-6b49-49e1-947e-b3ea36ff1dd4 182391 0 2024-04-15 08:24:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 658675f0-4b46-448d-9444-3927c4331cfa 0xc003dffa77 0xc003dffa78}] [] [{kube-controller-manager Update v1 2024-04-15 08:24:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"658675f0-4b46-448d-9444-3927c4331cfa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:24:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8pvc9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8pvc9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.96,PodIP:,StartTime:2024-04-15 08:24:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.499: INFO: Pod "webserver-deployment-7b75d79cf5-lh5ld" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lh5ld webserver-deployment-7b75d79cf5- deployment-7230  4cbcd9e7-6323-4c17-9be7-f3609c5c15ac 182453 0 2024-04-15 08:24:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 658675f0-4b46-448d-9444-3927c4331cfa 0xc003dffc67 0xc003dffc68}] [] [{kube-controller-manager Update v1 2024-04-15 08:24:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"658675f0-4b46-448d-9444-3927c4331cfa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w4fvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w4fvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.505: INFO: Pod "webserver-deployment-7b75d79cf5-pqpfq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-pqpfq webserver-deployment-7b75d79cf5- deployment-7230  b020cce6-c0f2-4b75-9e1f-b96d83d35263 182385 0 2024-04-15 08:24:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 658675f0-4b46-448d-9444-3927c4331cfa 0xc003dffde0 0xc003dffde1}] [] [{kube-controller-manager Update v1 2024-04-15 08:24:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"658675f0-4b46-448d-9444-3927c4331cfa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:24:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t6p26,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t6p26,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.199,PodIP:,StartTime:2024-04-15 08:24:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.505: INFO: Pod "webserver-deployment-7b75d79cf5-qbk5r" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-qbk5r webserver-deployment-7b75d79cf5- deployment-7230  50715d51-fbf0-4f74-bfc0-44cf0635db62 182417 0 2024-04-15 08:24:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 658675f0-4b46-448d-9444-3927c4331cfa 0xc003dfffc7 0xc003dfffc8}] [] [{kube-controller-manager Update v1 2024-04-15 08:24:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"658675f0-4b46-448d-9444-3927c4331cfa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:24:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8hn8j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8hn8j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:24:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.120,PodIP:,StartTime:2024-04-15 08:24:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:24:02.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7230" for this suite. @ 04/15/24 08:24:02.569
• [6.671 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 04/15/24 08:24:02.611
  Apr 15 08:24:02.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename events @ 04/15/24 08:24:02.62
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:24:03.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:24:03.014
  STEP: creating a test event @ 04/15/24 08:24:03.023
  STEP: listing events in all namespaces @ 04/15/24 08:24:03.075
  STEP: listing events in test namespace @ 04/15/24 08:24:03.101
  STEP: listing events with field selection filtering on source @ 04/15/24 08:24:03.137
  STEP: listing events with field selection filtering on reportingController @ 04/15/24 08:24:03.159
  STEP: getting the test event @ 04/15/24 08:24:03.175
  STEP: patching the test event @ 04/15/24 08:24:03.19
  STEP: getting the test event @ 04/15/24 08:24:03.282
  STEP: updating the test event @ 04/15/24 08:24:03.299
  STEP: getting the test event @ 04/15/24 08:24:03.356
  STEP: deleting the test event @ 04/15/24 08:24:03.365
  STEP: listing events in all namespaces @ 04/15/24 08:24:03.401
  STEP: listing events in test namespace @ 04/15/24 08:24:03.466
  Apr 15 08:24:03.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-9896" for this suite. @ 04/15/24 08:24:03.501
• [0.910 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 04/15/24 08:24:03.526
  Apr 15 08:24:03.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename job @ 04/15/24 08:24:03.529
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:24:03.567
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:24:03.573
  STEP: Creating a job @ 04/15/24 08:24:03.579
  STEP: Ensuring job reaches completions @ 04/15/24 08:24:03.593
  Apr 15 08:24:15.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-4445" for this suite. @ 04/15/24 08:24:15.617
• [12.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 04/15/24 08:24:15.644
  Apr 15 08:24:15.644: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename services @ 04/15/24 08:24:15.646
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:24:15.694
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:24:15.7
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-6678 @ 04/15/24 08:24:15.706
  STEP: changing the ExternalName service to type=NodePort @ 04/15/24 08:24:15.72
  STEP: creating replication controller externalname-service in namespace services-6678 @ 04/15/24 08:24:15.772
  I0415 08:24:15.791602      14 runners.go:194] Created replication controller with name: externalname-service, namespace: services-6678, replica count: 2
  I0415 08:24:18.844237      14 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 15 08:24:18.844: INFO: Creating new exec pod
  Apr 15 08:24:21.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-6678 exec execpodrkknn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 15 08:24:22.509: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 15 08:24:22.509: INFO: stdout: "externalname-service-25fpp"
  Apr 15 08:24:22.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-6678 exec execpodrkknn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.35.213 80'
  Apr 15 08:24:22.827: INFO: stderr: "+ + nc -v -t -w 2 10.233.35.213 80\necho hostName\nConnection to 10.233.35.213 80 port [tcp/http] succeeded!\n"
  Apr 15 08:24:22.827: INFO: stdout: "externalname-service-25fpp"
  Apr 15 08:24:22.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-6678 exec execpodrkknn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.96 30984'
  Apr 15 08:24:23.094: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.96 30984\nConnection to 192.168.121.96 30984 port [tcp/*] succeeded!\n"
  Apr 15 08:24:23.094: INFO: stdout: "externalname-service-25fpp"
  Apr 15 08:24:23.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-6678 exec execpodrkknn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.120 30984'
  Apr 15 08:24:23.375: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.120 30984\nConnection to 192.168.121.120 30984 port [tcp/*] succeeded!\n"
  Apr 15 08:24:23.375: INFO: stdout: "externalname-service-vv5rw"
  Apr 15 08:24:23.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 08:24:23.387: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-6678" for this suite. @ 04/15/24 08:24:23.433
• [7.801 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 04/15/24 08:24:23.451
  Apr 15 08:24:23.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 08:24:23.453
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:24:23.495
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:24:23.501
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/15/24 08:24:23.508
  Apr 15 08:24:23.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-3790 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Apr 15 08:24:23.671: INFO: stderr: ""
  Apr 15 08:24:23.671: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 04/15/24 08:24:23.671
  Apr 15 08:24:23.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-3790 delete pods e2e-test-httpd-pod'
  Apr 15 08:24:25.674: INFO: stderr: ""
  Apr 15 08:24:25.674: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 15 08:24:25.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3790" for this suite. @ 04/15/24 08:24:25.691
• [2.270 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 04/15/24 08:24:25.721
  Apr 15 08:24:25.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename gc @ 04/15/24 08:24:25.723
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:24:25.765
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:24:25.773
  Apr 15 08:24:25.881: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"1d3c01a7-c8ba-4ea4-af73-3bc95471795f", Controller:(*bool)(0xc00460d3e2), BlockOwnerDeletion:(*bool)(0xc00460d3e3)}}
  Apr 15 08:24:25.900: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"d9c17817-597a-4bf9-bf1b-cbbb70c172ba", Controller:(*bool)(0xc00460d682), BlockOwnerDeletion:(*bool)(0xc00460d683)}}
  Apr 15 08:24:25.964: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"fbf55944-64de-40b2-8083-53055b62f51c", Controller:(*bool)(0xc00460d8b2), BlockOwnerDeletion:(*bool)(0xc00460d8b3)}}
  Apr 15 08:24:30.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2722" for this suite. @ 04/15/24 08:24:31
• [5.293 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 04/15/24 08:24:31.031
  Apr 15 08:24:31.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 04/15/24 08:24:31.035
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:24:31.071
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:24:31.075
  STEP: creating a target pod @ 04/15/24 08:24:31.079
  STEP: adding an ephemeral container @ 04/15/24 08:24:33.132
  STEP: checking pod container endpoints @ 04/15/24 08:24:37.184
  Apr 15 08:24:37.184: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2057 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 08:24:37.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 08:24:37.188: INFO: ExecWithOptions: Clientset creation
  Apr 15 08:24:37.188: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-2057/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Apr 15 08:24:37.307: INFO: Exec stderr: ""
  Apr 15 08:24:37.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-2057" for this suite. @ 04/15/24 08:24:37.335
• [6.319 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 04/15/24 08:24:37.355
  Apr 15 08:24:37.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename runtimeclass @ 04/15/24 08:24:37.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:24:37.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:24:37.394
  STEP: Deleting RuntimeClass runtimeclass-9551-delete-me @ 04/15/24 08:24:37.407
  STEP: Waiting for the RuntimeClass to disappear @ 04/15/24 08:24:37.419
  Apr 15 08:24:37.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9551" for this suite. @ 04/15/24 08:24:37.456
• [0.113 seconds]
------------------------------
SSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 04/15/24 08:24:37.471
  Apr 15 08:24:37.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename prestop @ 04/15/24 08:24:37.475
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:24:37.504
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:24:37.507
  STEP: Creating server pod server in namespace prestop-3570 @ 04/15/24 08:24:37.513
  STEP: Waiting for pods to come up. @ 04/15/24 08:24:37.528
  STEP: Creating tester pod tester in namespace prestop-3570 @ 04/15/24 08:24:39.555
  STEP: Deleting pre-stop pod @ 04/15/24 08:24:41.585
  Apr 15 08:24:46.613: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Apr 15 08:24:46.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 04/15/24 08:24:46.631
  STEP: Destroying namespace "prestop-3570" for this suite. @ 04/15/24 08:24:46.67
• [9.246 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 04/15/24 08:24:46.72
  Apr 15 08:24:46.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename webhook @ 04/15/24 08:24:46.723
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:24:46.758
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:24:46.763
  STEP: Setting up server cert @ 04/15/24 08:24:46.827
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 08:24:48.271
  STEP: Deploying the webhook pod @ 04/15/24 08:24:48.298
  STEP: Wait for the deployment to be ready @ 04/15/24 08:24:48.319
  Apr 15 08:24:48.331: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 04/15/24 08:24:50.353
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 08:24:50.381
  Apr 15 08:24:51.381: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 15 08:24:51.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8710-crds.webhook.example.com via the AdmissionRegistration API @ 04/15/24 08:24:51.936
  STEP: Creating a custom resource while v1 is storage version @ 04/15/24 08:24:51.989
  STEP: Patching Custom Resource Definition to set v2 as storage @ 04/15/24 08:24:54.335
  STEP: Patching the custom resource while v2 is storage version @ 04/15/24 08:24:54.355
  Apr 15 08:24:54.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3128" for this suite. @ 04/15/24 08:24:55.097
  STEP: Destroying namespace "webhook-markers-9855" for this suite. @ 04/15/24 08:24:55.121
• [8.437 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 04/15/24 08:24:55.162
  Apr 15 08:24:55.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 08:24:55.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:24:55.246
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:24:55.26
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 08:24:55.271
  STEP: Saw pod success @ 04/15/24 08:24:59.325
  Apr 15 08:24:59.332: INFO: Trying to get logs from node ahz3daisheng-3 pod downwardapi-volume-eb6ccfde-a0b5-44bb-9f05-fee01d123e22 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 08:24:59.352
  Apr 15 08:24:59.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8396" for this suite. @ 04/15/24 08:24:59.447
• [4.307 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 04/15/24 08:24:59.473
  Apr 15 08:24:59.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename security-context-test @ 04/15/24 08:24:59.475
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:24:59.531
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:24:59.538
  Apr 15 08:25:03.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9481" for this suite. @ 04/15/24 08:25:03.656
• [4.200 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 04/15/24 08:25:03.677
  Apr 15 08:25:03.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubelet-test @ 04/15/24 08:25:03.68
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:25:03.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:25:03.742
  STEP: Waiting for pod completion @ 04/15/24 08:25:03.772
  Apr 15 08:25:07.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6186" for this suite. @ 04/15/24 08:25:07.846
• [4.186 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 04/15/24 08:25:07.888
  Apr 15 08:25:07.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename configmap @ 04/15/24 08:25:07.892
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:25:07.934
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:25:07.942
  STEP: Creating configMap with name configmap-test-upd-d809f633-cf3a-448c-b3ee-c334edfc6aff @ 04/15/24 08:25:07.964
  STEP: Creating the pod @ 04/15/24 08:25:07.98
  STEP: Updating configmap configmap-test-upd-d809f633-cf3a-448c-b3ee-c334edfc6aff @ 04/15/24 08:25:10.084
  STEP: waiting to observe update in volume @ 04/15/24 08:25:10.098
  Apr 15 08:26:24.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1556" for this suite. @ 04/15/24 08:26:24.976
• [77.105 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 04/15/24 08:26:24.993
  Apr 15 08:26:24.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename secrets @ 04/15/24 08:26:24.995
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:26:25.043
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:26:25.053
  Apr 15 08:26:25.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7138" for this suite. @ 04/15/24 08:26:25.168
• [0.191 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 04/15/24 08:26:25.184
  Apr 15 08:26:25.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 08:26:25.186
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:26:25.229
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:26:25.236
  STEP: creating Agnhost RC @ 04/15/24 08:26:25.242
  Apr 15 08:26:25.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6475 create -f -'
  Apr 15 08:26:25.884: INFO: stderr: ""
  Apr 15 08:26:25.884: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/15/24 08:26:25.884
  Apr 15 08:26:26.906: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 08:26:26.906: INFO: Found 0 / 1
  Apr 15 08:26:27.895: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 08:26:27.895: INFO: Found 1 / 1
  Apr 15 08:26:27.895: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 04/15/24 08:26:27.895
  Apr 15 08:26:27.902: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 08:26:27.902: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 15 08:26:27.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6475 patch pod agnhost-primary-lxj9t -p {"metadata":{"annotations":{"x":"y"}}}'
  Apr 15 08:26:28.088: INFO: stderr: ""
  Apr 15 08:26:28.088: INFO: stdout: "pod/agnhost-primary-lxj9t patched\n"
  STEP: checking annotations @ 04/15/24 08:26:28.088
  Apr 15 08:26:28.097: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 08:26:28.097: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 15 08:26:28.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6475" for this suite. @ 04/15/24 08:26:28.124
• [2.959 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 04/15/24 08:26:28.151
  Apr 15 08:26:28.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename configmap @ 04/15/24 08:26:28.153
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:26:28.192
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:26:28.2
  STEP: Creating configMap with name configmap-test-volume-map-1842f68b-b8b9-4641-9db3-56d392ad76d9 @ 04/15/24 08:26:28.21
  STEP: Creating a pod to test consume configMaps @ 04/15/24 08:26:28.223
  STEP: Saw pod success @ 04/15/24 08:26:32.281
  Apr 15 08:26:32.291: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-configmaps-d217cadb-775e-4088-9dbb-c03e30cdf434 container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 08:26:32.31
  Apr 15 08:26:32.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4959" for this suite. @ 04/15/24 08:26:32.369
• [4.235 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 04/15/24 08:26:32.393
  Apr 15 08:26:32.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename webhook @ 04/15/24 08:26:32.395
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:26:32.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:26:32.441
  STEP: Setting up server cert @ 04/15/24 08:26:32.499
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 08:26:33.351
  STEP: Deploying the webhook pod @ 04/15/24 08:26:33.386
  STEP: Wait for the deployment to be ready @ 04/15/24 08:26:33.436
  Apr 15 08:26:33.471: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/15/24 08:26:35.497
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 08:26:35.524
  Apr 15 08:26:36.524: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 04/15/24 08:26:36.533
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/15/24 08:26:36.533
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 04/15/24 08:26:36.574
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 04/15/24 08:26:37.603
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/15/24 08:26:37.608
  STEP: Having no error when timeout is longer than webhook latency @ 04/15/24 08:26:38.724
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/15/24 08:26:38.724
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 04/15/24 08:26:43.815
  STEP: Registering slow webhook via the AdmissionRegistration API @ 04/15/24 08:26:43.816
  Apr 15 08:26:48.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9296" for this suite. @ 04/15/24 08:26:49.064
  STEP: Destroying namespace "webhook-markers-5486" for this suite. @ 04/15/24 08:26:49.084
• [16.706 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 04/15/24 08:26:49.126
  Apr 15 08:26:49.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename replicaset @ 04/15/24 08:26:49.138
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:26:49.179
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:26:49.187
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 04/15/24 08:26:49.193
  STEP: When a replicaset with a matching selector is created @ 04/15/24 08:26:51.251
  STEP: Then the orphan pod is adopted @ 04/15/24 08:26:51.261
  STEP: When the matched label of one of its pods change @ 04/15/24 08:26:52.278
  Apr 15 08:26:52.285: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 04/15/24 08:26:52.309
  Apr 15 08:26:53.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7481" for this suite. @ 04/15/24 08:26:53.347
• [4.252 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 04/15/24 08:26:53.381
  Apr 15 08:26:53.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/15/24 08:26:53.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:26:53.445
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:26:53.449
  Apr 15 08:26:53.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 08:26:59.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-1265" for this suite. @ 04/15/24 08:26:59.975
• [6.609 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 04/15/24 08:26:59.997
  Apr 15 08:26:59.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename configmap @ 04/15/24 08:27:00.002
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:27:00.04
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:27:00.046
  STEP: Creating configMap with name configmap-test-volume-1caf6938-c047-4cff-85b7-1408f761abb8 @ 04/15/24 08:27:00.051
  STEP: Creating a pod to test consume configMaps @ 04/15/24 08:27:00.059
  STEP: Saw pod success @ 04/15/24 08:27:04.109
  Apr 15 08:27:04.123: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-configmaps-f2b77116-6c3d-48e5-addd-06f8b8786e3e container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 08:27:04.141
  Apr 15 08:27:04.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9156" for this suite. @ 04/15/24 08:27:04.187
• [4.207 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 04/15/24 08:27:04.205
  Apr 15 08:27:04.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename var-expansion @ 04/15/24 08:27:04.207
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:27:04.25
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:27:04.26
  STEP: creating the pod with failed condition @ 04/15/24 08:27:04.268
  STEP: updating the pod @ 04/15/24 08:29:04.292
  Apr 15 08:29:04.831: INFO: Successfully updated pod "var-expansion-0ac7efee-e215-47ed-a2c5-6bf9013571c8"
  STEP: waiting for pod running @ 04/15/24 08:29:04.831
  STEP: deleting the pod gracefully @ 04/15/24 08:29:06.861
  Apr 15 08:29:06.861: INFO: Deleting pod "var-expansion-0ac7efee-e215-47ed-a2c5-6bf9013571c8" in namespace "var-expansion-6326"
  Apr 15 08:29:06.875: INFO: Wait up to 5m0s for pod "var-expansion-0ac7efee-e215-47ed-a2c5-6bf9013571c8" to be fully deleted
  Apr 15 08:29:39.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-6326" for this suite. @ 04/15/24 08:29:39.087
• [154.897 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 04/15/24 08:29:39.106
  Apr 15 08:29:39.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename secrets @ 04/15/24 08:29:39.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:29:39.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:29:39.163
  STEP: Creating secret with name secret-test-map-31ae9244-e427-47fc-8b20-d3b839df43f4 @ 04/15/24 08:29:39.171
  STEP: Creating a pod to test consume secrets @ 04/15/24 08:29:39.184
  STEP: Saw pod success @ 04/15/24 08:29:43.231
  Apr 15 08:29:43.239: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-secrets-d4df68d1-f8cc-4650-816a-42ee3432503f container secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 08:29:43.281
  Apr 15 08:29:43.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-877" for this suite. @ 04/15/24 08:29:43.331
• [4.242 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 04/15/24 08:29:43.357
  Apr 15 08:29:43.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename secrets @ 04/15/24 08:29:43.36
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:29:43.398
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:29:43.404
  STEP: creating a secret @ 04/15/24 08:29:43.412
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 04/15/24 08:29:43.423
  STEP: patching the secret @ 04/15/24 08:29:43.432
  STEP: deleting the secret using a LabelSelector @ 04/15/24 08:29:43.452
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 04/15/24 08:29:43.474
  Apr 15 08:29:43.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6018" for this suite. @ 04/15/24 08:29:43.495
• [0.150 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 04/15/24 08:29:43.507
  Apr 15 08:29:43.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename pods @ 04/15/24 08:29:43.51
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:29:43.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:29:43.557
  STEP: creating pod @ 04/15/24 08:29:43.565
  Apr 15 08:29:45.611: INFO: Pod pod-hostip-cf37135d-98e7-447e-b95d-929ad4750c87 has hostIP: 192.168.121.199
  Apr 15 08:29:45.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9870" for this suite. @ 04/15/24 08:29:45.623
• [2.136 seconds]
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 04/15/24 08:29:45.644
  Apr 15 08:29:45.644: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename emptydir-wrapper @ 04/15/24 08:29:45.647
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:29:45.682
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:29:45.688
  Apr 15 08:29:47.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 04/15/24 08:29:47.764
  STEP: Cleaning up the configmap @ 04/15/24 08:29:47.779
  STEP: Cleaning up the pod @ 04/15/24 08:29:47.791
  STEP: Destroying namespace "emptydir-wrapper-9417" for this suite. @ 04/15/24 08:29:47.822
• [2.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 04/15/24 08:29:47.854
  Apr 15 08:29:47.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename deployment @ 04/15/24 08:29:47.857
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:29:47.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:29:47.891
  STEP: creating a Deployment @ 04/15/24 08:29:47.905
  STEP: waiting for Deployment to be created @ 04/15/24 08:29:47.916
  STEP: waiting for all Replicas to be Ready @ 04/15/24 08:29:47.919
  Apr 15 08:29:47.922: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 15 08:29:47.923: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 15 08:29:47.947: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 15 08:29:47.947: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 15 08:29:47.999: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 15 08:29:47.999: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 15 08:29:48.040: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 15 08:29:48.041: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Apr 15 08:29:49.186: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Apr 15 08:29:49.186: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Apr 15 08:29:49.509: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 04/15/24 08:29:49.51
  W0415 08:29:49.541758      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 15 08:29:49.545: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 04/15/24 08:29:49.545
  Apr 15 08:29:49.550: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 0
  Apr 15 08:29:49.550: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 0
  Apr 15 08:29:49.550: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 0
  Apr 15 08:29:49.550: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 0
  Apr 15 08:29:49.550: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 0
  Apr 15 08:29:49.550: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 0
  Apr 15 08:29:49.550: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 0
  Apr 15 08:29:49.550: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 0
  Apr 15 08:29:49.551: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 1
  Apr 15 08:29:49.551: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 1
  Apr 15 08:29:49.551: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2
  Apr 15 08:29:49.551: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2
  Apr 15 08:29:49.551: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2
  Apr 15 08:29:49.551: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2
  Apr 15 08:29:49.588: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2
  Apr 15 08:29:49.588: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2
  Apr 15 08:29:49.615: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2
  Apr 15 08:29:49.616: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2
  Apr 15 08:29:49.670: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 1
  Apr 15 08:29:49.670: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 1
  Apr 15 08:29:49.701: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 1
  Apr 15 08:29:49.702: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 1
  Apr 15 08:29:51.215: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2
  Apr 15 08:29:51.215: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2
  Apr 15 08:29:51.285: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 1
  STEP: listing Deployments @ 04/15/24 08:29:51.286
  Apr 15 08:29:51.293: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 04/15/24 08:29:51.294
  Apr 15 08:29:51.321: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 04/15/24 08:29:51.322
  Apr 15 08:29:51.337: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 15 08:29:51.376: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 15 08:29:51.427: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 15 08:29:51.501: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 15 08:29:52.276: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 15 08:29:52.321: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 15 08:29:52.371: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 15 08:29:52.407: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Apr 15 08:29:53.584: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 04/15/24 08:29:53.641
  STEP: fetching the DeploymentStatus @ 04/15/24 08:29:53.657
  Apr 15 08:29:53.668: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 1
  Apr 15 08:29:53.669: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 1
  Apr 15 08:29:53.669: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 1
  Apr 15 08:29:53.670: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 1
  Apr 15 08:29:53.670: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2
  Apr 15 08:29:53.670: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2
  Apr 15 08:29:53.670: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2
  Apr 15 08:29:53.671: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 2
  Apr 15 08:29:53.671: INFO: observed Deployment test-deployment in namespace deployment-2349 with ReadyReplicas 3
  STEP: deleting the Deployment @ 04/15/24 08:29:53.671
  Apr 15 08:29:53.689: INFO: observed event type MODIFIED
  Apr 15 08:29:53.691: INFO: observed event type MODIFIED
  Apr 15 08:29:53.692: INFO: observed event type MODIFIED
  Apr 15 08:29:53.692: INFO: observed event type MODIFIED
  Apr 15 08:29:53.692: INFO: observed event type MODIFIED
  Apr 15 08:29:53.692: INFO: observed event type MODIFIED
  Apr 15 08:29:53.694: INFO: observed event type MODIFIED
  Apr 15 08:29:53.695: INFO: observed event type MODIFIED
  Apr 15 08:29:53.695: INFO: observed event type MODIFIED
  Apr 15 08:29:53.695: INFO: observed event type MODIFIED
  Apr 15 08:29:53.703: INFO: Log out all the ReplicaSets if there is no deployment created
  Apr 15 08:29:53.713: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-2349  640f2d20-360a-46a5-b3d5-2dbcb4c93f6b 184404 4 2024-04-15 08:29:49 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment cfb7f6cb-0685-4a12-b1fb-0727b2dd2bc4 0xc004207227 0xc004207228}] [] [{kube-controller-manager Update apps/v1 2024-04-15 08:29:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cfb7f6cb-0685-4a12-b1fb-0727b2dd2bc4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 08:29:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042072b0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Apr 15 08:29:53.723: INFO: pod: "test-deployment-5b5dcbcd95-85q5q":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-85q5q test-deployment-5b5dcbcd95- deployment-2349  87f4b69c-576c-4cab-8fd7-470ef949ab7b 184399 0 2024-04-15 08:29:49 +0000 UTC 2024-04-15 08:29:54 +0000 UTC 0xc003e3fc78 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 640f2d20-360a-46a5-b3d5-2dbcb4c93f6b 0xc003e3fca7 0xc003e3fca8}] [] [{kube-controller-manager Update v1 2024-04-15 08:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"640f2d20-360a-46a5-b3d5-2dbcb4c93f6b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:29:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fpj6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fpj6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:29:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.199,PodIP:10.233.66.195,StartTime:2024-04-15 08:29:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 08:29:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://e15005f6d8fc5050ca43e4c5828fe6c67404a82a9734231384eec890162eaa75,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.195,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 15 08:29:53.727: INFO: ReplicaSet "test-deployment-6c79cbbc7f":
  &ReplicaSet{ObjectMeta:{test-deployment-6c79cbbc7f  deployment-2349  1f0769bd-cba4-47f9-8bdb-95fdaf086875 184310 3 2024-04-15 08:29:47 +0000 UTC <nil> <nil> map[pod-template-hash:6c79cbbc7f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment cfb7f6cb-0685-4a12-b1fb-0727b2dd2bc4 0xc004207317 0xc004207318}] [] [{kube-controller-manager Update apps/v1 2024-04-15 08:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cfb7f6cb-0685-4a12-b1fb-0727b2dd2bc4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 08:29:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6c79cbbc7f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6c79cbbc7f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.47 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042073a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Apr 15 08:29:53.739: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-2349  56ac88a7-1fbb-4194-a502-1d2627d74829 184393 2 2024-04-15 08:29:51 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment cfb7f6cb-0685-4a12-b1fb-0727b2dd2bc4 0xc004207407 0xc004207408}] [] [{kube-controller-manager Update apps/v1 2024-04-15 08:29:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cfb7f6cb-0685-4a12-b1fb-0727b2dd2bc4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 08:29:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004207490 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Apr 15 08:29:53.754: INFO: pod: "test-deployment-6fc78d85c6-4gtvg":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-4gtvg test-deployment-6fc78d85c6- deployment-2349  7a1331e6-6be9-41dc-907d-933a8a9ce1f3 184347 0 2024-04-15 08:29:51 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 56ac88a7-1fbb-4194-a502-1d2627d74829 0xc004143bf7 0xc004143bf8}] [] [{kube-controller-manager Update v1 2024-04-15 08:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56ac88a7-1fbb-4194-a502-1d2627d74829\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:29:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nmvx5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nmvx5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:29:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:29:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:29:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.199,PodIP:10.233.66.196,StartTime:2024-04-15 08:29:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 08:29:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://a23655bb004ebbc0210ce7fbfaa1c0b30927c6028eb0cab8e641303abff701db,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.196,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 15 08:29:53.757: INFO: pod: "test-deployment-6fc78d85c6-z9f5h":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-z9f5h test-deployment-6fc78d85c6- deployment-2349  c2bc358d-5c23-4086-ace2-564a7aeef4bf 184392 0 2024-04-15 08:29:52 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 56ac88a7-1fbb-4194-a502-1d2627d74829 0xc004143de7 0xc004143de8}] [] [{kube-controller-manager Update v1 2024-04-15 08:29:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56ac88a7-1fbb-4194-a502-1d2627d74829\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:29:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7z4sf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7z4sf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:29:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:29:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:29:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:29:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.120,PodIP:10.233.65.20,StartTime:2024-04-15 08:29:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 08:29:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://29219bd3b88e3ddc03f98b73969bc79a2b236edcc045b4e55b79e8fb5a7192c6,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.20,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Apr 15 08:29:53.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2349" for this suite. @ 04/15/24 08:29:53.78
• [5.938 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 04/15/24 08:29:53.798
  Apr 15 08:29:53.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename podtemplate @ 04/15/24 08:29:53.804
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:29:53.835
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:29:53.841
  Apr 15 08:29:53.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-7936" for this suite. @ 04/15/24 08:29:53.93
• [0.145 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 04/15/24 08:29:53.959
  Apr 15 08:29:53.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 08:29:53.965
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:29:53.993
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:29:53.998
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 08:29:54.004
  STEP: Saw pod success @ 04/15/24 08:29:58.048
  Apr 15 08:29:58.054: INFO: Trying to get logs from node ahz3daisheng-3 pod downwardapi-volume-7ff2ad95-d7e9-4cbe-8753-260ec09f30e3 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 08:29:58.068
  Apr 15 08:29:58.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5586" for this suite. @ 04/15/24 08:29:58.116
• [4.173 seconds]
------------------------------
SS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 04/15/24 08:29:58.133
  Apr 15 08:29:58.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename deployment @ 04/15/24 08:29:58.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:29:58.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:29:58.186
  Apr 15 08:29:58.192: INFO: Creating simple deployment test-new-deployment
  Apr 15 08:29:58.219: INFO: deployment "test-new-deployment" doesn't have the required revision set
  STEP: getting scale subresource @ 04/15/24 08:30:00.246
  STEP: updating a scale subresource @ 04/15/24 08:30:00.252
  STEP: verifying the deployment Spec.Replicas was modified @ 04/15/24 08:30:00.268
  STEP: Patch a scale subresource @ 04/15/24 08:30:00.275
  Apr 15 08:30:00.310: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-6817  b6ee3fc8-70a7-4808-b26c-d1fa0ec258a6 184541 3 2024-04-15 08:29:58 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2024-04-15 08:29:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 08:29:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041cfe38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-04-15 08:29:59 +0000 UTC,LastTransitionTime:2024-04-15 08:29:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2024-04-15 08:29:59 +0000 UTC,LastTransitionTime:2024-04-15 08:29:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 15 08:30:00.319: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-6817  47484e81-5ade-44c0-acd6-26e96f0bd77e 184540 2 2024-04-15 08:29:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment b6ee3fc8-70a7-4808-b26c-d1fa0ec258a6 0xc003a6cd67 0xc003a6cd68}] [] [{kube-controller-manager Update apps/v1 2024-04-15 08:29:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2024-04-15 08:30:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6ee3fc8-70a7-4808-b26c-d1fa0ec258a6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a6cdf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 08:30:00.337: INFO: Pod "test-new-deployment-67bd4bf6dc-78d9s" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-78d9s test-new-deployment-67bd4bf6dc- deployment-6817  18cfa34f-5754-47ff-bb94-1b8bd361ee78 184542 0 2024-04-15 08:30:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 47484e81-5ade-44c0-acd6-26e96f0bd77e 0xc004356207 0xc004356208}] [] [{kube-controller-manager Update v1 2024-04-15 08:30:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47484e81-5ade-44c0-acd6-26e96f0bd77e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nmlr9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nmlr9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:30:00.340: INFO: Pod "test-new-deployment-67bd4bf6dc-ptwtj" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-ptwtj test-new-deployment-67bd4bf6dc- deployment-6817  2dd3ab22-59a0-447d-a74e-2340582b6502 184492 0 2024-04-15 08:29:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 47484e81-5ade-44c0-acd6-26e96f0bd77e 0xc004356350 0xc004356351}] [] [{kube-controller-manager Update v1 2024-04-15 08:29:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47484e81-5ade-44c0-acd6-26e96f0bd77e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:29:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cs9f7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cs9f7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:29:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:29:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:29:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:29:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.199,PodIP:10.233.66.198,StartTime:2024-04-15 08:29:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 08:29:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://1478a4b0f8085b9b3508d0a1d27e3b4157431958713321a42e34dff705ccdaf1,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.198,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:30:00.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6817" for this suite. @ 04/15/24 08:30:00.37
• [2.253 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 04/15/24 08:30:00.394
  Apr 15 08:30:00.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename var-expansion @ 04/15/24 08:30:00.396
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:30:00.432
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:30:00.441
  STEP: Creating a pod to test substitution in container's command @ 04/15/24 08:30:00.449
  STEP: Saw pod success @ 04/15/24 08:30:02.481
  Apr 15 08:30:02.487: INFO: Trying to get logs from node ahz3daisheng-3 pod var-expansion-2bf7f5e5-67ee-4a39-96cd-5ef4b282c7b9 container dapi-container: <nil>
  STEP: delete the pod @ 04/15/24 08:30:02.501
  Apr 15 08:30:02.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2901" for this suite. @ 04/15/24 08:30:02.542
• [2.163 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 04/15/24 08:30:02.558
  Apr 15 08:30:02.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename job @ 04/15/24 08:30:02.561
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:30:02.594
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:30:02.598
  STEP: Creating Indexed job @ 04/15/24 08:30:02.604
  STEP: Ensuring job reaches completions @ 04/15/24 08:30:02.615
  STEP: Ensuring pods with index for job exist @ 04/15/24 08:30:12.623
  Apr 15 08:30:12.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7060" for this suite. @ 04/15/24 08:30:12.64
• [10.194 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 04/15/24 08:30:12.757
  Apr 15 08:30:12.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename disruption @ 04/15/24 08:30:12.76
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:30:12.855
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:30:12.86
  STEP: Creating a pdb that targets all three pods in a test replica set @ 04/15/24 08:30:12.865
  STEP: Waiting for the pdb to be processed @ 04/15/24 08:30:12.875
  STEP: First trying to evict a pod which shouldn't be evictable @ 04/15/24 08:30:14.905
  STEP: Waiting for all pods to be running @ 04/15/24 08:30:14.905
  Apr 15 08:30:14.913: INFO: pods: 0 < 3
  STEP: locating a running pod @ 04/15/24 08:30:16.925
  STEP: Updating the pdb to allow a pod to be evicted @ 04/15/24 08:30:16.954
  STEP: Waiting for the pdb to be processed @ 04/15/24 08:30:16.971
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 04/15/24 08:30:18.992
  STEP: Waiting for all pods to be running @ 04/15/24 08:30:18.993
  STEP: Waiting for the pdb to observed all healthy pods @ 04/15/24 08:30:19
  STEP: Patching the pdb to disallow a pod to be evicted @ 04/15/24 08:30:19.049
  STEP: Waiting for the pdb to be processed @ 04/15/24 08:30:19.087
  STEP: Waiting for all pods to be running @ 04/15/24 08:30:21.104
  STEP: locating a running pod @ 04/15/24 08:30:21.112
  STEP: Deleting the pdb to allow a pod to be evicted @ 04/15/24 08:30:21.133
  STEP: Waiting for the pdb to be deleted @ 04/15/24 08:30:21.145
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 04/15/24 08:30:21.15
  STEP: Waiting for all pods to be running @ 04/15/24 08:30:21.151
  Apr 15 08:30:21.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-281" for this suite. @ 04/15/24 08:30:21.209
• [8.480 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 04/15/24 08:30:21.239
  Apr 15 08:30:21.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename secrets @ 04/15/24 08:30:21.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:30:21.442
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:30:21.448
  STEP: Creating secret with name s-test-opt-del-8db974c7-4512-4b5c-afe9-fc5539b560ec @ 04/15/24 08:30:21.476
  STEP: Creating secret with name s-test-opt-upd-b3140d74-63fb-496c-b9ec-25205ed5f130 @ 04/15/24 08:30:21.492
  STEP: Creating the pod @ 04/15/24 08:30:21.518
  STEP: Deleting secret s-test-opt-del-8db974c7-4512-4b5c-afe9-fc5539b560ec @ 04/15/24 08:30:23.642
  STEP: Updating secret s-test-opt-upd-b3140d74-63fb-496c-b9ec-25205ed5f130 @ 04/15/24 08:30:23.654
  STEP: Creating secret with name s-test-opt-create-c37a2f2f-3626-4fa0-ba5e-5d431a660147 @ 04/15/24 08:30:23.666
  STEP: waiting to observe update in volume @ 04/15/24 08:30:23.677
  Apr 15 08:30:25.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9218" for this suite. @ 04/15/24 08:30:25.754
• [4.534 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 04/15/24 08:30:25.793
  Apr 15 08:30:25.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename pod-network-test @ 04/15/24 08:30:25.795
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:30:25.841
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:30:25.846
  STEP: Performing setup for networking test in namespace pod-network-test-4152 @ 04/15/24 08:30:25.851
  STEP: creating a selector @ 04/15/24 08:30:25.851
  STEP: Creating the service pods in kubernetes @ 04/15/24 08:30:25.851
  Apr 15 08:30:25.852: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 04/15/24 08:30:48.058
  Apr 15 08:30:50.127: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 15 08:30:50.127: INFO: Going to poll 10.233.64.249 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Apr 15 08:30:50.134: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.249:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4152 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 08:30:50.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 08:30:50.137: INFO: ExecWithOptions: Clientset creation
  Apr 15 08:30:50.137: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4152/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.249%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 15 08:30:50.319: INFO: Found all 1 expected endpoints: [netserver-0]
  Apr 15 08:30:50.320: INFO: Going to poll 10.233.65.22 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Apr 15 08:30:50.328: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.22:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4152 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 08:30:50.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 08:30:50.330: INFO: ExecWithOptions: Clientset creation
  Apr 15 08:30:50.330: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4152/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.65.22%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 15 08:30:50.479: INFO: Found all 1 expected endpoints: [netserver-1]
  Apr 15 08:30:50.479: INFO: Going to poll 10.233.66.207 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Apr 15 08:30:50.485: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.207:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4152 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 08:30:50.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 08:30:50.488: INFO: ExecWithOptions: Clientset creation
  Apr 15 08:30:50.488: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4152/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.207%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 15 08:30:50.615: INFO: Found all 1 expected endpoints: [netserver-2]
  Apr 15 08:30:50.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4152" for this suite. @ 04/15/24 08:30:50.632
• [24.855 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 04/15/24 08:30:50.649
  Apr 15 08:30:50.649: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename services @ 04/15/24 08:30:50.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:30:50.692
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:30:50.699
  STEP: creating service in namespace services-7648 @ 04/15/24 08:30:50.708
  STEP: creating service affinity-clusterip-transition in namespace services-7648 @ 04/15/24 08:30:50.709
  STEP: creating replication controller affinity-clusterip-transition in namespace services-7648 @ 04/15/24 08:30:50.753
  I0415 08:30:50.778527      14 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-7648, replica count: 3
  I0415 08:30:53.830657      14 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 15 08:30:53.855: INFO: Creating new exec pod
  Apr 15 08:30:56.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-7648 exec execpod-affinitym8zsh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Apr 15 08:30:57.384: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Apr 15 08:30:57.384: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 08:30:57.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-7648 exec execpod-affinitym8zsh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.60.63 80'
  Apr 15 08:30:57.692: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.60.63 80\nConnection to 10.233.60.63 80 port [tcp/http] succeeded!\n"
  Apr 15 08:30:57.692: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 08:30:57.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-7648 exec execpod-affinitym8zsh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.60.63:80/ ; done'
  Apr 15 08:30:58.253: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n"
  Apr 15 08:30:58.253: INFO: stdout: "\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-25kxc\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-25kxc\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-25kxc\naffinity-clusterip-transition-25kxc\naffinity-clusterip-transition-wwnpq\naffinity-clusterip-transition-25kxc\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-25kxc\naffinity-clusterip-transition-wwnpq\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-25kxc"
  Apr 15 08:30:58.253: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.253: INFO: Received response from host: affinity-clusterip-transition-25kxc
  Apr 15 08:30:58.253: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.253: INFO: Received response from host: affinity-clusterip-transition-25kxc
  Apr 15 08:30:58.253: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.253: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.253: INFO: Received response from host: affinity-clusterip-transition-25kxc
  Apr 15 08:30:58.253: INFO: Received response from host: affinity-clusterip-transition-25kxc
  Apr 15 08:30:58.253: INFO: Received response from host: affinity-clusterip-transition-wwnpq
  Apr 15 08:30:58.253: INFO: Received response from host: affinity-clusterip-transition-25kxc
  Apr 15 08:30:58.253: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.253: INFO: Received response from host: affinity-clusterip-transition-25kxc
  Apr 15 08:30:58.253: INFO: Received response from host: affinity-clusterip-transition-wwnpq
  Apr 15 08:30:58.253: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.253: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.253: INFO: Received response from host: affinity-clusterip-transition-25kxc
  Apr 15 08:30:58.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-7648 exec execpod-affinitym8zsh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.60.63:80/ ; done'
  Apr 15 08:30:58.733: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.60.63:80/\n"
  Apr 15 08:30:58.733: INFO: stdout: "\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-c269g\naffinity-clusterip-transition-c269g"
  Apr 15 08:30:58.733: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.733: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.733: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.733: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.733: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.733: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.733: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.733: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.733: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.733: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.733: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.733: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.733: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.733: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.733: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.733: INFO: Received response from host: affinity-clusterip-transition-c269g
  Apr 15 08:30:58.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 08:30:58.744: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7648, will wait for the garbage collector to delete the pods @ 04/15/24 08:30:58.765
  Apr 15 08:30:58.838: INFO: Deleting ReplicationController affinity-clusterip-transition took: 15.237496ms
  Apr 15 08:30:58.940: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 102.04957ms
  STEP: Destroying namespace "services-7648" for this suite. @ 04/15/24 08:31:01.083
• [10.448 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 04/15/24 08:31:01.104
  Apr 15 08:31:01.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename sched-pred @ 04/15/24 08:31:01.107
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:31:01.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:31:01.203
  Apr 15 08:31:01.209: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 15 08:31:01.230: INFO: Waiting for terminating namespaces to be deleted...
  Apr 15 08:31:01.236: INFO: 
  Logging pods the apiserver thinks is on node ahz3daisheng-1 before test
  Apr 15 08:31:01.253: INFO: rs-szglj from disruption-281 started at 2024-04-15 08:30:21 +0000 UTC (1 container statuses recorded)
  Apr 15 08:31:01.253: INFO: 	Container donothing ready: false, restart count 0
  Apr 15 08:31:01.253: INFO: coredns-5d78c9869d-hwtz8 from kube-system started at 2024-04-15 08:21:15 +0000 UTC (1 container statuses recorded)
  Apr 15 08:31:01.253: INFO: 	Container coredns ready: true, restart count 0
  Apr 15 08:31:01.253: INFO: kube-addon-manager-ahz3daisheng-1 from kube-system started at 2024-04-15 06:17:06 +0000 UTC (1 container statuses recorded)
  Apr 15 08:31:01.253: INFO: 	Container kube-addon-manager ready: true, restart count 5
  Apr 15 08:31:01.253: INFO: kube-apiserver-ahz3daisheng-1 from kube-system started at 2024-04-15 06:17:06 +0000 UTC (1 container statuses recorded)
  Apr 15 08:31:01.253: INFO: 	Container kube-apiserver ready: true, restart count 5
  Apr 15 08:31:01.253: INFO: kube-controller-manager-ahz3daisheng-1 from kube-system started at 2024-04-15 06:17:06 +0000 UTC (1 container statuses recorded)
  Apr 15 08:31:01.253: INFO: 	Container kube-controller-manager ready: true, restart count 5
  Apr 15 08:31:01.253: INFO: kube-flannel-ds-k86c4 from kube-system started at 2024-04-15 08:21:18 +0000 UTC (1 container statuses recorded)
  Apr 15 08:31:01.253: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 08:31:01.253: INFO: kube-proxy-8g55q from kube-system started at 2024-04-15 08:21:17 +0000 UTC (1 container statuses recorded)
  Apr 15 08:31:01.253: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 08:31:01.253: INFO: kube-scheduler-ahz3daisheng-1 from kube-system started at 2024-04-15 06:17:06 +0000 UTC (1 container statuses recorded)
  Apr 15 08:31:01.253: INFO: 	Container kube-scheduler ready: true, restart count 5
  Apr 15 08:31:01.253: INFO: sonobuoy-systemd-logs-daemon-set-10fc9ca8ef6747d1-9lf7d from sonobuoy started at 2024-04-15 08:22:14 +0000 UTC (2 container statuses recorded)
  Apr 15 08:31:01.253: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 08:31:01.253: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 15 08:31:01.253: INFO: 
  Logging pods the apiserver thinks is on node ahz3daisheng-2 before test
  Apr 15 08:31:01.270: INFO: kube-addon-manager-ahz3daisheng-2 from kube-system started at 2024-04-15 06:17:33 +0000 UTC (1 container statuses recorded)
  Apr 15 08:31:01.271: INFO: 	Container kube-addon-manager ready: true, restart count 5
  Apr 15 08:31:01.271: INFO: kube-apiserver-ahz3daisheng-2 from kube-system started at 2024-04-15 06:17:33 +0000 UTC (1 container statuses recorded)
  Apr 15 08:31:01.271: INFO: 	Container kube-apiserver ready: true, restart count 5
  Apr 15 08:31:01.272: INFO: kube-controller-manager-ahz3daisheng-2 from kube-system started at 2024-04-15 06:17:33 +0000 UTC (1 container statuses recorded)
  Apr 15 08:31:01.272: INFO: 	Container kube-controller-manager ready: true, restart count 5
  Apr 15 08:31:01.272: INFO: kube-flannel-ds-5c2gp from kube-system started at 2024-04-15 08:21:17 +0000 UTC (1 container statuses recorded)
  Apr 15 08:31:01.273: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 08:31:01.273: INFO: kube-proxy-llrhn from kube-system started at 2024-04-15 08:21:16 +0000 UTC (1 container statuses recorded)
  Apr 15 08:31:01.273: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 08:31:01.273: INFO: kube-scheduler-ahz3daisheng-2 from kube-system started at 2024-04-15 06:17:33 +0000 UTC (1 container statuses recorded)
  Apr 15 08:31:01.274: INFO: 	Container kube-scheduler ready: true, restart count 5
  Apr 15 08:31:01.274: INFO: sonobuoy-systemd-logs-daemon-set-10fc9ca8ef6747d1-r768m from sonobuoy started at 2024-04-15 08:22:14 +0000 UTC (2 container statuses recorded)
  Apr 15 08:31:01.274: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 08:31:01.275: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 15 08:31:01.275: INFO: 
  Logging pods the apiserver thinks is on node ahz3daisheng-3 before test
  Apr 15 08:31:01.290: INFO: coredns-5d78c9869d-zxbll from kube-system started at 2024-04-15 08:21:15 +0000 UTC (1 container statuses recorded)
  Apr 15 08:31:01.290: INFO: 	Container coredns ready: true, restart count 0
  Apr 15 08:31:01.290: INFO: kube-flannel-ds-2d8nc from kube-system started at 2024-04-15 08:21:17 +0000 UTC (1 container statuses recorded)
  Apr 15 08:31:01.290: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 08:31:01.290: INFO: kube-proxy-66n59 from kube-system started at 2024-04-15 08:21:16 +0000 UTC (1 container statuses recorded)
  Apr 15 08:31:01.290: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 08:31:01.290: INFO: sonobuoy from sonobuoy started at 2024-04-15 08:22:13 +0000 UTC (1 container statuses recorded)
  Apr 15 08:31:01.290: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 15 08:31:01.290: INFO: sonobuoy-e2e-job-dd62b7a77ff54dbf from sonobuoy started at 2024-04-15 08:22:14 +0000 UTC (2 container statuses recorded)
  Apr 15 08:31:01.290: INFO: 	Container e2e ready: true, restart count 0
  Apr 15 08:31:01.290: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 08:31:01.290: INFO: sonobuoy-systemd-logs-daemon-set-10fc9ca8ef6747d1-qhkjw from sonobuoy started at 2024-04-15 08:22:14 +0000 UTC (2 container statuses recorded)
  Apr 15 08:31:01.290: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 08:31:01.290: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node ahz3daisheng-1 @ 04/15/24 08:31:01.336
  STEP: verifying the node has the label node ahz3daisheng-2 @ 04/15/24 08:31:01.367
  STEP: verifying the node has the label node ahz3daisheng-3 @ 04/15/24 08:31:01.393
  Apr 15 08:31:01.421: INFO: Pod rs-szglj requesting resource cpu=0m on Node ahz3daisheng-1
  Apr 15 08:31:01.421: INFO: Pod coredns-5d78c9869d-hwtz8 requesting resource cpu=100m on Node ahz3daisheng-1
  Apr 15 08:31:01.421: INFO: Pod coredns-5d78c9869d-zxbll requesting resource cpu=100m on Node ahz3daisheng-3
  Apr 15 08:31:01.421: INFO: Pod kube-addon-manager-ahz3daisheng-1 requesting resource cpu=5m on Node ahz3daisheng-1
  Apr 15 08:31:01.421: INFO: Pod kube-addon-manager-ahz3daisheng-2 requesting resource cpu=5m on Node ahz3daisheng-2
  Apr 15 08:31:01.421: INFO: Pod kube-apiserver-ahz3daisheng-1 requesting resource cpu=250m on Node ahz3daisheng-1
  Apr 15 08:31:01.421: INFO: Pod kube-apiserver-ahz3daisheng-2 requesting resource cpu=250m on Node ahz3daisheng-2
  Apr 15 08:31:01.421: INFO: Pod kube-controller-manager-ahz3daisheng-1 requesting resource cpu=200m on Node ahz3daisheng-1
  Apr 15 08:31:01.421: INFO: Pod kube-controller-manager-ahz3daisheng-2 requesting resource cpu=200m on Node ahz3daisheng-2
  Apr 15 08:31:01.421: INFO: Pod kube-flannel-ds-2d8nc requesting resource cpu=100m on Node ahz3daisheng-3
  Apr 15 08:31:01.421: INFO: Pod kube-flannel-ds-5c2gp requesting resource cpu=100m on Node ahz3daisheng-2
  Apr 15 08:31:01.421: INFO: Pod kube-flannel-ds-k86c4 requesting resource cpu=100m on Node ahz3daisheng-1
  Apr 15 08:31:01.421: INFO: Pod kube-proxy-66n59 requesting resource cpu=0m on Node ahz3daisheng-3
  Apr 15 08:31:01.421: INFO: Pod kube-proxy-8g55q requesting resource cpu=0m on Node ahz3daisheng-1
  Apr 15 08:31:01.421: INFO: Pod kube-proxy-llrhn requesting resource cpu=0m on Node ahz3daisheng-2
  Apr 15 08:31:01.421: INFO: Pod kube-scheduler-ahz3daisheng-1 requesting resource cpu=100m on Node ahz3daisheng-1
  Apr 15 08:31:01.421: INFO: Pod kube-scheduler-ahz3daisheng-2 requesting resource cpu=100m on Node ahz3daisheng-2
  Apr 15 08:31:01.421: INFO: Pod sonobuoy requesting resource cpu=0m on Node ahz3daisheng-3
  Apr 15 08:31:01.421: INFO: Pod sonobuoy-e2e-job-dd62b7a77ff54dbf requesting resource cpu=0m on Node ahz3daisheng-3
  Apr 15 08:31:01.421: INFO: Pod sonobuoy-systemd-logs-daemon-set-10fc9ca8ef6747d1-9lf7d requesting resource cpu=0m on Node ahz3daisheng-1
  Apr 15 08:31:01.421: INFO: Pod sonobuoy-systemd-logs-daemon-set-10fc9ca8ef6747d1-qhkjw requesting resource cpu=0m on Node ahz3daisheng-3
  Apr 15 08:31:01.421: INFO: Pod sonobuoy-systemd-logs-daemon-set-10fc9ca8ef6747d1-r768m requesting resource cpu=0m on Node ahz3daisheng-2
  STEP: Starting Pods to consume most of the cluster CPU. @ 04/15/24 08:31:01.421
  Apr 15 08:31:01.421: INFO: Creating a pod which consumes cpu=980m on Node ahz3daisheng-3
  Apr 15 08:31:01.439: INFO: Creating a pod which consumes cpu=591m on Node ahz3daisheng-1
  Apr 15 08:31:01.450: INFO: Creating a pod which consumes cpu=661m on Node ahz3daisheng-2
  STEP: Creating another pod that requires unavailable amount of CPU. @ 04/15/24 08:31:05.497
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-3ede0057-c7be-4b6b-91bc-e7868912166a.17c666e98be56f57], Reason = [Scheduled], Message = [Successfully assigned sched-pred-715/filler-pod-3ede0057-c7be-4b6b-91bc-e7868912166a to ahz3daisheng-1] @ 04/15/24 08:31:05.508
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-3ede0057-c7be-4b6b-91bc-e7868912166a.17c666e9e4053a3a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/15/24 08:31:05.508
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-3ede0057-c7be-4b6b-91bc-e7868912166a.17c666e9ee1cc14a], Reason = [Created], Message = [Created container filler-pod-3ede0057-c7be-4b6b-91bc-e7868912166a] @ 04/15/24 08:31:05.508
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-3ede0057-c7be-4b6b-91bc-e7868912166a.17c666e9efeaea50], Reason = [Started], Message = [Started container filler-pod-3ede0057-c7be-4b6b-91bc-e7868912166a] @ 04/15/24 08:31:05.508
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-51826d47-b16b-4b3c-ac98-d1a5bb321f95.17c666e98989fd94], Reason = [Scheduled], Message = [Successfully assigned sched-pred-715/filler-pod-51826d47-b16b-4b3c-ac98-d1a5bb321f95 to ahz3daisheng-3] @ 04/15/24 08:31:05.508
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-51826d47-b16b-4b3c-ac98-d1a5bb321f95.17c666e9a546352a], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/15/24 08:31:05.508
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-51826d47-b16b-4b3c-ac98-d1a5bb321f95.17c666e9ad479f5f], Reason = [Created], Message = [Created container filler-pod-51826d47-b16b-4b3c-ac98-d1a5bb321f95] @ 04/15/24 08:31:05.508
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-51826d47-b16b-4b3c-ac98-d1a5bb321f95.17c666e9b081a0f4], Reason = [Started], Message = [Started container filler-pod-51826d47-b16b-4b3c-ac98-d1a5bb321f95] @ 04/15/24 08:31:05.508
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-74bec002-fe2b-42d5-b07e-cd46c4e5ac61.17c666e98c03758a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-715/filler-pod-74bec002-fe2b-42d5-b07e-cd46c4e5ac61 to ahz3daisheng-2] @ 04/15/24 08:31:05.508
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-74bec002-fe2b-42d5-b07e-cd46c4e5ac61.17c666e9a5a9ffea], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 04/15/24 08:31:05.508
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-74bec002-fe2b-42d5-b07e-cd46c4e5ac61.17c666e9aeb507c6], Reason = [Created], Message = [Created container filler-pod-74bec002-fe2b-42d5-b07e-cd46c4e5ac61] @ 04/15/24 08:31:05.508
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-74bec002-fe2b-42d5-b07e-cd46c4e5ac61.17c666e9b5811376], Reason = [Started], Message = [Started container filler-pod-74bec002-fe2b-42d5-b07e-cd46c4e5ac61] @ 04/15/24 08:31:05.508
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.17c666ea7a99636f], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] @ 04/15/24 08:31:05.554
  STEP: removing the label node off the node ahz3daisheng-1 @ 04/15/24 08:31:06.534
  STEP: verifying the node doesn't have the label node @ 04/15/24 08:31:06.563
  STEP: removing the label node off the node ahz3daisheng-2 @ 04/15/24 08:31:06.571
  STEP: verifying the node doesn't have the label node @ 04/15/24 08:31:06.596
  STEP: removing the label node off the node ahz3daisheng-3 @ 04/15/24 08:31:06.606
  STEP: verifying the node doesn't have the label node @ 04/15/24 08:31:06.635
  Apr 15 08:31:06.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-715" for this suite. @ 04/15/24 08:31:06.658
• [5.575 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 04/15/24 08:31:06.692
  Apr 15 08:31:06.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 08:31:06.695
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:31:06.765
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:31:06.777
  STEP: creating Agnhost RC @ 04/15/24 08:31:06.8
  Apr 15 08:31:06.800: INFO: namespace kubectl-1921
  Apr 15 08:31:06.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-1921 create -f -'
  Apr 15 08:31:07.732: INFO: stderr: ""
  Apr 15 08:31:07.732: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/15/24 08:31:07.732
  Apr 15 08:31:08.740: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 08:31:08.740: INFO: Found 0 / 1
  Apr 15 08:31:09.742: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 08:31:09.742: INFO: Found 1 / 1
  Apr 15 08:31:09.742: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Apr 15 08:31:09.749: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 08:31:09.749: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 15 08:31:09.749: INFO: wait on agnhost-primary startup in kubectl-1921 
  Apr 15 08:31:09.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-1921 logs agnhost-primary-sm7gq agnhost-primary'
  Apr 15 08:31:09.936: INFO: stderr: ""
  Apr 15 08:31:09.936: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 04/15/24 08:31:09.936
  Apr 15 08:31:09.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-1921 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Apr 15 08:31:10.109: INFO: stderr: ""
  Apr 15 08:31:10.109: INFO: stdout: "service/rm2 exposed\n"
  Apr 15 08:31:10.125: INFO: Service rm2 in namespace kubectl-1921 found.
  STEP: exposing service @ 04/15/24 08:31:12.178
  Apr 15 08:31:12.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-1921 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Apr 15 08:31:12.499: INFO: stderr: ""
  Apr 15 08:31:12.500: INFO: stdout: "service/rm3 exposed\n"
  Apr 15 08:31:12.525: INFO: Service rm3 in namespace kubectl-1921 found.
  Apr 15 08:31:14.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1921" for this suite. @ 04/15/24 08:31:14.548
• [7.872 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 04/15/24 08:31:14.585
  Apr 15 08:31:14.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename gc @ 04/15/24 08:31:14.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:31:14.621
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:31:14.626
  STEP: create the rc @ 04/15/24 08:31:14.631
  W0415 08:31:14.645469      14 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 04/15/24 08:31:19.669
  STEP: wait for all pods to be garbage collected @ 04/15/24 08:31:19.688
  STEP: Gathering metrics @ 04/15/24 08:31:24.732
  Apr 15 08:31:24.917: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 15 08:31:24.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6221" for this suite. @ 04/15/24 08:31:24.928
• [10.365 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 04/15/24 08:31:24.964
  Apr 15 08:31:24.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename configmap @ 04/15/24 08:31:24.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:31:24.996
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:31:25
  Apr 15 08:31:25.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4971" for this suite. @ 04/15/24 08:31:25.102
• [0.154 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 04/15/24 08:31:25.122
  Apr 15 08:31:25.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename disruption @ 04/15/24 08:31:25.126
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:31:25.165
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:31:25.17
  STEP: Waiting for the pdb to be processed @ 04/15/24 08:31:25.186
  STEP: Waiting for all pods to be running @ 04/15/24 08:31:27.269
  Apr 15 08:31:27.279: INFO: running pods: 0 < 3
  Apr 15 08:31:29.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4632" for this suite. @ 04/15/24 08:31:29.305
• [4.196 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 04/15/24 08:31:29.324
  Apr 15 08:31:29.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename webhook @ 04/15/24 08:31:29.327
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:31:29.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:31:29.356
  STEP: Setting up server cert @ 04/15/24 08:31:29.397
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 08:31:29.905
  STEP: Deploying the webhook pod @ 04/15/24 08:31:29.925
  STEP: Wait for the deployment to be ready @ 04/15/24 08:31:29.956
  Apr 15 08:31:29.973: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/15/24 08:31:31.999
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 08:31:32.015
  Apr 15 08:31:33.016: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 15 08:31:33.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 04/15/24 08:31:33.542
  STEP: Creating a custom resource that should be denied by the webhook @ 04/15/24 08:31:33.573
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 04/15/24 08:31:35.653
  STEP: Updating the custom resource with disallowed data should be denied @ 04/15/24 08:31:35.663
  STEP: Deleting the custom resource should be denied @ 04/15/24 08:31:35.681
  STEP: Remove the offending key and value from the custom resource data @ 04/15/24 08:31:35.693
  STEP: Deleting the updated custom resource should be successful @ 04/15/24 08:31:35.711
  Apr 15 08:31:35.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3971" for this suite. @ 04/15/24 08:31:36.428
  STEP: Destroying namespace "webhook-markers-4731" for this suite. @ 04/15/24 08:31:36.45
• [7.144 seconds]
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 04/15/24 08:31:36.469
  Apr 15 08:31:36.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename controllerrevisions @ 04/15/24 08:31:36.472
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:31:36.514
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:31:36.52
  STEP: Creating DaemonSet "e2e-kjm9h-daemon-set" @ 04/15/24 08:31:36.612
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/15/24 08:31:36.629
  Apr 15 08:31:36.659: INFO: Number of nodes with available pods controlled by daemonset e2e-kjm9h-daemon-set: 0
  Apr 15 08:31:36.660: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  Apr 15 08:31:37.703: INFO: Number of nodes with available pods controlled by daemonset e2e-kjm9h-daemon-set: 0
  Apr 15 08:31:37.703: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  Apr 15 08:31:38.687: INFO: Number of nodes with available pods controlled by daemonset e2e-kjm9h-daemon-set: 3
  Apr 15 08:31:38.688: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-kjm9h-daemon-set
  STEP: Confirm DaemonSet "e2e-kjm9h-daemon-set" successfully created with "daemonset-name=e2e-kjm9h-daemon-set" label @ 04/15/24 08:31:38.696
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-kjm9h-daemon-set" @ 04/15/24 08:31:38.717
  Apr 15 08:31:38.730: INFO: Located ControllerRevision: "e2e-kjm9h-daemon-set-56fc567d6d"
  STEP: Patching ControllerRevision "e2e-kjm9h-daemon-set-56fc567d6d" @ 04/15/24 08:31:38.738
  Apr 15 08:31:38.756: INFO: e2e-kjm9h-daemon-set-56fc567d6d has been patched
  STEP: Create a new ControllerRevision @ 04/15/24 08:31:38.756
  Apr 15 08:31:38.770: INFO: Created ControllerRevision: e2e-kjm9h-daemon-set-7fccb77c74
  STEP: Confirm that there are two ControllerRevisions @ 04/15/24 08:31:38.77
  Apr 15 08:31:38.770: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 15 08:31:38.784: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-kjm9h-daemon-set-56fc567d6d" @ 04/15/24 08:31:38.784
  STEP: Confirm that there is only one ControllerRevision @ 04/15/24 08:31:38.799
  Apr 15 08:31:38.799: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 15 08:31:38.807: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-kjm9h-daemon-set-7fccb77c74" @ 04/15/24 08:31:38.818
  Apr 15 08:31:38.873: INFO: e2e-kjm9h-daemon-set-7fccb77c74 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 04/15/24 08:31:38.873
  W0415 08:31:38.901313      14 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 04/15/24 08:31:38.901
  Apr 15 08:31:38.901: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 15 08:31:39.909: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 15 08:31:39.924: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-kjm9h-daemon-set-7fccb77c74=updated" @ 04/15/24 08:31:39.925
  STEP: Confirm that there is only one ControllerRevision @ 04/15/24 08:31:39.953
  Apr 15 08:31:39.953: INFO: Requesting list of ControllerRevisions to confirm quantity
  Apr 15 08:31:39.962: INFO: Found 1 ControllerRevisions
  Apr 15 08:31:39.976: INFO: ControllerRevision "e2e-kjm9h-daemon-set-57487554c9" has revision 3
  STEP: Deleting DaemonSet "e2e-kjm9h-daemon-set" @ 04/15/24 08:31:39.994
  STEP: deleting DaemonSet.extensions e2e-kjm9h-daemon-set in namespace controllerrevisions-3415, will wait for the garbage collector to delete the pods @ 04/15/24 08:31:39.994
  Apr 15 08:31:40.111: INFO: Deleting DaemonSet.extensions e2e-kjm9h-daemon-set took: 43.699876ms
  Apr 15 08:31:40.313: INFO: Terminating DaemonSet.extensions e2e-kjm9h-daemon-set pods took: 201.618633ms
  Apr 15 08:31:42.123: INFO: Number of nodes with available pods controlled by daemonset e2e-kjm9h-daemon-set: 0
  Apr 15 08:31:42.123: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-kjm9h-daemon-set
  Apr 15 08:31:42.129: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"185680"},"items":null}

  Apr 15 08:31:42.136: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"185680"},"items":null}

  Apr 15 08:31:42.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-3415" for this suite. @ 04/15/24 08:31:42.177
• [5.721 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 04/15/24 08:31:42.193
  Apr 15 08:31:42.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename containers @ 04/15/24 08:31:42.196
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:31:42.242
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:31:42.249
  STEP: Creating a pod to test override all @ 04/15/24 08:31:42.253
  STEP: Saw pod success @ 04/15/24 08:31:46.293
  Apr 15 08:31:46.298: INFO: Trying to get logs from node ahz3daisheng-3 pod client-containers-5f75e828-2169-47de-96b2-4d771a8d4032 container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 08:31:46.329
  Apr 15 08:31:46.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-3970" for this suite. @ 04/15/24 08:31:46.369
• [4.194 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 04/15/24 08:31:46.405
  Apr 15 08:31:46.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename sched-preemption @ 04/15/24 08:31:46.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:31:46.446
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:31:46.453
  Apr 15 08:31:46.484: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 15 08:32:46.534: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 04/15/24 08:32:46.546
  Apr 15 08:32:46.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename sched-preemption-path @ 04/15/24 08:32:46.549
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:32:46.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:32:46.599
  STEP: Finding an available node @ 04/15/24 08:32:46.606
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/15/24 08:32:46.606
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/15/24 08:32:48.655
  Apr 15 08:32:48.705: INFO: found a healthy node: ahz3daisheng-3
  Apr 15 08:32:54.884: INFO: pods created so far: [1 1 1]
  Apr 15 08:32:54.884: INFO: length of pods created so far: 3
  Apr 15 08:32:58.904: INFO: pods created so far: [2 2 1]
  Apr 15 08:33:05.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 08:33:06.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-8596" for this suite. @ 04/15/24 08:33:06.163
  STEP: Destroying namespace "sched-preemption-56" for this suite. @ 04/15/24 08:33:06.18
• [79.789 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 04/15/24 08:33:06.204
  Apr 15 08:33:06.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename server-version @ 04/15/24 08:33:06.207
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:33:06.258
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:33:06.265
  STEP: Request ServerVersion @ 04/15/24 08:33:06.27
  STEP: Confirm major version @ 04/15/24 08:33:06.272
  Apr 15 08:33:06.273: INFO: Major version: 1
  STEP: Confirm minor version @ 04/15/24 08:33:06.273
  Apr 15 08:33:06.273: INFO: cleanMinorVersion: 27
  Apr 15 08:33:06.274: INFO: Minor version: 27
  Apr 15 08:33:06.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-1889" for this suite. @ 04/15/24 08:33:06.285
• [0.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 04/15/24 08:33:06.307
  Apr 15 08:33:06.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename security-context @ 04/15/24 08:33:06.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:33:06.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:33:06.354
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 04/15/24 08:33:06.36
  STEP: Saw pod success @ 04/15/24 08:33:10.406
  Apr 15 08:33:10.420: INFO: Trying to get logs from node ahz3daisheng-3 pod security-context-2c5aa39c-251e-4ac7-b253-8c9f0da40e85 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 08:33:10.442
  Apr 15 08:33:10.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-3296" for this suite. @ 04/15/24 08:33:10.495
• [4.213 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 04/15/24 08:33:10.521
  Apr 15 08:33:10.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename dns @ 04/15/24 08:33:10.524
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:33:10.565
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:33:10.576
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3796.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3796.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 04/15/24 08:33:10.582
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3796.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3796.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 04/15/24 08:33:10.582
  STEP: creating a pod to probe /etc/hosts @ 04/15/24 08:33:10.582
  STEP: submitting the pod to kubernetes @ 04/15/24 08:33:10.583
  STEP: retrieving the pod @ 04/15/24 08:33:12.622
  STEP: looking for the results for each expected name from probers @ 04/15/24 08:33:12.629
  Apr 15 08:33:12.669: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-3796/dns-test-a0d271d2-70e1-409c-876d-d3941b1f1800: the server could not find the requested resource (get pods dns-test-a0d271d2-70e1-409c-876d-d3941b1f1800)
  Apr 15 08:33:12.669: INFO: Lookups using dns-3796/dns-test-a0d271d2-70e1-409c-876d-d3941b1f1800 failed for: [jessie_hosts@dns-querier-1]

  Apr 15 08:33:17.720: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-3796/dns-test-a0d271d2-70e1-409c-876d-d3941b1f1800: the server could not find the requested resource (get pods dns-test-a0d271d2-70e1-409c-876d-d3941b1f1800)
  Apr 15 08:33:17.722: INFO: Lookups using dns-3796/dns-test-a0d271d2-70e1-409c-876d-d3941b1f1800 failed for: [jessie_hosts@dns-querier-1]

  Apr 15 08:33:22.713: INFO: DNS probes using dns-3796/dns-test-a0d271d2-70e1-409c-876d-d3941b1f1800 succeeded

  Apr 15 08:33:22.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 08:33:22.729
  STEP: Destroying namespace "dns-3796" for this suite. @ 04/15/24 08:33:22.758
• [12.254 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 04/15/24 08:33:22.778
  Apr 15 08:33:22.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 08:33:22.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:33:22.833
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:33:22.838
  STEP: Counting existing ResourceQuota @ 04/15/24 08:33:22.845
  STEP: Creating a ResourceQuota @ 04/15/24 08:33:27.853
  STEP: Ensuring resource quota status is calculated @ 04/15/24 08:33:27.87
  STEP: Creating a Pod that fits quota @ 04/15/24 08:33:29.882
  STEP: Ensuring ResourceQuota status captures the pod usage @ 04/15/24 08:33:29.933
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 04/15/24 08:33:31.943
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 04/15/24 08:33:31.95
  STEP: Ensuring a pod cannot update its resource requirements @ 04/15/24 08:33:31.956
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 04/15/24 08:33:31.968
  STEP: Deleting the pod @ 04/15/24 08:33:33.979
  STEP: Ensuring resource quota status released the pod usage @ 04/15/24 08:33:34.013
  Apr 15 08:33:36.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4405" for this suite. @ 04/15/24 08:33:36.036
• [13.272 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 04/15/24 08:33:36.055
  Apr 15 08:33:36.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 08:33:36.058
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:33:36.115
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:33:36.123
  STEP: Creating a pod to test downward api env vars @ 04/15/24 08:33:36.129
  STEP: Saw pod success @ 04/15/24 08:33:40.193
  Apr 15 08:33:40.200: INFO: Trying to get logs from node ahz3daisheng-3 pod downward-api-c60e2785-2933-493b-b335-79901b1e2b07 container dapi-container: <nil>
  STEP: delete the pod @ 04/15/24 08:33:40.225
  Apr 15 08:33:40.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4835" for this suite. @ 04/15/24 08:33:40.269
• [4.232 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:875
  STEP: Creating a kubernetes client @ 04/15/24 08:33:40.292
  Apr 15 08:33:40.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename daemonsets @ 04/15/24 08:33:40.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:33:40.334
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:33:40.343
  STEP: Creating simple DaemonSet "daemon-set" @ 04/15/24 08:33:40.421
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/15/24 08:33:40.437
  Apr 15 08:33:40.465: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 08:33:40.465: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  Apr 15 08:33:41.504: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 08:33:41.504: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  Apr 15 08:33:42.485: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 15 08:33:42.485: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  Apr 15 08:33:43.488: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 15 08:33:43.488: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 04/15/24 08:33:43.495
  Apr 15 08:33:43.503: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 04/15/24 08:33:43.503
  Apr 15 08:33:43.530: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 04/15/24 08:33:43.53
  Apr 15 08:33:43.536: INFO: Observed &DaemonSet event: ADDED
  Apr 15 08:33:43.537: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 08:33:43.538: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 08:33:43.540: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 08:33:43.541: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 08:33:43.542: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 08:33:43.542: INFO: Found daemon set daemon-set in namespace daemonsets-9416 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 15 08:33:43.543: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 04/15/24 08:33:43.544
  STEP: watching for the daemon set status to be patched @ 04/15/24 08:33:43.561
  Apr 15 08:33:43.566: INFO: Observed &DaemonSet event: ADDED
  Apr 15 08:33:43.567: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 08:33:43.567: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 08:33:43.568: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 08:33:43.569: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 08:33:43.569: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 08:33:43.570: INFO: Observed daemon set daemon-set in namespace daemonsets-9416 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 15 08:33:43.570: INFO: Observed &DaemonSet event: MODIFIED
  Apr 15 08:33:43.570: INFO: Found daemon set daemon-set in namespace daemonsets-9416 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Apr 15 08:33:43.571: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 04/15/24 08:33:43.58
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9416, will wait for the garbage collector to delete the pods @ 04/15/24 08:33:43.58
  Apr 15 08:33:43.655: INFO: Deleting DaemonSet.extensions daemon-set took: 17.463733ms
  Apr 15 08:33:43.756: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.21584ms
  Apr 15 08:33:45.765: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 08:33:45.765: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 15 08:33:45.772: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"186301"},"items":null}

  Apr 15 08:33:45.777: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"186301"},"items":null}

  Apr 15 08:33:45.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9416" for this suite. @ 04/15/24 08:33:45.827
• [5.548 seconds]
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:305
  STEP: Creating a kubernetes client @ 04/15/24 08:33:45.841
  Apr 15 08:33:45.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename daemonsets @ 04/15/24 08:33:45.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:33:45.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:33:45.903
  STEP: Creating a simple DaemonSet "daemon-set" @ 04/15/24 08:33:45.958
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/15/24 08:33:45.972
  Apr 15 08:33:45.996: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 08:33:45.997: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  Apr 15 08:33:47.019: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 08:33:47.019: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  Apr 15 08:33:48.016: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 15 08:33:48.016: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  Apr 15 08:33:49.018: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 15 08:33:49.018: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 04/15/24 08:33:49.034
  Apr 15 08:33:49.090: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 15 08:33:49.090: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  Apr 15 08:33:50.109: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 15 08:33:50.109: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  Apr 15 08:33:51.108: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 15 08:33:51.108: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 04/15/24 08:33:51.108
  STEP: Deleting DaemonSet "daemon-set" @ 04/15/24 08:33:51.119
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2822, will wait for the garbage collector to delete the pods @ 04/15/24 08:33:51.12
  Apr 15 08:33:51.196: INFO: Deleting DaemonSet.extensions daemon-set took: 18.345815ms
  Apr 15 08:33:51.297: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.733956ms
  Apr 15 08:33:52.808: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 08:33:52.808: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 15 08:33:52.814: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"186415"},"items":null}

  Apr 15 08:33:52.819: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"186415"},"items":null}

  Apr 15 08:33:52.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2822" for this suite. @ 04/15/24 08:33:52.907
• [7.081 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 04/15/24 08:33:52.923
  Apr 15 08:33:52.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 08:33:52.927
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:33:52.957
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:33:52.961
  STEP: creating all guestbook components @ 04/15/24 08:33:52.965
  Apr 15 08:33:52.965: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Apr 15 08:33:52.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-4613 create -f -'
  Apr 15 08:33:53.843: INFO: stderr: ""
  Apr 15 08:33:53.843: INFO: stdout: "service/agnhost-replica created\n"
  Apr 15 08:33:53.843: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Apr 15 08:33:53.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-4613 create -f -'
  Apr 15 08:33:54.464: INFO: stderr: ""
  Apr 15 08:33:54.464: INFO: stdout: "service/agnhost-primary created\n"
  Apr 15 08:33:54.465: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Apr 15 08:33:54.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-4613 create -f -'
  Apr 15 08:33:55.071: INFO: stderr: ""
  Apr 15 08:33:55.072: INFO: stdout: "service/frontend created\n"
  Apr 15 08:33:55.072: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.47
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Apr 15 08:33:55.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-4613 create -f -'
  Apr 15 08:33:55.630: INFO: stderr: ""
  Apr 15 08:33:55.630: INFO: stdout: "deployment.apps/frontend created\n"
  Apr 15 08:33:55.630: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.47
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Apr 15 08:33:55.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-4613 create -f -'
  Apr 15 08:33:56.374: INFO: stderr: ""
  Apr 15 08:33:56.374: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Apr 15 08:33:56.374: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.47
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Apr 15 08:33:56.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-4613 create -f -'
  Apr 15 08:33:57.368: INFO: stderr: ""
  Apr 15 08:33:57.369: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 04/15/24 08:33:57.369
  Apr 15 08:33:57.370: INFO: Waiting for all frontend pods to be Running.
  Apr 15 08:33:57.421: INFO: Waiting for frontend to serve content.
  Apr 15 08:33:57.487: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
  Apr 15 08:34:02.518: INFO: Trying to add a new entry to the guestbook.
  Apr 15 08:34:02.562: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 04/15/24 08:34:02.585
  Apr 15 08:34:02.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-4613 delete --grace-period=0 --force -f -'
  Apr 15 08:34:02.785: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 15 08:34:02.785: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 04/15/24 08:34:02.786
  Apr 15 08:34:02.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-4613 delete --grace-period=0 --force -f -'
  Apr 15 08:34:03.007: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 15 08:34:03.007: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 04/15/24 08:34:03.007
  Apr 15 08:34:03.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-4613 delete --grace-period=0 --force -f -'
  Apr 15 08:34:03.224: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 15 08:34:03.224: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 04/15/24 08:34:03.224
  Apr 15 08:34:03.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-4613 delete --grace-period=0 --force -f -'
  Apr 15 08:34:03.390: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 15 08:34:03.390: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 04/15/24 08:34:03.39
  Apr 15 08:34:03.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-4613 delete --grace-period=0 --force -f -'
  Apr 15 08:34:03.634: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 15 08:34:03.635: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 04/15/24 08:34:03.635
  Apr 15 08:34:03.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-4613 delete --grace-period=0 --force -f -'
  Apr 15 08:34:03.909: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 15 08:34:03.909: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Apr 15 08:34:03.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4613" for this suite. @ 04/15/24 08:34:03.926
• [11.030 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 04/15/24 08:34:03.953
  Apr 15 08:34:03.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename configmap @ 04/15/24 08:34:03.956
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:34:04.005
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:34:04.015
  STEP: Creating configMap with name configmap-test-volume-map-883d4212-98b9-4af9-aa1a-4946cc7274f2 @ 04/15/24 08:34:04.026
  STEP: Creating a pod to test consume configMaps @ 04/15/24 08:34:04.049
  STEP: Saw pod success @ 04/15/24 08:34:08.11
  Apr 15 08:34:08.118: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-configmaps-aa46b619-fe6a-466c-933c-7bd6b3525b4c container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 08:34:08.136
  Apr 15 08:34:08.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3260" for this suite. @ 04/15/24 08:34:08.17
• [4.228 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 04/15/24 08:34:08.182
  Apr 15 08:34:08.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 08:34:08.184
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:34:08.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:34:08.22
  STEP: Creating pod busybox-32b4994b-036e-4089-9a77-e574ade13e9c in namespace container-probe-9506 @ 04/15/24 08:34:08.225
  Apr 15 08:34:10.266: INFO: Started pod busybox-32b4994b-036e-4089-9a77-e574ade13e9c in namespace container-probe-9506
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/15/24 08:34:10.266
  Apr 15 08:34:10.273: INFO: Initial restart count of pod busybox-32b4994b-036e-4089-9a77-e574ade13e9c is 0
  Apr 15 08:35:00.504: INFO: Restart count of pod container-probe-9506/busybox-32b4994b-036e-4089-9a77-e574ade13e9c is now 1 (50.230836434s elapsed)
  Apr 15 08:35:00.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 08:35:00.516
  STEP: Destroying namespace "container-probe-9506" for this suite. @ 04/15/24 08:35:00.543
• [52.378 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:329
  STEP: Creating a kubernetes client @ 04/15/24 08:35:00.57
  Apr 15 08:35:00.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename statefulset @ 04/15/24 08:35:00.572
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:35:00.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:35:00.619
  STEP: Creating service test in namespace statefulset-269 @ 04/15/24 08:35:00.631
  STEP: Creating a new StatefulSet @ 04/15/24 08:35:00.677
  Apr 15 08:35:00.723: INFO: Found 0 stateful pods, waiting for 3
  Apr 15 08:35:10.740: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 08:35:10.741: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 08:35:10.741: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 04/15/24 08:35:10.762
  Apr 15 08:35:10.793: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 04/15/24 08:35:10.794
  STEP: Not applying an update when the partition is greater than the number of replicas @ 04/15/24 08:35:20.849
  STEP: Performing a canary update @ 04/15/24 08:35:20.849
  Apr 15 08:35:20.882: INFO: Updating stateful set ss2
  Apr 15 08:35:20.902: INFO: Waiting for Pod statefulset-269/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 04/15/24 08:35:30.921
  Apr 15 08:35:31.009: INFO: Found 1 stateful pods, waiting for 3
  Apr 15 08:35:41.023: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 08:35:41.023: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 08:35:41.024: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 04/15/24 08:35:41.044
  Apr 15 08:35:41.083: INFO: Updating stateful set ss2
  Apr 15 08:35:41.113: INFO: Waiting for Pod statefulset-269/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Apr 15 08:35:51.168: INFO: Updating stateful set ss2
  Apr 15 08:35:51.196: INFO: Waiting for StatefulSet statefulset-269/ss2 to complete update
  Apr 15 08:35:51.197: INFO: Waiting for Pod statefulset-269/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Apr 15 08:36:01.215: INFO: Deleting all statefulset in ns statefulset-269
  Apr 15 08:36:01.220: INFO: Scaling statefulset ss2 to 0
  Apr 15 08:36:11.256: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 08:36:11.261: INFO: Deleting statefulset ss2
  Apr 15 08:36:11.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-269" for this suite. @ 04/15/24 08:36:11.306
• [70.749 seconds]
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 04/15/24 08:36:11.319
  Apr 15 08:36:11.319: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename svcaccounts @ 04/15/24 08:36:11.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:36:11.365
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:36:11.37
  STEP: creating a ServiceAccount @ 04/15/24 08:36:11.374
  STEP: watching for the ServiceAccount to be added @ 04/15/24 08:36:11.391
  STEP: patching the ServiceAccount @ 04/15/24 08:36:11.401
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 04/15/24 08:36:11.412
  STEP: deleting the ServiceAccount @ 04/15/24 08:36:11.419
  Apr 15 08:36:11.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-555" for this suite. @ 04/15/24 08:36:11.454
• [0.147 seconds]
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 04/15/24 08:36:11.468
  Apr 15 08:36:11.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubelet-test @ 04/15/24 08:36:11.472
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:36:11.504
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:36:11.51
  Apr 15 08:36:13.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8858" for this suite. @ 04/15/24 08:36:13.604
• [2.145 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 04/15/24 08:36:13.617
  Apr 15 08:36:13.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 08:36:13.619
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:36:13.67
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:36:13.675
  Apr 15 08:37:13.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-1407" for this suite. @ 04/15/24 08:37:13.714
• [60.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 04/15/24 08:37:13.734
  Apr 15 08:37:13.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/15/24 08:37:13.737
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:37:13.777
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:37:13.784
  Apr 15 08:37:13.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/15/24 08:37:15.713
  Apr 15 08:37:15.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4358 --namespace=crd-publish-openapi-4358 create -f -'
  Apr 15 08:37:17.230: INFO: stderr: ""
  Apr 15 08:37:17.230: INFO: stdout: "e2e-test-crd-publish-openapi-4738-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Apr 15 08:37:17.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4358 --namespace=crd-publish-openapi-4358 delete e2e-test-crd-publish-openapi-4738-crds test-cr'
  Apr 15 08:37:17.385: INFO: stderr: ""
  Apr 15 08:37:17.386: INFO: stdout: "e2e-test-crd-publish-openapi-4738-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Apr 15 08:37:17.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4358 --namespace=crd-publish-openapi-4358 apply -f -'
  Apr 15 08:37:17.822: INFO: stderr: ""
  Apr 15 08:37:17.823: INFO: stdout: "e2e-test-crd-publish-openapi-4738-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Apr 15 08:37:17.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4358 --namespace=crd-publish-openapi-4358 delete e2e-test-crd-publish-openapi-4738-crds test-cr'
  Apr 15 08:37:17.949: INFO: stderr: ""
  Apr 15 08:37:17.949: INFO: stdout: "e2e-test-crd-publish-openapi-4738-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 04/15/24 08:37:17.949
  Apr 15 08:37:17.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4358 explain e2e-test-crd-publish-openapi-4738-crds'
  Apr 15 08:37:18.342: INFO: stderr: ""
  Apr 15 08:37:18.342: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-4738-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  Apr 15 08:37:20.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4358" for this suite. @ 04/15/24 08:37:20.366
• [6.653 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 04/15/24 08:37:20.398
  Apr 15 08:37:20.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 08:37:20.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:37:20.459
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:37:20.464
  STEP: Creating a pod to test downward api env vars @ 04/15/24 08:37:20.47
  STEP: Saw pod success @ 04/15/24 08:37:24.526
  Apr 15 08:37:24.532: INFO: Trying to get logs from node ahz3daisheng-3 pod downward-api-041be917-7a47-4a00-9431-adb2acc975bf container dapi-container: <nil>
  STEP: delete the pod @ 04/15/24 08:37:24.552
  Apr 15 08:37:24.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6651" for this suite. @ 04/15/24 08:37:24.588
• [4.201 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 04/15/24 08:37:24.602
  Apr 15 08:37:24.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubelet-test @ 04/15/24 08:37:24.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:37:24.655
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:37:24.662
  Apr 15 08:37:28.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-701" for this suite. @ 04/15/24 08:37:28.77
• [4.182 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 04/15/24 08:37:28.788
  Apr 15 08:37:28.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename webhook @ 04/15/24 08:37:28.792
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:37:28.824
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:37:28.83
  STEP: Setting up server cert @ 04/15/24 08:37:28.874
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 08:37:29.702
  STEP: Deploying the webhook pod @ 04/15/24 08:37:29.718
  STEP: Wait for the deployment to be ready @ 04/15/24 08:37:29.741
  Apr 15 08:37:29.754: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 04/15/24 08:37:31.779
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 08:37:31.803
  Apr 15 08:37:32.808: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 04/15/24 08:37:32.823
  STEP: create a namespace for the webhook @ 04/15/24 08:37:32.866
  STEP: create a configmap should be unconditionally rejected by the webhook @ 04/15/24 08:37:32.899
  Apr 15 08:37:32.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7298" for this suite. @ 04/15/24 08:37:32.993
  STEP: Destroying namespace "webhook-markers-4131" for this suite. @ 04/15/24 08:37:33.006
  STEP: Destroying namespace "fail-closed-namespace-4075" for this suite. @ 04/15/24 08:37:33.025
• [4.250 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 04/15/24 08:37:33.05
  Apr 15 08:37:33.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubectl-logs @ 04/15/24 08:37:33.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:37:33.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:37:33.116
  STEP: creating an pod @ 04/15/24 08:37:33.122
  Apr 15 08:37:33.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-logs-319 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.47 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Apr 15 08:37:33.293: INFO: stderr: ""
  Apr 15 08:37:33.293: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 04/15/24 08:37:33.293
  Apr 15 08:37:33.293: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  Apr 15 08:37:35.313: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 04/15/24 08:37:35.313
  Apr 15 08:37:35.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-logs-319 logs logs-generator logs-generator'
  Apr 15 08:37:35.507: INFO: stderr: ""
  Apr 15 08:37:35.507: INFO: stdout: "I0415 08:37:34.171398       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/4q7 246\nI0415 08:37:34.371085       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/4slz 502\nI0415 08:37:34.573193       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/mcwg 352\nI0415 08:37:34.771689       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/dxs4 394\nI0415 08:37:34.970769       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/ck4l 572\nI0415 08:37:35.171303       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/dtq 255\nI0415 08:37:35.370736       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/q66 305\n"
  STEP: limiting log lines @ 04/15/24 08:37:35.507
  Apr 15 08:37:35.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-logs-319 logs logs-generator logs-generator --tail=1'
  Apr 15 08:37:35.680: INFO: stderr: ""
  Apr 15 08:37:35.680: INFO: stdout: "I0415 08:37:35.572120       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/pnhn 202\n"
  Apr 15 08:37:35.680: INFO: got output "I0415 08:37:35.572120       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/pnhn 202\n"
  STEP: limiting log bytes @ 04/15/24 08:37:35.68
  Apr 15 08:37:35.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-logs-319 logs logs-generator logs-generator --limit-bytes=1'
  Apr 15 08:37:35.864: INFO: stderr: ""
  Apr 15 08:37:35.864: INFO: stdout: "I"
  Apr 15 08:37:35.864: INFO: got output "I"
  STEP: exposing timestamps @ 04/15/24 08:37:35.864
  Apr 15 08:37:35.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-logs-319 logs logs-generator logs-generator --tail=1 --timestamps'
  Apr 15 08:37:36.034: INFO: stderr: ""
  Apr 15 08:37:36.034: INFO: stdout: "2024-04-15T08:37:35.971114125Z I0415 08:37:35.971013       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/t6cj 209\n"
  Apr 15 08:37:36.034: INFO: got output "2024-04-15T08:37:35.971114125Z I0415 08:37:35.971013       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/t6cj 209\n"
  STEP: restricting to a time range @ 04/15/24 08:37:36.034
  Apr 15 08:37:38.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-logs-319 logs logs-generator logs-generator --since=1s'
  Apr 15 08:37:38.714: INFO: stderr: ""
  Apr 15 08:37:38.714: INFO: stdout: "I0415 08:37:37.770705       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/pg7f 304\nI0415 08:37:37.971188       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/ckn 285\nI0415 08:37:38.171685       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/6xm 352\nI0415 08:37:38.371138       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/d28 472\nI0415 08:37:38.571606       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/fhj 484\n"
  Apr 15 08:37:38.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-logs-319 logs logs-generator logs-generator --since=24h'
  Apr 15 08:37:38.924: INFO: stderr: ""
  Apr 15 08:37:38.924: INFO: stdout: "I0415 08:37:34.171398       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/4q7 246\nI0415 08:37:34.371085       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/4slz 502\nI0415 08:37:34.573193       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/mcwg 352\nI0415 08:37:34.771689       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/dxs4 394\nI0415 08:37:34.970769       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/ck4l 572\nI0415 08:37:35.171303       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/dtq 255\nI0415 08:37:35.370736       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/q66 305\nI0415 08:37:35.572120       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/pnhn 202\nI0415 08:37:35.772509       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/4jmk 400\nI0415 08:37:35.971013       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/t6cj 209\nI0415 08:37:36.171874       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/27r 202\nI0415 08:37:36.371431       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/vzpf 502\nI0415 08:37:36.570808       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/vld 495\nI0415 08:37:36.771278       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/szgs 224\nI0415 08:37:36.970741       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/67js 595\nI0415 08:37:37.171346       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/gh2t 397\nI0415 08:37:37.370776       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/4grt 487\nI0415 08:37:37.571271       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/6g99 358\nI0415 08:37:37.770705       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/pg7f 304\nI0415 08:37:37.971188       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/ckn 285\nI0415 08:37:38.171685       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/6xm 352\nI0415 08:37:38.371138       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/d28 472\nI0415 08:37:38.571606       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/fhj 484\nI0415 08:37:38.772502       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/nsx 423\n"
  Apr 15 08:37:38.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-logs-319 delete pod logs-generator'
  Apr 15 08:37:39.845: INFO: stderr: ""
  Apr 15 08:37:39.845: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Apr 15 08:37:39.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-319" for this suite. @ 04/15/24 08:37:39.858
• [6.824 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 04/15/24 08:37:39.875
  Apr 15 08:37:39.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 08:37:39.877
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:37:39.916
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:37:39.921
  Apr 15 08:37:39.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-1539 version'
  Apr 15 08:37:40.091: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Apr 15 08:37:40.091: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.12\", GitCommit:\"12031002905c0410706974560cbdf2dad9278919\", GitTreeState:\"clean\", BuildDate:\"2024-03-15T02:15:31Z\", GoVersion:\"go1.21.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.12\", GitCommit:\"12031002905c0410706974560cbdf2dad9278919\", GitTreeState:\"clean\", BuildDate:\"2024-03-15T02:06:14Z\", GoVersion:\"go1.21.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Apr 15 08:37:40.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1539" for this suite. @ 04/15/24 08:37:40.102
• [0.239 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 04/15/24 08:37:40.117
  Apr 15 08:37:40.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/15/24 08:37:40.124
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:37:40.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:37:40.158
  STEP: create the container to handle the HTTPGet hook request. @ 04/15/24 08:37:40.175
  STEP: create the pod with lifecycle hook @ 04/15/24 08:37:42.215
  STEP: delete the pod with lifecycle hook @ 04/15/24 08:37:44.258
  STEP: check prestop hook @ 04/15/24 08:37:46.297
  Apr 15 08:37:46.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-4850" for this suite. @ 04/15/24 08:37:46.34
• [6.239 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 04/15/24 08:37:46.362
  Apr 15 08:37:46.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/15/24 08:37:46.364
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:37:46.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:37:46.408
  Apr 15 08:37:46.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/15/24 08:37:48.332
  Apr 15 08:37:48.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-935 --namespace=crd-publish-openapi-935 create -f -'
  Apr 15 08:37:50.082: INFO: stderr: ""
  Apr 15 08:37:50.082: INFO: stdout: "e2e-test-crd-publish-openapi-8598-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Apr 15 08:37:50.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-935 --namespace=crd-publish-openapi-935 delete e2e-test-crd-publish-openapi-8598-crds test-cr'
  Apr 15 08:37:50.369: INFO: stderr: ""
  Apr 15 08:37:50.369: INFO: stdout: "e2e-test-crd-publish-openapi-8598-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Apr 15 08:37:50.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-935 --namespace=crd-publish-openapi-935 apply -f -'
  Apr 15 08:37:51.633: INFO: stderr: ""
  Apr 15 08:37:51.633: INFO: stdout: "e2e-test-crd-publish-openapi-8598-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Apr 15 08:37:51.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-935 --namespace=crd-publish-openapi-935 delete e2e-test-crd-publish-openapi-8598-crds test-cr'
  Apr 15 08:37:51.845: INFO: stderr: ""
  Apr 15 08:37:51.846: INFO: stdout: "e2e-test-crd-publish-openapi-8598-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 04/15/24 08:37:51.846
  Apr 15 08:37:51.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-935 explain e2e-test-crd-publish-openapi-8598-crds'
  Apr 15 08:37:52.455: INFO: stderr: ""
  Apr 15 08:37:52.455: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-8598-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Apr 15 08:37:54.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-935" for this suite. @ 04/15/24 08:37:54.442
• [8.093 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 04/15/24 08:37:54.462
  Apr 15 08:37:54.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename runtimeclass @ 04/15/24 08:37:54.467
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:37:54.504
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:37:54.51
  Apr 15 08:37:56.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6434" for this suite. @ 04/15/24 08:37:56.574
• [2.124 seconds]
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 04/15/24 08:37:56.587
  Apr 15 08:37:56.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename cronjob @ 04/15/24 08:37:56.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:37:56.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:37:56.619
  STEP: Creating a cronjob @ 04/15/24 08:37:56.64
  STEP: Ensuring more than one job is running at a time @ 04/15/24 08:37:56.738
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 04/15/24 08:39:00.747
  STEP: Removing cronjob @ 04/15/24 08:39:00.759
  Apr 15 08:39:00.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3471" for this suite. @ 04/15/24 08:39:00.819
• [64.256 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 04/15/24 08:39:00.845
  Apr 15 08:39:00.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename emptydir-wrapper @ 04/15/24 08:39:00.847
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:39:00.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:39:00.909
  STEP: Creating 50 configmaps @ 04/15/24 08:39:00.915
  STEP: Creating RC which spawns configmap-volume pods @ 04/15/24 08:39:01.386
  Apr 15 08:39:01.430: INFO: Pod name wrapped-volume-race-4a4fb28d-79ad-42fa-8ade-9d89f8ddc28c: Found 0 pods out of 5
  Apr 15 08:39:06.455: INFO: Pod name wrapped-volume-race-4a4fb28d-79ad-42fa-8ade-9d89f8ddc28c: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/15/24 08:39:06.455
  STEP: Creating RC which spawns configmap-volume pods @ 04/15/24 08:39:06.518
  Apr 15 08:39:06.558: INFO: Pod name wrapped-volume-race-888a2e02-9181-425e-b6d8-34f4e9b67f94: Found 0 pods out of 5
  Apr 15 08:39:11.595: INFO: Pod name wrapped-volume-race-888a2e02-9181-425e-b6d8-34f4e9b67f94: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/15/24 08:39:11.596
  STEP: Creating RC which spawns configmap-volume pods @ 04/15/24 08:39:11.659
  Apr 15 08:39:11.704: INFO: Pod name wrapped-volume-race-aa51c788-ab0b-4e0a-b988-793a1b9b6de6: Found 0 pods out of 5
  Apr 15 08:39:16.781: INFO: Pod name wrapped-volume-race-aa51c788-ab0b-4e0a-b988-793a1b9b6de6: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 04/15/24 08:39:16.784
  Apr 15 08:39:16.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-aa51c788-ab0b-4e0a-b988-793a1b9b6de6 in namespace emptydir-wrapper-5601, will wait for the garbage collector to delete the pods @ 04/15/24 08:39:16.86
  Apr 15 08:39:16.950: INFO: Deleting ReplicationController wrapped-volume-race-aa51c788-ab0b-4e0a-b988-793a1b9b6de6 took: 31.741069ms
  Apr 15 08:39:17.052: INFO: Terminating ReplicationController wrapped-volume-race-aa51c788-ab0b-4e0a-b988-793a1b9b6de6 pods took: 101.723956ms
  STEP: deleting ReplicationController wrapped-volume-race-888a2e02-9181-425e-b6d8-34f4e9b67f94 in namespace emptydir-wrapper-5601, will wait for the garbage collector to delete the pods @ 04/15/24 08:39:19.353
  Apr 15 08:39:19.439: INFO: Deleting ReplicationController wrapped-volume-race-888a2e02-9181-425e-b6d8-34f4e9b67f94 took: 22.481559ms
  Apr 15 08:39:19.639: INFO: Terminating ReplicationController wrapped-volume-race-888a2e02-9181-425e-b6d8-34f4e9b67f94 pods took: 200.795026ms
  STEP: deleting ReplicationController wrapped-volume-race-4a4fb28d-79ad-42fa-8ade-9d89f8ddc28c in namespace emptydir-wrapper-5601, will wait for the garbage collector to delete the pods @ 04/15/24 08:39:21.441
  Apr 15 08:39:21.516: INFO: Deleting ReplicationController wrapped-volume-race-4a4fb28d-79ad-42fa-8ade-9d89f8ddc28c took: 14.749795ms
  Apr 15 08:39:21.616: INFO: Terminating ReplicationController wrapped-volume-race-4a4fb28d-79ad-42fa-8ade-9d89f8ddc28c pods took: 100.78754ms
  STEP: Cleaning up the configMaps @ 04/15/24 08:39:23.418
  STEP: Destroying namespace "emptydir-wrapper-5601" for this suite. @ 04/15/24 08:39:24.085
• [23.253 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 04/15/24 08:39:24.106
  Apr 15 08:39:24.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 08:39:24.108
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:39:24.143
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:39:24.147
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 08:39:24.153
  STEP: Saw pod success @ 04/15/24 08:39:28.217
  Apr 15 08:39:28.223: INFO: Trying to get logs from node ahz3daisheng-3 pod downwardapi-volume-e22f064a-6751-40f0-9da1-496421fb1a02 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 08:39:28.256
  Apr 15 08:39:28.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-923" for this suite. @ 04/15/24 08:39:28.297
• [4.204 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 04/15/24 08:39:28.319
  Apr 15 08:39:28.319: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename secrets @ 04/15/24 08:39:28.321
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:39:28.356
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:39:28.363
  STEP: Creating secret with name secret-test-6365c4e0-3484-4108-9293-24152846b947 @ 04/15/24 08:39:28.367
  STEP: Creating a pod to test consume secrets @ 04/15/24 08:39:28.375
  STEP: Saw pod success @ 04/15/24 08:39:32.424
  Apr 15 08:39:32.429: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-secrets-5300a001-b316-4bd5-aaef-d342a56a9eed container secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 08:39:32.444
  Apr 15 08:39:32.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2167" for this suite. @ 04/15/24 08:39:32.487
• [4.182 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 04/15/24 08:39:32.51
  Apr 15 08:39:32.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename dns @ 04/15/24 08:39:32.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:39:32.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:39:32.55
  STEP: Creating a test externalName service @ 04/15/24 08:39:32.555
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2277.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2277.svc.cluster.local; sleep 1; done
   @ 04/15/24 08:39:32.565
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2277.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2277.svc.cluster.local; sleep 1; done
   @ 04/15/24 08:39:32.565
  STEP: creating a pod to probe DNS @ 04/15/24 08:39:32.565
  STEP: submitting the pod to kubernetes @ 04/15/24 08:39:32.566
  STEP: retrieving the pod @ 04/15/24 08:39:34.606
  STEP: looking for the results for each expected name from probers @ 04/15/24 08:39:34.613
  Apr 15 08:39:34.645: INFO: DNS probes using dns-test-c52b39a3-9d07-44e2-a9e1-530109d4ab39 succeeded

  STEP: changing the externalName to bar.example.com @ 04/15/24 08:39:34.646
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2277.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2277.svc.cluster.local; sleep 1; done
   @ 04/15/24 08:39:34.664
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2277.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2277.svc.cluster.local; sleep 1; done
   @ 04/15/24 08:39:34.664
  STEP: creating a second pod to probe DNS @ 04/15/24 08:39:34.664
  STEP: submitting the pod to kubernetes @ 04/15/24 08:39:34.664
  STEP: retrieving the pod @ 04/15/24 08:39:38.718
  STEP: looking for the results for each expected name from probers @ 04/15/24 08:39:38.733
  Apr 15 08:39:38.753: INFO: File wheezy_udp@dns-test-service-3.dns-2277.svc.cluster.local from pod  dns-2277/dns-test-af202587-befa-48c6-ba32-3249a0383748 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 08:39:38.765: INFO: File jessie_udp@dns-test-service-3.dns-2277.svc.cluster.local from pod  dns-2277/dns-test-af202587-befa-48c6-ba32-3249a0383748 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 08:39:38.765: INFO: Lookups using dns-2277/dns-test-af202587-befa-48c6-ba32-3249a0383748 failed for: [wheezy_udp@dns-test-service-3.dns-2277.svc.cluster.local jessie_udp@dns-test-service-3.dns-2277.svc.cluster.local]

  Apr 15 08:39:43.775: INFO: File wheezy_udp@dns-test-service-3.dns-2277.svc.cluster.local from pod  dns-2277/dns-test-af202587-befa-48c6-ba32-3249a0383748 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 08:39:43.785: INFO: File jessie_udp@dns-test-service-3.dns-2277.svc.cluster.local from pod  dns-2277/dns-test-af202587-befa-48c6-ba32-3249a0383748 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 08:39:43.785: INFO: Lookups using dns-2277/dns-test-af202587-befa-48c6-ba32-3249a0383748 failed for: [wheezy_udp@dns-test-service-3.dns-2277.svc.cluster.local jessie_udp@dns-test-service-3.dns-2277.svc.cluster.local]

  Apr 15 08:39:48.778: INFO: File wheezy_udp@dns-test-service-3.dns-2277.svc.cluster.local from pod  dns-2277/dns-test-af202587-befa-48c6-ba32-3249a0383748 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 08:39:48.788: INFO: File jessie_udp@dns-test-service-3.dns-2277.svc.cluster.local from pod  dns-2277/dns-test-af202587-befa-48c6-ba32-3249a0383748 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 08:39:48.788: INFO: Lookups using dns-2277/dns-test-af202587-befa-48c6-ba32-3249a0383748 failed for: [wheezy_udp@dns-test-service-3.dns-2277.svc.cluster.local jessie_udp@dns-test-service-3.dns-2277.svc.cluster.local]

  Apr 15 08:39:53.784: INFO: File wheezy_udp@dns-test-service-3.dns-2277.svc.cluster.local from pod  dns-2277/dns-test-af202587-befa-48c6-ba32-3249a0383748 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 08:39:53.796: INFO: File jessie_udp@dns-test-service-3.dns-2277.svc.cluster.local from pod  dns-2277/dns-test-af202587-befa-48c6-ba32-3249a0383748 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 08:39:53.796: INFO: Lookups using dns-2277/dns-test-af202587-befa-48c6-ba32-3249a0383748 failed for: [wheezy_udp@dns-test-service-3.dns-2277.svc.cluster.local jessie_udp@dns-test-service-3.dns-2277.svc.cluster.local]

  Apr 15 08:39:58.774: INFO: File wheezy_udp@dns-test-service-3.dns-2277.svc.cluster.local from pod  dns-2277/dns-test-af202587-befa-48c6-ba32-3249a0383748 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 08:39:58.785: INFO: File jessie_udp@dns-test-service-3.dns-2277.svc.cluster.local from pod  dns-2277/dns-test-af202587-befa-48c6-ba32-3249a0383748 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 08:39:58.785: INFO: Lookups using dns-2277/dns-test-af202587-befa-48c6-ba32-3249a0383748 failed for: [wheezy_udp@dns-test-service-3.dns-2277.svc.cluster.local jessie_udp@dns-test-service-3.dns-2277.svc.cluster.local]

  Apr 15 08:40:03.776: INFO: File wheezy_udp@dns-test-service-3.dns-2277.svc.cluster.local from pod  dns-2277/dns-test-af202587-befa-48c6-ba32-3249a0383748 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 08:40:03.786: INFO: File jessie_udp@dns-test-service-3.dns-2277.svc.cluster.local from pod  dns-2277/dns-test-af202587-befa-48c6-ba32-3249a0383748 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Apr 15 08:40:03.786: INFO: Lookups using dns-2277/dns-test-af202587-befa-48c6-ba32-3249a0383748 failed for: [wheezy_udp@dns-test-service-3.dns-2277.svc.cluster.local jessie_udp@dns-test-service-3.dns-2277.svc.cluster.local]

  Apr 15 08:40:08.789: INFO: DNS probes using dns-test-af202587-befa-48c6-ba32-3249a0383748 succeeded

  STEP: changing the service to type=ClusterIP @ 04/15/24 08:40:08.789
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2277.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2277.svc.cluster.local; sleep 1; done
   @ 04/15/24 08:40:08.851
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2277.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2277.svc.cluster.local; sleep 1; done
   @ 04/15/24 08:40:08.851
  STEP: creating a third pod to probe DNS @ 04/15/24 08:40:08.851
  STEP: submitting the pod to kubernetes @ 04/15/24 08:40:08.882
  STEP: retrieving the pod @ 04/15/24 08:40:10.962
  STEP: looking for the results for each expected name from probers @ 04/15/24 08:40:10.968
  Apr 15 08:40:11.000: INFO: DNS probes using dns-test-c255927c-95da-4d3e-9109-eb4b71398833 succeeded

  Apr 15 08:40:11.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 08:40:11.014
  STEP: deleting the pod @ 04/15/24 08:40:11.049
  STEP: deleting the pod @ 04/15/24 08:40:11.108
  STEP: deleting the test externalName service @ 04/15/24 08:40:11.214
  STEP: Destroying namespace "dns-2277" for this suite. @ 04/15/24 08:40:11.301
• [38.818 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 04/15/24 08:40:11.352
  Apr 15 08:40:11.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename sched-pred @ 04/15/24 08:40:11.355
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:40:11.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:40:11.411
  Apr 15 08:40:11.417: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 15 08:40:11.436: INFO: Waiting for terminating namespaces to be deleted...
  Apr 15 08:40:11.449: INFO: 
  Logging pods the apiserver thinks is on node ahz3daisheng-1 before test
  Apr 15 08:40:11.469: INFO: coredns-5d78c9869d-hwtz8 from kube-system started at 2024-04-15 08:21:15 +0000 UTC (1 container statuses recorded)
  Apr 15 08:40:11.470: INFO: 	Container coredns ready: true, restart count 0
  Apr 15 08:40:11.470: INFO: kube-addon-manager-ahz3daisheng-1 from kube-system started at 2024-04-15 06:17:06 +0000 UTC (1 container statuses recorded)
  Apr 15 08:40:11.471: INFO: 	Container kube-addon-manager ready: true, restart count 5
  Apr 15 08:40:11.472: INFO: kube-apiserver-ahz3daisheng-1 from kube-system started at 2024-04-15 06:17:06 +0000 UTC (1 container statuses recorded)
  Apr 15 08:40:11.472: INFO: 	Container kube-apiserver ready: true, restart count 5
  Apr 15 08:40:11.473: INFO: kube-controller-manager-ahz3daisheng-1 from kube-system started at 2024-04-15 06:17:06 +0000 UTC (1 container statuses recorded)
  Apr 15 08:40:11.473: INFO: 	Container kube-controller-manager ready: true, restart count 5
  Apr 15 08:40:11.474: INFO: kube-flannel-ds-k86c4 from kube-system started at 2024-04-15 08:21:18 +0000 UTC (1 container statuses recorded)
  Apr 15 08:40:11.474: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 08:40:11.475: INFO: kube-proxy-8g55q from kube-system started at 2024-04-15 08:21:17 +0000 UTC (1 container statuses recorded)
  Apr 15 08:40:11.475: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 08:40:11.476: INFO: kube-scheduler-ahz3daisheng-1 from kube-system started at 2024-04-15 06:17:06 +0000 UTC (1 container statuses recorded)
  Apr 15 08:40:11.476: INFO: 	Container kube-scheduler ready: true, restart count 5
  Apr 15 08:40:11.477: INFO: sonobuoy-systemd-logs-daemon-set-10fc9ca8ef6747d1-9lf7d from sonobuoy started at 2024-04-15 08:22:14 +0000 UTC (2 container statuses recorded)
  Apr 15 08:40:11.477: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 08:40:11.478: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 15 08:40:11.479: INFO: 
  Logging pods the apiserver thinks is on node ahz3daisheng-2 before test
  Apr 15 08:40:11.497: INFO: kube-addon-manager-ahz3daisheng-2 from kube-system started at 2024-04-15 06:17:33 +0000 UTC (1 container statuses recorded)
  Apr 15 08:40:11.497: INFO: 	Container kube-addon-manager ready: true, restart count 5
  Apr 15 08:40:11.498: INFO: kube-apiserver-ahz3daisheng-2 from kube-system started at 2024-04-15 06:17:33 +0000 UTC (1 container statuses recorded)
  Apr 15 08:40:11.499: INFO: 	Container kube-apiserver ready: true, restart count 5
  Apr 15 08:40:11.499: INFO: kube-controller-manager-ahz3daisheng-2 from kube-system started at 2024-04-15 06:17:33 +0000 UTC (1 container statuses recorded)
  Apr 15 08:40:11.500: INFO: 	Container kube-controller-manager ready: true, restart count 5
  Apr 15 08:40:11.501: INFO: kube-flannel-ds-5c2gp from kube-system started at 2024-04-15 08:21:17 +0000 UTC (1 container statuses recorded)
  Apr 15 08:40:11.501: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 08:40:11.502: INFO: kube-proxy-llrhn from kube-system started at 2024-04-15 08:21:16 +0000 UTC (1 container statuses recorded)
  Apr 15 08:40:11.503: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 08:40:11.504: INFO: kube-scheduler-ahz3daisheng-2 from kube-system started at 2024-04-15 06:17:33 +0000 UTC (1 container statuses recorded)
  Apr 15 08:40:11.504: INFO: 	Container kube-scheduler ready: true, restart count 5
  Apr 15 08:40:11.505: INFO: sonobuoy-systemd-logs-daemon-set-10fc9ca8ef6747d1-r768m from sonobuoy started at 2024-04-15 08:22:14 +0000 UTC (2 container statuses recorded)
  Apr 15 08:40:11.505: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 08:40:11.506: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 15 08:40:11.506: INFO: 
  Logging pods the apiserver thinks is on node ahz3daisheng-3 before test
  Apr 15 08:40:11.529: INFO: coredns-5d78c9869d-zxbll from kube-system started at 2024-04-15 08:21:15 +0000 UTC (1 container statuses recorded)
  Apr 15 08:40:11.529: INFO: 	Container coredns ready: true, restart count 0
  Apr 15 08:40:11.529: INFO: kube-flannel-ds-2d8nc from kube-system started at 2024-04-15 08:21:17 +0000 UTC (1 container statuses recorded)
  Apr 15 08:40:11.529: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 08:40:11.529: INFO: kube-proxy-66n59 from kube-system started at 2024-04-15 08:21:16 +0000 UTC (1 container statuses recorded)
  Apr 15 08:40:11.529: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 08:40:11.529: INFO: sonobuoy from sonobuoy started at 2024-04-15 08:22:13 +0000 UTC (1 container statuses recorded)
  Apr 15 08:40:11.529: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 15 08:40:11.529: INFO: sonobuoy-e2e-job-dd62b7a77ff54dbf from sonobuoy started at 2024-04-15 08:22:14 +0000 UTC (2 container statuses recorded)
  Apr 15 08:40:11.529: INFO: 	Container e2e ready: true, restart count 0
  Apr 15 08:40:11.529: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 08:40:11.529: INFO: sonobuoy-systemd-logs-daemon-set-10fc9ca8ef6747d1-qhkjw from sonobuoy started at 2024-04-15 08:22:14 +0000 UTC (2 container statuses recorded)
  Apr 15 08:40:11.529: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 08:40:11.529: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/15/24 08:40:11.53
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/15/24 08:40:13.581
  STEP: Trying to apply a random label on the found node. @ 04/15/24 08:40:13.612
  STEP: verifying the node has the label kubernetes.io/e2e-96315356-cbcc-4ad5-8f4b-aa40a727bd32 42 @ 04/15/24 08:40:13.642
  STEP: Trying to relaunch the pod, now with labels. @ 04/15/24 08:40:13.652
  STEP: removing the label kubernetes.io/e2e-96315356-cbcc-4ad5-8f4b-aa40a727bd32 off the node ahz3daisheng-3 @ 04/15/24 08:40:15.739
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-96315356-cbcc-4ad5-8f4b-aa40a727bd32 @ 04/15/24 08:40:15.768
  Apr 15 08:40:15.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-2226" for this suite. @ 04/15/24 08:40:15.793
• [4.457 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 04/15/24 08:40:15.812
  Apr 15 08:40:15.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename ingressclass @ 04/15/24 08:40:15.816
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:40:15.862
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:40:15.869
  STEP: getting /apis @ 04/15/24 08:40:15.877
  STEP: getting /apis/networking.k8s.io @ 04/15/24 08:40:15.894
  STEP: getting /apis/networking.k8s.iov1 @ 04/15/24 08:40:15.898
  STEP: creating @ 04/15/24 08:40:15.905
  STEP: getting @ 04/15/24 08:40:15.942
  STEP: listing @ 04/15/24 08:40:15.95
  STEP: watching @ 04/15/24 08:40:15.957
  Apr 15 08:40:15.957: INFO: starting watch
  STEP: patching @ 04/15/24 08:40:15.961
  STEP: updating @ 04/15/24 08:40:15.973
  Apr 15 08:40:15.986: INFO: waiting for watch events with expected annotations
  Apr 15 08:40:15.986: INFO: saw patched and updated annotations
  STEP: deleting @ 04/15/24 08:40:15.987
  STEP: deleting a collection @ 04/15/24 08:40:16.01
  Apr 15 08:40:16.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-909" for this suite. @ 04/15/24 08:40:16.061
• [0.262 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 04/15/24 08:40:16.082
  Apr 15 08:40:16.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 08:40:16.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:40:16.122
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:40:16.134
  STEP: Creating projection with secret that has name projected-secret-test-map-8029df86-46ed-4292-b25f-7b42c24249a4 @ 04/15/24 08:40:16.144
  STEP: Creating a pod to test consume secrets @ 04/15/24 08:40:16.159
  STEP: Saw pod success @ 04/15/24 08:40:20.227
  Apr 15 08:40:20.235: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-projected-secrets-e0976228-afbe-43a0-a74e-3234f8f6e643 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 08:40:20.252
  Apr 15 08:40:20.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6728" for this suite. @ 04/15/24 08:40:20.312
• [4.247 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 04/15/24 08:40:20.343
  Apr 15 08:40:20.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename podtemplate @ 04/15/24 08:40:20.347
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:40:20.4
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:40:20.412
  STEP: Create a pod template @ 04/15/24 08:40:20.424
  STEP: Replace a pod template @ 04/15/24 08:40:20.442
  Apr 15 08:40:20.471: INFO: Found updated podtemplate annotation: "true"

  Apr 15 08:40:20.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-5475" for this suite. @ 04/15/24 08:40:20.486
• [0.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 04/15/24 08:40:20.51
  Apr 15 08:40:20.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename cronjob @ 04/15/24 08:40:20.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:40:20.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:40:20.559
  STEP: Creating a suspended cronjob @ 04/15/24 08:40:20.567
  STEP: Ensuring no jobs are scheduled @ 04/15/24 08:40:20.583
  STEP: Ensuring no job exists by listing jobs explicitly @ 04/15/24 08:45:20.602
  STEP: Removing cronjob @ 04/15/24 08:45:20.611
  Apr 15 08:45:20.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7491" for this suite. @ 04/15/24 08:45:20.657
• [300.189 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 04/15/24 08:45:20.702
  Apr 15 08:45:20.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename events @ 04/15/24 08:45:20.707
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:45:20.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:45:20.813
  STEP: Create set of events @ 04/15/24 08:45:20.825
  STEP: get a list of Events with a label in the current namespace @ 04/15/24 08:45:20.877
  STEP: delete a list of events @ 04/15/24 08:45:20.888
  Apr 15 08:45:20.888: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 04/15/24 08:45:20.936
  Apr 15 08:45:20.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5684" for this suite. @ 04/15/24 08:45:20.955
• [0.267 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 04/15/24 08:45:20.972
  Apr 15 08:45:20.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename job @ 04/15/24 08:45:20.974
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:45:21.016
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:45:21.022
  STEP: Creating a suspended job @ 04/15/24 08:45:21.033
  STEP: Patching the Job @ 04/15/24 08:45:21.052
  STEP: Watching for Job to be patched @ 04/15/24 08:45:21.103
  Apr 15 08:45:21.111: INFO: Event ADDED observed for Job e2e-nb8wh in namespace job-9818 with labels: map[e2e-job-label:e2e-nb8wh] and annotations: map[batch.kubernetes.io/job-tracking:]
  Apr 15 08:45:21.112: INFO: Event MODIFIED observed for Job e2e-nb8wh in namespace job-9818 with labels: map[e2e-job-label:e2e-nb8wh] and annotations: map[batch.kubernetes.io/job-tracking:]
  Apr 15 08:45:21.112: INFO: Event MODIFIED found for Job e2e-nb8wh in namespace job-9818 with labels: map[e2e-job-label:e2e-nb8wh e2e-nb8wh:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 04/15/24 08:45:21.113
  STEP: Watching for Job to be updated @ 04/15/24 08:45:21.15
  Apr 15 08:45:21.154: INFO: Event MODIFIED found for Job e2e-nb8wh in namespace job-9818 with labels: map[e2e-job-label:e2e-nb8wh e2e-nb8wh:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 15 08:45:21.154: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 04/15/24 08:45:21.154
  Apr 15 08:45:21.159: INFO: Job: e2e-nb8wh as labels: map[e2e-job-label:e2e-nb8wh e2e-nb8wh:patched]
  STEP: Waiting for job to complete @ 04/15/24 08:45:21.16
  STEP: Delete a job collection with a labelselector @ 04/15/24 08:45:29.174
  STEP: Watching for Job to be deleted @ 04/15/24 08:45:29.192
  Apr 15 08:45:29.197: INFO: Event MODIFIED observed for Job e2e-nb8wh in namespace job-9818 with labels: map[e2e-job-label:e2e-nb8wh e2e-nb8wh:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 15 08:45:29.198: INFO: Event MODIFIED observed for Job e2e-nb8wh in namespace job-9818 with labels: map[e2e-job-label:e2e-nb8wh e2e-nb8wh:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 15 08:45:29.199: INFO: Event MODIFIED observed for Job e2e-nb8wh in namespace job-9818 with labels: map[e2e-job-label:e2e-nb8wh e2e-nb8wh:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 15 08:45:29.199: INFO: Event MODIFIED observed for Job e2e-nb8wh in namespace job-9818 with labels: map[e2e-job-label:e2e-nb8wh e2e-nb8wh:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 15 08:45:29.200: INFO: Event MODIFIED observed for Job e2e-nb8wh in namespace job-9818 with labels: map[e2e-job-label:e2e-nb8wh e2e-nb8wh:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 15 08:45:29.201: INFO: Event MODIFIED observed for Job e2e-nb8wh in namespace job-9818 with labels: map[e2e-job-label:e2e-nb8wh e2e-nb8wh:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 15 08:45:29.201: INFO: Event MODIFIED observed for Job e2e-nb8wh in namespace job-9818 with labels: map[e2e-job-label:e2e-nb8wh e2e-nb8wh:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Apr 15 08:45:29.201: INFO: Event DELETED found for Job e2e-nb8wh in namespace job-9818 with labels: map[e2e-job-label:e2e-nb8wh e2e-nb8wh:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 04/15/24 08:45:29.202
  Apr 15 08:45:29.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9818" for this suite. @ 04/15/24 08:45:29.224
• [8.281 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 04/15/24 08:45:29.27
  Apr 15 08:45:29.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 08:45:29.274
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:45:29.31
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:45:29.315
  Apr 15 08:45:29.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9070 create -f -'
  Apr 15 08:45:31.157: INFO: stderr: ""
  Apr 15 08:45:31.157: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Apr 15 08:45:31.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9070 create -f -'
  Apr 15 08:45:31.929: INFO: stderr: ""
  Apr 15 08:45:31.929: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 04/15/24 08:45:31.929
  Apr 15 08:45:32.942: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 08:45:32.942: INFO: Found 0 / 1
  Apr 15 08:45:33.939: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 08:45:33.939: INFO: Found 1 / 1
  Apr 15 08:45:33.939: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Apr 15 08:45:33.948: INFO: Selector matched 1 pods for map[app:agnhost]
  Apr 15 08:45:33.948: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Apr 15 08:45:33.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9070 describe pod agnhost-primary-pgvwp'
  Apr 15 08:45:34.153: INFO: stderr: ""
  Apr 15 08:45:34.153: INFO: stdout: "Name:             agnhost-primary-pgvwp\nNamespace:        kubectl-9070\nPriority:         0\nService Account:  default\nNode:             ahz3daisheng-3/192.168.121.199\nStart Time:       Mon, 15 Apr 2024 08:45:31 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.66.19\nIPs:\n  IP:           10.233.66.19\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://eb8ca5587d9023319d177dcd9b176f219ba6e4ad5836d10aec4037e6cd8bea4e\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.47\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:c9997bf8d2e223d7d2a0078dcfb11a653e9b16cf09418829ec03e1d57ca9628a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 15 Apr 2024 08:45:31 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-75qbv (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-75qbv:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-9070/agnhost-primary-pgvwp to ahz3daisheng-3\n  Normal  Pulled     3s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.47\" already present on machine\n  Normal  Created    3s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
  Apr 15 08:45:34.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9070 describe rc agnhost-primary'
  Apr 15 08:45:34.355: INFO: stderr: ""
  Apr 15 08:45:34.355: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9070\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.47\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-pgvwp\n"
  Apr 15 08:45:34.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9070 describe service agnhost-primary'
  Apr 15 08:45:34.589: INFO: stderr: ""
  Apr 15 08:45:34.589: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9070\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.19.244\nIPs:               10.233.19.244\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.66.19:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Apr 15 08:45:34.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9070 describe node ahz3daisheng-1'
  Apr 15 08:45:34.878: INFO: stderr: ""
  Apr 15 08:45:34.878: INFO: stdout: "Name:               ahz3daisheng-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ahz3daisheng-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"6e:43:8f:50:6b:cb\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.121.96\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 14 Apr 2024 14:54:15 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ahz3daisheng-1\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 15 Apr 2024 08:45:27 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 15 Apr 2024 08:21:22 +0000   Mon, 15 Apr 2024 08:21:22 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Mon, 15 Apr 2024 08:45:12 +0000   Mon, 15 Apr 2024 06:09:09 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 15 Apr 2024 08:45:12 +0000   Mon, 15 Apr 2024 06:09:09 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 15 Apr 2024 08:45:12 +0000   Mon, 15 Apr 2024 06:09:09 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 15 Apr 2024 08:45:12 +0000   Mon, 15 Apr 2024 06:17:11 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.96\n  Hostname:    ahz3daisheng-1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  115008636Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8123560Ki\n  pods:               110\nAllocatable:\n  cpu:                1600m\n  ephemeral-storage:  111880401014\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3273896Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 3e08940a2d74468babdd8daf47776181\n  System UUID:                3e08940a-2d74-468b-abdd-8daf47776181\n  Boot ID:                    04a8e65e-f887-459f-af8d-995e124aabe6\n  Kernel Version:             6.5.0-27-generic\n  OS Image:                   Ubuntu 22.04.4 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.27.4\n  Kubelet Version:            v1.27.12\n  Kube-Proxy Version:         v1.27.12\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-5d78c9869d-hwtz8                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (5%)     24m\n  kube-system                 kube-addon-manager-ahz3daisheng-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         24m\n  kube-system                 kube-apiserver-ahz3daisheng-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         24m\n  kube-system                 kube-controller-manager-ahz3daisheng-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         24m\n  kube-system                 kube-flannel-ds-k86c4                                      100m (6%)     0 (0%)      50Mi (1%)        0 (0%)         24m\n  kube-system                 kube-proxy-8g55q                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         24m\n  kube-system                 kube-scheduler-ahz3daisheng-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         24m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-10fc9ca8ef6747d1-9lf7d    0 (0%)        0 (0%)      0 (0%)           0 (0%)         23m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                755m (47%)  0 (0%)\n  memory             170Mi (5%)  170Mi (5%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type    Reason    Age   From        Message\n  ----    ------    ----  ----        -------\n  Normal  Starting  24m   kube-proxy  \n"
  Apr 15 08:45:34.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9070 describe namespace kubectl-9070'
  Apr 15 08:45:35.054: INFO: stderr: ""
  Apr 15 08:45:35.054: INFO: stdout: "Name:         kubectl-9070\nLabels:       e2e-framework=kubectl\n              e2e-run=b296a868-5d29-45ef-ab0f-9de0eeb55529\n              kubernetes.io/metadata.name=kubectl-9070\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Apr 15 08:45:35.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9070" for this suite. @ 04/15/24 08:45:35.066
• [5.815 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 04/15/24 08:45:35.09
  Apr 15 08:45:35.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename svcaccounts @ 04/15/24 08:45:35.092
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:45:35.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:45:35.174
  Apr 15 08:45:35.187: INFO: Got root ca configmap in namespace "svcaccounts-1313"
  Apr 15 08:45:35.200: INFO: Deleted root ca configmap in namespace "svcaccounts-1313"
  STEP: waiting for a new root ca configmap created @ 04/15/24 08:45:35.702
  Apr 15 08:45:35.714: INFO: Recreated root ca configmap in namespace "svcaccounts-1313"
  Apr 15 08:45:35.732: INFO: Updated root ca configmap in namespace "svcaccounts-1313"
  STEP: waiting for the root ca configmap reconciled @ 04/15/24 08:45:36.234
  Apr 15 08:45:36.245: INFO: Reconciled root ca configmap in namespace "svcaccounts-1313"
  Apr 15 08:45:36.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1313" for this suite. @ 04/15/24 08:45:36.261
• [1.188 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:638
  STEP: Creating a kubernetes client @ 04/15/24 08:45:36.287
  Apr 15 08:45:36.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename statefulset @ 04/15/24 08:45:36.291
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:45:36.32
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:45:36.327
  STEP: Creating service test in namespace statefulset-3971 @ 04/15/24 08:45:36.333
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 04/15/24 08:45:36.348
  STEP: Creating stateful set ss in namespace statefulset-3971 @ 04/15/24 08:45:36.358
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3971 @ 04/15/24 08:45:36.383
  Apr 15 08:45:36.391: INFO: Found 0 stateful pods, waiting for 1
  Apr 15 08:45:46.404: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 04/15/24 08:45:46.404
  Apr 15 08:45:46.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-3971 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 08:45:46.698: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 08:45:46.698: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 08:45:46.698: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 15 08:45:46.709: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Apr 15 08:45:56.765: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 15 08:45:56.766: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 08:45:56.815: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999298s
  Apr 15 08:45:57.825: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993067704s
  Apr 15 08:45:58.866: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.978867442s
  Apr 15 08:45:59.875: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.942180474s
  Apr 15 08:46:00.885: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.93269745s
  Apr 15 08:46:01.896: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.92303217s
  Apr 15 08:46:02.904: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.911728339s
  Apr 15 08:46:03.916: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.903612899s
  Apr 15 08:46:04.925: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.892154244s
  Apr 15 08:46:05.936: INFO: Verifying statefulset ss doesn't scale past 1 for another 883.163365ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3971 @ 04/15/24 08:46:06.937
  Apr 15 08:46:06.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-3971 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 15 08:46:07.274: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 15 08:46:07.274: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 15 08:46:07.274: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 15 08:46:07.282: INFO: Found 1 stateful pods, waiting for 3
  Apr 15 08:46:17.301: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 08:46:17.302: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 08:46:17.302: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 04/15/24 08:46:17.302
  STEP: Scale down will halt with unhealthy stateful pod @ 04/15/24 08:46:17.302
  Apr 15 08:46:17.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-3971 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 08:46:17.650: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 08:46:17.650: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 08:46:17.650: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 15 08:46:17.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-3971 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 08:46:18.002: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 08:46:18.002: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 08:46:18.002: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 15 08:46:18.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-3971 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 08:46:18.393: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 08:46:18.394: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 08:46:18.394: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 15 08:46:18.394: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 08:46:18.407: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
  Apr 15 08:46:28.431: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 15 08:46:28.431: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Apr 15 08:46:28.431: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Apr 15 08:46:28.470: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999562s
  Apr 15 08:46:29.484: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.984735607s
  Apr 15 08:46:30.496: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.971170893s
  Apr 15 08:46:31.507: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.958790145s
  Apr 15 08:46:32.519: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.947831806s
  Apr 15 08:46:33.531: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.9354825s
  Apr 15 08:46:34.540: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.923859819s
  Apr 15 08:46:35.554: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.915212084s
  Apr 15 08:46:36.564: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.900786483s
  Apr 15 08:46:37.574: INFO: Verifying statefulset ss doesn't scale past 3 for another 889.952216ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3971 @ 04/15/24 08:46:38.575
  Apr 15 08:46:38.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-3971 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 15 08:46:38.903: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 15 08:46:38.903: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 15 08:46:38.903: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 15 08:46:38.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-3971 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 15 08:46:39.233: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 15 08:46:39.233: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 15 08:46:39.234: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 15 08:46:39.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-3971 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 15 08:46:39.541: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 15 08:46:39.541: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 15 08:46:39.541: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 15 08:46:39.541: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 04/15/24 08:46:49.581
  Apr 15 08:46:49.581: INFO: Deleting all statefulset in ns statefulset-3971
  Apr 15 08:46:49.590: INFO: Scaling statefulset ss to 0
  Apr 15 08:46:49.618: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 08:46:49.625: INFO: Deleting statefulset ss
  Apr 15 08:46:49.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3971" for this suite. @ 04/15/24 08:46:49.662
• [73.399 seconds]
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 04/15/24 08:46:49.687
  Apr 15 08:46:49.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/15/24 08:46:49.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:46:49.746
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:46:49.751
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 04/15/24 08:46:49.756
  Apr 15 08:46:49.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 04/15/24 08:46:58.286
  Apr 15 08:46:58.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 08:47:00.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 08:47:07.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3188" for this suite. @ 04/15/24 08:47:07.674
• [18.001 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:96
  STEP: Creating a kubernetes client @ 04/15/24 08:47:07.703
  Apr 15 08:47:07.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename aggregator @ 04/15/24 08:47:07.707
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:47:07.742
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:47:07.747
  Apr 15 08:47:07.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Registering the sample API server. @ 04/15/24 08:47:07.757
  Apr 15 08:47:08.919: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Apr 15 08:47:08.986: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
  Apr 15 08:47:11.083: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 08:47:13.096: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 08:47:15.098: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 08:47:17.094: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 08:47:19.102: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 08:47:21.094: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 08:47:23.094: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 08:47:25.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 08:47:27.096: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 08:47:29.109: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bf7768968\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 08:47:31.233: INFO: Waited 128.098495ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 04/15/24 08:47:31.324
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 04/15/24 08:47:31.332
  STEP: List APIServices @ 04/15/24 08:47:31.35
  Apr 15 08:47:31.364: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 04/15/24 08:47:31.364
  Apr 15 08:47:31.397: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 04/15/24 08:47:31.397
  Apr 15 08:47:31.417: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2024, time.April, 15, 8, 47, 31, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 04/15/24 08:47:31.417
  Apr 15 08:47:31.424: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2024-04-15 08:47:31 +0000 UTC Passed all checks passed}
  Apr 15 08:47:31.424: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 15 08:47:31.424: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 04/15/24 08:47:31.424
  Apr 15 08:47:31.456: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1764938273" @ 04/15/24 08:47:31.456
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 04/15/24 08:47:31.482
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 04/15/24 08:47:31.495
  STEP: Patch APIService Status @ 04/15/24 08:47:31.503
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 04/15/24 08:47:31.517
  Apr 15 08:47:31.529: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2024-04-15 08:47:31 +0000 UTC Passed all checks passed}
  Apr 15 08:47:31.530: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 15 08:47:31.530: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Apr 15 08:47:31.531: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 04/15/24 08:47:31.531
  STEP: Confirm that the generated APIService has been deleted @ 04/15/24 08:47:31.542
  Apr 15 08:47:31.542: INFO: Requesting list of APIServices to confirm quantity
  Apr 15 08:47:31.553: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Apr 15 08:47:31.554: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Apr 15 08:47:31.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-5202" for this suite. @ 04/15/24 08:47:31.953
• [24.267 seconds]
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 04/15/24 08:47:31.97
  Apr 15 08:47:31.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename certificates @ 04/15/24 08:47:31.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:47:32.017
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:47:32.027
  STEP: getting /apis @ 04/15/24 08:47:33.512
  STEP: getting /apis/certificates.k8s.io @ 04/15/24 08:47:33.52
  STEP: getting /apis/certificates.k8s.io/v1 @ 04/15/24 08:47:33.522
  STEP: creating @ 04/15/24 08:47:33.524
  STEP: getting @ 04/15/24 08:47:33.562
  STEP: listing @ 04/15/24 08:47:33.568
  STEP: watching @ 04/15/24 08:47:33.579
  Apr 15 08:47:33.579: INFO: starting watch
  STEP: patching @ 04/15/24 08:47:33.582
  STEP: updating @ 04/15/24 08:47:33.596
  Apr 15 08:47:33.607: INFO: waiting for watch events with expected annotations
  Apr 15 08:47:33.607: INFO: saw patched and updated annotations
  STEP: getting /approval @ 04/15/24 08:47:33.608
  STEP: patching /approval @ 04/15/24 08:47:33.614
  STEP: updating /approval @ 04/15/24 08:47:33.625
  STEP: getting /status @ 04/15/24 08:47:33.636
  STEP: patching /status @ 04/15/24 08:47:33.643
  STEP: updating /status @ 04/15/24 08:47:33.655
  STEP: deleting @ 04/15/24 08:47:33.667
  STEP: deleting a collection @ 04/15/24 08:47:33.69
  Apr 15 08:47:33.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-6432" for this suite. @ 04/15/24 08:47:33.725
• [1.766 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 04/15/24 08:47:33.739
  Apr 15 08:47:33.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename hostport @ 04/15/24 08:47:33.741
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:47:33.771
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:47:33.778
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 04/15/24 08:47:33.792
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.120 on the node which pod1 resides and expect scheduled @ 04/15/24 08:47:35.844
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.120 but use UDP protocol on the node which pod2 resides @ 04/15/24 08:47:37.893
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 04/15/24 08:47:41.955
  Apr 15 08:47:41.955: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.120 http://127.0.0.1:54323/hostname] Namespace:hostport-9257 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 08:47:41.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 08:47:41.957: INFO: ExecWithOptions: Clientset creation
  Apr 15 08:47:41.957: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-9257/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.120+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.120, port: 54323 @ 04/15/24 08:47:42.183
  Apr 15 08:47:42.183: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.120:54323/hostname] Namespace:hostport-9257 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 08:47:42.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 08:47:42.185: INFO: ExecWithOptions: Clientset creation
  Apr 15 08:47:42.186: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-9257/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.120%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.120, port: 54323 UDP @ 04/15/24 08:47:42.352
  Apr 15 08:47:42.352: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.121.120 54323] Namespace:hostport-9257 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 08:47:42.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 08:47:42.354: INFO: ExecWithOptions: Clientset creation
  Apr 15 08:47:42.354: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-9257/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.121.120+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  Apr 15 08:47:47.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-9257" for this suite. @ 04/15/24 08:47:47.486
• [13.767 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 04/15/24 08:47:47.508
  Apr 15 08:47:47.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 08:47:47.51
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:47:47.541
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:47:47.545
  STEP: Creating a pod to test emptydir volume type on node default medium @ 04/15/24 08:47:47.551
  STEP: Saw pod success @ 04/15/24 08:47:51.601
  Apr 15 08:47:51.608: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-4b5b9606-b0fa-4738-9d18-40855691d431 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 08:47:51.643
  Apr 15 08:47:51.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2952" for this suite. @ 04/15/24 08:47:51.69
• [4.194 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 04/15/24 08:47:51.706
  Apr 15 08:47:51.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename sched-pred @ 04/15/24 08:47:51.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:47:51.736
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:47:51.745
  Apr 15 08:47:51.749: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 15 08:47:51.777: INFO: Waiting for terminating namespaces to be deleted...
  Apr 15 08:47:51.784: INFO: 
  Logging pods the apiserver thinks is on node ahz3daisheng-1 before test
  Apr 15 08:47:51.801: INFO: coredns-5d78c9869d-hwtz8 from kube-system started at 2024-04-15 08:21:15 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.801: INFO: 	Container coredns ready: true, restart count 0
  Apr 15 08:47:51.801: INFO: kube-addon-manager-ahz3daisheng-1 from kube-system started at 2024-04-15 06:17:06 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.801: INFO: 	Container kube-addon-manager ready: true, restart count 5
  Apr 15 08:47:51.801: INFO: kube-apiserver-ahz3daisheng-1 from kube-system started at 2024-04-15 06:17:06 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.801: INFO: 	Container kube-apiserver ready: true, restart count 5
  Apr 15 08:47:51.801: INFO: kube-controller-manager-ahz3daisheng-1 from kube-system started at 2024-04-15 06:17:06 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.801: INFO: 	Container kube-controller-manager ready: true, restart count 5
  Apr 15 08:47:51.801: INFO: kube-flannel-ds-k86c4 from kube-system started at 2024-04-15 08:21:18 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.801: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 08:47:51.801: INFO: kube-proxy-8g55q from kube-system started at 2024-04-15 08:21:17 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.801: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 08:47:51.801: INFO: kube-scheduler-ahz3daisheng-1 from kube-system started at 2024-04-15 06:17:06 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.801: INFO: 	Container kube-scheduler ready: true, restart count 5
  Apr 15 08:47:51.801: INFO: sonobuoy-systemd-logs-daemon-set-10fc9ca8ef6747d1-9lf7d from sonobuoy started at 2024-04-15 08:22:14 +0000 UTC (2 container statuses recorded)
  Apr 15 08:47:51.801: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 08:47:51.801: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 15 08:47:51.801: INFO: 
  Logging pods the apiserver thinks is on node ahz3daisheng-2 before test
  Apr 15 08:47:51.823: INFO: e2e-host-exec from hostport-9257 started at 2024-04-15 08:47:39 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.823: INFO: 	Container e2e-host-exec ready: true, restart count 0
  Apr 15 08:47:51.823: INFO: pod1 from hostport-9257 started at 2024-04-15 08:47:33 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.824: INFO: 	Container agnhost ready: true, restart count 0
  Apr 15 08:47:51.824: INFO: pod2 from hostport-9257 started at 2024-04-15 08:47:35 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.824: INFO: 	Container agnhost ready: true, restart count 0
  Apr 15 08:47:51.825: INFO: pod3 from hostport-9257 started at 2024-04-15 08:47:37 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.825: INFO: 	Container agnhost ready: true, restart count 0
  Apr 15 08:47:51.825: INFO: kube-addon-manager-ahz3daisheng-2 from kube-system started at 2024-04-15 06:17:33 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.825: INFO: 	Container kube-addon-manager ready: true, restart count 5
  Apr 15 08:47:51.826: INFO: kube-apiserver-ahz3daisheng-2 from kube-system started at 2024-04-15 06:17:33 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.826: INFO: 	Container kube-apiserver ready: true, restart count 5
  Apr 15 08:47:51.826: INFO: kube-controller-manager-ahz3daisheng-2 from kube-system started at 2024-04-15 06:17:33 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.826: INFO: 	Container kube-controller-manager ready: true, restart count 5
  Apr 15 08:47:51.826: INFO: kube-flannel-ds-5c2gp from kube-system started at 2024-04-15 08:21:17 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.827: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 08:47:51.827: INFO: kube-proxy-llrhn from kube-system started at 2024-04-15 08:21:16 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.827: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 08:47:51.828: INFO: kube-scheduler-ahz3daisheng-2 from kube-system started at 2024-04-15 06:17:33 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.828: INFO: 	Container kube-scheduler ready: true, restart count 5
  Apr 15 08:47:51.828: INFO: sonobuoy-systemd-logs-daemon-set-10fc9ca8ef6747d1-r768m from sonobuoy started at 2024-04-15 08:22:14 +0000 UTC (2 container statuses recorded)
  Apr 15 08:47:51.828: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 08:47:51.828: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 15 08:47:51.828: INFO: 
  Logging pods the apiserver thinks is on node ahz3daisheng-3 before test
  Apr 15 08:47:51.846: INFO: coredns-5d78c9869d-zxbll from kube-system started at 2024-04-15 08:21:15 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.846: INFO: 	Container coredns ready: true, restart count 0
  Apr 15 08:47:51.846: INFO: kube-flannel-ds-2d8nc from kube-system started at 2024-04-15 08:21:17 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.846: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 08:47:51.846: INFO: kube-proxy-66n59 from kube-system started at 2024-04-15 08:21:16 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.846: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 08:47:51.846: INFO: sonobuoy from sonobuoy started at 2024-04-15 08:22:13 +0000 UTC (1 container statuses recorded)
  Apr 15 08:47:51.846: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 15 08:47:51.846: INFO: sonobuoy-e2e-job-dd62b7a77ff54dbf from sonobuoy started at 2024-04-15 08:22:14 +0000 UTC (2 container statuses recorded)
  Apr 15 08:47:51.847: INFO: 	Container e2e ready: true, restart count 0
  Apr 15 08:47:51.847: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 08:47:51.847: INFO: sonobuoy-systemd-logs-daemon-set-10fc9ca8ef6747d1-qhkjw from sonobuoy started at 2024-04-15 08:22:14 +0000 UTC (2 container statuses recorded)
  Apr 15 08:47:51.847: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 08:47:51.847: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 04/15/24 08:47:51.847
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.17c667d4cc58b25f], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] @ 04/15/24 08:47:51.927
  Apr 15 08:47:52.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-6580" for this suite. @ 04/15/24 08:47:52.938
• [1.255 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 04/15/24 08:47:52.961
  Apr 15 08:47:52.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename watch @ 04/15/24 08:47:52.965
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:47:53.001
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:47:53.01
  STEP: getting a starting resourceVersion @ 04/15/24 08:47:53.016
  STEP: starting a background goroutine to produce watch events @ 04/15/24 08:47:53.022
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 04/15/24 08:47:53.022
  Apr 15 08:47:55.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8688" for this suite. @ 04/15/24 08:47:55.829
• [2.919 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 04/15/24 08:47:55.881
  Apr 15 08:47:55.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 08:47:55.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:47:55.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:47:55.922
  STEP: Creating a ResourceQuota @ 04/15/24 08:47:55.928
  STEP: Getting a ResourceQuota @ 04/15/24 08:47:55.938
  STEP: Listing all ResourceQuotas with LabelSelector @ 04/15/24 08:47:55.947
  STEP: Patching the ResourceQuota @ 04/15/24 08:47:55.958
  STEP: Deleting a Collection of ResourceQuotas @ 04/15/24 08:47:55.973
  STEP: Verifying the deleted ResourceQuota @ 04/15/24 08:47:56.001
  Apr 15 08:47:56.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6315" for this suite. @ 04/15/24 08:47:56.024
• [0.155 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 04/15/24 08:47:56.04
  Apr 15 08:47:56.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename gc @ 04/15/24 08:47:56.044
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:47:56.086
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:47:56.092
  STEP: create the rc @ 04/15/24 08:47:56.109
  W0415 08:47:56.121131      14 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 04/15/24 08:48:02.19
  STEP: wait for the rc to be deleted @ 04/15/24 08:48:02.274
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 04/15/24 08:48:07.289
  STEP: Gathering metrics @ 04/15/24 08:48:37.369
  Apr 15 08:48:37.629: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 15 08:48:37.639: INFO: Deleting pod "simpletest.rc-2dlhn" in namespace "gc-7358"
  Apr 15 08:48:37.732: INFO: Deleting pod "simpletest.rc-2kjtr" in namespace "gc-7358"
  Apr 15 08:48:37.878: INFO: Deleting pod "simpletest.rc-2ljd5" in namespace "gc-7358"
  Apr 15 08:48:37.976: INFO: Deleting pod "simpletest.rc-2mlbp" in namespace "gc-7358"
  Apr 15 08:48:38.012: INFO: Deleting pod "simpletest.rc-2rjv7" in namespace "gc-7358"
  Apr 15 08:48:38.088: INFO: Deleting pod "simpletest.rc-2x2qt" in namespace "gc-7358"
  Apr 15 08:48:38.185: INFO: Deleting pod "simpletest.rc-44q7l" in namespace "gc-7358"
  Apr 15 08:48:38.254: INFO: Deleting pod "simpletest.rc-52pjp" in namespace "gc-7358"
  Apr 15 08:48:38.314: INFO: Deleting pod "simpletest.rc-52v9c" in namespace "gc-7358"
  Apr 15 08:48:38.388: INFO: Deleting pod "simpletest.rc-57s9c" in namespace "gc-7358"
  Apr 15 08:48:38.509: INFO: Deleting pod "simpletest.rc-5dmxz" in namespace "gc-7358"
  Apr 15 08:48:38.630: INFO: Deleting pod "simpletest.rc-5g8sr" in namespace "gc-7358"
  Apr 15 08:48:38.683: INFO: Deleting pod "simpletest.rc-5vdzl" in namespace "gc-7358"
  Apr 15 08:48:38.849: INFO: Deleting pod "simpletest.rc-5xmhz" in namespace "gc-7358"
  Apr 15 08:48:38.951: INFO: Deleting pod "simpletest.rc-6mc8v" in namespace "gc-7358"
  Apr 15 08:48:39.168: INFO: Deleting pod "simpletest.rc-6msll" in namespace "gc-7358"
  Apr 15 08:48:39.252: INFO: Deleting pod "simpletest.rc-6nsqf" in namespace "gc-7358"
  Apr 15 08:48:39.282: INFO: Deleting pod "simpletest.rc-6ssm2" in namespace "gc-7358"
  Apr 15 08:48:39.366: INFO: Deleting pod "simpletest.rc-6xfmn" in namespace "gc-7358"
  Apr 15 08:48:39.416: INFO: Deleting pod "simpletest.rc-79mkp" in namespace "gc-7358"
  Apr 15 08:48:39.506: INFO: Deleting pod "simpletest.rc-7qdmm" in namespace "gc-7358"
  Apr 15 08:48:39.554: INFO: Deleting pod "simpletest.rc-7wwp9" in namespace "gc-7358"
  Apr 15 08:48:39.639: INFO: Deleting pod "simpletest.rc-8c6lc" in namespace "gc-7358"
  Apr 15 08:48:39.698: INFO: Deleting pod "simpletest.rc-8t6w6" in namespace "gc-7358"
  Apr 15 08:48:39.863: INFO: Deleting pod "simpletest.rc-8vkn8" in namespace "gc-7358"
  Apr 15 08:48:40.050: INFO: Deleting pod "simpletest.rc-97mvl" in namespace "gc-7358"
  Apr 15 08:48:40.229: INFO: Deleting pod "simpletest.rc-9dgsx" in namespace "gc-7358"
  Apr 15 08:48:40.410: INFO: Deleting pod "simpletest.rc-9p5wt" in namespace "gc-7358"
  Apr 15 08:48:40.529: INFO: Deleting pod "simpletest.rc-9v4hl" in namespace "gc-7358"
  Apr 15 08:48:40.624: INFO: Deleting pod "simpletest.rc-b8w5w" in namespace "gc-7358"
  Apr 15 08:48:41.014: INFO: Deleting pod "simpletest.rc-bb8kt" in namespace "gc-7358"
  Apr 15 08:48:41.087: INFO: Deleting pod "simpletest.rc-bbvkw" in namespace "gc-7358"
  Apr 15 08:48:41.191: INFO: Deleting pod "simpletest.rc-br6gj" in namespace "gc-7358"
  Apr 15 08:48:41.291: INFO: Deleting pod "simpletest.rc-bslrl" in namespace "gc-7358"
  Apr 15 08:48:41.360: INFO: Deleting pod "simpletest.rc-bwdm4" in namespace "gc-7358"
  Apr 15 08:48:41.442: INFO: Deleting pod "simpletest.rc-cb859" in namespace "gc-7358"
  Apr 15 08:48:41.522: INFO: Deleting pod "simpletest.rc-cf455" in namespace "gc-7358"
  Apr 15 08:48:41.589: INFO: Deleting pod "simpletest.rc-d4gzm" in namespace "gc-7358"
  Apr 15 08:48:41.662: INFO: Deleting pod "simpletest.rc-dqk54" in namespace "gc-7358"
  Apr 15 08:48:41.846: INFO: Deleting pod "simpletest.rc-dwzjf" in namespace "gc-7358"
  Apr 15 08:48:42.002: INFO: Deleting pod "simpletest.rc-dzxgl" in namespace "gc-7358"
  Apr 15 08:48:42.162: INFO: Deleting pod "simpletest.rc-f4nsw" in namespace "gc-7358"
  Apr 15 08:48:42.330: INFO: Deleting pod "simpletest.rc-fkwwv" in namespace "gc-7358"
  Apr 15 08:48:42.468: INFO: Deleting pod "simpletest.rc-fzhfk" in namespace "gc-7358"
  Apr 15 08:48:42.589: INFO: Deleting pod "simpletest.rc-gbtcc" in namespace "gc-7358"
  Apr 15 08:48:42.733: INFO: Deleting pod "simpletest.rc-gdcz5" in namespace "gc-7358"
  Apr 15 08:48:43.103: INFO: Deleting pod "simpletest.rc-gh5p8" in namespace "gc-7358"
  Apr 15 08:48:43.409: INFO: Deleting pod "simpletest.rc-gp5z2" in namespace "gc-7358"
  Apr 15 08:48:43.495: INFO: Deleting pod "simpletest.rc-h9kvt" in namespace "gc-7358"
  Apr 15 08:48:43.561: INFO: Deleting pod "simpletest.rc-hsrh6" in namespace "gc-7358"
  Apr 15 08:48:43.626: INFO: Deleting pod "simpletest.rc-hzkp5" in namespace "gc-7358"
  Apr 15 08:48:43.698: INFO: Deleting pod "simpletest.rc-jdcvg" in namespace "gc-7358"
  Apr 15 08:48:43.764: INFO: Deleting pod "simpletest.rc-krkk6" in namespace "gc-7358"
  Apr 15 08:48:43.831: INFO: Deleting pod "simpletest.rc-kscsc" in namespace "gc-7358"
  Apr 15 08:48:43.888: INFO: Deleting pod "simpletest.rc-l5bqt" in namespace "gc-7358"
  Apr 15 08:48:43.966: INFO: Deleting pod "simpletest.rc-lgqwp" in namespace "gc-7358"
  Apr 15 08:48:44.030: INFO: Deleting pod "simpletest.rc-mbjt9" in namespace "gc-7358"
  Apr 15 08:48:44.137: INFO: Deleting pod "simpletest.rc-mmz6r" in namespace "gc-7358"
  Apr 15 08:48:44.200: INFO: Deleting pod "simpletest.rc-n6z52" in namespace "gc-7358"
  Apr 15 08:48:44.338: INFO: Deleting pod "simpletest.rc-nlxtl" in namespace "gc-7358"
  Apr 15 08:48:44.391: INFO: Deleting pod "simpletest.rc-nntnj" in namespace "gc-7358"
  Apr 15 08:48:44.449: INFO: Deleting pod "simpletest.rc-nqcd7" in namespace "gc-7358"
  Apr 15 08:48:44.523: INFO: Deleting pod "simpletest.rc-nwzmb" in namespace "gc-7358"
  Apr 15 08:48:44.647: INFO: Deleting pod "simpletest.rc-p4zdj" in namespace "gc-7358"
  Apr 15 08:48:44.924: INFO: Deleting pod "simpletest.rc-pcf8h" in namespace "gc-7358"
  Apr 15 08:48:44.969: INFO: Deleting pod "simpletest.rc-pfrtv" in namespace "gc-7358"
  Apr 15 08:48:45.066: INFO: Deleting pod "simpletest.rc-pk7sc" in namespace "gc-7358"
  Apr 15 08:48:45.201: INFO: Deleting pod "simpletest.rc-pkt7m" in namespace "gc-7358"
  Apr 15 08:48:45.321: INFO: Deleting pod "simpletest.rc-pkv9r" in namespace "gc-7358"
  Apr 15 08:48:45.362: INFO: Deleting pod "simpletest.rc-pm5cv" in namespace "gc-7358"
  Apr 15 08:48:45.425: INFO: Deleting pod "simpletest.rc-q5gns" in namespace "gc-7358"
  Apr 15 08:48:45.540: INFO: Deleting pod "simpletest.rc-qlspj" in namespace "gc-7358"
  Apr 15 08:48:45.594: INFO: Deleting pod "simpletest.rc-qmxhj" in namespace "gc-7358"
  Apr 15 08:48:45.672: INFO: Deleting pod "simpletest.rc-rcd85" in namespace "gc-7358"
  Apr 15 08:48:45.784: INFO: Deleting pod "simpletest.rc-rcgjl" in namespace "gc-7358"
  Apr 15 08:48:45.892: INFO: Deleting pod "simpletest.rc-rmcpk" in namespace "gc-7358"
  Apr 15 08:48:45.971: INFO: Deleting pod "simpletest.rc-rn4f6" in namespace "gc-7358"
  Apr 15 08:48:46.016: INFO: Deleting pod "simpletest.rc-rpdh2" in namespace "gc-7358"
  Apr 15 08:48:46.132: INFO: Deleting pod "simpletest.rc-rwqqq" in namespace "gc-7358"
  Apr 15 08:48:46.252: INFO: Deleting pod "simpletest.rc-stdrl" in namespace "gc-7358"
  Apr 15 08:48:46.365: INFO: Deleting pod "simpletest.rc-svnwm" in namespace "gc-7358"
  Apr 15 08:48:46.455: INFO: Deleting pod "simpletest.rc-t47mk" in namespace "gc-7358"
  Apr 15 08:48:46.547: INFO: Deleting pod "simpletest.rc-tcdnm" in namespace "gc-7358"
  Apr 15 08:48:46.598: INFO: Deleting pod "simpletest.rc-tczgt" in namespace "gc-7358"
  Apr 15 08:48:46.657: INFO: Deleting pod "simpletest.rc-vcd48" in namespace "gc-7358"
  Apr 15 08:48:46.701: INFO: Deleting pod "simpletest.rc-vl2nh" in namespace "gc-7358"
  Apr 15 08:48:46.797: INFO: Deleting pod "simpletest.rc-vpq7t" in namespace "gc-7358"
  Apr 15 08:48:46.867: INFO: Deleting pod "simpletest.rc-wbfll" in namespace "gc-7358"
  Apr 15 08:48:47.012: INFO: Deleting pod "simpletest.rc-x66rp" in namespace "gc-7358"
  Apr 15 08:48:47.124: INFO: Deleting pod "simpletest.rc-xc885" in namespace "gc-7358"
  Apr 15 08:48:47.210: INFO: Deleting pod "simpletest.rc-xhvfc" in namespace "gc-7358"
  Apr 15 08:48:47.269: INFO: Deleting pod "simpletest.rc-xjxvl" in namespace "gc-7358"
  Apr 15 08:48:47.308: INFO: Deleting pod "simpletest.rc-xm586" in namespace "gc-7358"
  Apr 15 08:48:47.335: INFO: Deleting pod "simpletest.rc-xwz85" in namespace "gc-7358"
  Apr 15 08:48:47.392: INFO: Deleting pod "simpletest.rc-z6w5f" in namespace "gc-7358"
  Apr 15 08:48:47.440: INFO: Deleting pod "simpletest.rc-z82dm" in namespace "gc-7358"
  Apr 15 08:48:47.471: INFO: Deleting pod "simpletest.rc-z8g7q" in namespace "gc-7358"
  Apr 15 08:48:47.574: INFO: Deleting pod "simpletest.rc-zdsmh" in namespace "gc-7358"
  Apr 15 08:48:47.632: INFO: Deleting pod "simpletest.rc-zfvbc" in namespace "gc-7358"
  Apr 15 08:48:47.696: INFO: Deleting pod "simpletest.rc-zrk49" in namespace "gc-7358"
  Apr 15 08:48:47.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7358" for this suite. @ 04/15/24 08:48:47.783
• [51.767 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 04/15/24 08:48:47.814
  Apr 15 08:48:47.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename configmap @ 04/15/24 08:48:47.826
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:48:47.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:48:47.912
  STEP: Creating configMap with name configmap-test-volume-map-bb9fdcf4-4a9f-4032-9325-729d1cb2e9d2 @ 04/15/24 08:48:47.917
  STEP: Creating a pod to test consume configMaps @ 04/15/24 08:48:47.928
  STEP: Saw pod success @ 04/15/24 08:48:52.025
  Apr 15 08:48:52.032: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-configmaps-323b9445-9749-437c-9fd3-986ed13c9d6e container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 08:48:52.044
  Apr 15 08:48:52.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7718" for this suite. @ 04/15/24 08:48:52.08
• [4.279 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 04/15/24 08:48:52.093
  Apr 15 08:48:52.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename watch @ 04/15/24 08:48:52.096
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:48:52.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:48:52.118
  STEP: creating a new configmap @ 04/15/24 08:48:52.122
  STEP: modifying the configmap once @ 04/15/24 08:48:52.13
  STEP: modifying the configmap a second time @ 04/15/24 08:48:52.142
  STEP: deleting the configmap @ 04/15/24 08:48:52.154
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 04/15/24 08:48:52.164
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 04/15/24 08:48:52.166
  Apr 15 08:48:52.166: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2075  00134a69-c721-4d38-b98c-4da7c60c49bd 191698 0 2024-04-15 08:48:52 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2024-04-15 08:48:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 08:48:52.167: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2075  00134a69-c721-4d38-b98c-4da7c60c49bd 191699 0 2024-04-15 08:48:52 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2024-04-15 08:48:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 08:48:52.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2075" for this suite. @ 04/15/24 08:48:52.175
• [0.092 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 04/15/24 08:48:52.186
  Apr 15 08:48:52.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename endpointslice @ 04/15/24 08:48:52.188
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:48:52.213
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:48:52.216
  STEP: referencing a single matching pod @ 04/15/24 08:48:57.36
  STEP: referencing matching pods with named port @ 04/15/24 08:49:02.388
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 04/15/24 08:49:07.403
  STEP: recreating EndpointSlices after they've been deleted @ 04/15/24 08:49:12.422
  Apr 15 08:49:12.468: INFO: EndpointSlice for Service endpointslice-7598/example-named-port not found
  Apr 15 08:49:22.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7598" for this suite. @ 04/15/24 08:49:22.504
• [30.336 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 04/15/24 08:49:22.527
  Apr 15 08:49:22.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 08:49:22.529
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:49:22.561
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:49:22.567
  STEP: Counting existing ResourceQuota @ 04/15/24 08:49:39.584
  STEP: Creating a ResourceQuota @ 04/15/24 08:49:44.593
  STEP: Ensuring resource quota status is calculated @ 04/15/24 08:49:44.605
  STEP: Creating a ConfigMap @ 04/15/24 08:49:46.617
  STEP: Ensuring resource quota status captures configMap creation @ 04/15/24 08:49:46.648
  STEP: Deleting a ConfigMap @ 04/15/24 08:49:48.656
  STEP: Ensuring resource quota status released usage @ 04/15/24 08:49:48.703
  Apr 15 08:49:50.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2716" for this suite. @ 04/15/24 08:49:50.725
• [28.210 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 04/15/24 08:49:50.738
  Apr 15 08:49:50.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 08:49:50.74
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:49:50.767
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:49:50.773
  STEP: Creating pod liveness-2071321a-51c6-427e-9273-4bcd38988c1f in namespace container-probe-862 @ 04/15/24 08:49:50.78
  Apr 15 08:49:52.830: INFO: Started pod liveness-2071321a-51c6-427e-9273-4bcd38988c1f in namespace container-probe-862
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/15/24 08:49:52.83
  Apr 15 08:49:52.836: INFO: Initial restart count of pod liveness-2071321a-51c6-427e-9273-4bcd38988c1f is 0
  Apr 15 08:50:13.011: INFO: Restart count of pod container-probe-862/liveness-2071321a-51c6-427e-9273-4bcd38988c1f is now 1 (20.175084638s elapsed)
  Apr 15 08:50:13.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 08:50:13.024
  STEP: Destroying namespace "container-probe-862" for this suite. @ 04/15/24 08:50:13.058
• [22.340 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 04/15/24 08:50:13.087
  Apr 15 08:50:13.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename services @ 04/15/24 08:50:13.089
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:50:13.115
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:50:13.125
  STEP: fetching services @ 04/15/24 08:50:13.132
  Apr 15 08:50:13.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5884" for this suite. @ 04/15/24 08:50:13.153
• [0.082 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 04/15/24 08:50:13.186
  Apr 15 08:50:13.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename lease-test @ 04/15/24 08:50:13.188
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:50:13.222
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:50:13.228
  Apr 15 08:50:13.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-6011" for this suite. @ 04/15/24 08:50:13.377
• [0.205 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 04/15/24 08:50:13.397
  Apr 15 08:50:13.397: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename init-container @ 04/15/24 08:50:13.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:50:13.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:50:13.445
  STEP: creating the pod @ 04/15/24 08:50:13.454
  Apr 15 08:50:13.455: INFO: PodSpec: initContainers in spec.initContainers
  Apr 15 08:50:16.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3109" for this suite. @ 04/15/24 08:50:16.469
• [3.088 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 04/15/24 08:50:16.489
  Apr 15 08:50:16.490: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename job @ 04/15/24 08:50:16.492
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:50:16.537
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:50:16.548
  STEP: Creating a job @ 04/15/24 08:50:16.552
  STEP: Ensuring active pods == parallelism @ 04/15/24 08:50:16.562
  STEP: Orphaning one of the Job's Pods @ 04/15/24 08:50:18.572
  Apr 15 08:50:19.113: INFO: Successfully updated pod "adopt-release-gdn7z"
  STEP: Checking that the Job readopts the Pod @ 04/15/24 08:50:19.113
  STEP: Removing the labels from the Job's Pod @ 04/15/24 08:50:21.135
  Apr 15 08:50:21.666: INFO: Successfully updated pod "adopt-release-gdn7z"
  STEP: Checking that the Job releases the Pod @ 04/15/24 08:50:21.666
  Apr 15 08:50:23.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2383" for this suite. @ 04/15/24 08:50:23.703
• [7.229 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 04/15/24 08:50:23.723
  Apr 15 08:50:23.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename deployment @ 04/15/24 08:50:23.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:50:23.758
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:50:23.765
  STEP: creating a Deployment @ 04/15/24 08:50:23.78
  Apr 15 08:50:23.781: INFO: Creating simple deployment test-deployment-x8k5r
  Apr 15 08:50:23.811: INFO: deployment "test-deployment-x8k5r" doesn't have the required revision set
  STEP: Getting /status @ 04/15/24 08:50:25.842
  Apr 15 08:50:25.852: INFO: Deployment test-deployment-x8k5r has Conditions: [{Available True 2024-04-15 08:50:25 +0000 UTC 2024-04-15 08:50:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2024-04-15 08:50:25 +0000 UTC 2024-04-15 08:50:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x8k5r-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 04/15/24 08:50:25.853
  Apr 15 08:50:25.878: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 8, 50, 25, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 50, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 8, 50, 25, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 8, 50, 23, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-x8k5r-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 04/15/24 08:50:25.879
  Apr 15 08:50:25.884: INFO: Observed &Deployment event: ADDED
  Apr 15 08:50:25.885: INFO: Observed Deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 08:50:23 +0000 UTC 2024-04-15 08:50:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-x8k5r-5994cf9475"}
  Apr 15 08:50:25.886: INFO: Observed &Deployment event: MODIFIED
  Apr 15 08:50:25.886: INFO: Observed Deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 08:50:23 +0000 UTC 2024-04-15 08:50:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-x8k5r-5994cf9475"}
  Apr 15 08:50:25.887: INFO: Observed Deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-04-15 08:50:23 +0000 UTC 2024-04-15 08:50:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 15 08:50:25.888: INFO: Observed &Deployment event: MODIFIED
  Apr 15 08:50:25.888: INFO: Observed Deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-04-15 08:50:23 +0000 UTC 2024-04-15 08:50:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 15 08:50:25.889: INFO: Observed Deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 08:50:23 +0000 UTC 2024-04-15 08:50:23 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-x8k5r-5994cf9475" is progressing.}
  Apr 15 08:50:25.890: INFO: Observed &Deployment event: MODIFIED
  Apr 15 08:50:25.892: INFO: Observed Deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-04-15 08:50:25 +0000 UTC 2024-04-15 08:50:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 15 08:50:25.892: INFO: Observed Deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 08:50:25 +0000 UTC 2024-04-15 08:50:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x8k5r-5994cf9475" has successfully progressed.}
  Apr 15 08:50:25.893: INFO: Observed &Deployment event: MODIFIED
  Apr 15 08:50:25.893: INFO: Observed Deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-04-15 08:50:25 +0000 UTC 2024-04-15 08:50:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 15 08:50:25.893: INFO: Observed Deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 08:50:25 +0000 UTC 2024-04-15 08:50:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x8k5r-5994cf9475" has successfully progressed.}
  Apr 15 08:50:25.893: INFO: Found Deployment test-deployment-x8k5r in namespace deployment-3787 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 15 08:50:25.893: INFO: Deployment test-deployment-x8k5r has an updated status
  STEP: patching the Statefulset Status @ 04/15/24 08:50:25.893
  Apr 15 08:50:25.893: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 15 08:50:25.911: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 04/15/24 08:50:25.911
  Apr 15 08:50:25.916: INFO: Observed &Deployment event: ADDED
  Apr 15 08:50:25.916: INFO: Observed deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 08:50:23 +0000 UTC 2024-04-15 08:50:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-x8k5r-5994cf9475"}
  Apr 15 08:50:25.916: INFO: Observed &Deployment event: MODIFIED
  Apr 15 08:50:25.916: INFO: Observed deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 08:50:23 +0000 UTC 2024-04-15 08:50:23 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-x8k5r-5994cf9475"}
  Apr 15 08:50:25.916: INFO: Observed deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-04-15 08:50:23 +0000 UTC 2024-04-15 08:50:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 15 08:50:25.917: INFO: Observed &Deployment event: MODIFIED
  Apr 15 08:50:25.917: INFO: Observed deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-04-15 08:50:23 +0000 UTC 2024-04-15 08:50:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Apr 15 08:50:25.917: INFO: Observed deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 08:50:23 +0000 UTC 2024-04-15 08:50:23 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-x8k5r-5994cf9475" is progressing.}
  Apr 15 08:50:25.918: INFO: Observed &Deployment event: MODIFIED
  Apr 15 08:50:25.918: INFO: Observed deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-04-15 08:50:25 +0000 UTC 2024-04-15 08:50:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 15 08:50:25.918: INFO: Observed deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 08:50:25 +0000 UTC 2024-04-15 08:50:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x8k5r-5994cf9475" has successfully progressed.}
  Apr 15 08:50:25.919: INFO: Observed &Deployment event: MODIFIED
  Apr 15 08:50:25.921: INFO: Observed deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-04-15 08:50:25 +0000 UTC 2024-04-15 08:50:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Apr 15 08:50:25.921: INFO: Observed deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-04-15 08:50:25 +0000 UTC 2024-04-15 08:50:23 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-x8k5r-5994cf9475" has successfully progressed.}
  Apr 15 08:50:25.921: INFO: Observed deployment test-deployment-x8k5r in namespace deployment-3787 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 15 08:50:25.922: INFO: Observed &Deployment event: MODIFIED
  Apr 15 08:50:25.923: INFO: Found deployment test-deployment-x8k5r in namespace deployment-3787 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Apr 15 08:50:25.923: INFO: Deployment test-deployment-x8k5r has a patched status
  Apr 15 08:50:25.948: INFO: Deployment "test-deployment-x8k5r":
  &Deployment{ObjectMeta:{test-deployment-x8k5r  deployment-3787  a5810f73-1bde-4709-850e-91b9e0b34094 192639 1 2024-04-15 08:50:23 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2024-04-15 08:50:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2024-04-15 08:50:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2024-04-15 08:50:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040e53a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-x8k5r-5994cf9475",LastUpdateTime:2024-04-15 08:50:25 +0000 UTC,LastTransitionTime:2024-04-15 08:50:25 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 15 08:50:25.974: INFO: New ReplicaSet "test-deployment-x8k5r-5994cf9475" of Deployment "test-deployment-x8k5r":
  &ReplicaSet{ObjectMeta:{test-deployment-x8k5r-5994cf9475  deployment-3787  3429d18b-27d2-4ca8-9813-0587c1384335 192635 1 2024-04-15 08:50:23 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-x8k5r a5810f73-1bde-4709-850e-91b9e0b34094 0xc002ec8b00 0xc002ec8b01}] [] [{kube-controller-manager Update apps/v1 2024-04-15 08:50:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5810f73-1bde-4709-850e-91b9e0b34094\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 08:50:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002ec8ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 08:50:25.988: INFO: Pod "test-deployment-x8k5r-5994cf9475-7sd47" is available:
  &Pod{ObjectMeta:{test-deployment-x8k5r-5994cf9475-7sd47 test-deployment-x8k5r-5994cf9475- deployment-3787  5a123a83-47b6-4865-b5a6-ae7952dc1335 192634 0 2024-04-15 08:50:23 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-x8k5r-5994cf9475 3429d18b-27d2-4ca8-9813-0587c1384335 0xc002ec8f60 0xc002ec8f61}] [] [{kube-controller-manager Update v1 2024-04-15 08:50:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3429d18b-27d2-4ca8-9813-0587c1384335\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 08:50:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4kv95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4kv95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:50:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:50:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:50:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 08:50:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.120,PodIP:10.233.65.71,StartTime:2024-04-15 08:50:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 08:50:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://1f76a6c298c7efaa217b4945fc62be6382c95ca38da1767f81c1d4051028c780,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.71,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 08:50:25.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3787" for this suite. @ 04/15/24 08:50:26.003
• [2.295 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 04/15/24 08:50:26.032
  Apr 15 08:50:26.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 08:50:26.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:50:26.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:50:26.088
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 04/15/24 08:50:26.095
  STEP: Saw pod success @ 04/15/24 08:50:30.135
  Apr 15 08:50:30.143: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-87109793-de79-4c20-ac7f-0ec661096344 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 08:50:30.173
  Apr 15 08:50:30.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3184" for this suite. @ 04/15/24 08:50:30.229
• [4.213 seconds]
------------------------------
S
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 04/15/24 08:50:30.246
  Apr 15 08:50:30.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 08:50:30.25
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:50:30.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:50:30.303
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 08:50:30.309
  STEP: Saw pod success @ 04/15/24 08:50:34.383
  Apr 15 08:50:34.391: INFO: Trying to get logs from node ahz3daisheng-3 pod downwardapi-volume-140f614e-ab1f-49fd-a234-547b60910584 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 08:50:34.404
  Apr 15 08:50:34.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7403" for this suite. @ 04/15/24 08:50:34.447
• [4.218 seconds]
------------------------------
SSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 04/15/24 08:50:34.466
  Apr 15 08:50:34.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename svc-latency @ 04/15/24 08:50:34.47
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:50:34.505
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:50:34.518
  Apr 15 08:50:34.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-2241 @ 04/15/24 08:50:34.529
  I0415 08:50:34.545335      14 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2241, replica count: 1
  I0415 08:50:35.598017      14 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0415 08:50:36.598493      14 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 15 08:50:36.730: INFO: Created: latency-svc-n6gvl
  Apr 15 08:50:36.741: INFO: Got endpoints: latency-svc-n6gvl [41.560311ms]
  Apr 15 08:50:36.786: INFO: Created: latency-svc-kh296
  Apr 15 08:50:36.797: INFO: Created: latency-svc-nbp59
  Apr 15 08:50:36.797: INFO: Got endpoints: latency-svc-kh296 [56.237804ms]
  Apr 15 08:50:36.812: INFO: Created: latency-svc-m2jvm
  Apr 15 08:50:36.820: INFO: Got endpoints: latency-svc-nbp59 [78.113141ms]
  Apr 15 08:50:36.831: INFO: Got endpoints: latency-svc-m2jvm [88.38581ms]
  Apr 15 08:50:36.841: INFO: Created: latency-svc-lhfvh
  Apr 15 08:50:36.866: INFO: Got endpoints: latency-svc-lhfvh [122.9616ms]
  Apr 15 08:50:36.866: INFO: Created: latency-svc-xqh4p
  Apr 15 08:50:36.867: INFO: Got endpoints: latency-svc-xqh4p [123.751334ms]
  Apr 15 08:50:36.887: INFO: Created: latency-svc-kwhqg
  Apr 15 08:50:36.893: INFO: Got endpoints: latency-svc-kwhqg [150.421187ms]
  Apr 15 08:50:37.082: INFO: Created: latency-svc-72pdg
  Apr 15 08:50:37.116: INFO: Created: latency-svc-qq5rd
  Apr 15 08:50:37.122: INFO: Created: latency-svc-vprbt
  Apr 15 08:50:37.129: INFO: Created: latency-svc-d45l7
  Apr 15 08:50:37.129: INFO: Created: latency-svc-6m4sl
  Apr 15 08:50:37.130: INFO: Created: latency-svc-xg6t2
  Apr 15 08:50:37.142: INFO: Created: latency-svc-9f9ft
  Apr 15 08:50:37.143: INFO: Created: latency-svc-8gr6g
  Apr 15 08:50:37.144: INFO: Created: latency-svc-ddrq7
  Apr 15 08:50:37.144: INFO: Created: latency-svc-v76zr
  Apr 15 08:50:37.156: INFO: Created: latency-svc-ccm2m
  Apr 15 08:50:37.157: INFO: Created: latency-svc-vhfps
  Apr 15 08:50:37.157: INFO: Created: latency-svc-qxtmg
  Apr 15 08:50:37.158: INFO: Created: latency-svc-tmxpt
  Apr 15 08:50:37.159: INFO: Created: latency-svc-66bm2
  Apr 15 08:50:37.159: INFO: Got endpoints: latency-svc-vprbt [414.686468ms]
  Apr 15 08:50:37.159: INFO: Got endpoints: latency-svc-72pdg [415.839719ms]
  Apr 15 08:50:37.194: INFO: Got endpoints: latency-svc-xg6t2 [328.044958ms]
  Apr 15 08:50:37.199: INFO: Got endpoints: latency-svc-8gr6g [368.452624ms]
  Apr 15 08:50:37.201: INFO: Got endpoints: latency-svc-9f9ft [456.388591ms]
  Apr 15 08:50:37.206: INFO: Got endpoints: latency-svc-ddrq7 [311.550386ms]
  Apr 15 08:50:37.227: INFO: Got endpoints: latency-svc-v76zr [482.056075ms]
  Apr 15 08:50:37.245: INFO: Created: latency-svc-stdr9
  Apr 15 08:50:37.275: INFO: Got endpoints: latency-svc-qxtmg [477.071473ms]
  Apr 15 08:50:37.275: INFO: Got endpoints: latency-svc-6m4sl [531.451354ms]
  Apr 15 08:50:37.276: INFO: Got endpoints: latency-svc-ccm2m [408.794577ms]
  Apr 15 08:50:37.276: INFO: Got endpoints: latency-svc-qq5rd [532.024599ms]
  Apr 15 08:50:37.276: INFO: Got endpoints: latency-svc-d45l7 [531.657835ms]
  Apr 15 08:50:37.283: INFO: Created: latency-svc-pvmfg
  Apr 15 08:50:37.307: INFO: Got endpoints: latency-svc-vhfps [486.601287ms]
  Apr 15 08:50:37.319: INFO: Created: latency-svc-26q4k
  Apr 15 08:50:37.330: INFO: Got endpoints: latency-svc-tmxpt [587.290416ms]
  Apr 15 08:50:37.331: INFO: Got endpoints: latency-svc-stdr9 [171.015544ms]
  Apr 15 08:50:37.360: INFO: Created: latency-svc-gsr2k
  Apr 15 08:50:37.378: INFO: Got endpoints: latency-svc-gsr2k [178.386163ms]
  Apr 15 08:50:37.378: INFO: Got endpoints: latency-svc-26q4k [183.635681ms]
  Apr 15 08:50:37.379: INFO: Got endpoints: latency-svc-66bm2 [634.62978ms]
  Apr 15 08:50:37.380: INFO: Got endpoints: latency-svc-pvmfg [220.103083ms]
  Apr 15 08:50:37.400: INFO: Created: latency-svc-2mq2b
  Apr 15 08:50:37.409: INFO: Created: latency-svc-xbpv9
  Apr 15 08:50:37.429: INFO: Created: latency-svc-fzdvv
  Apr 15 08:50:37.449: INFO: Got endpoints: latency-svc-xbpv9 [241.92022ms]
  Apr 15 08:50:37.451: INFO: Got endpoints: latency-svc-2mq2b [249.930592ms]
  Apr 15 08:50:37.495: INFO: Got endpoints: latency-svc-fzdvv [268.174912ms]
  Apr 15 08:50:37.499: INFO: Created: latency-svc-qpt6r
  Apr 15 08:50:37.500: INFO: Created: latency-svc-75hrx
  Apr 15 08:50:37.509: INFO: Created: latency-svc-nnn5j
  Apr 15 08:50:37.546: INFO: Created: latency-svc-vjkps
  Apr 15 08:50:37.575: INFO: Created: latency-svc-8mtfn
  Apr 15 08:50:37.585: INFO: Got endpoints: latency-svc-75hrx [308.427734ms]
  Apr 15 08:50:37.587: INFO: Got endpoints: latency-svc-qpt6r [312.747811ms]
  Apr 15 08:50:37.589: INFO: Got endpoints: latency-svc-vjkps [311.490467ms]
  Apr 15 08:50:37.590: INFO: Got endpoints: latency-svc-nnn5j [313.43193ms]
  Apr 15 08:50:37.621: INFO: Created: latency-svc-f25lf
  Apr 15 08:50:37.635: INFO: Got endpoints: latency-svc-f25lf [358.596265ms]
  Apr 15 08:50:37.636: INFO: Got endpoints: latency-svc-8mtfn [328.833576ms]
  Apr 15 08:50:37.651: INFO: Created: latency-svc-2b2fb
  Apr 15 08:50:37.660: INFO: Created: latency-svc-b6vhl
  Apr 15 08:50:37.682: INFO: Got endpoints: latency-svc-2b2fb [352.16279ms]
  Apr 15 08:50:37.682: INFO: Got endpoints: latency-svc-b6vhl [350.739587ms]
  Apr 15 08:50:37.693: INFO: Created: latency-svc-5n8sn
  Apr 15 08:50:37.712: INFO: Got endpoints: latency-svc-5n8sn [333.361692ms]
  Apr 15 08:50:37.719: INFO: Created: latency-svc-2bl8x
  Apr 15 08:50:37.735: INFO: Created: latency-svc-9b5rk
  Apr 15 08:50:37.739: INFO: Got endpoints: latency-svc-2bl8x [359.837485ms]
  Apr 15 08:50:37.758: INFO: Got endpoints: latency-svc-9b5rk [377.949946ms]
  Apr 15 08:50:37.770: INFO: Created: latency-svc-pjm2n
  Apr 15 08:50:37.786: INFO: Created: latency-svc-qzmq9
  Apr 15 08:50:37.813: INFO: Got endpoints: latency-svc-qzmq9 [362.125663ms]
  Apr 15 08:50:37.814: INFO: Got endpoints: latency-svc-pjm2n [435.738324ms]
  Apr 15 08:50:37.838: INFO: Created: latency-svc-7xz4q
  Apr 15 08:50:37.865: INFO: Got endpoints: latency-svc-7xz4q [415.171758ms]
  Apr 15 08:50:37.866: INFO: Created: latency-svc-nqgvb
  Apr 15 08:50:37.875: INFO: Got endpoints: latency-svc-nqgvb [377.887767ms]
  Apr 15 08:50:37.888: INFO: Created: latency-svc-8brhj
  Apr 15 08:50:37.900: INFO: Created: latency-svc-nzvtg
  Apr 15 08:50:37.910: INFO: Got endpoints: latency-svc-8brhj [324.481036ms]
  Apr 15 08:50:37.929: INFO: Created: latency-svc-gmmfl
  Apr 15 08:50:37.930: INFO: Got endpoints: latency-svc-nzvtg [342.525871ms]
  Apr 15 08:50:37.942: INFO: Got endpoints: latency-svc-gmmfl [353.191844ms]
  Apr 15 08:50:37.957: INFO: Created: latency-svc-tgztj
  Apr 15 08:50:37.967: INFO: Created: latency-svc-lclgd
  Apr 15 08:50:37.975: INFO: Got endpoints: latency-svc-tgztj [384.903488ms]
  Apr 15 08:50:37.984: INFO: Got endpoints: latency-svc-lclgd [348.696317ms]
  Apr 15 08:50:37.996: INFO: Created: latency-svc-n47dx
  Apr 15 08:50:38.009: INFO: Created: latency-svc-8hk66
  Apr 15 08:50:38.009: INFO: Got endpoints: latency-svc-n47dx [373.261301ms]
  Apr 15 08:50:38.050: INFO: Created: latency-svc-fzbkl
  Apr 15 08:50:38.061: INFO: Got endpoints: latency-svc-8hk66 [378.256927ms]
  Apr 15 08:50:38.076: INFO: Created: latency-svc-bn6ds
  Apr 15 08:50:38.091: INFO: Got endpoints: latency-svc-fzbkl [408.564456ms]
  Apr 15 08:50:38.097: INFO: Got endpoints: latency-svc-bn6ds [384.939266ms]
  Apr 15 08:50:38.109: INFO: Created: latency-svc-96hd2
  Apr 15 08:50:38.127: INFO: Got endpoints: latency-svc-96hd2 [387.663092ms]
  Apr 15 08:50:38.127: INFO: Created: latency-svc-97d4k
  Apr 15 08:50:38.145: INFO: Got endpoints: latency-svc-97d4k [387.578233ms]
  Apr 15 08:50:38.157: INFO: Created: latency-svc-jkxb8
  Apr 15 08:50:38.170: INFO: Created: latency-svc-twmp7
  Apr 15 08:50:38.171: INFO: Got endpoints: latency-svc-jkxb8 [357.356516ms]
  Apr 15 08:50:38.179: INFO: Created: latency-svc-rjb95
  Apr 15 08:50:38.195: INFO: Got endpoints: latency-svc-twmp7 [381.149692ms]
  Apr 15 08:50:38.198: INFO: Created: latency-svc-hmd5z
  Apr 15 08:50:38.213: INFO: Got endpoints: latency-svc-hmd5z [338.048124ms]
  Apr 15 08:50:38.214: INFO: Got endpoints: latency-svc-rjb95 [348.827451ms]
  Apr 15 08:50:38.223: INFO: Created: latency-svc-vvlzm
  Apr 15 08:50:38.230: INFO: Created: latency-svc-9l2h5
  Apr 15 08:50:38.245: INFO: Got endpoints: latency-svc-vvlzm [334.476648ms]
  Apr 15 08:50:38.249: INFO: Created: latency-svc-hzp4p
  Apr 15 08:50:38.262: INFO: Created: latency-svc-npfsn
  Apr 15 08:50:38.269: INFO: Created: latency-svc-ppbqd
  Apr 15 08:50:38.285: INFO: Created: latency-svc-fq5v2
  Apr 15 08:50:38.290: INFO: Got endpoints: latency-svc-9l2h5 [359.694734ms]
  Apr 15 08:50:38.308: INFO: Created: latency-svc-rg76h
  Apr 15 08:50:38.308: INFO: Created: latency-svc-8hd6w
  Apr 15 08:50:38.342: INFO: Got endpoints: latency-svc-hzp4p [400.210945ms]
  Apr 15 08:50:38.389: INFO: Got endpoints: latency-svc-npfsn [414.304931ms]
  Apr 15 08:50:38.409: INFO: Created: latency-svc-xcbmm
  Apr 15 08:50:38.411: INFO: Created: latency-svc-mvr64
  Apr 15 08:50:38.410: INFO: Created: latency-svc-c7zpx
  Apr 15 08:50:38.414: INFO: Created: latency-svc-pbkf5
  Apr 15 08:50:38.415: INFO: Created: latency-svc-fw7nh
  Apr 15 08:50:38.426: INFO: Created: latency-svc-rvvx9
  Apr 15 08:50:38.430: INFO: Created: latency-svc-t96kn
  Apr 15 08:50:38.431: INFO: Created: latency-svc-f94vz
  Apr 15 08:50:38.431: INFO: Created: latency-svc-bnf9p
  Apr 15 08:50:38.433: INFO: Created: latency-svc-gg2d8
  Apr 15 08:50:38.433: INFO: Created: latency-svc-5sv8k
  Apr 15 08:50:38.439: INFO: Got endpoints: latency-svc-ppbqd [454.534241ms]
  Apr 15 08:50:38.459: INFO: Created: latency-svc-lmr7p
  Apr 15 08:50:38.491: INFO: Got endpoints: latency-svc-fq5v2 [480.981094ms]
  Apr 15 08:50:38.512: INFO: Created: latency-svc-wplvj
  Apr 15 08:50:38.542: INFO: Got endpoints: latency-svc-8hd6w [480.961905ms]
  Apr 15 08:50:38.561: INFO: Created: latency-svc-6htqh
  Apr 15 08:50:38.590: INFO: Got endpoints: latency-svc-rg76h [498.395172ms]
  Apr 15 08:50:38.607: INFO: Created: latency-svc-hcq8g
  Apr 15 08:50:38.640: INFO: Got endpoints: latency-svc-xcbmm [542.876003ms]
  Apr 15 08:50:38.669: INFO: Created: latency-svc-c2k2d
  Apr 15 08:50:38.691: INFO: Got endpoints: latency-svc-pbkf5 [545.801646ms]
  Apr 15 08:50:38.722: INFO: Created: latency-svc-l2ww4
  Apr 15 08:50:38.746: INFO: Got endpoints: latency-svc-mvr64 [618.616034ms]
  Apr 15 08:50:38.766: INFO: Created: latency-svc-sxhnm
  Apr 15 08:50:38.794: INFO: Got endpoints: latency-svc-c7zpx [622.567934ms]
  Apr 15 08:50:38.827: INFO: Created: latency-svc-t9skn
  Apr 15 08:50:38.848: INFO: Got endpoints: latency-svc-fw7nh [557.041006ms]
  Apr 15 08:50:38.912: INFO: Got endpoints: latency-svc-rvvx9 [522.086119ms]
  Apr 15 08:50:38.949: INFO: Got endpoints: latency-svc-f94vz [606.523496ms]
  Apr 15 08:50:38.951: INFO: Created: latency-svc-rtq4x
  Apr 15 08:50:38.968: INFO: Created: latency-svc-fnwtk
  Apr 15 08:50:38.989: INFO: Created: latency-svc-ltdm2
  Apr 15 08:50:39.003: INFO: Got endpoints: latency-svc-t96kn [788.845471ms]
  Apr 15 08:50:39.023: INFO: Created: latency-svc-9gxcb
  Apr 15 08:50:39.040: INFO: Got endpoints: latency-svc-bnf9p [794.841792ms]
  Apr 15 08:50:39.063: INFO: Created: latency-svc-cf2sw
  Apr 15 08:50:39.091: INFO: Got endpoints: latency-svc-5sv8k [895.247631ms]
  Apr 15 08:50:39.117: INFO: Created: latency-svc-pjftx
  Apr 15 08:50:39.147: INFO: Got endpoints: latency-svc-gg2d8 [933.658201ms]
  Apr 15 08:50:39.176: INFO: Created: latency-svc-fk94t
  Apr 15 08:50:39.197: INFO: Got endpoints: latency-svc-lmr7p [758.49651ms]
  Apr 15 08:50:39.221: INFO: Created: latency-svc-rvwvs
  Apr 15 08:50:39.248: INFO: Got endpoints: latency-svc-wplvj [757.266627ms]
  Apr 15 08:50:39.274: INFO: Created: latency-svc-mtjhn
  Apr 15 08:50:39.293: INFO: Got endpoints: latency-svc-6htqh [750.946156ms]
  Apr 15 08:50:39.327: INFO: Created: latency-svc-6nnvx
  Apr 15 08:50:39.350: INFO: Got endpoints: latency-svc-hcq8g [759.429191ms]
  Apr 15 08:50:39.367: INFO: Created: latency-svc-mgxhl
  Apr 15 08:50:39.391: INFO: Got endpoints: latency-svc-c2k2d [750.59096ms]
  Apr 15 08:50:39.417: INFO: Created: latency-svc-r64sw
  Apr 15 08:50:39.445: INFO: Got endpoints: latency-svc-l2ww4 [753.362841ms]
  Apr 15 08:50:39.466: INFO: Created: latency-svc-kh2lp
  Apr 15 08:50:39.490: INFO: Got endpoints: latency-svc-sxhnm [744.27155ms]
  Apr 15 08:50:39.514: INFO: Created: latency-svc-r2jdv
  Apr 15 08:50:39.543: INFO: Got endpoints: latency-svc-t9skn [748.378315ms]
  Apr 15 08:50:39.570: INFO: Created: latency-svc-rnqsr
  Apr 15 08:50:39.597: INFO: Got endpoints: latency-svc-rtq4x [747.627057ms]
  Apr 15 08:50:39.621: INFO: Created: latency-svc-m6j7v
  Apr 15 08:50:39.641: INFO: Got endpoints: latency-svc-fnwtk [729.56893ms]
  Apr 15 08:50:39.665: INFO: Created: latency-svc-kqgzx
  Apr 15 08:50:39.690: INFO: Got endpoints: latency-svc-ltdm2 [739.519672ms]
  Apr 15 08:50:39.718: INFO: Created: latency-svc-ss6db
  Apr 15 08:50:39.747: INFO: Got endpoints: latency-svc-9gxcb [744.098306ms]
  Apr 15 08:50:39.770: INFO: Created: latency-svc-rmjdf
  Apr 15 08:50:39.792: INFO: Got endpoints: latency-svc-cf2sw [752.142289ms]
  Apr 15 08:50:39.812: INFO: Created: latency-svc-kn8mp
  Apr 15 08:50:39.844: INFO: Got endpoints: latency-svc-pjftx [753.315299ms]
  Apr 15 08:50:39.867: INFO: Created: latency-svc-85rkr
  Apr 15 08:50:39.899: INFO: Got endpoints: latency-svc-fk94t [751.11627ms]
  Apr 15 08:50:39.923: INFO: Created: latency-svc-4k96v
  Apr 15 08:50:39.944: INFO: Got endpoints: latency-svc-rvwvs [746.614447ms]
  Apr 15 08:50:39.969: INFO: Created: latency-svc-sfjfq
  Apr 15 08:50:39.990: INFO: Got endpoints: latency-svc-mtjhn [741.416955ms]
  Apr 15 08:50:40.013: INFO: Created: latency-svc-g9pnr
  Apr 15 08:50:40.050: INFO: Got endpoints: latency-svc-6nnvx [757.103856ms]
  Apr 15 08:50:40.084: INFO: Created: latency-svc-x47bx
  Apr 15 08:50:40.092: INFO: Got endpoints: latency-svc-mgxhl [742.092588ms]
  Apr 15 08:50:40.118: INFO: Created: latency-svc-dfr8b
  Apr 15 08:50:40.146: INFO: Got endpoints: latency-svc-r64sw [754.021823ms]
  Apr 15 08:50:40.162: INFO: Created: latency-svc-tssrb
  Apr 15 08:50:40.192: INFO: Got endpoints: latency-svc-kh2lp [746.223614ms]
  Apr 15 08:50:40.212: INFO: Created: latency-svc-7kshc
  Apr 15 08:50:40.242: INFO: Got endpoints: latency-svc-r2jdv [751.497765ms]
  Apr 15 08:50:40.261: INFO: Created: latency-svc-ffpg9
  Apr 15 08:50:40.289: INFO: Got endpoints: latency-svc-rnqsr [746.451166ms]
  Apr 15 08:50:40.308: INFO: Created: latency-svc-qdxfd
  Apr 15 08:50:40.341: INFO: Got endpoints: latency-svc-m6j7v [743.510517ms]
  Apr 15 08:50:40.367: INFO: Created: latency-svc-22rql
  Apr 15 08:50:40.390: INFO: Got endpoints: latency-svc-kqgzx [748.177601ms]
  Apr 15 08:50:40.410: INFO: Created: latency-svc-dmvbr
  Apr 15 08:50:40.444: INFO: Got endpoints: latency-svc-ss6db [754.184359ms]
  Apr 15 08:50:40.471: INFO: Created: latency-svc-mth8n
  Apr 15 08:50:40.491: INFO: Got endpoints: latency-svc-rmjdf [743.207017ms]
  Apr 15 08:50:40.520: INFO: Created: latency-svc-689gz
  Apr 15 08:50:40.553: INFO: Got endpoints: latency-svc-kn8mp [760.261833ms]
  Apr 15 08:50:40.572: INFO: Created: latency-svc-hqltd
  Apr 15 08:50:40.592: INFO: Got endpoints: latency-svc-85rkr [747.344862ms]
  Apr 15 08:50:40.615: INFO: Created: latency-svc-5kkqv
  Apr 15 08:50:40.642: INFO: Got endpoints: latency-svc-4k96v [743.045783ms]
  Apr 15 08:50:40.702: INFO: Got endpoints: latency-svc-sfjfq [757.626565ms]
  Apr 15 08:50:40.716: INFO: Created: latency-svc-mqhbv
  Apr 15 08:50:40.727: INFO: Created: latency-svc-bcccg
  Apr 15 08:50:40.745: INFO: Got endpoints: latency-svc-g9pnr [754.995242ms]
  Apr 15 08:50:40.804: INFO: Got endpoints: latency-svc-x47bx [753.312005ms]
  Apr 15 08:50:40.821: INFO: Created: latency-svc-rh59c
  Apr 15 08:50:40.844: INFO: Got endpoints: latency-svc-dfr8b [751.519578ms]
  Apr 15 08:50:40.848: INFO: Created: latency-svc-7rpmd
  Apr 15 08:50:40.874: INFO: Created: latency-svc-vsnn5
  Apr 15 08:50:40.900: INFO: Got endpoints: latency-svc-tssrb [754.562536ms]
  Apr 15 08:50:40.923: INFO: Created: latency-svc-9dd5w
  Apr 15 08:50:40.941: INFO: Got endpoints: latency-svc-7kshc [748.397461ms]
  Apr 15 08:50:40.967: INFO: Created: latency-svc-q76pk
  Apr 15 08:50:40.992: INFO: Got endpoints: latency-svc-ffpg9 [749.892473ms]
  Apr 15 08:50:41.013: INFO: Created: latency-svc-6lhb9
  Apr 15 08:50:41.040: INFO: Got endpoints: latency-svc-qdxfd [750.926673ms]
  Apr 15 08:50:41.059: INFO: Created: latency-svc-bjnqz
  Apr 15 08:50:41.100: INFO: Got endpoints: latency-svc-22rql [757.856176ms]
  Apr 15 08:50:41.128: INFO: Created: latency-svc-b62f7
  Apr 15 08:50:41.143: INFO: Got endpoints: latency-svc-dmvbr [752.186857ms]
  Apr 15 08:50:41.163: INFO: Created: latency-svc-gm6k9
  Apr 15 08:50:41.189: INFO: Got endpoints: latency-svc-mth8n [744.433397ms]
  Apr 15 08:50:41.208: INFO: Created: latency-svc-7vr96
  Apr 15 08:50:41.245: INFO: Got endpoints: latency-svc-689gz [754.173634ms]
  Apr 15 08:50:41.265: INFO: Created: latency-svc-ts2gr
  Apr 15 08:50:41.290: INFO: Got endpoints: latency-svc-hqltd [736.383881ms]
  Apr 15 08:50:41.310: INFO: Created: latency-svc-zd77p
  Apr 15 08:50:41.341: INFO: Got endpoints: latency-svc-5kkqv [748.750248ms]
  Apr 15 08:50:41.360: INFO: Created: latency-svc-rpldv
  Apr 15 08:50:41.400: INFO: Got endpoints: latency-svc-mqhbv [758.057166ms]
  Apr 15 08:50:41.429: INFO: Created: latency-svc-4fdpz
  Apr 15 08:50:41.443: INFO: Got endpoints: latency-svc-bcccg [741.085813ms]
  Apr 15 08:50:41.470: INFO: Created: latency-svc-xc8r4
  Apr 15 08:50:41.492: INFO: Got endpoints: latency-svc-rh59c [746.2426ms]
  Apr 15 08:50:41.516: INFO: Created: latency-svc-tj99w
  Apr 15 08:50:41.543: INFO: Got endpoints: latency-svc-7rpmd [735.597272ms]
  Apr 15 08:50:41.563: INFO: Created: latency-svc-8btvz
  Apr 15 08:50:41.597: INFO: Got endpoints: latency-svc-vsnn5 [750.946957ms]
  Apr 15 08:50:41.634: INFO: Created: latency-svc-7rvdw
  Apr 15 08:50:41.641: INFO: Got endpoints: latency-svc-9dd5w [740.317427ms]
  Apr 15 08:50:41.665: INFO: Created: latency-svc-b8vst
  Apr 15 08:50:41.694: INFO: Got endpoints: latency-svc-q76pk [752.717494ms]
  Apr 15 08:50:41.729: INFO: Created: latency-svc-wpknp
  Apr 15 08:50:41.738: INFO: Got endpoints: latency-svc-6lhb9 [745.610418ms]
  Apr 15 08:50:41.760: INFO: Created: latency-svc-dwn64
  Apr 15 08:50:41.791: INFO: Got endpoints: latency-svc-bjnqz [750.017095ms]
  Apr 15 08:50:41.819: INFO: Created: latency-svc-662h8
  Apr 15 08:50:41.842: INFO: Got endpoints: latency-svc-b62f7 [742.125919ms]
  Apr 15 08:50:41.864: INFO: Created: latency-svc-hcn4b
  Apr 15 08:50:41.897: INFO: Got endpoints: latency-svc-gm6k9 [754.373561ms]
  Apr 15 08:50:41.916: INFO: Created: latency-svc-99xrc
  Apr 15 08:50:41.941: INFO: Got endpoints: latency-svc-7vr96 [751.655716ms]
  Apr 15 08:50:41.965: INFO: Created: latency-svc-tsmdc
  Apr 15 08:50:42.007: INFO: Got endpoints: latency-svc-ts2gr [760.885966ms]
  Apr 15 08:50:42.050: INFO: Created: latency-svc-nd2j2
  Apr 15 08:50:42.061: INFO: Got endpoints: latency-svc-zd77p [770.160747ms]
  Apr 15 08:50:42.096: INFO: Got endpoints: latency-svc-rpldv [755.082592ms]
  Apr 15 08:50:42.104: INFO: Created: latency-svc-lk8nd
  Apr 15 08:50:42.124: INFO: Created: latency-svc-95fl6
  Apr 15 08:50:42.146: INFO: Got endpoints: latency-svc-4fdpz [745.120737ms]
  Apr 15 08:50:42.167: INFO: Created: latency-svc-2hj76
  Apr 15 08:50:42.196: INFO: Got endpoints: latency-svc-xc8r4 [752.51048ms]
  Apr 15 08:50:42.220: INFO: Created: latency-svc-6s2vv
  Apr 15 08:50:42.241: INFO: Got endpoints: latency-svc-tj99w [748.906577ms]
  Apr 15 08:50:42.267: INFO: Created: latency-svc-fcs64
  Apr 15 08:50:42.290: INFO: Got endpoints: latency-svc-8btvz [745.82635ms]
  Apr 15 08:50:42.324: INFO: Created: latency-svc-pfns2
  Apr 15 08:50:42.346: INFO: Got endpoints: latency-svc-7rvdw [748.795142ms]
  Apr 15 08:50:42.375: INFO: Created: latency-svc-w67pt
  Apr 15 08:50:42.398: INFO: Got endpoints: latency-svc-b8vst [756.804195ms]
  Apr 15 08:50:42.422: INFO: Created: latency-svc-8w5pf
  Apr 15 08:50:42.448: INFO: Got endpoints: latency-svc-wpknp [754.192105ms]
  Apr 15 08:50:42.473: INFO: Created: latency-svc-r86pd
  Apr 15 08:50:42.494: INFO: Got endpoints: latency-svc-dwn64 [754.563513ms]
  Apr 15 08:50:42.516: INFO: Created: latency-svc-dmzlm
  Apr 15 08:50:42.545: INFO: Got endpoints: latency-svc-662h8 [751.539307ms]
  Apr 15 08:50:42.567: INFO: Created: latency-svc-r86g7
  Apr 15 08:50:42.587: INFO: Got endpoints: latency-svc-hcn4b [744.712005ms]
  Apr 15 08:50:42.614: INFO: Created: latency-svc-jc6d7
  Apr 15 08:50:42.641: INFO: Got endpoints: latency-svc-99xrc [743.912995ms]
  Apr 15 08:50:42.664: INFO: Created: latency-svc-7bx2s
  Apr 15 08:50:42.690: INFO: Got endpoints: latency-svc-tsmdc [748.0778ms]
  Apr 15 08:50:42.717: INFO: Created: latency-svc-7x2xr
  Apr 15 08:50:42.738: INFO: Got endpoints: latency-svc-nd2j2 [731.523837ms]
  Apr 15 08:50:42.792: INFO: Created: latency-svc-7d46k
  Apr 15 08:50:42.800: INFO: Got endpoints: latency-svc-lk8nd [738.048898ms]
  Apr 15 08:50:42.846: INFO: Created: latency-svc-ffrkk
  Apr 15 08:50:42.856: INFO: Got endpoints: latency-svc-95fl6 [760.010511ms]
  Apr 15 08:50:42.908: INFO: Got endpoints: latency-svc-2hj76 [761.065057ms]
  Apr 15 08:50:42.921: INFO: Created: latency-svc-8qzvq
  Apr 15 08:50:42.931: INFO: Created: latency-svc-hp6fp
  Apr 15 08:50:42.941: INFO: Got endpoints: latency-svc-6s2vv [743.961633ms]
  Apr 15 08:50:42.958: INFO: Created: latency-svc-7zsx5
  Apr 15 08:50:42.993: INFO: Got endpoints: latency-svc-fcs64 [751.74113ms]
  Apr 15 08:50:43.014: INFO: Created: latency-svc-vrfqk
  Apr 15 08:50:43.043: INFO: Got endpoints: latency-svc-pfns2 [752.413194ms]
  Apr 15 08:50:43.070: INFO: Created: latency-svc-lmm8z
  Apr 15 08:50:43.099: INFO: Got endpoints: latency-svc-w67pt [752.620471ms]
  Apr 15 08:50:43.121: INFO: Created: latency-svc-wdg9j
  Apr 15 08:50:43.144: INFO: Got endpoints: latency-svc-8w5pf [744.839321ms]
  Apr 15 08:50:43.164: INFO: Created: latency-svc-6dvx2
  Apr 15 08:50:43.193: INFO: Got endpoints: latency-svc-r86pd [744.65269ms]
  Apr 15 08:50:43.213: INFO: Created: latency-svc-qxl95
  Apr 15 08:50:43.248: INFO: Got endpoints: latency-svc-dmzlm [753.564174ms]
  Apr 15 08:50:43.269: INFO: Created: latency-svc-dgrt2
  Apr 15 08:50:43.291: INFO: Got endpoints: latency-svc-r86g7 [745.530855ms]
  Apr 15 08:50:43.317: INFO: Created: latency-svc-bwk92
  Apr 15 08:50:43.344: INFO: Got endpoints: latency-svc-jc6d7 [756.061861ms]
  Apr 15 08:50:43.370: INFO: Created: latency-svc-2tttt
  Apr 15 08:50:43.390: INFO: Got endpoints: latency-svc-7bx2s [748.346628ms]
  Apr 15 08:50:43.414: INFO: Created: latency-svc-mlrxf
  Apr 15 08:50:43.447: INFO: Got endpoints: latency-svc-7x2xr [757.36295ms]
  Apr 15 08:50:43.469: INFO: Created: latency-svc-q9mlm
  Apr 15 08:50:43.492: INFO: Got endpoints: latency-svc-7d46k [752.358886ms]
  Apr 15 08:50:43.517: INFO: Created: latency-svc-zmsdc
  Apr 15 08:50:43.546: INFO: Got endpoints: latency-svc-ffrkk [745.459027ms]
  Apr 15 08:50:43.576: INFO: Created: latency-svc-b2dvb
  Apr 15 08:50:43.598: INFO: Got endpoints: latency-svc-8qzvq [741.377527ms]
  Apr 15 08:50:43.627: INFO: Created: latency-svc-ngzh9
  Apr 15 08:50:43.640: INFO: Got endpoints: latency-svc-hp6fp [731.677677ms]
  Apr 15 08:50:43.677: INFO: Created: latency-svc-wxvdg
  Apr 15 08:50:43.693: INFO: Got endpoints: latency-svc-7zsx5 [751.845193ms]
  Apr 15 08:50:43.748: INFO: Created: latency-svc-7kwzw
  Apr 15 08:50:43.752: INFO: Got endpoints: latency-svc-vrfqk [758.71744ms]
  Apr 15 08:50:43.775: INFO: Created: latency-svc-njshr
  Apr 15 08:50:43.797: INFO: Got endpoints: latency-svc-lmm8z [753.968459ms]
  Apr 15 08:50:43.818: INFO: Created: latency-svc-wbq4f
  Apr 15 08:50:43.843: INFO: Got endpoints: latency-svc-wdg9j [743.937323ms]
  Apr 15 08:50:43.868: INFO: Created: latency-svc-ms78x
  Apr 15 08:50:43.891: INFO: Got endpoints: latency-svc-6dvx2 [747.695954ms]
  Apr 15 08:50:43.922: INFO: Created: latency-svc-d85c7
  Apr 15 08:50:43.945: INFO: Got endpoints: latency-svc-qxl95 [751.434415ms]
  Apr 15 08:50:44.003: INFO: Created: latency-svc-5nvv9
  Apr 15 08:50:44.008: INFO: Got endpoints: latency-svc-dgrt2 [759.584707ms]
  Apr 15 08:50:44.050: INFO: Created: latency-svc-88lgg
  Apr 15 08:50:44.052: INFO: Got endpoints: latency-svc-bwk92 [760.261087ms]
  Apr 15 08:50:44.091: INFO: Got endpoints: latency-svc-2tttt [746.080467ms]
  Apr 15 08:50:44.097: INFO: Created: latency-svc-nk6bb
  Apr 15 08:50:44.123: INFO: Created: latency-svc-87q2t
  Apr 15 08:50:44.143: INFO: Got endpoints: latency-svc-mlrxf [752.589087ms]
  Apr 15 08:50:44.169: INFO: Created: latency-svc-cx26t
  Apr 15 08:50:44.191: INFO: Got endpoints: latency-svc-q9mlm [743.406206ms]
  Apr 15 08:50:44.217: INFO: Created: latency-svc-44rzg
  Apr 15 08:50:44.242: INFO: Got endpoints: latency-svc-zmsdc [749.901692ms]
  Apr 15 08:50:44.275: INFO: Created: latency-svc-9pfnc
  Apr 15 08:50:44.294: INFO: Got endpoints: latency-svc-b2dvb [747.112814ms]
  Apr 15 08:50:44.320: INFO: Created: latency-svc-7rr6f
  Apr 15 08:50:44.345: INFO: Got endpoints: latency-svc-ngzh9 [747.025847ms]
  Apr 15 08:50:44.367: INFO: Created: latency-svc-rxqnk
  Apr 15 08:50:44.392: INFO: Got endpoints: latency-svc-wxvdg [751.219439ms]
  Apr 15 08:50:44.415: INFO: Created: latency-svc-bsj4l
  Apr 15 08:50:44.442: INFO: Got endpoints: latency-svc-7kwzw [749.520709ms]
  Apr 15 08:50:44.467: INFO: Created: latency-svc-6dhjn
  Apr 15 08:50:44.497: INFO: Got endpoints: latency-svc-njshr [744.088561ms]
  Apr 15 08:50:44.527: INFO: Created: latency-svc-g4whq
  Apr 15 08:50:44.552: INFO: Got endpoints: latency-svc-wbq4f [754.161193ms]
  Apr 15 08:50:44.578: INFO: Created: latency-svc-cvx7m
  Apr 15 08:50:44.599: INFO: Got endpoints: latency-svc-ms78x [754.611702ms]
  Apr 15 08:50:44.649: INFO: Got endpoints: latency-svc-d85c7 [756.972591ms]
  Apr 15 08:50:44.733: INFO: Got endpoints: latency-svc-5nvv9 [787.176729ms]
  Apr 15 08:50:44.753: INFO: Got endpoints: latency-svc-88lgg [744.572835ms]
  Apr 15 08:50:44.795: INFO: Got endpoints: latency-svc-nk6bb [742.390037ms]
  Apr 15 08:50:44.855: INFO: Got endpoints: latency-svc-87q2t [763.452652ms]
  Apr 15 08:50:44.897: INFO: Got endpoints: latency-svc-cx26t [754.086673ms]
  Apr 15 08:50:44.946: INFO: Got endpoints: latency-svc-44rzg [754.993841ms]
  Apr 15 08:50:44.994: INFO: Got endpoints: latency-svc-9pfnc [750.827488ms]
  Apr 15 08:50:45.043: INFO: Got endpoints: latency-svc-7rr6f [748.972507ms]
  Apr 15 08:50:45.096: INFO: Got endpoints: latency-svc-rxqnk [750.773416ms]
  Apr 15 08:50:45.146: INFO: Got endpoints: latency-svc-bsj4l [754.133684ms]
  Apr 15 08:50:45.195: INFO: Got endpoints: latency-svc-6dhjn [752.756573ms]
  Apr 15 08:50:45.253: INFO: Got endpoints: latency-svc-g4whq [754.716879ms]
  Apr 15 08:50:45.293: INFO: Got endpoints: latency-svc-cvx7m [740.422887ms]
  Apr 15 08:50:45.293: INFO: Latencies: [56.237804ms 78.113141ms 88.38581ms 122.9616ms 123.751334ms 150.421187ms 171.015544ms 178.386163ms 183.635681ms 220.103083ms 241.92022ms 249.930592ms 268.174912ms 308.427734ms 311.490467ms 311.550386ms 312.747811ms 313.43193ms 324.481036ms 328.044958ms 328.833576ms 333.361692ms 334.476648ms 338.048124ms 342.525871ms 348.696317ms 348.827451ms 350.739587ms 352.16279ms 353.191844ms 357.356516ms 358.596265ms 359.694734ms 359.837485ms 362.125663ms 368.452624ms 373.261301ms 377.887767ms 377.949946ms 378.256927ms 381.149692ms 384.903488ms 384.939266ms 387.578233ms 387.663092ms 400.210945ms 408.564456ms 408.794577ms 414.304931ms 414.686468ms 415.171758ms 415.839719ms 435.738324ms 454.534241ms 456.388591ms 477.071473ms 480.961905ms 480.981094ms 482.056075ms 486.601287ms 498.395172ms 522.086119ms 531.451354ms 531.657835ms 532.024599ms 542.876003ms 545.801646ms 557.041006ms 587.290416ms 606.523496ms 618.616034ms 622.567934ms 634.62978ms 729.56893ms 731.523837ms 731.677677ms 735.597272ms 736.383881ms 738.048898ms 739.519672ms 740.317427ms 740.422887ms 741.085813ms 741.377527ms 741.416955ms 742.092588ms 742.125919ms 742.390037ms 743.045783ms 743.207017ms 743.406206ms 743.510517ms 743.912995ms 743.937323ms 743.961633ms 744.088561ms 744.098306ms 744.27155ms 744.433397ms 744.572835ms 744.65269ms 744.712005ms 744.839321ms 745.120737ms 745.459027ms 745.530855ms 745.610418ms 745.82635ms 746.080467ms 746.223614ms 746.2426ms 746.451166ms 746.614447ms 747.025847ms 747.112814ms 747.344862ms 747.627057ms 747.695954ms 748.0778ms 748.177601ms 748.346628ms 748.378315ms 748.397461ms 748.750248ms 748.795142ms 748.906577ms 748.972507ms 749.520709ms 749.892473ms 749.901692ms 750.017095ms 750.59096ms 750.773416ms 750.827488ms 750.926673ms 750.946156ms 750.946957ms 751.11627ms 751.219439ms 751.434415ms 751.497765ms 751.519578ms 751.539307ms 751.655716ms 751.74113ms 751.845193ms 752.142289ms 752.186857ms 752.358886ms 752.413194ms 752.51048ms 752.589087ms 752.620471ms 752.717494ms 752.756573ms 753.312005ms 753.315299ms 753.362841ms 753.564174ms 753.968459ms 754.021823ms 754.086673ms 754.133684ms 754.161193ms 754.173634ms 754.184359ms 754.192105ms 754.373561ms 754.562536ms 754.563513ms 754.611702ms 754.716879ms 754.993841ms 754.995242ms 755.082592ms 756.061861ms 756.804195ms 756.972591ms 757.103856ms 757.266627ms 757.36295ms 757.626565ms 757.856176ms 758.057166ms 758.49651ms 758.71744ms 759.429191ms 759.584707ms 760.010511ms 760.261087ms 760.261833ms 760.885966ms 761.065057ms 763.452652ms 770.160747ms 787.176729ms 788.845471ms 794.841792ms 895.247631ms 933.658201ms]
  Apr 15 08:50:45.294: INFO: 50 %ile: 744.65269ms
  Apr 15 08:50:45.294: INFO: 90 %ile: 757.36295ms
  Apr 15 08:50:45.294: INFO: 99 %ile: 895.247631ms
  Apr 15 08:50:45.294: INFO: Total sample count: 200
  Apr 15 08:50:45.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-2241" for this suite. @ 04/15/24 08:50:45.315
• [10.873 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 04/15/24 08:50:45.346
  Apr 15 08:50:45.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename containers @ 04/15/24 08:50:45.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:50:45.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:50:45.41
  STEP: Creating a pod to test override arguments @ 04/15/24 08:50:45.42
  STEP: Saw pod success @ 04/15/24 08:50:49.482
  Apr 15 08:50:49.491: INFO: Trying to get logs from node ahz3daisheng-3 pod client-containers-f39b410e-a3f0-4606-8ca1-88984b17a12b container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 08:50:49.506
  Apr 15 08:50:49.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-9130" for this suite. @ 04/15/24 08:50:49.573
• [4.251 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 04/15/24 08:50:49.599
  Apr 15 08:50:49.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 08:50:49.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:50:49.645
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:50:49.65
  STEP: create deployment with httpd image @ 04/15/24 08:50:49.656
  Apr 15 08:50:49.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9313 create -f -'
  Apr 15 08:50:50.307: INFO: stderr: ""
  Apr 15 08:50:50.307: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 04/15/24 08:50:50.307
  Apr 15 08:50:50.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9313 diff -f -'
  Apr 15 08:50:51.100: INFO: rc: 1
  Apr 15 08:50:51.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9313 delete -f -'
  Apr 15 08:50:51.444: INFO: stderr: ""
  Apr 15 08:50:51.444: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Apr 15 08:50:51.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9313" for this suite. @ 04/15/24 08:50:51.476
• [1.925 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 04/15/24 08:50:51.525
  Apr 15 08:50:51.525: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename csiinlinevolumes @ 04/15/24 08:50:51.529
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:50:51.591
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:50:51.606
  STEP: creating @ 04/15/24 08:50:51.615
  STEP: getting @ 04/15/24 08:50:51.718
  STEP: listing @ 04/15/24 08:50:51.79
  STEP: deleting @ 04/15/24 08:50:51.813
  Apr 15 08:50:51.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-6452" for this suite. @ 04/15/24 08:50:51.931
• [0.436 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 04/15/24 08:50:51.982
  Apr 15 08:50:51.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 08:50:51.984
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:50:52.036
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:50:52.062
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 08:50:52.079
  STEP: Saw pod success @ 04/15/24 08:50:56.194
  Apr 15 08:50:56.207: INFO: Trying to get logs from node ahz3daisheng-3 pod downwardapi-volume-54faa577-7db1-4581-8cb4-cb88875f19ad container client-container: <nil>
  STEP: delete the pod @ 04/15/24 08:50:56.232
  Apr 15 08:50:56.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-942" for this suite. @ 04/15/24 08:50:56.31
• [4.370 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:748
  STEP: Creating a kubernetes client @ 04/15/24 08:50:56.357
  Apr 15 08:50:56.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename statefulset @ 04/15/24 08:50:56.361
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:50:56.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:50:56.428
  STEP: Creating service test in namespace statefulset-899 @ 04/15/24 08:50:56.442
  STEP: Creating stateful set ss in namespace statefulset-899 @ 04/15/24 08:50:56.461
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-899 @ 04/15/24 08:50:56.502
  Apr 15 08:50:56.528: INFO: Found 0 stateful pods, waiting for 1
  Apr 15 08:51:06.542: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 04/15/24 08:51:06.542
  Apr 15 08:51:06.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-899 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 08:51:06.854: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 08:51:06.854: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 08:51:06.854: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 15 08:51:06.883: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Apr 15 08:51:16.892: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 15 08:51:16.892: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 08:51:16.934: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Apr 15 08:51:16.935: INFO: ss-0  ahz3daisheng-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:50:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:50:56 +0000 UTC  }]
  Apr 15 08:51:16.936: INFO: 
  Apr 15 08:51:16.936: INFO: StatefulSet ss has not reached scale 3, at 1
  Apr 15 08:51:17.947: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.985816s
  Apr 15 08:51:18.958: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973484899s
  Apr 15 08:51:19.972: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.963634914s
  Apr 15 08:51:20.982: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.949429607s
  Apr 15 08:51:21.994: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.940519028s
  Apr 15 08:51:23.005: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.927482132s
  Apr 15 08:51:24.014: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.917681519s
  Apr 15 08:51:25.025: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.908499403s
  Apr 15 08:51:26.035: INFO: Verifying statefulset ss doesn't scale past 3 for another 897.288309ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-899 @ 04/15/24 08:51:27.036
  Apr 15 08:51:27.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-899 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 15 08:51:27.331: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 15 08:51:27.331: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 15 08:51:27.331: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 15 08:51:27.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-899 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 15 08:51:27.662: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Apr 15 08:51:27.662: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 15 08:51:27.662: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 15 08:51:27.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-899 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 15 08:51:27.969: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Apr 15 08:51:27.969: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 15 08:51:27.969: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 15 08:51:27.988: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 08:51:27.989: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 08:51:27.989: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 04/15/24 08:51:27.989
  Apr 15 08:51:28.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-899 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 08:51:28.320: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 08:51:28.321: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 08:51:28.321: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 15 08:51:28.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-899 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 08:51:28.630: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 08:51:28.630: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 08:51:28.630: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 15 08:51:28.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-899 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 08:51:28.987: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 08:51:28.987: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 08:51:28.987: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 15 08:51:28.987: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 08:51:28.996: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  Apr 15 08:51:39.013: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Apr 15 08:51:39.013: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Apr 15 08:51:39.013: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Apr 15 08:51:39.040: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Apr 15 08:51:39.040: INFO: ss-0  ahz3daisheng-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:50:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:50:56 +0000 UTC  }]
  Apr 15 08:51:39.040: INFO: ss-1  ahz3daisheng-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:16 +0000 UTC  }]
  Apr 15 08:51:39.040: INFO: ss-2  ahz3daisheng-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:29 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:16 +0000 UTC  }]
  Apr 15 08:51:39.040: INFO: 
  Apr 15 08:51:39.040: INFO: StatefulSet ss has not reached scale 0, at 3
  Apr 15 08:51:40.053: INFO: POD   NODE            PHASE      GRACE  CONDITIONS
  Apr 15 08:51:40.053: INFO: ss-1  ahz3daisheng-1  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:16 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:29 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:29 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:16 +0000 UTC  }]
  Apr 15 08:51:40.053: INFO: ss-2  ahz3daisheng-2  Succeeded  0s     [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:16 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:29 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:29 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 08:51:16 +0000 UTC  }]
  Apr 15 08:51:40.053: INFO: 
  Apr 15 08:51:40.053: INFO: StatefulSet ss has not reached scale 0, at 2
  Apr 15 08:51:41.059: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.977160067s
  Apr 15 08:51:42.068: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.970429775s
  Apr 15 08:51:43.077: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.961070123s
  Apr 15 08:51:44.085: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.952687876s
  Apr 15 08:51:45.095: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.943607614s
  Apr 15 08:51:46.103: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.934390947s
  Apr 15 08:51:47.114: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.926304782s
  Apr 15 08:51:48.123: INFO: Verifying statefulset ss doesn't scale past 0 for another 915.725713ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-899 @ 04/15/24 08:51:49.123
  Apr 15 08:51:49.135: INFO: Scaling statefulset ss to 0
  Apr 15 08:51:49.161: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 08:51:49.168: INFO: Deleting all statefulset in ns statefulset-899
  Apr 15 08:51:49.173: INFO: Scaling statefulset ss to 0
  Apr 15 08:51:49.192: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 08:51:49.197: INFO: Deleting statefulset ss
  Apr 15 08:51:49.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-899" for this suite. @ 04/15/24 08:51:49.244
• [52.907 seconds]
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 04/15/24 08:51:49.266
  Apr 15 08:51:49.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename security-context-test @ 04/15/24 08:51:49.269
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:51:49.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:51:49.304
  Apr 15 08:51:53.387: INFO: Got logs for pod "busybox-privileged-false-b75fb445-cb1b-42ac-82a1-d0b68e99296d": "ip: RTNETLINK answers: Operation not permitted\n"
  Apr 15 08:51:53.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-3512" for this suite. @ 04/15/24 08:51:53.402
• [4.153 seconds]
------------------------------
SSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 04/15/24 08:51:53.422
  Apr 15 08:51:53.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 08:51:53.425
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:51:53.477
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:51:53.481
  STEP: Creating a pod to test downward api env vars @ 04/15/24 08:51:53.487
  STEP: Saw pod success @ 04/15/24 08:51:57.542
  Apr 15 08:51:57.553: INFO: Trying to get logs from node ahz3daisheng-3 pod downward-api-b6d2196a-17f2-4b12-920a-686bc2b27c19 container dapi-container: <nil>
  STEP: delete the pod @ 04/15/24 08:51:57.568
  Apr 15 08:51:57.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1372" for this suite. @ 04/15/24 08:51:57.619
• [4.214 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 04/15/24 08:51:57.653
  Apr 15 08:51:57.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 08:51:57.657
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:51:57.686
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:51:57.692
  STEP: Counting existing ResourceQuota @ 04/15/24 08:51:57.697
  STEP: Creating a ResourceQuota @ 04/15/24 08:52:02.709
  STEP: Ensuring resource quota status is calculated @ 04/15/24 08:52:02.738
  STEP: Creating a ReplicaSet @ 04/15/24 08:52:04.758
  STEP: Ensuring resource quota status captures replicaset creation @ 04/15/24 08:52:04.798
  STEP: Deleting a ReplicaSet @ 04/15/24 08:52:06.811
  STEP: Ensuring resource quota status released usage @ 04/15/24 08:52:06.841
  Apr 15 08:52:08.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3540" for this suite. @ 04/15/24 08:52:08.865
• [11.229 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 04/15/24 08:52:08.884
  Apr 15 08:52:08.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename sched-preemption @ 04/15/24 08:52:08.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:52:08.92
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:52:08.925
  Apr 15 08:52:08.961: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 15 08:53:09.029: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 04/15/24 08:53:09.043
  Apr 15 08:53:09.138: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Apr 15 08:53:09.179: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Apr 15 08:53:09.282: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Apr 15 08:53:09.329: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Apr 15 08:53:09.445: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Apr 15 08:53:09.481: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 04/15/24 08:53:09.481
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 04/15/24 08:53:11.561
  Apr 15 08:53:15.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-5882" for this suite. @ 04/15/24 08:53:15.818
• [66.949 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 04/15/24 08:53:15.841
  Apr 15 08:53:15.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 08:53:15.847
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:53:15.875
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:53:15.89
  STEP: creating a replication controller @ 04/15/24 08:53:15.904
  Apr 15 08:53:15.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 create -f -'
  Apr 15 08:53:16.839: INFO: stderr: ""
  Apr 15 08:53:16.839: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/15/24 08:53:16.841
  Apr 15 08:53:16.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 15 08:53:17.029: INFO: stderr: ""
  Apr 15 08:53:17.029: INFO: stdout: "update-demo-nautilus-h4v2s update-demo-nautilus-zs29w "
  Apr 15 08:53:17.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 get pods update-demo-nautilus-h4v2s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 15 08:53:17.199: INFO: stderr: ""
  Apr 15 08:53:17.199: INFO: stdout: ""
  Apr 15 08:53:17.199: INFO: update-demo-nautilus-h4v2s is created but not running
  Apr 15 08:53:22.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 15 08:53:22.423: INFO: stderr: ""
  Apr 15 08:53:22.424: INFO: stdout: "update-demo-nautilus-h4v2s update-demo-nautilus-zs29w "
  Apr 15 08:53:22.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 get pods update-demo-nautilus-h4v2s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 15 08:53:22.657: INFO: stderr: ""
  Apr 15 08:53:22.657: INFO: stdout: "true"
  Apr 15 08:53:22.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 get pods update-demo-nautilus-h4v2s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 15 08:53:22.864: INFO: stderr: ""
  Apr 15 08:53:22.865: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 15 08:53:22.865: INFO: validating pod update-demo-nautilus-h4v2s
  Apr 15 08:53:22.919: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 15 08:53:22.919: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 15 08:53:22.919: INFO: update-demo-nautilus-h4v2s is verified up and running
  Apr 15 08:53:22.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 get pods update-demo-nautilus-zs29w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 15 08:53:23.092: INFO: stderr: ""
  Apr 15 08:53:23.093: INFO: stdout: "true"
  Apr 15 08:53:23.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 get pods update-demo-nautilus-zs29w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 15 08:53:23.280: INFO: stderr: ""
  Apr 15 08:53:23.280: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 15 08:53:23.280: INFO: validating pod update-demo-nautilus-zs29w
  Apr 15 08:53:23.298: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 15 08:53:23.298: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 15 08:53:23.298: INFO: update-demo-nautilus-zs29w is verified up and running
  STEP: scaling down the replication controller @ 04/15/24 08:53:23.298
  Apr 15 08:53:23.319: INFO: scanned /root for discovery docs: <nil>
  Apr 15 08:53:23.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  Apr 15 08:53:24.518: INFO: stderr: ""
  Apr 15 08:53:24.518: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/15/24 08:53:24.518
  Apr 15 08:53:24.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 15 08:53:24.675: INFO: stderr: ""
  Apr 15 08:53:24.675: INFO: stdout: "update-demo-nautilus-h4v2s "
  Apr 15 08:53:24.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 get pods update-demo-nautilus-h4v2s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 15 08:53:24.860: INFO: stderr: ""
  Apr 15 08:53:24.861: INFO: stdout: "true"
  Apr 15 08:53:24.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 get pods update-demo-nautilus-h4v2s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 15 08:53:25.026: INFO: stderr: ""
  Apr 15 08:53:25.026: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 15 08:53:25.026: INFO: validating pod update-demo-nautilus-h4v2s
  Apr 15 08:53:25.036: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 15 08:53:25.036: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 15 08:53:25.036: INFO: update-demo-nautilus-h4v2s is verified up and running
  STEP: scaling up the replication controller @ 04/15/24 08:53:25.036
  Apr 15 08:53:25.047: INFO: scanned /root for discovery docs: <nil>
  Apr 15 08:53:25.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  Apr 15 08:53:26.263: INFO: stderr: ""
  Apr 15 08:53:26.263: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/15/24 08:53:26.263
  Apr 15 08:53:26.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 15 08:53:26.480: INFO: stderr: ""
  Apr 15 08:53:26.480: INFO: stdout: "update-demo-nautilus-4g75b update-demo-nautilus-h4v2s "
  Apr 15 08:53:26.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 get pods update-demo-nautilus-4g75b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 15 08:53:26.635: INFO: stderr: ""
  Apr 15 08:53:26.635: INFO: stdout: "true"
  Apr 15 08:53:26.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 get pods update-demo-nautilus-4g75b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 15 08:53:26.773: INFO: stderr: ""
  Apr 15 08:53:26.773: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 15 08:53:26.773: INFO: validating pod update-demo-nautilus-4g75b
  Apr 15 08:53:26.788: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 15 08:53:26.789: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 15 08:53:26.789: INFO: update-demo-nautilus-4g75b is verified up and running
  Apr 15 08:53:26.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 get pods update-demo-nautilus-h4v2s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 15 08:53:26.960: INFO: stderr: ""
  Apr 15 08:53:26.960: INFO: stdout: "true"
  Apr 15 08:53:26.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 get pods update-demo-nautilus-h4v2s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 15 08:53:27.112: INFO: stderr: ""
  Apr 15 08:53:27.112: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 15 08:53:27.112: INFO: validating pod update-demo-nautilus-h4v2s
  Apr 15 08:53:27.132: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 15 08:53:27.132: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 15 08:53:27.132: INFO: update-demo-nautilus-h4v2s is verified up and running
  STEP: using delete to clean up resources @ 04/15/24 08:53:27.132
  Apr 15 08:53:27.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 delete --grace-period=0 --force -f -'
  Apr 15 08:53:27.287: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 15 08:53:27.287: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Apr 15 08:53:27.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 get rc,svc -l name=update-demo --no-headers'
  Apr 15 08:53:27.599: INFO: stderr: "No resources found in kubectl-6474 namespace.\n"
  Apr 15 08:53:27.599: INFO: stdout: ""
  Apr 15 08:53:27.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-6474 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 15 08:53:27.797: INFO: stderr: ""
  Apr 15 08:53:27.797: INFO: stdout: ""
  Apr 15 08:53:27.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6474" for this suite. @ 04/15/24 08:53:27.809
• [11.983 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 04/15/24 08:53:27.825
  Apr 15 08:53:27.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 08:53:27.827
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:53:27.856
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:53:27.861
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 08:53:27.867
  STEP: Saw pod success @ 04/15/24 08:53:31.922
  Apr 15 08:53:31.929: INFO: Trying to get logs from node ahz3daisheng-3 pod downwardapi-volume-1cc7292b-a47f-4581-9da6-b041ba334929 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 08:53:31.973
  Apr 15 08:53:32.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-638" for this suite. @ 04/15/24 08:53:32.031
• [4.221 seconds]
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 04/15/24 08:53:32.046
  Apr 15 08:53:32.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename namespaces @ 04/15/24 08:53:32.049
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:53:32.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:53:32.087
  STEP: Read namespace status @ 04/15/24 08:53:32.1
  Apr 15 08:53:32.106: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 04/15/24 08:53:32.107
  Apr 15 08:53:32.131: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 04/15/24 08:53:32.132
  Apr 15 08:53:32.153: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Apr 15 08:53:32.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3568" for this suite. @ 04/15/24 08:53:32.166
• [0.134 seconds]
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:1028
  STEP: Creating a kubernetes client @ 04/15/24 08:53:32.182
  Apr 15 08:53:32.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename statefulset @ 04/15/24 08:53:32.184
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:53:32.211
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:53:32.216
  STEP: Creating service test in namespace statefulset-9119 @ 04/15/24 08:53:32.221
  STEP: Creating statefulset ss in namespace statefulset-9119 @ 04/15/24 08:53:32.246
  Apr 15 08:53:32.274: INFO: Found 0 stateful pods, waiting for 1
  Apr 15 08:53:42.284: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 04/15/24 08:53:42.302
  STEP: Getting /status @ 04/15/24 08:53:42.321
  Apr 15 08:53:42.330: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 04/15/24 08:53:42.33
  Apr 15 08:53:42.352: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 04/15/24 08:53:42.352
  Apr 15 08:53:42.358: INFO: Observed &StatefulSet event: ADDED
  Apr 15 08:53:42.358: INFO: Found Statefulset ss in namespace statefulset-9119 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Apr 15 08:53:42.358: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 04/15/24 08:53:42.358
  Apr 15 08:53:42.358: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Apr 15 08:53:42.373: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 04/15/24 08:53:42.373
  Apr 15 08:53:42.379: INFO: Observed &StatefulSet event: ADDED
  Apr 15 08:53:42.379: INFO: Deleting all statefulset in ns statefulset-9119
  Apr 15 08:53:42.387: INFO: Scaling statefulset ss to 0
  Apr 15 08:53:52.434: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 08:53:52.441: INFO: Deleting statefulset ss
  Apr 15 08:53:52.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9119" for this suite. @ 04/15/24 08:53:52.511
• [20.352 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 04/15/24 08:53:52.542
  Apr 15 08:53:52.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename events @ 04/15/24 08:53:52.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:53:52.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:53:52.603
  STEP: Create set of events @ 04/15/24 08:53:52.608
  Apr 15 08:53:52.628: INFO: created test-event-1
  Apr 15 08:53:52.637: INFO: created test-event-2
  Apr 15 08:53:52.646: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 04/15/24 08:53:52.646
  STEP: delete collection of events @ 04/15/24 08:53:52.652
  Apr 15 08:53:52.652: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 04/15/24 08:53:52.796
  Apr 15 08:53:52.796: INFO: requesting list of events to confirm quantity
  Apr 15 08:53:52.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8482" for this suite. @ 04/15/24 08:53:52.819
• [0.293 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 04/15/24 08:53:52.843
  Apr 15 08:53:52.843: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename dns @ 04/15/24 08:53:52.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:53:52.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:53:52.891
  STEP: Creating a test headless service @ 04/15/24 08:53:52.895
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7611 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7611;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7611 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7611;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7611.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7611.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7611.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7611.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7611.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7611.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7611.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7611.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7611.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7611.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7611.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7611.svc;check="$$(dig +notcp +noall +answer +search 58.58.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.58.58_udp@PTR;check="$$(dig +tcp +noall +answer +search 58.58.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.58.58_tcp@PTR;sleep 1; done
   @ 04/15/24 08:53:52.953
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7611 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7611;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7611 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7611;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7611.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7611.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7611.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7611.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7611.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7611.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7611.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7611.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7611.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7611.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7611.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7611.svc;check="$$(dig +notcp +noall +answer +search 58.58.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.58.58_udp@PTR;check="$$(dig +tcp +noall +answer +search 58.58.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.58.58_tcp@PTR;sleep 1; done
   @ 04/15/24 08:53:52.953
  STEP: creating a pod to probe DNS @ 04/15/24 08:53:52.954
  STEP: submitting the pod to kubernetes @ 04/15/24 08:53:52.955
  STEP: retrieving the pod @ 04/15/24 08:53:55.007
  STEP: looking for the results for each expected name from probers @ 04/15/24 08:53:55.015
  Apr 15 08:53:55.031: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:53:55.043: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:53:55.053: INFO: Unable to read wheezy_udp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:53:55.062: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:53:55.071: INFO: Unable to read wheezy_udp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:53:55.079: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:53:55.088: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:53:55.098: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:53:55.156: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:53:55.164: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:53:55.173: INFO: Unable to read jessie_udp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:53:55.182: INFO: Unable to read jessie_tcp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:53:55.191: INFO: Unable to read jessie_udp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:53:55.199: INFO: Unable to read jessie_tcp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:53:55.211: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:53:55.219: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:53:55.265: INFO: Lookups using dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7611 wheezy_tcp@dns-test-service.dns-7611 wheezy_udp@dns-test-service.dns-7611.svc wheezy_tcp@dns-test-service.dns-7611.svc wheezy_udp@_http._tcp.dns-test-service.dns-7611.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7611.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7611 jessie_tcp@dns-test-service.dns-7611 jessie_udp@dns-test-service.dns-7611.svc jessie_tcp@dns-test-service.dns-7611.svc jessie_udp@_http._tcp.dns-test-service.dns-7611.svc jessie_tcp@_http._tcp.dns-test-service.dns-7611.svc]

  Apr 15 08:54:00.273: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:00.285: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:00.294: INFO: Unable to read wheezy_udp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:00.301: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:00.308: INFO: Unable to read wheezy_udp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:00.317: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:00.324: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:00.333: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:00.372: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:00.380: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:00.389: INFO: Unable to read jessie_udp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:00.396: INFO: Unable to read jessie_tcp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:00.404: INFO: Unable to read jessie_udp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:00.413: INFO: Unable to read jessie_tcp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:00.421: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:00.430: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:00.464: INFO: Lookups using dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7611 wheezy_tcp@dns-test-service.dns-7611 wheezy_udp@dns-test-service.dns-7611.svc wheezy_tcp@dns-test-service.dns-7611.svc wheezy_udp@_http._tcp.dns-test-service.dns-7611.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7611.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7611 jessie_tcp@dns-test-service.dns-7611 jessie_udp@dns-test-service.dns-7611.svc jessie_tcp@dns-test-service.dns-7611.svc jessie_udp@_http._tcp.dns-test-service.dns-7611.svc jessie_tcp@_http._tcp.dns-test-service.dns-7611.svc]

  Apr 15 08:54:05.280: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:05.290: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:05.302: INFO: Unable to read wheezy_udp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:05.311: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:05.323: INFO: Unable to read wheezy_udp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:05.338: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:05.350: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:05.361: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:05.431: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:05.441: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:05.450: INFO: Unable to read jessie_udp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:05.463: INFO: Unable to read jessie_tcp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:05.473: INFO: Unable to read jessie_udp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:05.482: INFO: Unable to read jessie_tcp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:05.492: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:05.500: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:05.546: INFO: Lookups using dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7611 wheezy_tcp@dns-test-service.dns-7611 wheezy_udp@dns-test-service.dns-7611.svc wheezy_tcp@dns-test-service.dns-7611.svc wheezy_udp@_http._tcp.dns-test-service.dns-7611.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7611.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7611 jessie_tcp@dns-test-service.dns-7611 jessie_udp@dns-test-service.dns-7611.svc jessie_tcp@dns-test-service.dns-7611.svc jessie_udp@_http._tcp.dns-test-service.dns-7611.svc jessie_tcp@_http._tcp.dns-test-service.dns-7611.svc]

  Apr 15 08:54:10.275: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:10.282: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:10.289: INFO: Unable to read wheezy_udp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:10.297: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:10.302: INFO: Unable to read wheezy_udp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:10.310: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:10.319: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:10.325: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:10.381: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:10.394: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:10.402: INFO: Unable to read jessie_udp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:10.416: INFO: Unable to read jessie_tcp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:10.424: INFO: Unable to read jessie_udp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:10.443: INFO: Unable to read jessie_tcp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:10.453: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:10.467: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:10.508: INFO: Lookups using dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7611 wheezy_tcp@dns-test-service.dns-7611 wheezy_udp@dns-test-service.dns-7611.svc wheezy_tcp@dns-test-service.dns-7611.svc wheezy_udp@_http._tcp.dns-test-service.dns-7611.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7611.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7611 jessie_tcp@dns-test-service.dns-7611 jessie_udp@dns-test-service.dns-7611.svc jessie_tcp@dns-test-service.dns-7611.svc jessie_udp@_http._tcp.dns-test-service.dns-7611.svc jessie_tcp@_http._tcp.dns-test-service.dns-7611.svc]

  Apr 15 08:54:15.277: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:15.285: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:15.296: INFO: Unable to read wheezy_udp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:15.309: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:15.323: INFO: Unable to read wheezy_udp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:15.332: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:15.340: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:15.347: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:15.384: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:15.394: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:15.403: INFO: Unable to read jessie_udp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:15.411: INFO: Unable to read jessie_tcp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:15.419: INFO: Unable to read jessie_udp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:15.429: INFO: Unable to read jessie_tcp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:15.435: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:15.444: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:15.478: INFO: Lookups using dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7611 wheezy_tcp@dns-test-service.dns-7611 wheezy_udp@dns-test-service.dns-7611.svc wheezy_tcp@dns-test-service.dns-7611.svc wheezy_udp@_http._tcp.dns-test-service.dns-7611.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7611.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7611 jessie_tcp@dns-test-service.dns-7611 jessie_udp@dns-test-service.dns-7611.svc jessie_tcp@dns-test-service.dns-7611.svc jessie_udp@_http._tcp.dns-test-service.dns-7611.svc jessie_tcp@_http._tcp.dns-test-service.dns-7611.svc]

  Apr 15 08:54:20.277: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:20.289: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:20.298: INFO: Unable to read wheezy_udp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:20.315: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:20.326: INFO: Unable to read wheezy_udp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:20.341: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:20.354: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:20.366: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:20.417: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:20.425: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:20.434: INFO: Unable to read jessie_udp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:20.443: INFO: Unable to read jessie_tcp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:20.449: INFO: Unable to read jessie_udp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:20.458: INFO: Unable to read jessie_tcp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:20.466: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:20.478: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:20.520: INFO: Lookups using dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7611 wheezy_tcp@dns-test-service.dns-7611 wheezy_udp@dns-test-service.dns-7611.svc wheezy_tcp@dns-test-service.dns-7611.svc wheezy_udp@_http._tcp.dns-test-service.dns-7611.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7611.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7611 jessie_tcp@dns-test-service.dns-7611 jessie_udp@dns-test-service.dns-7611.svc jessie_tcp@dns-test-service.dns-7611.svc jessie_udp@_http._tcp.dns-test-service.dns-7611.svc jessie_tcp@_http._tcp.dns-test-service.dns-7611.svc]

  Apr 15 08:54:25.368: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:25.380: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:25.394: INFO: Unable to read jessie_udp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:25.408: INFO: Unable to read jessie_tcp@dns-test-service.dns-7611 from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:25.416: INFO: Unable to read jessie_udp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:25.424: INFO: Unable to read jessie_tcp@dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:25.432: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:25.441: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7611.svc from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:25.470: INFO: Lookups using dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3 failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7611 jessie_tcp@dns-test-service.dns-7611 jessie_udp@dns-test-service.dns-7611.svc jessie_tcp@dns-test-service.dns-7611.svc jessie_udp@_http._tcp.dns-test-service.dns-7611.svc jessie_tcp@_http._tcp.dns-test-service.dns-7611.svc]

  Apr 15 08:54:30.419: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3: the server could not find the requested resource (get pods dns-test-29aba830-106b-4a18-bf92-94204196d4e3)
  Apr 15 08:54:30.531: INFO: Lookups using dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3 failed for: [jessie_udp@dns-test-service]

  Apr 15 08:54:35.481: INFO: DNS probes using dns-7611/dns-test-29aba830-106b-4a18-bf92-94204196d4e3 succeeded

  Apr 15 08:54:35.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 08:54:35.495
  STEP: deleting the test service @ 04/15/24 08:54:35.557
  STEP: deleting the test headless service @ 04/15/24 08:54:35.655
  STEP: Destroying namespace "dns-7611" for this suite. @ 04/15/24 08:54:35.734
• [42.923 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 04/15/24 08:54:35.798
  Apr 15 08:54:35.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename var-expansion @ 04/15/24 08:54:35.809
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:54:35.848
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:54:35.861
  Apr 15 08:54:37.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 08:54:37.928: INFO: Deleting pod "var-expansion-36ecf7c5-9bf9-4126-ac4a-a7c556010255" in namespace "var-expansion-1847"
  Apr 15 08:54:37.944: INFO: Wait up to 5m0s for pod "var-expansion-36ecf7c5-9bf9-4126-ac4a-a7c556010255" to be fully deleted
  STEP: Destroying namespace "var-expansion-1847" for this suite. @ 04/15/24 08:54:39.97
• [4.191 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 04/15/24 08:54:39.99
  Apr 15 08:54:39.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 08:54:39.994
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:54:40.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:54:40.043
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 04/15/24 08:54:40.048
  STEP: Saw pod success @ 04/15/24 08:54:44.103
  Apr 15 08:54:44.111: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-3e535508-d390-4ca6-8db0-bd9915d9dac0 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 08:54:44.126
  Apr 15 08:54:44.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9851" for this suite. @ 04/15/24 08:54:44.169
• [4.201 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 04/15/24 08:54:44.193
  Apr 15 08:54:44.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 08:54:44.195
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:54:44.233
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:54:44.241
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 08:54:44.247
  STEP: Saw pod success @ 04/15/24 08:54:48.311
  Apr 15 08:54:48.317: INFO: Trying to get logs from node ahz3daisheng-3 pod downwardapi-volume-303189d2-6773-4a31-9663-bd731bbf7968 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 08:54:48.334
  Apr 15 08:54:48.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8204" for this suite. @ 04/15/24 08:54:48.378
• [4.202 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 04/15/24 08:54:48.398
  Apr 15 08:54:48.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 08:54:48.401
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:54:48.432
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:54:48.437
  STEP: Creating pod test-grpc-7808e322-e1dd-4db0-9558-67773632acd5 in namespace container-probe-3932 @ 04/15/24 08:54:48.446
  Apr 15 08:54:50.478: INFO: Started pod test-grpc-7808e322-e1dd-4db0-9558-67773632acd5 in namespace container-probe-3932
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/15/24 08:54:50.478
  Apr 15 08:54:50.484: INFO: Initial restart count of pod test-grpc-7808e322-e1dd-4db0-9558-67773632acd5 is 0
  Apr 15 08:58:51.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 08:58:51.75
  STEP: Destroying namespace "container-probe-3932" for this suite. @ 04/15/24 08:58:51.789
• [243.446 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:899
  STEP: Creating a kubernetes client @ 04/15/24 08:58:51.846
  Apr 15 08:58:51.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename statefulset @ 04/15/24 08:58:51.853
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:58:51.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:58:51.897
  STEP: Creating service test in namespace statefulset-7712 @ 04/15/24 08:58:51.903
  STEP: Creating statefulset ss in namespace statefulset-7712 @ 04/15/24 08:58:51.923
  Apr 15 08:58:51.976: INFO: Found 0 stateful pods, waiting for 1
  Apr 15 08:59:01.985: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 04/15/24 08:59:01.997
  STEP: updating a scale subresource @ 04/15/24 08:59:02.005
  STEP: verifying the statefulset Spec.Replicas was modified @ 04/15/24 08:59:02.02
  STEP: Patch a scale subresource @ 04/15/24 08:59:02.025
  STEP: verifying the statefulset Spec.Replicas was modified @ 04/15/24 08:59:02.045
  Apr 15 08:59:02.064: INFO: Deleting all statefulset in ns statefulset-7712
  Apr 15 08:59:02.097: INFO: Scaling statefulset ss to 0
  Apr 15 08:59:12.157: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 08:59:12.162: INFO: Deleting statefulset ss
  Apr 15 08:59:12.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7712" for this suite. @ 04/15/24 08:59:12.219
• [20.398 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 04/15/24 08:59:12.258
  Apr 15 08:59:12.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename configmap @ 04/15/24 08:59:12.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:59:12.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:59:12.301
  STEP: Creating configMap that has name configmap-test-emptyKey-5f6e09d4-46a5-44fe-8ba0-95006499ad43 @ 04/15/24 08:59:12.308
  Apr 15 08:59:12.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-748" for this suite. @ 04/15/24 08:59:12.322
• [0.078 seconds]
------------------------------
S
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 04/15/24 08:59:12.341
  Apr 15 08:59:12.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename cronjob @ 04/15/24 08:59:12.347
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 08:59:12.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 08:59:12.39
  STEP: Creating a ForbidConcurrent cronjob @ 04/15/24 08:59:12.396
  STEP: Ensuring a job is scheduled @ 04/15/24 08:59:12.407
  STEP: Ensuring exactly one is scheduled @ 04/15/24 09:00:00.416
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 04/15/24 09:00:00.423
  STEP: Ensuring no more jobs are scheduled @ 04/15/24 09:00:00.432
  STEP: Removing cronjob @ 04/15/24 09:05:00.456
  Apr 15 09:05:00.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5958" for this suite. @ 04/15/24 09:05:00.482
• [348.158 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 04/15/24 09:05:00.502
  Apr 15 09:05:00.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-runtime @ 04/15/24 09:05:00.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:05:00.545
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:05:00.556
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 04/15/24 09:05:00.579
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 04/15/24 09:05:19.748
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 04/15/24 09:05:19.754
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 04/15/24 09:05:19.767
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 04/15/24 09:05:19.768
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 04/15/24 09:05:19.813
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 04/15/24 09:05:22.874
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 04/15/24 09:05:23.888
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 04/15/24 09:05:23.903
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 04/15/24 09:05:23.903
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 04/15/24 09:05:23.944
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 04/15/24 09:05:24.958
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 04/15/24 09:05:26.982
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 04/15/24 09:05:26.993
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 04/15/24 09:05:26.993
  Apr 15 09:05:27.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6439" for this suite. @ 04/15/24 09:05:27.049
• [26.558 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 04/15/24 09:05:27.062
  Apr 15 09:05:27.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename configmap @ 04/15/24 09:05:27.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:05:27.086
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:05:27.092
  STEP: Creating configMap configmap-8186/configmap-test-52e77f92-500d-4433-bb30-d1909d550f26 @ 04/15/24 09:05:27.098
  STEP: Creating a pod to test consume configMaps @ 04/15/24 09:05:27.112
  STEP: Saw pod success @ 04/15/24 09:05:31.153
  Apr 15 09:05:31.158: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-configmaps-3eda1a77-1fff-45e6-b969-6af1a9d2ee66 container env-test: <nil>
  STEP: delete the pod @ 04/15/24 09:05:31.198
  Apr 15 09:05:31.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8186" for this suite. @ 04/15/24 09:05:31.239
• [4.191 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 04/15/24 09:05:31.257
  Apr 15 09:05:31.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename services @ 04/15/24 09:05:31.26
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:05:31.285
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:05:31.291
  STEP: creating service endpoint-test2 in namespace services-9643 @ 04/15/24 09:05:31.297
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9643 to expose endpoints map[] @ 04/15/24 09:05:31.323
  Apr 15 09:05:31.349: INFO: successfully validated that service endpoint-test2 in namespace services-9643 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-9643 @ 04/15/24 09:05:31.349
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9643 to expose endpoints map[pod1:[80]] @ 04/15/24 09:05:33.397
  Apr 15 09:05:33.424: INFO: successfully validated that service endpoint-test2 in namespace services-9643 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 04/15/24 09:05:33.424
  Apr 15 09:05:33.425: INFO: Creating new exec pod
  Apr 15 09:05:36.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-9643 exec execpodsq4xt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 15 09:05:36.855: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 15 09:05:36.855: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 09:05:36.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-9643 exec execpodsq4xt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.39.120 80'
  Apr 15 09:05:37.153: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.39.120 80\nConnection to 10.233.39.120 80 port [tcp/http] succeeded!\n"
  Apr 15 09:05:37.153: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-9643 @ 04/15/24 09:05:37.153
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9643 to expose endpoints map[pod1:[80] pod2:[80]] @ 04/15/24 09:05:39.189
  Apr 15 09:05:39.218: INFO: successfully validated that service endpoint-test2 in namespace services-9643 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 04/15/24 09:05:39.218
  Apr 15 09:05:40.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-9643 exec execpodsq4xt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 15 09:05:40.527: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 15 09:05:40.528: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 09:05:40.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-9643 exec execpodsq4xt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.39.120 80'
  Apr 15 09:05:40.823: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.39.120 80\nConnection to 10.233.39.120 80 port [tcp/http] succeeded!\n"
  Apr 15 09:05:40.823: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-9643 @ 04/15/24 09:05:40.823
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9643 to expose endpoints map[pod2:[80]] @ 04/15/24 09:05:40.848
  Apr 15 09:05:40.889: INFO: successfully validated that service endpoint-test2 in namespace services-9643 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 04/15/24 09:05:40.889
  Apr 15 09:05:41.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-9643 exec execpodsq4xt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Apr 15 09:05:42.137: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Apr 15 09:05:42.137: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 09:05:42.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-9643 exec execpodsq4xt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.39.120 80'
  Apr 15 09:05:42.391: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.39.120 80\nConnection to 10.233.39.120 80 port [tcp/http] succeeded!\n"
  Apr 15 09:05:42.391: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-9643 @ 04/15/24 09:05:42.391
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9643 to expose endpoints map[] @ 04/15/24 09:05:42.433
  Apr 15 09:05:42.468: INFO: successfully validated that service endpoint-test2 in namespace services-9643 exposes endpoints map[]
  Apr 15 09:05:42.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9643" for this suite. @ 04/15/24 09:05:42.526
• [11.287 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 04/15/24 09:05:42.546
  Apr 15 09:05:42.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:05:42.549
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:05:42.589
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:05:42.594
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 09:05:42.599
  STEP: Saw pod success @ 04/15/24 09:05:46.649
  Apr 15 09:05:46.656: INFO: Trying to get logs from node ahz3daisheng-3 pod downwardapi-volume-c88d1b6c-d13d-4b2c-b115-0489f17a0078 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 09:05:46.67
  Apr 15 09:05:46.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2248" for this suite. @ 04/15/24 09:05:46.715
• [4.192 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 04/15/24 09:05:46.747
  Apr 15 09:05:46.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename sched-preemption @ 04/15/24 09:05:46.749
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:05:46.777
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:05:46.782
  Apr 15 09:05:46.813: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 15 09:06:46.911: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 04/15/24 09:06:46.932
  Apr 15 09:06:46.993: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Apr 15 09:06:47.034: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Apr 15 09:06:47.094: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Apr 15 09:06:47.148: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Apr 15 09:06:47.186: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Apr 15 09:06:47.205: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 04/15/24 09:06:47.205
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 04/15/24 09:06:49.296
  Apr 15 09:06:55.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-5177" for this suite. @ 04/15/24 09:06:55.61
• [68.890 seconds]
------------------------------
SS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 04/15/24 09:06:55.643
  Apr 15 09:06:55.644: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename endpointslice @ 04/15/24 09:06:55.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:06:55.686
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:06:55.694
  Apr 15 09:06:55.736: INFO: Endpoints addresses: [192.168.121.120 192.168.121.96] , ports: [6443]
  Apr 15 09:06:55.736: INFO: EndpointSlices addresses: [192.168.121.120 192.168.121.96] , ports: [6443]
  Apr 15 09:06:55.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-1723" for this suite. @ 04/15/24 09:06:55.746
• [0.117 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 04/15/24 09:06:55.761
  Apr 15 09:06:55.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename pod-network-test @ 04/15/24 09:06:55.763
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:06:55.794
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:06:55.799
  STEP: Performing setup for networking test in namespace pod-network-test-5328 @ 04/15/24 09:06:55.804
  STEP: creating a selector @ 04/15/24 09:06:55.805
  STEP: Creating the service pods in kubernetes @ 04/15/24 09:06:55.805
  Apr 15 09:06:55.805: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 04/15/24 09:07:08.038
  Apr 15 09:07:10.125: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 15 09:07:10.126: INFO: Going to poll 10.233.64.49 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Apr 15 09:07:10.135: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.49 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5328 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:07:10.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:07:10.136: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:07:10.136: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5328/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.49+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 15 09:07:11.282: INFO: Found all 1 expected endpoints: [netserver-0]
  Apr 15 09:07:11.282: INFO: Going to poll 10.233.65.81 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Apr 15 09:07:11.290: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.81 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5328 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:07:11.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:07:11.291: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:07:11.291: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5328/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.65.81+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 15 09:07:12.405: INFO: Found all 1 expected endpoints: [netserver-1]
  Apr 15 09:07:12.406: INFO: Going to poll 10.233.66.97 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Apr 15 09:07:12.414: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.97 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5328 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:07:12.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:07:12.417: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:07:12.417: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5328/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.97+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 15 09:07:13.593: INFO: Found all 1 expected endpoints: [netserver-2]
  Apr 15 09:07:13.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-5328" for this suite. @ 04/15/24 09:07:13.606
• [17.860 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 04/15/24 09:07:13.622
  Apr 15 09:07:13.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 09:07:13.624
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:07:13.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:07:13.665
  STEP: validating api versions @ 04/15/24 09:07:13.672
  Apr 15 09:07:13.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-4620 api-versions'
  Apr 15 09:07:13.891: INFO: stderr: ""
  Apr 15 09:07:13.891: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Apr 15 09:07:13.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4620" for this suite. @ 04/15/24 09:07:13.913
• [0.307 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 04/15/24 09:07:13.931
  Apr 15 09:07:13.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:07:13.933
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:07:13.962
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:07:13.968
  STEP: Creating projection with secret that has name projected-secret-test-map-41cd8097-9299-4b96-9e92-4ed3a9a301fa @ 04/15/24 09:07:13.978
  STEP: Creating a pod to test consume secrets @ 04/15/24 09:07:13.986
  STEP: Saw pod success @ 04/15/24 09:07:18.028
  Apr 15 09:07:18.036: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-projected-secrets-fa8db69e-f6bb-4a7e-beb4-2deea6efab2b container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 09:07:18.068
  Apr 15 09:07:18.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5458" for this suite. @ 04/15/24 09:07:18.109
• [4.195 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 04/15/24 09:07:18.13
  Apr 15 09:07:18.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename subpath @ 04/15/24 09:07:18.134
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:07:18.166
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:07:18.171
  STEP: Setting up data @ 04/15/24 09:07:18.179
  STEP: Creating pod pod-subpath-test-projected-5sc2 @ 04/15/24 09:07:18.198
  STEP: Creating a pod to test atomic-volume-subpath @ 04/15/24 09:07:18.198
  STEP: Saw pod success @ 04/15/24 09:07:42.352
  Apr 15 09:07:42.361: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-subpath-test-projected-5sc2 container test-container-subpath-projected-5sc2: <nil>
  STEP: delete the pod @ 04/15/24 09:07:42.38
  STEP: Deleting pod pod-subpath-test-projected-5sc2 @ 04/15/24 09:07:42.412
  Apr 15 09:07:42.412: INFO: Deleting pod "pod-subpath-test-projected-5sc2" in namespace "subpath-7898"
  Apr 15 09:07:42.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7898" for this suite. @ 04/15/24 09:07:42.43
• [24.320 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 04/15/24 09:07:42.452
  Apr 15 09:07:42.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename svcaccounts @ 04/15/24 09:07:42.455
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:07:42.482
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:07:42.489
  Apr 15 09:07:42.519: INFO: created pod
  STEP: Saw pod success @ 04/15/24 09:07:46.542
  Apr 15 09:08:16.546: INFO: polling logs
  Apr 15 09:08:16.563: INFO: Pod logs: 
  I0415 09:07:43.266652       1 log.go:245] OK: Got token
  I0415 09:07:43.267885       1 log.go:245] validating with in-cluster discovery
  I0415 09:07:43.269184       1 log.go:245] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0415 09:07:43.269240       1 log.go:245] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9024:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:(*jwt.NumericDate)(0xc000013290), NotBefore:(*jwt.NumericDate)(0xc000013378), IssuedAt:(*jwt.NumericDate)(0xc0000132a0), ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9024", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ac0288fb-86d8-475c-b41b-6b91f194d8ff"}}}
  I0415 09:07:43.292973       1 log.go:245] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0415 09:07:43.306416       1 log.go:245] OK: Validated signature on JWT
  I0415 09:07:43.306610       1 log.go:245] OK: Got valid claims from token!
  I0415 09:07:43.306687       1 log.go:245] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-9024:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:(*jwt.NumericDate)(0xc0000c0a80), NotBefore:(*jwt.NumericDate)(0xc0000c0aa8), IssuedAt:(*jwt.NumericDate)(0xc0000c0a88), ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-9024", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ac0288fb-86d8-475c-b41b-6b91f194d8ff"}}}

  Apr 15 09:08:16.563: INFO: completed pod
  Apr 15 09:08:16.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9024" for this suite. @ 04/15/24 09:08:16.589
• [34.152 seconds]
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 04/15/24 09:08:16.605
  Apr 15 09:08:16.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 09:08:16.607
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:08:16.655
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:08:16.662
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 09:08:16.673
  STEP: Saw pod success @ 04/15/24 09:08:20.727
  Apr 15 09:08:20.734: INFO: Trying to get logs from node ahz3daisheng-3 pod downwardapi-volume-1ae2d976-3632-4fa1-b406-e69b1537d50f container client-container: <nil>
  STEP: delete the pod @ 04/15/24 09:08:20.758
  Apr 15 09:08:20.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2147" for this suite. @ 04/15/24 09:08:20.824
• [4.238 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 04/15/24 09:08:20.844
  Apr 15 09:08:20.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename services @ 04/15/24 09:08:20.849
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:08:20.884
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:08:20.89
  Apr 15 09:08:20.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8810" for this suite. @ 04/15/24 09:08:20.919
• [0.094 seconds]
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 04/15/24 09:08:20.941
  Apr 15 09:08:20.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename var-expansion @ 04/15/24 09:08:20.945
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:08:20.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:08:20.991
  STEP: creating the pod @ 04/15/24 09:08:20.997
  STEP: waiting for pod running @ 04/15/24 09:08:21.02
  STEP: creating a file in subpath @ 04/15/24 09:08:23.043
  Apr 15 09:08:23.052: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8718 PodName:var-expansion-56be9e1e-0b60-4172-bbb8-3ebaf6344a5b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:08:23.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:08:23.055: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:08:23.055: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-8718/pods/var-expansion-56be9e1e-0b60-4172-bbb8-3ebaf6344a5b/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 04/15/24 09:08:23.179
  Apr 15 09:08:23.191: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8718 PodName:var-expansion-56be9e1e-0b60-4172-bbb8-3ebaf6344a5b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:08:23.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:08:23.193: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:08:23.194: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-8718/pods/var-expansion-56be9e1e-0b60-4172-bbb8-3ebaf6344a5b/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 04/15/24 09:08:23.289
  Apr 15 09:08:23.818: INFO: Successfully updated pod "var-expansion-56be9e1e-0b60-4172-bbb8-3ebaf6344a5b"
  STEP: waiting for annotated pod running @ 04/15/24 09:08:23.818
  STEP: deleting the pod gracefully @ 04/15/24 09:08:23.828
  Apr 15 09:08:23.828: INFO: Deleting pod "var-expansion-56be9e1e-0b60-4172-bbb8-3ebaf6344a5b" in namespace "var-expansion-8718"
  Apr 15 09:08:23.847: INFO: Wait up to 5m0s for pod "var-expansion-56be9e1e-0b60-4172-bbb8-3ebaf6344a5b" to be fully deleted
  Apr 15 09:08:56.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8718" for this suite. @ 04/15/24 09:08:56.038
• [35.110 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 04/15/24 09:08:56.054
  Apr 15 09:08:56.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename services @ 04/15/24 09:08:56.06
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:08:56.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:08:56.093
  STEP: creating a Service @ 04/15/24 09:08:56.106
  STEP: watching for the Service to be added @ 04/15/24 09:08:56.133
  Apr 15 09:08:56.141: INFO: Found Service test-service-jlm4v in namespace services-6868 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Apr 15 09:08:56.143: INFO: Service test-service-jlm4v created
  STEP: Getting /status @ 04/15/24 09:08:56.144
  Apr 15 09:08:56.158: INFO: Service test-service-jlm4v has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 04/15/24 09:08:56.159
  STEP: watching for the Service to be patched @ 04/15/24 09:08:56.181
  Apr 15 09:08:56.186: INFO: observed Service test-service-jlm4v in namespace services-6868 with annotations: map[] & LoadBalancer: {[]}
  Apr 15 09:08:56.186: INFO: Found Service test-service-jlm4v in namespace services-6868 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Apr 15 09:08:56.187: INFO: Service test-service-jlm4v has service status patched
  STEP: updating the ServiceStatus @ 04/15/24 09:08:56.188
  Apr 15 09:08:56.211: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 04/15/24 09:08:56.211
  Apr 15 09:08:56.214: INFO: Observed Service test-service-jlm4v in namespace services-6868 with annotations: map[] & Conditions: {[]}
  Apr 15 09:08:56.215: INFO: Observed event: &Service{ObjectMeta:{test-service-jlm4v  services-6868  b0a10e64-97d3-4f89-823e-1b3a4fe1234d 197858 0 2024-04-15 09:08:56 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2024-04-15 09:08:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2024-04-15 09:08:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.47.201,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.47.201],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Apr 15 09:08:56.216: INFO: Found Service test-service-jlm4v in namespace services-6868 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Apr 15 09:08:56.216: INFO: Service test-service-jlm4v has service status updated
  STEP: patching the service @ 04/15/24 09:08:56.217
  STEP: watching for the Service to be patched @ 04/15/24 09:08:56.243
  Apr 15 09:08:56.246: INFO: observed Service test-service-jlm4v in namespace services-6868 with labels: map[test-service-static:true]
  Apr 15 09:08:56.246: INFO: observed Service test-service-jlm4v in namespace services-6868 with labels: map[test-service-static:true]
  Apr 15 09:08:56.247: INFO: observed Service test-service-jlm4v in namespace services-6868 with labels: map[test-service-static:true]
  Apr 15 09:08:56.248: INFO: Found Service test-service-jlm4v in namespace services-6868 with labels: map[test-service:patched test-service-static:true]
  Apr 15 09:08:56.248: INFO: Service test-service-jlm4v patched
  STEP: deleting the service @ 04/15/24 09:08:56.248
  STEP: watching for the Service to be deleted @ 04/15/24 09:08:56.281
  Apr 15 09:08:56.285: INFO: Observed event: ADDED
  Apr 15 09:08:56.286: INFO: Observed event: MODIFIED
  Apr 15 09:08:56.286: INFO: Observed event: MODIFIED
  Apr 15 09:08:56.286: INFO: Observed event: MODIFIED
  Apr 15 09:08:56.287: INFO: Found Service test-service-jlm4v in namespace services-6868 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Apr 15 09:08:56.287: INFO: Service test-service-jlm4v deleted
  Apr 15 09:08:56.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6868" for this suite. @ 04/15/24 09:08:56.298
• [0.256 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 04/15/24 09:08:56.311
  Apr 15 09:08:56.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename sysctl @ 04/15/24 09:08:56.313
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:08:56.338
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:08:56.343
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 04/15/24 09:08:56.35
  STEP: Watching for error events or started pod @ 04/15/24 09:08:56.366
  STEP: Waiting for pod completion @ 04/15/24 09:08:58.374
  STEP: Checking that the pod succeeded @ 04/15/24 09:08:58.388
  STEP: Getting logs from the pod @ 04/15/24 09:08:58.388
  STEP: Checking that the sysctl is actually updated @ 04/15/24 09:08:58.402
  Apr 15 09:08:58.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-3858" for this suite. @ 04/15/24 09:08:58.412
• [2.114 seconds]
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 04/15/24 09:08:58.427
  Apr 15 09:08:58.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename init-container @ 04/15/24 09:08:58.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:08:58.459
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:08:58.465
  STEP: creating the pod @ 04/15/24 09:08:58.472
  Apr 15 09:08:58.472: INFO: PodSpec: initContainers in spec.initContainers
  Apr 15 09:09:45.276: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-3cebd5de-b1a8-4c52-97f8-68459952c7e4", GenerateName:"", Namespace:"init-container-5131", SelfLink:"", UID:"f9a2fd4b-2f8d-42f2-b491-d2eaef31f01c", ResourceVersion:"198013", Generation:0, CreationTimestamp:time.Date(2024, time.April, 15, 9, 8, 58, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"472760074"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.April, 15, 9, 8, 58, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0013ce1b0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.April, 15, 9, 9, 45, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0013ce210), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-w86c8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003fd8100), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-w86c8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-w86c8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-w86c8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003e3eac8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ahz3daisheng-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000542230), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003e3eb50)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003e3eb70)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003e3eb78), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003e3eb7c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0010d40f0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.April, 15, 9, 8, 58, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.April, 15, 9, 8, 58, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.April, 15, 9, 8, 58, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.April, 15, 9, 8, 58, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.199", PodIP:"10.233.66.105", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.66.105"}}, StartTime:time.Date(2024, time.April, 15, 9, 8, 58, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000542310)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000542380)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://954bd6c72efb1c11bfcd1d73c90aeb611668acc4be4dca899c1b1b51dae5a7f0", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003fd8180), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003fd8160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc003e3ebf4), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Apr 15 09:09:45.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-5131" for this suite. @ 04/15/24 09:09:45.288
• [46.878 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 04/15/24 09:09:45.31
  Apr 15 09:09:45.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 09:09:45.315
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:09:45.347
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:09:45.353
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 09:09:45.362
  STEP: Saw pod success @ 04/15/24 09:09:49.415
  Apr 15 09:09:49.422: INFO: Trying to get logs from node ahz3daisheng-3 pod downwardapi-volume-8f586908-00ee-49e8-8e43-7df46d6e32b4 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 09:09:49.438
  Apr 15 09:09:49.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5165" for this suite. @ 04/15/24 09:09:49.489
• [4.196 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 04/15/24 09:09:49.509
  Apr 15 09:09:49.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename runtimeclass @ 04/15/24 09:09:49.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:09:49.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:09:49.553
  Apr 15 09:09:51.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9297" for this suite. @ 04/15/24 09:09:51.613
• [2.120 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 04/15/24 09:09:51.63
  Apr 15 09:09:51.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/15/24 09:09:51.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:09:51.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:09:51.711
  STEP: set up a multi version CRD @ 04/15/24 09:09:51.716
  Apr 15 09:09:51.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: rename a version @ 04/15/24 09:09:57.218
  STEP: check the new version name is served @ 04/15/24 09:09:57.246
  STEP: check the old version name is removed @ 04/15/24 09:09:59.284
  STEP: check the other version is not changed @ 04/15/24 09:10:00.263
  Apr 15 09:10:04.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1707" for this suite. @ 04/15/24 09:10:04.055
• [12.438 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 04/15/24 09:10:04.075
  Apr 15 09:10:04.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 09:10:04.077
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:10:04.107
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:10:04.113
  STEP: Creating the pod @ 04/15/24 09:10:04.117
  Apr 15 09:10:06.687: INFO: Successfully updated pod "labelsupdate3c3628a3-1a4d-48ab-b3e7-fb687fba167c"
  Apr 15 09:10:08.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9466" for this suite. @ 04/15/24 09:10:08.751
• [4.688 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 04/15/24 09:10:08.766
  Apr 15 09:10:08.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename disruption @ 04/15/24 09:10:08.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:10:08.8
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:10:08.805
  STEP: Waiting for the pdb to be processed @ 04/15/24 09:10:08.824
  STEP: Updating PodDisruptionBudget status @ 04/15/24 09:10:10.876
  STEP: Waiting for all pods to be running @ 04/15/24 09:10:10.891
  Apr 15 09:10:10.901: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 04/15/24 09:10:12.912
  STEP: Waiting for the pdb to be processed @ 04/15/24 09:10:12.937
  STEP: Patching PodDisruptionBudget status @ 04/15/24 09:10:12.951
  STEP: Waiting for the pdb to be processed @ 04/15/24 09:10:12.966
  Apr 15 09:10:12.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-6942" for this suite. @ 04/15/24 09:10:12.983
• [4.231 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 04/15/24 09:10:12.999
  Apr 15 09:10:12.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename services @ 04/15/24 09:10:13.001
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:10:13.024
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:10:13.03
  STEP: creating service in namespace services-9557 @ 04/15/24 09:10:13.036
  STEP: creating service affinity-clusterip in namespace services-9557 @ 04/15/24 09:10:13.037
  STEP: creating replication controller affinity-clusterip in namespace services-9557 @ 04/15/24 09:10:13.059
  I0415 09:10:13.072674      14 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-9557, replica count: 3
  I0415 09:10:16.125133      14 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 15 09:10:16.137: INFO: Creating new exec pod
  Apr 15 09:10:19.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-9557 exec execpod-affinity4dhgd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Apr 15 09:10:19.508: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Apr 15 09:10:19.508: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 09:10:19.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-9557 exec execpod-affinity4dhgd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.9.49 80'
  Apr 15 09:10:19.811: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.9.49 80\nConnection to 10.233.9.49 80 port [tcp/http] succeeded!\n"
  Apr 15 09:10:19.812: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 09:10:19.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-9557 exec execpod-affinity4dhgd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.9.49:80/ ; done'
  Apr 15 09:10:20.353: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.49:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.9.49:80/\n"
  Apr 15 09:10:20.353: INFO: stdout: "\naffinity-clusterip-tk7qb\naffinity-clusterip-tk7qb\naffinity-clusterip-tk7qb\naffinity-clusterip-tk7qb\naffinity-clusterip-tk7qb\naffinity-clusterip-tk7qb\naffinity-clusterip-tk7qb\naffinity-clusterip-tk7qb\naffinity-clusterip-tk7qb\naffinity-clusterip-tk7qb\naffinity-clusterip-tk7qb\naffinity-clusterip-tk7qb\naffinity-clusterip-tk7qb\naffinity-clusterip-tk7qb\naffinity-clusterip-tk7qb\naffinity-clusterip-tk7qb"
  Apr 15 09:10:20.353: INFO: Received response from host: affinity-clusterip-tk7qb
  Apr 15 09:10:20.353: INFO: Received response from host: affinity-clusterip-tk7qb
  Apr 15 09:10:20.353: INFO: Received response from host: affinity-clusterip-tk7qb
  Apr 15 09:10:20.353: INFO: Received response from host: affinity-clusterip-tk7qb
  Apr 15 09:10:20.353: INFO: Received response from host: affinity-clusterip-tk7qb
  Apr 15 09:10:20.353: INFO: Received response from host: affinity-clusterip-tk7qb
  Apr 15 09:10:20.353: INFO: Received response from host: affinity-clusterip-tk7qb
  Apr 15 09:10:20.353: INFO: Received response from host: affinity-clusterip-tk7qb
  Apr 15 09:10:20.353: INFO: Received response from host: affinity-clusterip-tk7qb
  Apr 15 09:10:20.353: INFO: Received response from host: affinity-clusterip-tk7qb
  Apr 15 09:10:20.353: INFO: Received response from host: affinity-clusterip-tk7qb
  Apr 15 09:10:20.353: INFO: Received response from host: affinity-clusterip-tk7qb
  Apr 15 09:10:20.353: INFO: Received response from host: affinity-clusterip-tk7qb
  Apr 15 09:10:20.353: INFO: Received response from host: affinity-clusterip-tk7qb
  Apr 15 09:10:20.353: INFO: Received response from host: affinity-clusterip-tk7qb
  Apr 15 09:10:20.353: INFO: Received response from host: affinity-clusterip-tk7qb
  Apr 15 09:10:20.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 09:10:20.367: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-9557, will wait for the garbage collector to delete the pods @ 04/15/24 09:10:20.401
  Apr 15 09:10:20.475: INFO: Deleting ReplicationController affinity-clusterip took: 16.053422ms
  Apr 15 09:10:20.576: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.106417ms
  STEP: Destroying namespace "services-9557" for this suite. @ 04/15/24 09:10:23.026
• [10.041 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 04/15/24 09:10:23.045
  Apr 15 09:10:23.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename configmap @ 04/15/24 09:10:23.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:10:23.094
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:10:23.099
  STEP: Creating configMap with name configmap-test-upd-3bba57da-95eb-4849-b924-d4cb25e735c3 @ 04/15/24 09:10:23.117
  STEP: Creating the pod @ 04/15/24 09:10:23.129
  STEP: Waiting for pod with text data @ 04/15/24 09:10:25.174
  STEP: Waiting for pod with binary data @ 04/15/24 09:10:25.191
  Apr 15 09:10:25.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1355" for this suite. @ 04/15/24 09:10:25.213
• [2.182 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 04/15/24 09:10:25.232
  Apr 15 09:10:25.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:10:25.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:10:25.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:10:25.268
  STEP: Creating configMap with name projected-configmap-test-volume-486ac2d2-3e31-496e-84ee-fe05a73c3f41 @ 04/15/24 09:10:25.274
  STEP: Creating a pod to test consume configMaps @ 04/15/24 09:10:25.286
  STEP: Saw pod success @ 04/15/24 09:10:29.354
  Apr 15 09:10:29.360: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-projected-configmaps-aeda26cf-5eb7-4308-a280-999c535f79b6 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 09:10:29.373
  Apr 15 09:10:29.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-612" for this suite. @ 04/15/24 09:10:29.417
• [4.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 04/15/24 09:10:29.462
  Apr 15 09:10:29.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename crd-webhook @ 04/15/24 09:10:29.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:10:29.505
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:10:29.512
  STEP: Setting up server cert @ 04/15/24 09:10:29.518
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 04/15/24 09:10:31.003
  STEP: Deploying the custom resource conversion webhook pod @ 04/15/24 09:10:31.023
  STEP: Wait for the deployment to be ready @ 04/15/24 09:10:31.046
  Apr 15 09:10:31.062: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 04/15/24 09:10:33.086
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 09:10:33.121
  Apr 15 09:10:34.122: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Apr 15 09:10:34.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Creating a v1 custom resource @ 04/15/24 09:10:37.023
  STEP: Create a v2 custom resource @ 04/15/24 09:10:37.062
  STEP: List CRs in v1 @ 04/15/24 09:10:37.087
  STEP: List CRs in v2 @ 04/15/24 09:10:37.302
  Apr 15 09:10:37.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-2576" for this suite. @ 04/15/24 09:10:38.001
• [8.560 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 04/15/24 09:10:38.026
  Apr 15 09:10:38.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename services @ 04/15/24 09:10:38.031
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:10:38.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:10:38.078
  STEP: creating service multi-endpoint-test in namespace services-6317 @ 04/15/24 09:10:38.086
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6317 to expose endpoints map[] @ 04/15/24 09:10:38.123
  Apr 15 09:10:38.163: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  Apr 15 09:10:39.179: INFO: successfully validated that service multi-endpoint-test in namespace services-6317 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-6317 @ 04/15/24 09:10:39.179
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6317 to expose endpoints map[pod1:[100]] @ 04/15/24 09:10:41.228
  Apr 15 09:10:41.254: INFO: successfully validated that service multi-endpoint-test in namespace services-6317 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-6317 @ 04/15/24 09:10:41.254
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6317 to expose endpoints map[pod1:[100] pod2:[101]] @ 04/15/24 09:10:43.298
  Apr 15 09:10:43.327: INFO: successfully validated that service multi-endpoint-test in namespace services-6317 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 04/15/24 09:10:43.327
  Apr 15 09:10:43.328: INFO: Creating new exec pod
  Apr 15 09:10:46.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-6317 exec execpodgswc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Apr 15 09:10:46.703: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Apr 15 09:10:46.703: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 09:10:46.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-6317 exec execpodgswc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.34.194 80'
  Apr 15 09:10:46.978: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.34.194 80\nConnection to 10.233.34.194 80 port [tcp/http] succeeded!\n"
  Apr 15 09:10:46.978: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 09:10:46.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-6317 exec execpodgswc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Apr 15 09:10:47.257: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Apr 15 09:10:47.257: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 09:10:47.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-6317 exec execpodgswc2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.34.194 81'
  Apr 15 09:10:47.577: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.34.194 81\nConnection to 10.233.34.194 81 port [tcp/*] succeeded!\n"
  Apr 15 09:10:47.577: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-6317 @ 04/15/24 09:10:47.577
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6317 to expose endpoints map[pod2:[101]] @ 04/15/24 09:10:47.613
  Apr 15 09:10:48.762: INFO: successfully validated that service multi-endpoint-test in namespace services-6317 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-6317 @ 04/15/24 09:10:48.763
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6317 to expose endpoints map[] @ 04/15/24 09:10:48.805
  Apr 15 09:10:48.834: INFO: successfully validated that service multi-endpoint-test in namespace services-6317 exposes endpoints map[]
  Apr 15 09:10:48.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6317" for this suite. @ 04/15/24 09:10:48.892
• [10.886 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 04/15/24 09:10:48.919
  Apr 15 09:10:48.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename webhook @ 04/15/24 09:10:48.921
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:10:48.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:10:48.959
  STEP: Setting up server cert @ 04/15/24 09:10:49.01
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 09:10:50.553
  STEP: Deploying the webhook pod @ 04/15/24 09:10:50.563
  STEP: Wait for the deployment to be ready @ 04/15/24 09:10:50.583
  Apr 15 09:10:50.602: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/15/24 09:10:52.632
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 09:10:52.67
  Apr 15 09:10:53.671: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 04/15/24 09:10:53.679
  STEP: create a pod that should be denied by the webhook @ 04/15/24 09:10:53.711
  STEP: create a pod that causes the webhook to hang @ 04/15/24 09:10:53.74
  STEP: create a configmap that should be denied by the webhook @ 04/15/24 09:11:03.755
  STEP: create a configmap that should be admitted by the webhook @ 04/15/24 09:11:03.793
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 04/15/24 09:11:03.815
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 04/15/24 09:11:03.841
  STEP: create a namespace that bypass the webhook @ 04/15/24 09:11:03.855
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 04/15/24 09:11:03.888
  Apr 15 09:11:03.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9443" for this suite. @ 04/15/24 09:11:04.037
  STEP: Destroying namespace "webhook-markers-9369" for this suite. @ 04/15/24 09:11:04.068
  STEP: Destroying namespace "exempted-namespace-5425" for this suite. @ 04/15/24 09:11:04.086
• [15.180 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 04/15/24 09:11:04.102
  Apr 15 09:11:04.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename gc @ 04/15/24 09:11:04.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:11:04.132
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:11:04.137
  STEP: create the deployment @ 04/15/24 09:11:04.143
  W0415 09:11:04.153354      14 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 04/15/24 09:11:04.153
  STEP: delete the deployment @ 04/15/24 09:11:04.841
  STEP: wait for all rs to be garbage collected @ 04/15/24 09:11:04.889
  STEP: expected 0 rs, got 1 rs @ 04/15/24 09:11:04.906
  STEP: expected 0 pods, got 2 pods @ 04/15/24 09:11:04.912
  STEP: Gathering metrics @ 04/15/24 09:11:05.43
  Apr 15 09:11:05.626: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 15 09:11:05.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1482" for this suite. @ 04/15/24 09:11:05.643
• [1.560 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 04/15/24 09:11:05.664
  Apr 15 09:11:05.664: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename endpointslicemirroring @ 04/15/24 09:11:05.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:11:05.695
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:11:05.701
  STEP: mirroring a new custom Endpoint @ 04/15/24 09:11:05.733
  Apr 15 09:11:05.763: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 04/15/24 09:11:07.771
  Apr 15 09:11:07.787: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  STEP: mirroring deletion of a custom Endpoint @ 04/15/24 09:11:09.798
  Apr 15 09:11:09.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-438" for this suite. @ 04/15/24 09:11:09.847
• [4.202 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 04/15/24 09:11:09.868
  Apr 15 09:11:09.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:11:09.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:11:09.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:11:09.907
  STEP: Creating configMap with name projected-configmap-test-volume-map-bb975ed3-b736-4039-af38-38072da011d2 @ 04/15/24 09:11:09.914
  STEP: Creating a pod to test consume configMaps @ 04/15/24 09:11:09.927
  STEP: Saw pod success @ 04/15/24 09:11:11.973
  Apr 15 09:11:11.980: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-projected-configmaps-460be330-ed0f-4ac9-96e9-735d8c1180e0 container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 09:11:11.997
  Apr 15 09:11:12.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8765" for this suite. @ 04/15/24 09:11:12.045
• [2.192 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 04/15/24 09:11:12.071
  Apr 15 09:11:12.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:11:12.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:11:12.102
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:11:12.11
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-1552e245-1a62-4ca9-aa5a-f7eeb75e44aa @ 04/15/24 09:11:12.137
  STEP: Creating the pod @ 04/15/24 09:11:12.144
  STEP: Updating configmap projected-configmap-test-upd-1552e245-1a62-4ca9-aa5a-f7eeb75e44aa @ 04/15/24 09:11:14.204
  STEP: waiting to observe update in volume @ 04/15/24 09:11:14.215
  Apr 15 09:11:16.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5223" for this suite. @ 04/15/24 09:11:16.264
• [4.208 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 04/15/24 09:11:16.289
  Apr 15 09:11:16.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename webhook @ 04/15/24 09:11:16.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:11:16.327
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:11:16.333
  STEP: Setting up server cert @ 04/15/24 09:11:16.414
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 09:11:17.176
  STEP: Deploying the webhook pod @ 04/15/24 09:11:17.195
  STEP: Wait for the deployment to be ready @ 04/15/24 09:11:17.229
  Apr 15 09:11:17.261: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/15/24 09:11:19.285
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 09:11:19.307
  Apr 15 09:11:20.308: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 04/15/24 09:11:20.316
  STEP: create a configmap that should be updated by the webhook @ 04/15/24 09:11:20.353
  Apr 15 09:11:20.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2253" for this suite. @ 04/15/24 09:11:20.523
  STEP: Destroying namespace "webhook-markers-554" for this suite. @ 04/15/24 09:11:20.544
• [4.269 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:443
  STEP: Creating a kubernetes client @ 04/15/24 09:11:20.573
  Apr 15 09:11:20.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename daemonsets @ 04/15/24 09:11:20.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:11:20.608
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:11:20.614
  Apr 15 09:11:20.797: INFO: Create a RollingUpdate DaemonSet
  Apr 15 09:11:20.812: INFO: Check that daemon pods launch on every node of the cluster
  Apr 15 09:11:20.837: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 09:11:20.837: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  Apr 15 09:11:21.877: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 09:11:21.877: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  Apr 15 09:11:22.885: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 15 09:11:22.886: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Apr 15 09:11:22.886: INFO: Update the DaemonSet to trigger a rollout
  Apr 15 09:11:22.908: INFO: Updating DaemonSet daemon-set
  Apr 15 09:11:23.955: INFO: Roll back the DaemonSet before rollout is complete
  Apr 15 09:11:23.980: INFO: Updating DaemonSet daemon-set
  Apr 15 09:11:23.981: INFO: Make sure DaemonSet rollback is complete
  Apr 15 09:11:23.997: INFO: Wrong image for pod: daemon-set-nxlzf. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Apr 15 09:11:23.998: INFO: Pod daemon-set-nxlzf is not available
  Apr 15 09:11:30.028: INFO: Pod daemon-set-mlk65 is not available
  STEP: Deleting DaemonSet "daemon-set" @ 04/15/24 09:11:30.051
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6320, will wait for the garbage collector to delete the pods @ 04/15/24 09:11:30.051
  Apr 15 09:11:30.118: INFO: Deleting DaemonSet.extensions daemon-set took: 10.395315ms
  Apr 15 09:11:30.219: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.978102ms
  Apr 15 09:11:32.228: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 09:11:32.228: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 15 09:11:32.235: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"199014"},"items":null}

  Apr 15 09:11:32.246: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"199014"},"items":null}

  Apr 15 09:11:32.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6320" for this suite. @ 04/15/24 09:11:32.283
• [11.723 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 04/15/24 09:11:32.302
  Apr 15 09:11:32.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename services @ 04/15/24 09:11:32.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:11:32.327
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:11:32.336
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-5833 @ 04/15/24 09:11:32.341
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 04/15/24 09:11:32.372
  STEP: creating service externalsvc in namespace services-5833 @ 04/15/24 09:11:32.372
  STEP: creating replication controller externalsvc in namespace services-5833 @ 04/15/24 09:11:32.408
  I0415 09:11:32.428596      14 runners.go:194] Created replication controller with name: externalsvc, namespace: services-5833, replica count: 2
  I0415 09:11:35.482432      14 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 04/15/24 09:11:35.491
  Apr 15 09:11:35.523: INFO: Creating new exec pod
  Apr 15 09:11:37.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-5833 exec execpodg8h9x -- /bin/sh -x -c nslookup nodeport-service.services-5833.svc.cluster.local'
  Apr 15 09:11:38.004: INFO: stderr: "+ nslookup nodeport-service.services-5833.svc.cluster.local\n"
  Apr 15 09:11:38.004: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-5833.svc.cluster.local\tcanonical name = externalsvc.services-5833.svc.cluster.local.\nName:\texternalsvc.services-5833.svc.cluster.local\nAddress: 10.233.63.161\n\n"
  Apr 15 09:11:38.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-5833, will wait for the garbage collector to delete the pods @ 04/15/24 09:11:38.017
  Apr 15 09:11:38.108: INFO: Deleting ReplicationController externalsvc took: 30.10033ms
  Apr 15 09:11:38.210: INFO: Terminating ReplicationController externalsvc pods took: 101.768856ms
  Apr 15 09:11:40.552: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-5833" for this suite. @ 04/15/24 09:11:40.577
• [8.290 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 04/15/24 09:11:40.596
  Apr 15 09:11:40.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename containers @ 04/15/24 09:11:40.598
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:11:40.627
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:11:40.633
  Apr 15 09:11:42.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-7156" for this suite. @ 04/15/24 09:11:42.695
• [2.111 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 04/15/24 09:11:42.708
  Apr 15 09:11:42.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 09:11:42.71
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:11:42.742
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:11:42.747
  STEP: Creating resourceQuota "e2e-rq-status-6d9rq" @ 04/15/24 09:11:42.761
  Apr 15 09:11:42.780: INFO: Resource quota "e2e-rq-status-6d9rq" reports spec: hard cpu limit of 500m
  Apr 15 09:11:42.780: INFO: Resource quota "e2e-rq-status-6d9rq" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-6d9rq" /status @ 04/15/24 09:11:42.78
  STEP: Confirm /status for "e2e-rq-status-6d9rq" resourceQuota via watch @ 04/15/24 09:11:42.796
  Apr 15 09:11:42.800: INFO: observed resourceQuota "e2e-rq-status-6d9rq" in namespace "resourcequota-6513" with hard status: v1.ResourceList(nil)
  Apr 15 09:11:42.800: INFO: Found resourceQuota "e2e-rq-status-6d9rq" in namespace "resourcequota-6513" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Apr 15 09:11:42.800: INFO: ResourceQuota "e2e-rq-status-6d9rq" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 04/15/24 09:11:42.821
  Apr 15 09:11:42.839: INFO: Resource quota "e2e-rq-status-6d9rq" reports spec: hard cpu limit of 1
  Apr 15 09:11:42.839: INFO: Resource quota "e2e-rq-status-6d9rq" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-6d9rq" /status @ 04/15/24 09:11:42.839
  STEP: Confirm /status for "e2e-rq-status-6d9rq" resourceQuota via watch @ 04/15/24 09:11:42.877
  Apr 15 09:11:42.879: INFO: observed resourceQuota "e2e-rq-status-6d9rq" in namespace "resourcequota-6513" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Apr 15 09:11:42.880: INFO: Found resourceQuota "e2e-rq-status-6d9rq" in namespace "resourcequota-6513" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Apr 15 09:11:42.880: INFO: ResourceQuota "e2e-rq-status-6d9rq" /status was patched
  STEP: Get "e2e-rq-status-6d9rq" /status @ 04/15/24 09:11:42.88
  Apr 15 09:11:42.887: INFO: Resourcequota "e2e-rq-status-6d9rq" reports status: hard cpu of 1
  Apr 15 09:11:42.887: INFO: Resourcequota "e2e-rq-status-6d9rq" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-6d9rq" /status before checking Spec is unchanged @ 04/15/24 09:11:42.895
  Apr 15 09:11:42.907: INFO: Resourcequota "e2e-rq-status-6d9rq" reports status: hard cpu of 2
  Apr 15 09:11:42.907: INFO: Resourcequota "e2e-rq-status-6d9rq" reports status: hard memory of 2Gi
  Apr 15 09:11:42.910: INFO: observed resourceQuota "e2e-rq-status-6d9rq" in namespace "resourcequota-6513" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Apr 15 09:11:42.910: INFO: Found resourceQuota "e2e-rq-status-6d9rq" in namespace "resourcequota-6513" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  Apr 15 09:15:52.922: INFO: ResourceQuota "e2e-rq-status-6d9rq" Spec was unchanged and /status reset
  Apr 15 09:15:52.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6513" for this suite. @ 04/15/24 09:15:52.938
• [250.245 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 04/15/24 09:15:52.955
  Apr 15 09:15:52.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename services @ 04/15/24 09:15:52.959
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:15:52.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:15:52.992
  STEP: creating service in namespace services-8806 @ 04/15/24 09:15:52.998
  STEP: creating service affinity-nodeport-transition in namespace services-8806 @ 04/15/24 09:15:52.999
  STEP: creating replication controller affinity-nodeport-transition in namespace services-8806 @ 04/15/24 09:15:53.032
  I0415 09:15:53.053320      14 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-8806, replica count: 3
  I0415 09:15:56.106025      14 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 15 09:15:56.195: INFO: Creating new exec pod
  Apr 15 09:15:59.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-8806 exec execpod-affinityh4knz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Apr 15 09:15:59.626: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Apr 15 09:15:59.626: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 09:15:59.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-8806 exec execpod-affinityh4knz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.48.123 80'
  Apr 15 09:15:59.887: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.48.123 80\nConnection to 10.233.48.123 80 port [tcp/http] succeeded!\n"
  Apr 15 09:15:59.887: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 09:15:59.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-8806 exec execpod-affinityh4knz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.199 30677'
  Apr 15 09:16:00.138: INFO: stderr: "+ nc -v -t -w 2 192.168.121.199+  30677\necho hostName\nConnection to 192.168.121.199 30677 port [tcp/*] succeeded!\n"
  Apr 15 09:16:00.138: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 09:16:00.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-8806 exec execpod-affinityh4knz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.96 30677'
  Apr 15 09:16:00.397: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.96 30677\nConnection to 192.168.121.96 30677 port [tcp/*] succeeded!\n"
  Apr 15 09:16:00.397: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 09:16:00.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-8806 exec execpod-affinityh4knz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.96:30677/ ; done'
  Apr 15 09:16:00.936: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n"
  Apr 15 09:16:00.936: INFO: stdout: "\naffinity-nodeport-transition-l85mp\naffinity-nodeport-transition-n8s42\naffinity-nodeport-transition-n8s42\naffinity-nodeport-transition-n8s42\naffinity-nodeport-transition-l85mp\naffinity-nodeport-transition-l85mp\naffinity-nodeport-transition-l85mp\naffinity-nodeport-transition-48c9w\naffinity-nodeport-transition-n8s42\naffinity-nodeport-transition-l85mp\naffinity-nodeport-transition-48c9w\naffinity-nodeport-transition-n8s42\naffinity-nodeport-transition-l85mp\naffinity-nodeport-transition-l85mp\naffinity-nodeport-transition-n8s42\naffinity-nodeport-transition-l85mp"
  Apr 15 09:16:00.936: INFO: Received response from host: affinity-nodeport-transition-l85mp
  Apr 15 09:16:00.936: INFO: Received response from host: affinity-nodeport-transition-n8s42
  Apr 15 09:16:00.936: INFO: Received response from host: affinity-nodeport-transition-n8s42
  Apr 15 09:16:00.936: INFO: Received response from host: affinity-nodeport-transition-n8s42
  Apr 15 09:16:00.936: INFO: Received response from host: affinity-nodeport-transition-l85mp
  Apr 15 09:16:00.936: INFO: Received response from host: affinity-nodeport-transition-l85mp
  Apr 15 09:16:00.936: INFO: Received response from host: affinity-nodeport-transition-l85mp
  Apr 15 09:16:00.936: INFO: Received response from host: affinity-nodeport-transition-48c9w
  Apr 15 09:16:00.936: INFO: Received response from host: affinity-nodeport-transition-n8s42
  Apr 15 09:16:00.936: INFO: Received response from host: affinity-nodeport-transition-l85mp
  Apr 15 09:16:00.936: INFO: Received response from host: affinity-nodeport-transition-48c9w
  Apr 15 09:16:00.936: INFO: Received response from host: affinity-nodeport-transition-n8s42
  Apr 15 09:16:00.936: INFO: Received response from host: affinity-nodeport-transition-l85mp
  Apr 15 09:16:00.936: INFO: Received response from host: affinity-nodeport-transition-l85mp
  Apr 15 09:16:00.936: INFO: Received response from host: affinity-nodeport-transition-n8s42
  Apr 15 09:16:00.936: INFO: Received response from host: affinity-nodeport-transition-l85mp
  Apr 15 09:16:00.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-8806 exec execpod-affinityh4knz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.96:30677/ ; done'
  Apr 15 09:16:01.417: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:30677/\n"
  Apr 15 09:16:01.417: INFO: stdout: "\naffinity-nodeport-transition-48c9w\naffinity-nodeport-transition-48c9w\naffinity-nodeport-transition-48c9w\naffinity-nodeport-transition-48c9w\naffinity-nodeport-transition-48c9w\naffinity-nodeport-transition-48c9w\naffinity-nodeport-transition-48c9w\naffinity-nodeport-transition-48c9w\naffinity-nodeport-transition-48c9w\naffinity-nodeport-transition-48c9w\naffinity-nodeport-transition-48c9w\naffinity-nodeport-transition-48c9w\naffinity-nodeport-transition-48c9w\naffinity-nodeport-transition-48c9w\naffinity-nodeport-transition-48c9w\naffinity-nodeport-transition-48c9w"
  Apr 15 09:16:01.417: INFO: Received response from host: affinity-nodeport-transition-48c9w
  Apr 15 09:16:01.417: INFO: Received response from host: affinity-nodeport-transition-48c9w
  Apr 15 09:16:01.417: INFO: Received response from host: affinity-nodeport-transition-48c9w
  Apr 15 09:16:01.417: INFO: Received response from host: affinity-nodeport-transition-48c9w
  Apr 15 09:16:01.417: INFO: Received response from host: affinity-nodeport-transition-48c9w
  Apr 15 09:16:01.417: INFO: Received response from host: affinity-nodeport-transition-48c9w
  Apr 15 09:16:01.418: INFO: Received response from host: affinity-nodeport-transition-48c9w
  Apr 15 09:16:01.418: INFO: Received response from host: affinity-nodeport-transition-48c9w
  Apr 15 09:16:01.418: INFO: Received response from host: affinity-nodeport-transition-48c9w
  Apr 15 09:16:01.418: INFO: Received response from host: affinity-nodeport-transition-48c9w
  Apr 15 09:16:01.418: INFO: Received response from host: affinity-nodeport-transition-48c9w
  Apr 15 09:16:01.418: INFO: Received response from host: affinity-nodeport-transition-48c9w
  Apr 15 09:16:01.418: INFO: Received response from host: affinity-nodeport-transition-48c9w
  Apr 15 09:16:01.418: INFO: Received response from host: affinity-nodeport-transition-48c9w
  Apr 15 09:16:01.418: INFO: Received response from host: affinity-nodeport-transition-48c9w
  Apr 15 09:16:01.418: INFO: Received response from host: affinity-nodeport-transition-48c9w
  Apr 15 09:16:01.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 09:16:01.428: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8806, will wait for the garbage collector to delete the pods @ 04/15/24 09:16:01.453
  Apr 15 09:16:01.523: INFO: Deleting ReplicationController affinity-nodeport-transition took: 12.390809ms
  Apr 15 09:16:01.624: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.248315ms
  STEP: Destroying namespace "services-8806" for this suite. @ 04/15/24 09:16:03.591
• [10.649 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 04/15/24 09:16:03.606
  Apr 15 09:16:03.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename pods @ 04/15/24 09:16:03.609
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:16:03.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:16:03.643
  STEP: Create a pod @ 04/15/24 09:16:03.649
  STEP: patching /status @ 04/15/24 09:16:05.685
  Apr 15 09:16:05.703: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Apr 15 09:16:05.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8766" for this suite. @ 04/15/24 09:16:05.715
• [2.121 seconds]
------------------------------
S
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 04/15/24 09:16:05.728
  Apr 15 09:16:05.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename endpointslice @ 04/15/24 09:16:05.731
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:16:05.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:16:05.767
  STEP: getting /apis @ 04/15/24 09:16:05.773
  STEP: getting /apis/discovery.k8s.io @ 04/15/24 09:16:05.782
  STEP: getting /apis/discovery.k8s.iov1 @ 04/15/24 09:16:05.784
  STEP: creating @ 04/15/24 09:16:05.787
  STEP: getting @ 04/15/24 09:16:05.815
  STEP: listing @ 04/15/24 09:16:05.82
  STEP: watching @ 04/15/24 09:16:05.828
  Apr 15 09:16:05.829: INFO: starting watch
  STEP: cluster-wide listing @ 04/15/24 09:16:05.831
  STEP: cluster-wide watching @ 04/15/24 09:16:05.837
  Apr 15 09:16:05.838: INFO: starting watch
  STEP: patching @ 04/15/24 09:16:05.84
  STEP: updating @ 04/15/24 09:16:05.851
  Apr 15 09:16:05.869: INFO: waiting for watch events with expected annotations
  Apr 15 09:16:05.869: INFO: saw patched and updated annotations
  STEP: deleting @ 04/15/24 09:16:05.87
  STEP: deleting a collection @ 04/15/24 09:16:05.891
  Apr 15 09:16:05.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-1435" for this suite. @ 04/15/24 09:16:05.932
• [0.222 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 04/15/24 09:16:05.954
  Apr 15 09:16:05.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename webhook @ 04/15/24 09:16:05.957
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:16:05.993
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:16:05.997
  STEP: Setting up server cert @ 04/15/24 09:16:06.05
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 09:16:06.712
  STEP: Deploying the webhook pod @ 04/15/24 09:16:06.729
  STEP: Wait for the deployment to be ready @ 04/15/24 09:16:06.747
  Apr 15 09:16:06.760: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 04/15/24 09:16:08.788
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 09:16:08.816
  Apr 15 09:16:09.816: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 04/15/24 09:16:09.823
  STEP: Creating a custom resource definition that should be denied by the webhook @ 04/15/24 09:16:09.86
  Apr 15 09:16:09.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:16:09.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6265" for this suite. @ 04/15/24 09:16:09.999
  STEP: Destroying namespace "webhook-markers-18" for this suite. @ 04/15/24 09:16:10.01
• [4.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 04/15/24 09:16:10.028
  Apr 15 09:16:10.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/15/24 09:16:10.03
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:16:10.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:16:10.073
  Apr 15 09:16:10.079: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:16:11.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6332" for this suite. @ 04/15/24 09:16:11.148
• [1.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 04/15/24 09:16:11.17
  Apr 15 09:16:11.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename configmap @ 04/15/24 09:16:11.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:16:11.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:16:11.211
  STEP: Creating configMap with name cm-test-opt-del-a5130024-1647-4735-bd30-7ba74a149db5 @ 04/15/24 09:16:11.225
  STEP: Creating configMap with name cm-test-opt-upd-cb22d25e-f7d4-4117-a7dc-78cd3a046cf3 @ 04/15/24 09:16:11.235
  STEP: Creating the pod @ 04/15/24 09:16:11.243
  STEP: Deleting configmap cm-test-opt-del-a5130024-1647-4735-bd30-7ba74a149db5 @ 04/15/24 09:16:13.412
  STEP: Updating configmap cm-test-opt-upd-cb22d25e-f7d4-4117-a7dc-78cd3a046cf3 @ 04/15/24 09:16:13.426
  STEP: Creating configMap with name cm-test-opt-create-74d63f25-5344-4f7b-995c-1ee60bd216a5 @ 04/15/24 09:16:13.437
  STEP: waiting to observe update in volume @ 04/15/24 09:16:13.447
  Apr 15 09:16:17.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5135" for this suite. @ 04/15/24 09:16:17.546
• [6.388 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 04/15/24 09:16:17.561
  Apr 15 09:16:17.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 09:16:17.563
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:16:17.59
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:16:17.596
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 04/15/24 09:16:17.603
  STEP: Saw pod success @ 04/15/24 09:16:21.654
  Apr 15 09:16:21.663: INFO: Trying to get logs from node ahz3daisheng-2 pod pod-41070a9d-0fd8-4455-93b5-2055d2adfc0d container test-container: <nil>
  STEP: delete the pod @ 04/15/24 09:16:21.704
  Apr 15 09:16:21.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7340" for this suite. @ 04/15/24 09:16:21.759
• [4.213 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 04/15/24 09:16:21.775
  Apr 15 09:16:21.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename secrets @ 04/15/24 09:16:21.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:16:21.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:16:21.81
  STEP: Creating projection with secret that has name secret-emptykey-test-5ef87e12-b463-4259-94c0-b5c208e76f5d @ 04/15/24 09:16:21.818
  Apr 15 09:16:21.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9379" for this suite. @ 04/15/24 09:16:21.832
• [0.069 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 04/15/24 09:16:21.849
  Apr 15 09:16:21.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename var-expansion @ 04/15/24 09:16:21.851
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:16:21.875
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:16:21.882
  STEP: Creating a pod to test substitution in container's args @ 04/15/24 09:16:21.887
  STEP: Saw pod success @ 04/15/24 09:16:25.937
  Apr 15 09:16:25.945: INFO: Trying to get logs from node ahz3daisheng-2 pod var-expansion-eb0da9c1-0596-465e-8eec-89db36c4bded container dapi-container: <nil>
  STEP: delete the pod @ 04/15/24 09:16:25.962
  Apr 15 09:16:25.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4336" for this suite. @ 04/15/24 09:16:26.007
• [4.172 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 04/15/24 09:16:26.023
  Apr 15 09:16:26.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename replicaset @ 04/15/24 09:16:26.027
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:16:26.058
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:16:26.063
  Apr 15 09:16:26.091: INFO: Pod name sample-pod: Found 0 pods out of 1
  Apr 15 09:16:31.105: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/15/24 09:16:31.105
  STEP: Scaling up "test-rs" replicaset  @ 04/15/24 09:16:31.106
  Apr 15 09:16:31.138: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 04/15/24 09:16:31.139
  W0415 09:16:31.155507      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 15 09:16:31.164: INFO: observed ReplicaSet test-rs in namespace replicaset-3685 with ReadyReplicas 1, AvailableReplicas 1
  Apr 15 09:16:31.183: INFO: observed ReplicaSet test-rs in namespace replicaset-3685 with ReadyReplicas 1, AvailableReplicas 1
  Apr 15 09:16:31.285: INFO: observed ReplicaSet test-rs in namespace replicaset-3685 with ReadyReplicas 1, AvailableReplicas 1
  Apr 15 09:16:31.311: INFO: observed ReplicaSet test-rs in namespace replicaset-3685 with ReadyReplicas 1, AvailableReplicas 1
  Apr 15 09:16:32.665: INFO: observed ReplicaSet test-rs in namespace replicaset-3685 with ReadyReplicas 2, AvailableReplicas 2
  Apr 15 09:16:33.321: INFO: observed Replicaset test-rs in namespace replicaset-3685 with ReadyReplicas 3 found true
  Apr 15 09:16:33.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3685" for this suite. @ 04/15/24 09:16:33.334
• [7.324 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 04/15/24 09:16:33.348
  Apr 15 09:16:33.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename namespaces @ 04/15/24 09:16:33.35
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:16:33.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:16:33.384
  STEP: Creating a test namespace @ 04/15/24 09:16:33.393
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:16:33.427
  STEP: Creating a service in the namespace @ 04/15/24 09:16:33.435
  STEP: Deleting the namespace @ 04/15/24 09:16:33.453
  STEP: Waiting for the namespace to be removed. @ 04/15/24 09:16:33.475
  STEP: Recreating the namespace @ 04/15/24 09:16:40.488
  STEP: Verifying there is no service in the namespace @ 04/15/24 09:16:40.541
  Apr 15 09:16:40.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-2848" for this suite. @ 04/15/24 09:16:40.564
  STEP: Destroying namespace "nsdeletetest-385" for this suite. @ 04/15/24 09:16:40.58
  Apr 15 09:16:40.589: INFO: Namespace nsdeletetest-385 was already deleted
  STEP: Destroying namespace "nsdeletetest-3181" for this suite. @ 04/15/24 09:16:40.59
• [7.255 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 04/15/24 09:16:40.606
  Apr 15 09:16:40.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 09:16:40.611
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:16:40.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:16:40.666
  STEP: Creating a pod to test downward api env vars @ 04/15/24 09:16:40.686
  STEP: Saw pod success @ 04/15/24 09:16:44.759
  Apr 15 09:16:44.767: INFO: Trying to get logs from node ahz3daisheng-3 pod downward-api-86d8d401-d0e6-4a14-ba55-78f22d148984 container dapi-container: <nil>
  STEP: delete the pod @ 04/15/24 09:16:44.786
  Apr 15 09:16:44.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1899" for this suite. @ 04/15/24 09:16:44.835
• [4.244 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 04/15/24 09:16:44.853
  Apr 15 09:16:44.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename svcaccounts @ 04/15/24 09:16:44.855
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:16:44.898
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:16:44.907
  STEP: Creating ServiceAccount "e2e-sa-48762"  @ 04/15/24 09:16:44.912
  Apr 15 09:16:44.920: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-48762"  @ 04/15/24 09:16:44.92
  Apr 15 09:16:44.938: INFO: AutomountServiceAccountToken: true
  Apr 15 09:16:44.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9069" for this suite. @ 04/15/24 09:16:44.957
• [0.121 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 04/15/24 09:16:44.977
  Apr 15 09:16:44.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename webhook @ 04/15/24 09:16:44.98
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:16:45.017
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:16:45.022
  STEP: Setting up server cert @ 04/15/24 09:16:45.088
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 09:16:45.59
  STEP: Deploying the webhook pod @ 04/15/24 09:16:45.609
  STEP: Wait for the deployment to be ready @ 04/15/24 09:16:45.648
  Apr 15 09:16:45.667: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 04/15/24 09:16:47.699
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 09:16:47.747
  Apr 15 09:16:48.748: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 15 09:16:48.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4092-crds.webhook.example.com via the AdmissionRegistration API @ 04/15/24 09:16:49.314
  STEP: Creating a custom resource that should be mutated by the webhook @ 04/15/24 09:16:49.368
  Apr 15 09:16:51.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8922" for this suite. @ 04/15/24 09:16:52.417
  STEP: Destroying namespace "webhook-markers-5483" for this suite. @ 04/15/24 09:16:52.433
• [7.470 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 04/15/24 09:16:52.454
  Apr 15 09:16:52.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename replication-controller @ 04/15/24 09:16:52.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:16:52.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:16:52.493
  Apr 15 09:16:52.500: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 04/15/24 09:16:53.53
  STEP: Checking rc "condition-test" has the desired failure condition set @ 04/15/24 09:16:53.55
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 04/15/24 09:16:54.572
  Apr 15 09:16:54.588: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 04/15/24 09:16:54.588
  Apr 15 09:16:55.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8505" for this suite. @ 04/15/24 09:16:55.621
• [3.185 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 04/15/24 09:16:55.647
  Apr 15 09:16:55.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 09:16:55.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:16:55.683
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:16:55.687
  STEP: Creating pod busybox-d319a1db-b02c-4403-90c2-a6afed972492 in namespace container-probe-5826 @ 04/15/24 09:16:55.693
  Apr 15 09:16:57.740: INFO: Started pod busybox-d319a1db-b02c-4403-90c2-a6afed972492 in namespace container-probe-5826
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/15/24 09:16:57.74
  Apr 15 09:16:57.763: INFO: Initial restart count of pod busybox-d319a1db-b02c-4403-90c2-a6afed972492 is 0
  Apr 15 09:20:59.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 09:20:59.033
  STEP: Destroying namespace "container-probe-5826" for this suite. @ 04/15/24 09:20:59.075
• [243.455 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 04/15/24 09:20:59.119
  Apr 15 09:20:59.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 09:20:59.123
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:20:59.152
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:20:59.181
  STEP: Counting existing ResourceQuota @ 04/15/24 09:20:59.186
  STEP: Creating a ResourceQuota @ 04/15/24 09:21:04.195
  STEP: Ensuring resource quota status is calculated @ 04/15/24 09:21:04.212
  STEP: Creating a Service @ 04/15/24 09:21:06.222
  STEP: Creating a NodePort Service @ 04/15/24 09:21:06.256
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 04/15/24 09:21:06.332
  STEP: Ensuring resource quota status captures service creation @ 04/15/24 09:21:06.386
  STEP: Deleting Services @ 04/15/24 09:21:08.396
  STEP: Ensuring resource quota status released usage @ 04/15/24 09:21:08.472
  Apr 15 09:21:10.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6773" for this suite. @ 04/15/24 09:21:10.499
• [11.395 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 04/15/24 09:21:10.519
  Apr 15 09:21:10.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-runtime @ 04/15/24 09:21:10.522
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:21:10.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:21:10.56
  STEP: create the container @ 04/15/24 09:21:10.565
  W0415 09:21:10.581929      14 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 04/15/24 09:21:10.582
  STEP: get the container status @ 04/15/24 09:21:13.619
  STEP: the container should be terminated @ 04/15/24 09:21:13.626
  STEP: the termination message should be set @ 04/15/24 09:21:13.626
  Apr 15 09:21:13.626: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 04/15/24 09:21:13.626
  Apr 15 09:21:13.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5474" for this suite. @ 04/15/24 09:21:13.677
• [3.178 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 04/15/24 09:21:13.7
  Apr 15 09:21:13.700: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 09:21:13.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:21:13.787
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:21:13.799
  STEP: Counting existing ResourceQuota @ 04/15/24 09:21:13.807
  STEP: Creating a ResourceQuota @ 04/15/24 09:21:18.828
  STEP: Ensuring resource quota status is calculated @ 04/15/24 09:21:18.844
  STEP: Creating a ReplicationController @ 04/15/24 09:21:20.854
  STEP: Ensuring resource quota status captures replication controller creation @ 04/15/24 09:21:20.876
  STEP: Deleting a ReplicationController @ 04/15/24 09:21:22.89
  STEP: Ensuring resource quota status released usage @ 04/15/24 09:21:22.902
  Apr 15 09:21:24.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8518" for this suite. @ 04/15/24 09:21:24.925
• [11.249 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 04/15/24 09:21:24.95
  Apr 15 09:21:24.950: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 09:21:24.953
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:21:24.982
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:21:24.987
  STEP: Creating a pod to test downward api env vars @ 04/15/24 09:21:24.993
  STEP: Saw pod success @ 04/15/24 09:21:29.035
  Apr 15 09:21:29.042: INFO: Trying to get logs from node ahz3daisheng-3 pod downward-api-16c07a2e-1dfb-41ba-b3a5-4267e1a0aba9 container dapi-container: <nil>
  STEP: delete the pod @ 04/15/24 09:21:29.103
  Apr 15 09:21:29.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2629" for this suite. @ 04/15/24 09:21:29.146
• [4.214 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 04/15/24 09:21:29.167
  Apr 15 09:21:29.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/15/24 09:21:29.17
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:21:29.211
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:21:29.216
  STEP: set up a multi version CRD @ 04/15/24 09:21:29.224
  Apr 15 09:21:29.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: mark a version not serverd @ 04/15/24 09:21:34.345
  STEP: check the unserved version gets removed @ 04/15/24 09:21:34.386
  STEP: check the other version is not changed @ 04/15/24 09:21:35.838
  Apr 15 09:21:39.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8181" for this suite. @ 04/15/24 09:21:39.436
• [10.292 seconds]
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 04/15/24 09:21:39.459
  Apr 15 09:21:39.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename gc @ 04/15/24 09:21:39.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:21:39.498
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:21:39.503
  STEP: create the deployment @ 04/15/24 09:21:39.51
  W0415 09:21:39.526240      14 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 04/15/24 09:21:39.526
  STEP: delete the deployment @ 04/15/24 09:21:39.76
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 04/15/24 09:21:39.776
  STEP: Gathering metrics @ 04/15/24 09:21:40.316
  Apr 15 09:21:40.572: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 15 09:21:40.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-528" for this suite. @ 04/15/24 09:21:40.595
• [1.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:177
  STEP: Creating a kubernetes client @ 04/15/24 09:21:40.609
  Apr 15 09:21:40.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename daemonsets @ 04/15/24 09:21:40.611
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:21:40.68
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:21:40.691
  STEP: Creating simple DaemonSet "daemon-set" @ 04/15/24 09:21:40.781
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/15/24 09:21:40.799
  Apr 15 09:21:40.822: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 09:21:40.822: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  Apr 15 09:21:41.839: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 15 09:21:41.840: INFO: Node ahz3daisheng-2 is running 0 daemon pod, expected 1
  Apr 15 09:21:42.838: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 15 09:21:42.838: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 04/15/24 09:21:42.845
  Apr 15 09:21:42.915: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 15 09:21:42.915: INFO: Node ahz3daisheng-2 is running 0 daemon pod, expected 1
  Apr 15 09:21:43.946: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 15 09:21:43.946: INFO: Node ahz3daisheng-2 is running 0 daemon pod, expected 1
  Apr 15 09:21:44.935: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 15 09:21:44.935: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/15/24 09:21:44.941
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2545, will wait for the garbage collector to delete the pods @ 04/15/24 09:21:44.941
  Apr 15 09:21:45.009: INFO: Deleting DaemonSet.extensions daemon-set took: 11.898554ms
  Apr 15 09:21:45.210: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.255221ms
  Apr 15 09:21:46.217: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 09:21:46.217: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 15 09:21:46.230: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"201128"},"items":null}

  Apr 15 09:21:46.236: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"201128"},"items":null}

  Apr 15 09:21:46.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2545" for this suite. @ 04/15/24 09:21:46.303
• [5.712 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:474
  STEP: Creating a kubernetes client @ 04/15/24 09:21:46.321
  Apr 15 09:21:46.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename field-validation @ 04/15/24 09:21:46.324
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:21:46.38
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:21:46.387
  Apr 15 09:21:46.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  W0415 09:21:49.159457      14 warnings.go:70] unknown field "alpha"
  W0415 09:21:49.159508      14 warnings.go:70] unknown field "beta"
  W0415 09:21:49.159521      14 warnings.go:70] unknown field "delta"
  W0415 09:21:49.159632      14 warnings.go:70] unknown field "epsilon"
  W0415 09:21:49.159702      14 warnings.go:70] unknown field "gamma"
  Apr 15 09:21:49.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-926" for this suite. @ 04/15/24 09:21:49.786
• [3.482 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 04/15/24 09:21:49.805
  Apr 15 09:21:49.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 09:21:49.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:21:49.845
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:21:49.853
  STEP: Creating a ResourceQuota with terminating scope @ 04/15/24 09:21:49.859
  STEP: Ensuring ResourceQuota status is calculated @ 04/15/24 09:21:49.873
  STEP: Creating a ResourceQuota with not terminating scope @ 04/15/24 09:21:51.885
  STEP: Ensuring ResourceQuota status is calculated @ 04/15/24 09:21:51.899
  STEP: Creating a long running pod @ 04/15/24 09:21:53.913
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 04/15/24 09:21:53.955
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 04/15/24 09:21:55.964
  STEP: Deleting the pod @ 04/15/24 09:21:57.972
  STEP: Ensuring resource quota status released the pod usage @ 04/15/24 09:21:58.011
  STEP: Creating a terminating pod @ 04/15/24 09:22:00.021
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 04/15/24 09:22:00.041
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 04/15/24 09:22:02.052
  STEP: Deleting the pod @ 04/15/24 09:22:04.061
  STEP: Ensuring resource quota status released the pod usage @ 04/15/24 09:22:04.083
  Apr 15 09:22:06.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-668" for this suite. @ 04/15/24 09:22:06.104
• [16.311 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 04/15/24 09:22:06.124
  Apr 15 09:22:06.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename pods @ 04/15/24 09:22:06.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:22:06.176
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:22:06.183
  STEP: creating a Pod with a static label @ 04/15/24 09:22:06.206
  STEP: watching for Pod to be ready @ 04/15/24 09:22:06.223
  Apr 15 09:22:06.227: INFO: observed Pod pod-test in namespace pods-2733 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Apr 15 09:22:06.238: INFO: observed Pod pod-test in namespace pods-2733 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 09:22:06 +0000 UTC  }]
  Apr 15 09:22:06.272: INFO: observed Pod pod-test in namespace pods-2733 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 09:22:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 09:22:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-04-15 09:22:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 09:22:06 +0000 UTC  }]
  Apr 15 09:22:07.522: INFO: Found Pod pod-test in namespace pods-2733 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 09:22:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 09:22:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 09:22:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-04-15 09:22:06 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 04/15/24 09:22:07.529
  STEP: getting the Pod and ensuring that it's patched @ 04/15/24 09:22:07.551
  STEP: replacing the Pod's status Ready condition to False @ 04/15/24 09:22:07.559
  STEP: check the Pod again to ensure its Ready conditions are False @ 04/15/24 09:22:07.581
  STEP: deleting the Pod via a Collection with a LabelSelector @ 04/15/24 09:22:07.581
  STEP: watching for the Pod to be deleted @ 04/15/24 09:22:07.597
  Apr 15 09:22:07.601: INFO: observed event type MODIFIED
  Apr 15 09:22:08.895: INFO: observed event type MODIFIED
  Apr 15 09:22:09.552: INFO: observed event type MODIFIED
  Apr 15 09:22:09.661: INFO: observed event type MODIFIED
  Apr 15 09:22:10.552: INFO: observed event type MODIFIED
  Apr 15 09:22:10.578: INFO: observed event type MODIFIED
  Apr 15 09:22:10.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2733" for this suite. @ 04/15/24 09:22:10.605
• [4.501 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 04/15/24 09:22:10.627
  Apr 15 09:22:10.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename security-context-test @ 04/15/24 09:22:10.629
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:22:10.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:22:10.678
  Apr 15 09:22:12.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-4959" for this suite. @ 04/15/24 09:22:12.759
• [2.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 04/15/24 09:22:12.828
  Apr 15 09:22:12.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename gc @ 04/15/24 09:22:12.83
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:22:12.92
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:22:12.926
  STEP: create the rc1 @ 04/15/24 09:22:12.943
  STEP: create the rc2 @ 04/15/24 09:22:12.955
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 04/15/24 09:22:19.204
  STEP: delete the rc simpletest-rc-to-be-deleted @ 04/15/24 09:22:22.312
  STEP: wait for the rc to be deleted @ 04/15/24 09:22:22.416
  Apr 15 09:22:27.512: INFO: 68 pods remaining
  Apr 15 09:22:27.512: INFO: 68 pods has nil DeletionTimestamp
  Apr 15 09:22:27.512: INFO: 
  STEP: Gathering metrics @ 04/15/24 09:22:32.451
  Apr 15 09:22:32.631: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 15 09:22:32.631: INFO: Deleting pod "simpletest-rc-to-be-deleted-242d8" in namespace "gc-7441"
  Apr 15 09:22:32.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-25rxd" in namespace "gc-7441"
  Apr 15 09:22:32.812: INFO: Deleting pod "simpletest-rc-to-be-deleted-295m6" in namespace "gc-7441"
  Apr 15 09:22:32.869: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hvv5" in namespace "gc-7441"
  Apr 15 09:22:32.987: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xkn8" in namespace "gc-7441"
  Apr 15 09:22:33.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jqkr" in namespace "gc-7441"
  Apr 15 09:22:33.122: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mcrf" in namespace "gc-7441"
  Apr 15 09:22:33.197: INFO: Deleting pod "simpletest-rc-to-be-deleted-4n8sm" in namespace "gc-7441"
  Apr 15 09:22:33.264: INFO: Deleting pod "simpletest-rc-to-be-deleted-4tqhq" in namespace "gc-7441"
  Apr 15 09:22:33.376: INFO: Deleting pod "simpletest-rc-to-be-deleted-56b2q" in namespace "gc-7441"
  Apr 15 09:22:33.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-56hk9" in namespace "gc-7441"
  Apr 15 09:22:33.495: INFO: Deleting pod "simpletest-rc-to-be-deleted-5htkq" in namespace "gc-7441"
  Apr 15 09:22:33.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-64vmv" in namespace "gc-7441"
  Apr 15 09:22:33.637: INFO: Deleting pod "simpletest-rc-to-be-deleted-69r6b" in namespace "gc-7441"
  Apr 15 09:22:33.694: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lpmn" in namespace "gc-7441"
  Apr 15 09:22:33.739: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dvkg" in namespace "gc-7441"
  Apr 15 09:22:33.801: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kf4l" in namespace "gc-7441"
  Apr 15 09:22:33.852: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qqlz" in namespace "gc-7441"
  Apr 15 09:22:33.930: INFO: Deleting pod "simpletest-rc-to-be-deleted-7snxs" in namespace "gc-7441"
  Apr 15 09:22:34.023: INFO: Deleting pod "simpletest-rc-to-be-deleted-8265l" in namespace "gc-7441"
  Apr 15 09:22:34.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f247" in namespace "gc-7441"
  Apr 15 09:22:34.130: INFO: Deleting pod "simpletest-rc-to-be-deleted-96bb9" in namespace "gc-7441"
  Apr 15 09:22:34.264: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jfc9" in namespace "gc-7441"
  Apr 15 09:22:34.343: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jx8z" in namespace "gc-7441"
  Apr 15 09:22:34.391: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mkkh" in namespace "gc-7441"
  Apr 15 09:22:34.473: INFO: Deleting pod "simpletest-rc-to-be-deleted-9pcpb" in namespace "gc-7441"
  Apr 15 09:22:34.560: INFO: Deleting pod "simpletest-rc-to-be-deleted-9s4sh" in namespace "gc-7441"
  Apr 15 09:22:34.607: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vdt5" in namespace "gc-7441"
  Apr 15 09:22:34.686: INFO: Deleting pod "simpletest-rc-to-be-deleted-9w2vd" in namespace "gc-7441"
  Apr 15 09:22:34.755: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmvlz" in namespace "gc-7441"
  Apr 15 09:22:34.841: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnknn" in namespace "gc-7441"
  Apr 15 09:22:34.952: INFO: Deleting pod "simpletest-rc-to-be-deleted-brpt7" in namespace "gc-7441"
  Apr 15 09:22:35.194: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvzjj" in namespace "gc-7441"
  Apr 15 09:22:35.255: INFO: Deleting pod "simpletest-rc-to-be-deleted-c59qc" in namespace "gc-7441"
  Apr 15 09:22:35.338: INFO: Deleting pod "simpletest-rc-to-be-deleted-cbcdv" in namespace "gc-7441"
  Apr 15 09:22:35.399: INFO: Deleting pod "simpletest-rc-to-be-deleted-ch8px" in namespace "gc-7441"
  Apr 15 09:22:35.458: INFO: Deleting pod "simpletest-rc-to-be-deleted-cpl5w" in namespace "gc-7441"
  Apr 15 09:22:35.492: INFO: Deleting pod "simpletest-rc-to-be-deleted-czb8m" in namespace "gc-7441"
  Apr 15 09:22:35.538: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4r57" in namespace "gc-7441"
  Apr 15 09:22:35.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6nnj" in namespace "gc-7441"
  Apr 15 09:22:35.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-dl9pb" in namespace "gc-7441"
  Apr 15 09:22:35.726: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5qbr" in namespace "gc-7441"
  Apr 15 09:22:35.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8dwd" in namespace "gc-7441"
  Apr 15 09:22:35.868: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsk2l" in namespace "gc-7441"
  Apr 15 09:22:36.030: INFO: Deleting pod "simpletest-rc-to-be-deleted-g6zps" in namespace "gc-7441"
  Apr 15 09:22:36.195: INFO: Deleting pod "simpletest-rc-to-be-deleted-gcw2d" in namespace "gc-7441"
  Apr 15 09:22:36.291: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmm89" in namespace "gc-7441"
  Apr 15 09:22:36.343: INFO: Deleting pod "simpletest-rc-to-be-deleted-gqjpk" in namespace "gc-7441"
  Apr 15 09:22:36.416: INFO: Deleting pod "simpletest-rc-to-be-deleted-h29r4" in namespace "gc-7441"
  Apr 15 09:22:36.470: INFO: Deleting pod "simpletest-rc-to-be-deleted-hc8zx" in namespace "gc-7441"
  Apr 15 09:22:36.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7441" for this suite. @ 04/15/24 09:22:36.558
• [23.780 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 04/15/24 09:22:36.61
  Apr 15 09:22:36.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename watch @ 04/15/24 09:22:36.613
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:22:36.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:22:36.67
  STEP: creating a watch on configmaps with a certain label @ 04/15/24 09:22:36.674
  STEP: creating a new configmap @ 04/15/24 09:22:36.676
  STEP: modifying the configmap once @ 04/15/24 09:22:36.758
  STEP: changing the label value of the configmap @ 04/15/24 09:22:36.881
  STEP: Expecting to observe a delete notification for the watched object @ 04/15/24 09:22:36.979
  Apr 15 09:22:36.979: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5476  23779430-b829-4c75-9177-fab14f83c3d0 202661 0 2024-04-15 09:22:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-04-15 09:22:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 09:22:36.981: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5476  23779430-b829-4c75-9177-fab14f83c3d0 202662 0 2024-04-15 09:22:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-04-15 09:22:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 09:22:36.982: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5476  23779430-b829-4c75-9177-fab14f83c3d0 202663 0 2024-04-15 09:22:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-04-15 09:22:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 04/15/24 09:22:36.982
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 04/15/24 09:22:36.999
  STEP: changing the label value of the configmap back @ 04/15/24 09:22:47
  STEP: modifying the configmap a third time @ 04/15/24 09:22:47.02
  STEP: deleting the configmap @ 04/15/24 09:22:47.038
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 04/15/24 09:22:47.049
  Apr 15 09:22:47.050: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5476  23779430-b829-4c75-9177-fab14f83c3d0 203092 0 2024-04-15 09:22:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-04-15 09:22:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 09:22:47.051: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5476  23779430-b829-4c75-9177-fab14f83c3d0 203094 0 2024-04-15 09:22:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-04-15 09:22:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 09:22:47.052: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5476  23779430-b829-4c75-9177-fab14f83c3d0 203096 0 2024-04-15 09:22:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-04-15 09:22:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 09:22:47.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5476" for this suite. @ 04/15/24 09:22:47.063
• [10.475 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 04/15/24 09:22:47.088
  Apr 15 09:22:47.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/15/24 09:22:47.101
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:22:47.158
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:22:47.169
  STEP: create the container to handle the HTTPGet hook request. @ 04/15/24 09:22:47.193
  STEP: create the pod with lifecycle hook @ 04/15/24 09:22:49.245
  STEP: check poststart hook @ 04/15/24 09:23:01.352
  STEP: delete the pod with lifecycle hook @ 04/15/24 09:23:01.386
  Apr 15 09:23:03.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-5847" for this suite. @ 04/15/24 09:23:03.424
• [16.348 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 04/15/24 09:23:03.438
  Apr 15 09:23:03.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename limitrange @ 04/15/24 09:23:03.44
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:23:03.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:23:03.488
  STEP: Creating LimitRange "e2e-limitrange-hscd5" in namespace "limitrange-1094" @ 04/15/24 09:23:03.492
  STEP: Creating another limitRange in another namespace @ 04/15/24 09:23:03.51
  Apr 15 09:23:03.548: INFO: Namespace "e2e-limitrange-hscd5-7070" created
  Apr 15 09:23:03.549: INFO: Creating LimitRange "e2e-limitrange-hscd5" in namespace "e2e-limitrange-hscd5-7070"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-hscd5" @ 04/15/24 09:23:03.56
  Apr 15 09:23:03.566: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-hscd5" in "limitrange-1094" namespace @ 04/15/24 09:23:03.566
  Apr 15 09:23:03.578: INFO: LimitRange "e2e-limitrange-hscd5" has been patched
  STEP: Delete LimitRange "e2e-limitrange-hscd5" by Collection with labelSelector: "e2e-limitrange-hscd5=patched" @ 04/15/24 09:23:03.579
  STEP: Confirm that the limitRange "e2e-limitrange-hscd5" has been deleted @ 04/15/24 09:23:03.596
  Apr 15 09:23:03.596: INFO: Requesting list of LimitRange to confirm quantity
  Apr 15 09:23:03.604: INFO: Found 0 LimitRange with label "e2e-limitrange-hscd5=patched"
  Apr 15 09:23:03.605: INFO: LimitRange "e2e-limitrange-hscd5" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-hscd5" @ 04/15/24 09:23:03.605
  Apr 15 09:23:03.610: INFO: Found 1 limitRange
  Apr 15 09:23:03.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-1094" for this suite. @ 04/15/24 09:23:03.62
  STEP: Destroying namespace "e2e-limitrange-hscd5-7070" for this suite. @ 04/15/24 09:23:03.631
• [0.205 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 04/15/24 09:23:03.652
  Apr 15 09:23:03.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename taint-single-pod @ 04/15/24 09:23:03.654
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:23:03.686
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:23:03.694
  Apr 15 09:23:03.700: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 15 09:24:03.753: INFO: Waiting for terminating namespaces to be deleted...
  Apr 15 09:24:03.764: INFO: Starting informer...
  STEP: Starting pod... @ 04/15/24 09:24:03.764
  Apr 15 09:24:04.010: INFO: Pod is running on ahz3daisheng-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 04/15/24 09:24:04.011
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/15/24 09:24:04.043
  STEP: Waiting short time to make sure Pod is queued for deletion @ 04/15/24 09:24:04.054
  Apr 15 09:24:04.054: INFO: Pod wasn't evicted. Proceeding
  Apr 15 09:24:04.054: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/15/24 09:24:04.107
  STEP: Waiting some time to make sure that toleration time passed. @ 04/15/24 09:24:04.115
  Apr 15 09:25:19.117: INFO: Pod wasn't evicted. Test successful
  Apr 15 09:25:19.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-3189" for this suite. @ 04/15/24 09:25:19.128
• [135.492 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 04/15/24 09:25:19.145
  Apr 15 09:25:19.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename webhook @ 04/15/24 09:25:19.147
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:25:19.18
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:25:19.188
  STEP: Setting up server cert @ 04/15/24 09:25:19.245
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 09:25:19.734
  STEP: Deploying the webhook pod @ 04/15/24 09:25:19.753
  STEP: Wait for the deployment to be ready @ 04/15/24 09:25:19.771
  Apr 15 09:25:19.785: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 04/15/24 09:25:21.808
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 09:25:21.832
  Apr 15 09:25:22.833: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 04/15/24 09:25:22.84
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 04/15/24 09:25:22.912
  STEP: Creating a configMap that should not be mutated @ 04/15/24 09:25:22.931
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 04/15/24 09:25:22.955
  STEP: Creating a configMap that should be mutated @ 04/15/24 09:25:22.973
  Apr 15 09:25:23.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2600" for this suite. @ 04/15/24 09:25:23.127
  STEP: Destroying namespace "webhook-markers-4309" for this suite. @ 04/15/24 09:25:23.143
• [4.018 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 04/15/24 09:25:23.164
  Apr 15 09:25:23.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:25:23.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:25:23.198
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:25:23.205
  STEP: Creating secret with name s-test-opt-del-720b1457-4e0c-4b8d-8280-09b749083bbc @ 04/15/24 09:25:23.218
  STEP: Creating secret with name s-test-opt-upd-f23d6437-494c-4f4d-b1f5-c64fe78908a1 @ 04/15/24 09:25:23.227
  STEP: Creating the pod @ 04/15/24 09:25:23.235
  STEP: Deleting secret s-test-opt-del-720b1457-4e0c-4b8d-8280-09b749083bbc @ 04/15/24 09:25:25.338
  STEP: Updating secret s-test-opt-upd-f23d6437-494c-4f4d-b1f5-c64fe78908a1 @ 04/15/24 09:25:25.351
  STEP: Creating secret with name s-test-opt-create-e1ec5595-9e8c-4e80-9d35-12c66bb2479f @ 04/15/24 09:25:25.359
  STEP: waiting to observe update in volume @ 04/15/24 09:25:25.367
  Apr 15 09:25:29.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2718" for this suite. @ 04/15/24 09:25:29.454
• [6.304 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 04/15/24 09:25:29.468
  Apr 15 09:25:29.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:25:29.47
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:25:29.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:25:29.504
  STEP: Creating projection with secret that has name projected-secret-test-acfda8de-9da0-4af0-9a27-a3b03e75bc60 @ 04/15/24 09:25:29.51
  STEP: Creating a pod to test consume secrets @ 04/15/24 09:25:29.52
  STEP: Saw pod success @ 04/15/24 09:25:33.569
  Apr 15 09:25:33.576: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-projected-secrets-097e36aa-a60f-42fb-a28e-b7e434e5b334 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 09:25:33.592
  Apr 15 09:25:33.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3767" for this suite. @ 04/15/24 09:25:33.642
• [4.188 seconds]
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 04/15/24 09:25:33.657
  Apr 15 09:25:33.658: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:25:33.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:25:33.687
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:25:33.695
  STEP: Creating configMap with name projected-configmap-test-volume-3b805f3a-3b2d-470d-865c-e771a44c4d89 @ 04/15/24 09:25:33.701
  STEP: Creating a pod to test consume configMaps @ 04/15/24 09:25:33.71
  STEP: Saw pod success @ 04/15/24 09:25:37.757
  Apr 15 09:25:37.764: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-projected-configmaps-c38dc457-3704-4322-8654-ac2f854775eb container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 09:25:37.78
  Apr 15 09:25:37.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9919" for this suite. @ 04/15/24 09:25:37.823
• [4.177 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 04/15/24 09:25:37.835
  Apr 15 09:25:37.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename var-expansion @ 04/15/24 09:25:37.837
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:25:37.865
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:25:37.874
  STEP: Creating a pod to test substitution in volume subpath @ 04/15/24 09:25:37.883
  STEP: Saw pod success @ 04/15/24 09:25:41.938
  Apr 15 09:25:41.948: INFO: Trying to get logs from node ahz3daisheng-3 pod var-expansion-37eec300-3f6c-46f9-bcbe-37143695cf9e container dapi-container: <nil>
  STEP: delete the pod @ 04/15/24 09:25:41.972
  Apr 15 09:25:42.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2695" for this suite. @ 04/15/24 09:25:42.014
• [4.192 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 04/15/24 09:25:42.031
  Apr 15 09:25:42.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:25:42.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:25:42.066
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:25:42.072
  STEP: Creating configMap with name projected-configmap-test-volume-map-01cddac6-ef35-48ff-8043-71de78f32feb @ 04/15/24 09:25:42.078
  STEP: Creating a pod to test consume configMaps @ 04/15/24 09:25:42.087
  STEP: Saw pod success @ 04/15/24 09:25:46.139
  Apr 15 09:25:46.146: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-projected-configmaps-1f46bd64-c7fd-44e4-bc7b-7db5b3e21719 container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 09:25:46.163
  Apr 15 09:25:46.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2942" for this suite. @ 04/15/24 09:25:46.212
• [4.194 seconds]
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 04/15/24 09:25:46.227
  Apr 15 09:25:46.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename subpath @ 04/15/24 09:25:46.231
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:25:46.265
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:25:46.27
  STEP: Setting up data @ 04/15/24 09:25:46.275
  STEP: Creating pod pod-subpath-test-configmap-n25q @ 04/15/24 09:25:46.292
  STEP: Creating a pod to test atomic-volume-subpath @ 04/15/24 09:25:46.292
  STEP: Saw pod success @ 04/15/24 09:26:08.426
  Apr 15 09:26:08.437: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-subpath-test-configmap-n25q container test-container-subpath-configmap-n25q: <nil>
  STEP: delete the pod @ 04/15/24 09:26:08.455
  STEP: Deleting pod pod-subpath-test-configmap-n25q @ 04/15/24 09:26:08.487
  Apr 15 09:26:08.487: INFO: Deleting pod "pod-subpath-test-configmap-n25q" in namespace "subpath-8096"
  Apr 15 09:26:08.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8096" for this suite. @ 04/15/24 09:26:08.504
• [22.296 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 04/15/24 09:26:08.524
  Apr 15 09:26:08.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename secrets @ 04/15/24 09:26:08.526
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:26:08.563
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:26:08.572
  STEP: Creating secret with name secret-test-183be59d-000d-4f3a-a746-d2c9da90ac70 @ 04/15/24 09:26:08.651
  STEP: Creating a pod to test consume secrets @ 04/15/24 09:26:08.731
  STEP: Saw pod success @ 04/15/24 09:26:12.848
  Apr 15 09:26:12.857: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-secrets-b37d3bd3-a5c0-4571-a262-ec893cbc6965 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 09:26:12.87
  Apr 15 09:26:12.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5738" for this suite. @ 04/15/24 09:26:12.922
  STEP: Destroying namespace "secret-namespace-5823" for this suite. @ 04/15/24 09:26:12.936
• [4.435 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:350
  STEP: Creating a kubernetes client @ 04/15/24 09:26:12.959
  Apr 15 09:26:12.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename field-validation @ 04/15/24 09:26:12.962
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:26:13.002
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:26:13.008
  Apr 15 09:26:13.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  W0415 09:26:13.018004      14 field_validation.go:423] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc0052adea0 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  W0415 09:26:15.896451      14 warnings.go:70] unknown field "alpha"
  W0415 09:26:15.896682      14 warnings.go:70] unknown field "beta"
  W0415 09:26:15.896900      14 warnings.go:70] unknown field "delta"
  W0415 09:26:15.897122      14 warnings.go:70] unknown field "epsilon"
  W0415 09:26:15.897254      14 warnings.go:70] unknown field "gamma"
  Apr 15 09:26:16.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2131" for this suite. @ 04/15/24 09:26:16.518
• [3.573 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 04/15/24 09:26:16.542
  Apr 15 09:26:16.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 09:26:16.545
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:26:16.584
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:26:16.592
  STEP: creating a replication controller @ 04/15/24 09:26:16.598
  Apr 15 09:26:16.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9535 create -f -'
  Apr 15 09:26:17.585: INFO: stderr: ""
  Apr 15 09:26:17.585: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 04/15/24 09:26:17.585
  Apr 15 09:26:17.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 15 09:26:17.804: INFO: stderr: ""
  Apr 15 09:26:17.804: INFO: stdout: "update-demo-nautilus-m785n update-demo-nautilus-vdkjb "
  Apr 15 09:26:17.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9535 get pods update-demo-nautilus-m785n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 15 09:26:17.968: INFO: stderr: ""
  Apr 15 09:26:17.968: INFO: stdout: ""
  Apr 15 09:26:17.968: INFO: update-demo-nautilus-m785n is created but not running
  Apr 15 09:26:22.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Apr 15 09:26:23.133: INFO: stderr: ""
  Apr 15 09:26:23.134: INFO: stdout: "update-demo-nautilus-m785n update-demo-nautilus-vdkjb "
  Apr 15 09:26:23.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9535 get pods update-demo-nautilus-m785n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 15 09:26:23.305: INFO: stderr: ""
  Apr 15 09:26:23.305: INFO: stdout: "true"
  Apr 15 09:26:23.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9535 get pods update-demo-nautilus-m785n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 15 09:26:23.473: INFO: stderr: ""
  Apr 15 09:26:23.473: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 15 09:26:23.473: INFO: validating pod update-demo-nautilus-m785n
  Apr 15 09:26:23.500: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 15 09:26:23.501: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 15 09:26:23.501: INFO: update-demo-nautilus-m785n is verified up and running
  Apr 15 09:26:23.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9535 get pods update-demo-nautilus-vdkjb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Apr 15 09:26:23.651: INFO: stderr: ""
  Apr 15 09:26:23.652: INFO: stdout: "true"
  Apr 15 09:26:23.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9535 get pods update-demo-nautilus-vdkjb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Apr 15 09:26:23.815: INFO: stderr: ""
  Apr 15 09:26:23.815: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Apr 15 09:26:23.815: INFO: validating pod update-demo-nautilus-vdkjb
  Apr 15 09:26:23.839: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Apr 15 09:26:23.839: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Apr 15 09:26:23.840: INFO: update-demo-nautilus-vdkjb is verified up and running
  STEP: using delete to clean up resources @ 04/15/24 09:26:23.84
  Apr 15 09:26:23.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9535 delete --grace-period=0 --force -f -'
  Apr 15 09:26:23.988: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 15 09:26:23.988: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Apr 15 09:26:23.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9535 get rc,svc -l name=update-demo --no-headers'
  Apr 15 09:26:24.210: INFO: stderr: "No resources found in kubectl-9535 namespace.\n"
  Apr 15 09:26:24.210: INFO: stdout: ""
  Apr 15 09:26:24.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9535 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 15 09:26:24.418: INFO: stderr: ""
  Apr 15 09:26:24.418: INFO: stdout: ""
  Apr 15 09:26:24.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9535" for this suite. @ 04/15/24 09:26:24.443
• [7.923 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 04/15/24 09:26:24.465
  Apr 15 09:26:24.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 09:26:24.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:26:24.5
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:26:24.507
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 04/15/24 09:26:24.514
  STEP: Saw pod success @ 04/15/24 09:26:26.555
  Apr 15 09:26:26.562: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-4d97440d-aac9-4ff2-a1ba-83165605e978 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 09:26:26.578
  Apr 15 09:26:26.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1833" for this suite. @ 04/15/24 09:26:26.62
• [2.168 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 04/15/24 09:26:26.642
  Apr 15 09:26:26.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 09:26:26.646
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:26:26.685
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:26:26.695
  STEP: Creating a ResourceQuota @ 04/15/24 09:26:26.704
  STEP: Getting a ResourceQuota @ 04/15/24 09:26:26.722
  STEP: Updating a ResourceQuota @ 04/15/24 09:26:26.736
  STEP: Verifying a ResourceQuota was modified @ 04/15/24 09:26:26.752
  STEP: Deleting a ResourceQuota @ 04/15/24 09:26:26.765
  STEP: Verifying the deleted ResourceQuota @ 04/15/24 09:26:26.78
  Apr 15 09:26:26.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1424" for this suite. @ 04/15/24 09:26:26.8
• [0.172 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 04/15/24 09:26:26.819
  Apr 15 09:26:26.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:26:26.823
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:26:26.886
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:26:26.893
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 09:26:26.901
  STEP: Saw pod success @ 04/15/24 09:26:30.952
  Apr 15 09:26:30.959: INFO: Trying to get logs from node ahz3daisheng-3 pod downwardapi-volume-d66da96f-0680-4a28-bddb-cf98b755285e container client-container: <nil>
  STEP: delete the pod @ 04/15/24 09:26:30.977
  Apr 15 09:26:31.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2959" for this suite. @ 04/15/24 09:26:31.019
• [4.217 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 04/15/24 09:26:31.039
  Apr 15 09:26:31.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename endpointslice @ 04/15/24 09:26:31.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:26:31.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:26:31.089
  Apr 15 09:26:31.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4575" for this suite. @ 04/15/24 09:26:31.208
• [0.184 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 04/15/24 09:26:31.228
  Apr 15 09:26:31.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:26:31.23
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:26:31.272
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:26:31.278
  STEP: Creating projection with secret that has name projected-secret-test-6cbbbe1f-3462-45f3-9c7f-986e65cf8680 @ 04/15/24 09:26:31.285
  STEP: Creating a pod to test consume secrets @ 04/15/24 09:26:31.299
  STEP: Saw pod success @ 04/15/24 09:26:35.347
  Apr 15 09:26:35.353: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-projected-secrets-baa5308a-aa89-4597-9566-544e0af0d172 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 09:26:35.366
  Apr 15 09:26:35.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1280" for this suite. @ 04/15/24 09:26:35.402
• [4.184 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 04/15/24 09:26:35.417
  Apr 15 09:26:35.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename cronjob @ 04/15/24 09:26:35.419
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:26:35.45
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:26:35.457
  STEP: Creating a cronjob @ 04/15/24 09:26:35.461
  STEP: creating @ 04/15/24 09:26:35.461
  STEP: getting @ 04/15/24 09:26:35.471
  STEP: listing @ 04/15/24 09:26:35.479
  STEP: watching @ 04/15/24 09:26:35.485
  Apr 15 09:26:35.485: INFO: starting watch
  STEP: cluster-wide listing @ 04/15/24 09:26:35.487
  STEP: cluster-wide watching @ 04/15/24 09:26:35.495
  Apr 15 09:26:35.495: INFO: starting watch
  STEP: patching @ 04/15/24 09:26:35.498
  STEP: updating @ 04/15/24 09:26:35.51
  Apr 15 09:26:35.530: INFO: waiting for watch events with expected annotations
  Apr 15 09:26:35.531: INFO: saw patched and updated annotations
  STEP: patching /status @ 04/15/24 09:26:35.531
  STEP: updating /status @ 04/15/24 09:26:35.547
  STEP: get /status @ 04/15/24 09:26:35.574
  STEP: deleting @ 04/15/24 09:26:35.585
  STEP: deleting a collection @ 04/15/24 09:26:35.617
  Apr 15 09:26:35.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3015" for this suite. @ 04/15/24 09:26:35.651
• [0.249 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 04/15/24 09:26:35.676
  Apr 15 09:26:35.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename webhook @ 04/15/24 09:26:35.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:26:35.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:26:35.718
  STEP: Setting up server cert @ 04/15/24 09:26:35.766
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 09:26:36.658
  STEP: Deploying the webhook pod @ 04/15/24 09:26:36.711
  STEP: Wait for the deployment to be ready @ 04/15/24 09:26:36.752
  Apr 15 09:26:36.766: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 04/15/24 09:26:38.789
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 09:26:38.813
  Apr 15 09:26:39.814: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Apr 15 09:26:39.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7201-crds.webhook.example.com via the AdmissionRegistration API @ 04/15/24 09:26:40.344
  STEP: Creating a custom resource that should be mutated by the webhook @ 04/15/24 09:26:40.384
  Apr 15 09:26:42.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3605" for this suite. @ 04/15/24 09:26:43.227
  STEP: Destroying namespace "webhook-markers-5325" for this suite. @ 04/15/24 09:26:43.243
• [7.581 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 04/15/24 09:26:43.264
  Apr 15 09:26:43.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 09:26:43.267
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:26:43.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:26:43.306
  STEP: validating cluster-info @ 04/15/24 09:26:43.315
  Apr 15 09:26:43.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-3172 cluster-info'
  Apr 15 09:26:43.508: INFO: stderr: ""
  Apr 15 09:26:43.508: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Apr 15 09:26:43.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3172" for this suite. @ 04/15/24 09:26:43.518
• [0.269 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 04/15/24 09:26:43.542
  Apr 15 09:26:43.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:26:43.545
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:26:43.594
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:26:43.6
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 09:26:43.607
  STEP: Saw pod success @ 04/15/24 09:26:45.661
  Apr 15 09:26:45.673: INFO: Trying to get logs from node ahz3daisheng-3 pod downwardapi-volume-a766b649-4a75-4aee-a210-ecfe038085ed container client-container: <nil>
  STEP: delete the pod @ 04/15/24 09:26:45.7
  Apr 15 09:26:45.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-860" for this suite. @ 04/15/24 09:26:45.753
• [2.224 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 04/15/24 09:26:45.779
  Apr 15 09:26:45.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename services @ 04/15/24 09:26:45.782
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:26:45.824
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:26:45.832
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-1473 @ 04/15/24 09:26:45.844
  STEP: changing the ExternalName service to type=ClusterIP @ 04/15/24 09:26:45.86
  STEP: creating replication controller externalname-service in namespace services-1473 @ 04/15/24 09:26:45.894
  I0415 09:26:45.916843      14 runners.go:194] Created replication controller with name: externalname-service, namespace: services-1473, replica count: 2
  I0415 09:26:48.969296      14 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 15 09:26:48.969: INFO: Creating new exec pod
  Apr 15 09:26:52.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-1473 exec execpodrxbfw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 15 09:26:54.324: INFO: rc: 1
  Apr 15 09:26:54.324: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-1473 exec execpodrxbfw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 externalname-service 80
  nc: connect to externalname-service port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  Apr 15 09:26:55.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-1473 exec execpodrxbfw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 15 09:26:57.626: INFO: rc: 1
  Apr 15 09:26:57.626: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-1473 exec execpodrxbfw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 externalname-service 80
  nc: connect to externalname-service port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  Apr 15 09:26:58.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-1473 exec execpodrxbfw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 15 09:26:58.625: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 15 09:26:58.625: INFO: stdout: ""
  Apr 15 09:26:59.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-1473 exec execpodrxbfw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 15 09:27:01.589: INFO: rc: 1
  Apr 15 09:27:01.589: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-1473 exec execpodrxbfw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 externalname-service 80
  nc: connect to externalname-service port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  Apr 15 09:27:02.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-1473 exec execpodrxbfw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Apr 15 09:27:02.597: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Apr 15 09:27:02.597: INFO: stdout: "externalname-service-dcz75"
  Apr 15 09:27:02.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-1473 exec execpodrxbfw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.34.86 80'
  Apr 15 09:27:02.889: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.34.86 80\nConnection to 10.233.34.86 80 port [tcp/http] succeeded!\n"
  Apr 15 09:27:02.889: INFO: stdout: "externalname-service-dcz75"
  Apr 15 09:27:02.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 09:27:02.900: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-1473" for this suite. @ 04/15/24 09:27:02.949
• [17.185 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 04/15/24 09:27:02.966
  Apr 15 09:27:02.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename webhook @ 04/15/24 09:27:02.969
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:27:03.006
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:27:03.011
  STEP: Setting up server cert @ 04/15/24 09:27:03.063
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 09:27:03.514
  STEP: Deploying the webhook pod @ 04/15/24 09:27:03.527
  STEP: Wait for the deployment to be ready @ 04/15/24 09:27:03.556
  Apr 15 09:27:03.590: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/15/24 09:27:05.613
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 09:27:05.63
  Apr 15 09:27:06.631: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 04/15/24 09:27:06.639
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 04/15/24 09:27:06.642
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 04/15/24 09:27:06.642
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 04/15/24 09:27:06.642
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 04/15/24 09:27:06.644
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 04/15/24 09:27:06.644
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 04/15/24 09:27:06.647
  Apr 15 09:27:06.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3455" for this suite. @ 04/15/24 09:27:06.749
  STEP: Destroying namespace "webhook-markers-3378" for this suite. @ 04/15/24 09:27:06.771
• [3.824 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 04/15/24 09:27:06.809
  Apr 15 09:27:06.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename webhook @ 04/15/24 09:27:06.813
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:27:06.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:27:06.912
  STEP: Setting up server cert @ 04/15/24 09:27:06.967
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 09:27:07.958
  STEP: Deploying the webhook pod @ 04/15/24 09:27:07.97
  STEP: Wait for the deployment to be ready @ 04/15/24 09:27:07.991
  Apr 15 09:27:08.010: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 04/15/24 09:27:10.062
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 09:27:10.11
  Apr 15 09:27:11.111: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 04/15/24 09:27:11.122
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/15/24 09:27:11.163
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 04/15/24 09:27:11.183
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/15/24 09:27:11.206
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 04/15/24 09:27:11.229
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 04/15/24 09:27:11.245
  Apr 15 09:27:11.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8224" for this suite. @ 04/15/24 09:27:11.41
  STEP: Destroying namespace "webhook-markers-1016" for this suite. @ 04/15/24 09:27:11.432
• [4.640 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 04/15/24 09:27:11.452
  Apr 15 09:27:11.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename replication-controller @ 04/15/24 09:27:11.455
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:27:11.492
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:27:11.499
  STEP: Given a ReplicationController is created @ 04/15/24 09:27:11.507
  STEP: When the matched label of one of its pods change @ 04/15/24 09:27:11.523
  Apr 15 09:27:11.537: INFO: Pod name pod-release: Found 0 pods out of 1
  Apr 15 09:27:16.553: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 04/15/24 09:27:16.574
  Apr 15 09:27:17.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3307" for this suite. @ 04/15/24 09:27:17.604
• [6.168 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 04/15/24 09:27:17.632
  Apr 15 09:27:17.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-runtime @ 04/15/24 09:27:17.636
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:27:17.678
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:27:17.688
  STEP: create the container @ 04/15/24 09:27:17.696
  W0415 09:27:17.718103      14 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/15/24 09:27:17.719
  STEP: get the container status @ 04/15/24 09:27:19.752
  STEP: the container should be terminated @ 04/15/24 09:27:19.76
  STEP: the termination message should be set @ 04/15/24 09:27:19.76
  Apr 15 09:27:19.761: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 04/15/24 09:27:19.761
  Apr 15 09:27:19.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7789" for this suite. @ 04/15/24 09:27:19.797
• [2.179 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 04/15/24 09:27:19.816
  Apr 15 09:27:19.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename var-expansion @ 04/15/24 09:27:19.819
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:27:19.851
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:27:19.857
  STEP: Creating a pod to test env composition @ 04/15/24 09:27:19.864
  STEP: Saw pod success @ 04/15/24 09:27:21.906
  Apr 15 09:27:21.915: INFO: Trying to get logs from node ahz3daisheng-3 pod var-expansion-66f3226a-174f-47be-aa85-77682b344cb8 container dapi-container: <nil>
  STEP: delete the pod @ 04/15/24 09:27:21.937
  Apr 15 09:27:21.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-822" for this suite. @ 04/15/24 09:27:21.985
• [2.184 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 04/15/24 09:27:22
  Apr 15 09:27:22.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:27:22.002
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:27:22.043
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:27:22.048
  STEP: Creating configMap with name cm-test-opt-del-eb057103-29bf-47d1-86ee-0f3c9cf2b9d7 @ 04/15/24 09:27:22.064
  STEP: Creating configMap with name cm-test-opt-upd-d3c1ee6e-e5de-4631-9e75-da86039744ba @ 04/15/24 09:27:22.077
  STEP: Creating the pod @ 04/15/24 09:27:22.089
  STEP: Deleting configmap cm-test-opt-del-eb057103-29bf-47d1-86ee-0f3c9cf2b9d7 @ 04/15/24 09:27:24.215
  STEP: Updating configmap cm-test-opt-upd-d3c1ee6e-e5de-4631-9e75-da86039744ba @ 04/15/24 09:27:24.234
  STEP: Creating configMap with name cm-test-opt-create-187e347b-8c95-455b-9dd1-e0115ce91c0a @ 04/15/24 09:27:24.25
  STEP: waiting to observe update in volume @ 04/15/24 09:27:24.261
  Apr 15 09:27:26.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7743" for this suite. @ 04/15/24 09:27:26.334
• [4.347 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 04/15/24 09:27:26.401
  Apr 15 09:27:26.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 09:27:26.406
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:27:26.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:27:26.47
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 04/15/24 09:27:26.478
  STEP: Saw pod success @ 04/15/24 09:27:30.551
  Apr 15 09:27:30.565: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-274f4006-f68b-4dec-8356-1467bd60c181 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 09:27:30.586
  Apr 15 09:27:30.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3008" for this suite. @ 04/15/24 09:27:30.65
• [4.273 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 04/15/24 09:27:30.681
  Apr 15 09:27:30.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename sched-preemption @ 04/15/24 09:27:30.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:27:30.731
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:27:30.742
  Apr 15 09:27:30.792: INFO: Waiting up to 1m0s for all nodes to be ready
  Apr 15 09:28:30.883: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 04/15/24 09:28:30.9
  Apr 15 09:28:30.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename sched-preemption-path @ 04/15/24 09:28:30.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:28:30.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:28:30.952
  Apr 15 09:28:30.991: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Apr 15 09:28:31.002: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Apr 15 09:28:31.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 09:28:31.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-317" for this suite. @ 04/15/24 09:28:31.212
  STEP: Destroying namespace "sched-preemption-1165" for this suite. @ 04/15/24 09:28:31.228
• [60.563 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 04/15/24 09:28:31.245
  Apr 15 09:28:31.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename svcaccounts @ 04/15/24 09:28:31.248
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:28:31.285
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:28:31.292
  STEP: reading a file in the container @ 04/15/24 09:28:33.352
  Apr 15 09:28:33.353: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1626 pod-service-account-562ab288-e341-42ed-bbe4-c395e7f46a0b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 04/15/24 09:28:33.643
  Apr 15 09:28:33.643: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1626 pod-service-account-562ab288-e341-42ed-bbe4-c395e7f46a0b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 04/15/24 09:28:33.916
  Apr 15 09:28:33.916: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1626 pod-service-account-562ab288-e341-42ed-bbe4-c395e7f46a0b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Apr 15 09:28:34.208: INFO: Got root ca configmap in namespace "svcaccounts-1626"
  Apr 15 09:28:34.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1626" for this suite. @ 04/15/24 09:28:34.222
• [2.992 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 04/15/24 09:28:34.247
  Apr 15 09:28:34.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename security-context-test @ 04/15/24 09:28:34.249
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:28:34.284
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:28:34.29
  Apr 15 09:28:36.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5536" for this suite. @ 04/15/24 09:28:36.371
• [2.150 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 04/15/24 09:28:36.405
  Apr 15 09:28:36.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:28:36.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:28:36.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:28:36.453
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 09:28:36.46
  STEP: Saw pod success @ 04/15/24 09:28:40.503
  Apr 15 09:28:40.510: INFO: Trying to get logs from node ahz3daisheng-3 pod downwardapi-volume-2fea4646-5016-4688-9d8c-109ab92501a1 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 09:28:40.526
  Apr 15 09:28:40.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9068" for this suite. @ 04/15/24 09:28:40.567
• [4.172 seconds]
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 04/15/24 09:28:40.579
  Apr 15 09:28:40.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename crd-webhook @ 04/15/24 09:28:40.581
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:28:40.62
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:28:40.625
  STEP: Setting up server cert @ 04/15/24 09:28:40.63
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 04/15/24 09:28:41.407
  STEP: Deploying the custom resource conversion webhook pod @ 04/15/24 09:28:41.428
  STEP: Wait for the deployment to be ready @ 04/15/24 09:28:41.454
  Apr 15 09:28:41.469: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  Apr 15 09:28:43.489: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-764f67665f\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 09:28:45.503: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-764f67665f\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 09:28:47.499: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-764f67665f\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 09:28:49.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-764f67665f\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 09:28:51.499: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 28, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-764f67665f\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 04/15/24 09:28:53.498
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 09:28:53.524
  Apr 15 09:28:54.526: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Apr 15 09:28:54.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Creating a v1 custom resource @ 04/15/24 09:28:57.435
  STEP: v2 custom resource should be converted @ 04/15/24 09:28:57.451
  Apr 15 09:28:57.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-1890" for this suite. @ 04/15/24 09:28:58.162
• [17.612 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 04/15/24 09:28:58.192
  Apr 15 09:28:58.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/15/24 09:28:58.194
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:28:58.229
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:28:58.234
  STEP: create the container to handle the HTTPGet hook request. @ 04/15/24 09:28:58.252
  STEP: create the pod with lifecycle hook @ 04/15/24 09:29:02.321
  STEP: check poststart hook @ 04/15/24 09:29:16.434
  STEP: delete the pod with lifecycle hook @ 04/15/24 09:29:16.491
  Apr 15 09:29:18.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1802" for this suite. @ 04/15/24 09:29:18.552
• [20.378 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 04/15/24 09:29:18.573
  Apr 15 09:29:18.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:29:18.576
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:29:18.616
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:29:18.623
  STEP: Creating configMap with name projected-configmap-test-volume-91f806fb-d778-4057-a2df-be791f82eb01 @ 04/15/24 09:29:18.63
  STEP: Creating a pod to test consume configMaps @ 04/15/24 09:29:18.642
  STEP: Saw pod success @ 04/15/24 09:29:22.749
  Apr 15 09:29:22.759: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-projected-configmaps-c6b5f5a0-7fb5-4c77-bd6f-b513ce2e08e7 container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 09:29:22.779
  Apr 15 09:29:22.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1392" for this suite. @ 04/15/24 09:29:22.835
• [4.280 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:289
  STEP: Creating a kubernetes client @ 04/15/24 09:29:22.86
  Apr 15 09:29:22.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename field-validation @ 04/15/24 09:29:22.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:29:22.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:29:22.976
  Apr 15 09:29:22.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:29:26.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7323" for this suite. @ 04/15/24 09:29:26.415
• [3.568 seconds]
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:318
  STEP: Creating a kubernetes client @ 04/15/24 09:29:26.432
  Apr 15 09:29:26.432: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename statefulset @ 04/15/24 09:29:26.435
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:29:26.473
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:29:26.481
  STEP: Creating service test in namespace statefulset-6508 @ 04/15/24 09:29:26.489
  STEP: Creating a new StatefulSet @ 04/15/24 09:29:26.509
  Apr 15 09:29:26.537: INFO: Found 0 stateful pods, waiting for 3
  Apr 15 09:29:36.552: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 09:29:36.552: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 09:29:36.553: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 09:29:36.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-6508 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 09:29:36.910: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 09:29:36.910: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 09:29:36.910: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 04/15/24 09:29:46.947
  Apr 15 09:29:46.978: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 04/15/24 09:29:46.979
  STEP: Updating Pods in reverse ordinal order @ 04/15/24 09:29:57.017
  Apr 15 09:29:57.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-6508 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 15 09:29:57.309: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 15 09:29:57.309: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 15 09:29:57.309: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  STEP: Rolling back to a previous revision @ 04/15/24 09:30:07.365
  Apr 15 09:30:07.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-6508 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Apr 15 09:30:07.629: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Apr 15 09:30:07.629: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Apr 15 09:30:07.629: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Apr 15 09:30:17.698: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 04/15/24 09:30:27.741
  Apr 15 09:30:27.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=statefulset-6508 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Apr 15 09:30:28.064: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Apr 15 09:30:28.064: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Apr 15 09:30:28.064: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Apr 15 09:30:38.138: INFO: Deleting all statefulset in ns statefulset-6508
  Apr 15 09:30:38.148: INFO: Scaling statefulset ss2 to 0
  Apr 15 09:30:48.199: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 09:30:48.209: INFO: Deleting statefulset ss2
  Apr 15 09:30:48.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6508" for this suite. @ 04/15/24 09:30:48.257
• [81.847 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:959
  STEP: Creating a kubernetes client @ 04/15/24 09:30:48.284
  Apr 15 09:30:48.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename statefulset @ 04/15/24 09:30:48.287
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:30:48.34
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:30:48.347
  STEP: Creating service test in namespace statefulset-3824 @ 04/15/24 09:30:48.358
  Apr 15 09:30:48.405: INFO: Found 0 stateful pods, waiting for 1
  Apr 15 09:30:58.422: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 04/15/24 09:30:58.443
  W0415 09:30:58.488184      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Apr 15 09:30:58.512: INFO: Found 1 stateful pods, waiting for 2
  Apr 15 09:31:08.529: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Apr 15 09:31:08.529: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 04/15/24 09:31:08.546
  STEP: Delete all of the StatefulSets @ 04/15/24 09:31:08.554
  STEP: Verify that StatefulSets have been deleted @ 04/15/24 09:31:08.574
  Apr 15 09:31:08.592: INFO: Deleting all statefulset in ns statefulset-3824
  Apr 15 09:31:08.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3824" for this suite. @ 04/15/24 09:31:08.649
• [20.438 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 04/15/24 09:31:08.723
  Apr 15 09:31:08.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename deployment @ 04/15/24 09:31:08.725
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:31:08.812
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:31:08.819
  Apr 15 09:31:08.855: INFO: Pod name rollover-pod: Found 0 pods out of 1
  Apr 15 09:31:13.863: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/15/24 09:31:13.863
  Apr 15 09:31:13.863: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  Apr 15 09:31:15.873: INFO: Creating deployment "test-rollover-deployment"
  Apr 15 09:31:15.901: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  Apr 15 09:31:17.921: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Apr 15 09:31:17.936: INFO: Ensure that both replica sets have 1 created replica
  Apr 15 09:31:17.956: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Apr 15 09:31:17.979: INFO: Updating deployment test-rollover-deployment
  Apr 15 09:31:17.979: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  Apr 15 09:31:19.997: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Apr 15 09:31:20.026: INFO: Make sure deployment "test-rollover-deployment" is complete
  Apr 15 09:31:20.040: INFO: all replica sets need to contain the pod-template-hash label
  Apr 15 09:31:20.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 9, 31, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 31, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 9, 31, 19, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 31, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65cd7886c5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 09:31:22.056: INFO: all replica sets need to contain the pod-template-hash label
  Apr 15 09:31:22.057: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 9, 31, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 31, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 9, 31, 19, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 31, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65cd7886c5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 09:31:24.061: INFO: all replica sets need to contain the pod-template-hash label
  Apr 15 09:31:24.062: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 9, 31, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 31, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 9, 31, 19, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 31, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65cd7886c5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 09:31:26.058: INFO: all replica sets need to contain the pod-template-hash label
  Apr 15 09:31:26.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 9, 31, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 31, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 9, 31, 19, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 31, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65cd7886c5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 09:31:28.056: INFO: all replica sets need to contain the pod-template-hash label
  Apr 15 09:31:28.056: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 9, 31, 15, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 31, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 9, 31, 19, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 31, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-65cd7886c5\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Apr 15 09:31:30.063: INFO: 
  Apr 15 09:31:30.063: INFO: Ensure that both old replica sets have no replicas
  Apr 15 09:31:30.086: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-9683  1351edde-71c9-458a-8772-aacf02a91262 206102 2 2024-04-15 09:31:15 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-04-15 09:31:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 09:31:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.47 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00343cfd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-04-15 09:31:15 +0000 UTC,LastTransitionTime:2024-04-15 09:31:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-65cd7886c5" has successfully progressed.,LastUpdateTime:2024-04-15 09:31:29 +0000 UTC,LastTransitionTime:2024-04-15 09:31:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 15 09:31:30.096: INFO: New ReplicaSet "test-rollover-deployment-65cd7886c5" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-65cd7886c5  deployment-9683  172df64a-b9ee-483d-bb4e-4ca9b7db4f56 206092 2 2024-04-15 09:31:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:65cd7886c5] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 1351edde-71c9-458a-8772-aacf02a91262 0xc00343d597 0xc00343d598}] [] [{kube-controller-manager Update apps/v1 2024-04-15 09:31:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1351edde-71c9-458a-8772-aacf02a91262\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 09:31:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 65cd7886c5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:65cd7886c5] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.47 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00343d648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 09:31:30.097: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Apr 15 09:31:30.097: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9683  e537aa7f-7000-4e0b-a196-2075d7694507 206101 2 2024-04-15 09:31:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 1351edde-71c9-458a-8772-aacf02a91262 0xc00343d337 0xc00343d338}] [] [{e2e.test Update apps/v1 2024-04-15 09:31:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 09:31:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1351edde-71c9-458a-8772-aacf02a91262\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2024-04-15 09:31:29 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00343d408 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 09:31:30.097: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-9683  6e82a296-7545-4175-8012-bcccd1e36a2a 206057 2 2024-04-15 09:31:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 1351edde-71c9-458a-8772-aacf02a91262 0xc00343d477 0xc00343d478}] [] [{kube-controller-manager Update apps/v1 2024-04-15 09:31:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1351edde-71c9-458a-8772-aacf02a91262\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 09:31:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00343d528 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 09:31:30.105: INFO: Pod "test-rollover-deployment-65cd7886c5-xtpqn" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-65cd7886c5-xtpqn test-rollover-deployment-65cd7886c5- deployment-9683  cd7d41d5-c0e6-4853-9df5-1fc3d96ffc0f 206072 0 2024-04-15 09:31:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:65cd7886c5] map[] [{apps/v1 ReplicaSet test-rollover-deployment-65cd7886c5 172df64a-b9ee-483d-bb4e-4ca9b7db4f56 0xc005911c57 0xc005911c58}] [] [{kube-controller-manager Update v1 2024-04-15 09:31:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"172df64a-b9ee-483d-bb4e-4ca9b7db4f56\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 09:31:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-twz9q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.47,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-twz9q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 09:31:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 09:31:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 09:31:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 09:31:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.199,PodIP:10.233.66.219,StartTime:2024-04-15 09:31:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 09:31:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.47,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:c9997bf8d2e223d7d2a0078dcfb11a653e9b16cf09418829ec03e1d57ca9628a,ContainerID:cri-o://8dab165124166ec411b3a936aaba870db501152b681b3c24b6be0f2689f78165,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.219,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 09:31:30.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9683" for this suite. @ 04/15/24 09:31:30.118
• [21.412 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 04/15/24 09:31:30.138
  Apr 15 09:31:30.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 09:31:30.142
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:31:30.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:31:30.185
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 04/15/24 09:31:30.19
  STEP: Saw pod success @ 04/15/24 09:31:34.25
  Apr 15 09:31:34.262: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-ee57da12-a582-4509-a7eb-137f781a722d container test-container: <nil>
  STEP: delete the pod @ 04/15/24 09:31:34.308
  Apr 15 09:31:34.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7231" for this suite. @ 04/15/24 09:31:34.36
• [4.240 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 04/15/24 09:31:34.407
  Apr 15 09:31:34.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 09:31:34.411
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:31:34.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:31:34.459
  STEP: creating the pod @ 04/15/24 09:31:34.467
  Apr 15 09:31:34.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-8983 create -f -'
  Apr 15 09:31:35.201: INFO: stderr: ""
  Apr 15 09:31:35.201: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 04/15/24 09:31:37.217
  Apr 15 09:31:37.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-8983 label pods pause testing-label=testing-label-value'
  Apr 15 09:31:37.401: INFO: stderr: ""
  Apr 15 09:31:37.401: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 04/15/24 09:31:37.401
  Apr 15 09:31:37.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-8983 get pod pause -L testing-label'
  Apr 15 09:31:37.564: INFO: stderr: ""
  Apr 15 09:31:37.564: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 04/15/24 09:31:37.564
  Apr 15 09:31:37.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-8983 label pods pause testing-label-'
  Apr 15 09:31:37.770: INFO: stderr: ""
  Apr 15 09:31:37.770: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 04/15/24 09:31:37.77
  Apr 15 09:31:37.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-8983 get pod pause -L testing-label'
  Apr 15 09:31:37.929: INFO: stderr: ""
  Apr 15 09:31:37.929: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
  STEP: using delete to clean up resources @ 04/15/24 09:31:37.929
  Apr 15 09:31:37.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-8983 delete --grace-period=0 --force -f -'
  Apr 15 09:31:38.095: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Apr 15 09:31:38.095: INFO: stdout: "pod \"pause\" force deleted\n"
  Apr 15 09:31:38.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-8983 get rc,svc -l name=pause --no-headers'
  Apr 15 09:31:38.265: INFO: stderr: "No resources found in kubectl-8983 namespace.\n"
  Apr 15 09:31:38.265: INFO: stdout: ""
  Apr 15 09:31:38.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-8983 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Apr 15 09:31:38.419: INFO: stderr: ""
  Apr 15 09:31:38.419: INFO: stdout: ""
  Apr 15 09:31:38.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8983" for this suite. @ 04/15/24 09:31:38.431
• [4.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 04/15/24 09:31:38.45
  Apr 15 09:31:38.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename gc @ 04/15/24 09:31:38.452
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:31:38.5
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:31:38.509
  STEP: create the rc @ 04/15/24 09:31:38.525
  W0415 09:31:38.537160      14 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 04/15/24 09:31:44.554
  STEP: wait for the rc to be deleted @ 04/15/24 09:31:44.604
  Apr 15 09:31:46.390: INFO: 83 pods remaining
  Apr 15 09:31:46.390: INFO: 80 pods has nil DeletionTimestamp
  Apr 15 09:31:46.390: INFO: 
  Apr 15 09:31:46.822: INFO: 71 pods remaining
  Apr 15 09:31:46.822: INFO: 68 pods has nil DeletionTimestamp
  Apr 15 09:31:46.822: INFO: 
  Apr 15 09:31:47.701: INFO: 59 pods remaining
  Apr 15 09:31:47.701: INFO: 59 pods has nil DeletionTimestamp
  Apr 15 09:31:47.702: INFO: 
  Apr 15 09:31:49.094: INFO: 42 pods remaining
  Apr 15 09:31:49.094: INFO: 39 pods has nil DeletionTimestamp
  Apr 15 09:31:49.095: INFO: 
  Apr 15 09:31:49.822: INFO: 30 pods remaining
  Apr 15 09:31:49.822: INFO: 28 pods has nil DeletionTimestamp
  Apr 15 09:31:49.823: INFO: 
  Apr 15 09:31:50.849: INFO: 19 pods remaining
  Apr 15 09:31:50.849: INFO: 18 pods has nil DeletionTimestamp
  Apr 15 09:31:50.850: INFO: 
  Apr 15 09:31:51.686: INFO: 1 pods remaining
  Apr 15 09:31:51.686: INFO: 0 pods has nil DeletionTimestamp
  Apr 15 09:31:51.686: INFO: 
  STEP: Gathering metrics @ 04/15/24 09:31:52.619
  Apr 15 09:31:53.584: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Apr 15 09:31:53.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3629" for this suite. @ 04/15/24 09:31:53.603
• [15.177 seconds]
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 04/15/24 09:31:53.628
  Apr 15 09:31:53.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename svcaccounts @ 04/15/24 09:31:53.645
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:31:53.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:31:53.713
  Apr 15 09:31:53.807: INFO: created pod pod-service-account-defaultsa
  Apr 15 09:31:53.807: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Apr 15 09:31:53.823: INFO: created pod pod-service-account-mountsa
  Apr 15 09:31:53.823: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Apr 15 09:31:53.865: INFO: created pod pod-service-account-nomountsa
  Apr 15 09:31:53.865: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Apr 15 09:31:53.931: INFO: created pod pod-service-account-defaultsa-mountspec
  Apr 15 09:31:53.931: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Apr 15 09:31:53.956: INFO: created pod pod-service-account-mountsa-mountspec
  Apr 15 09:31:53.956: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Apr 15 09:31:54.000: INFO: created pod pod-service-account-nomountsa-mountspec
  Apr 15 09:31:54.000: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Apr 15 09:31:54.040: INFO: created pod pod-service-account-defaultsa-nomountspec
  Apr 15 09:31:54.040: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Apr 15 09:31:54.089: INFO: created pod pod-service-account-mountsa-nomountspec
  Apr 15 09:31:54.089: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Apr 15 09:31:54.171: INFO: created pod pod-service-account-nomountsa-nomountspec
  Apr 15 09:31:54.171: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Apr 15 09:31:54.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3349" for this suite. @ 04/15/24 09:31:54.211
• [0.693 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 04/15/24 09:31:54.327
  Apr 15 09:31:54.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename ingress @ 04/15/24 09:31:54.329
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:31:54.4
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:31:54.405
  STEP: getting /apis @ 04/15/24 09:31:54.414
  STEP: getting /apis/networking.k8s.io @ 04/15/24 09:31:54.426
  STEP: getting /apis/networking.k8s.iov1 @ 04/15/24 09:31:54.429
  STEP: creating @ 04/15/24 09:31:54.433
  STEP: getting @ 04/15/24 09:31:54.493
  STEP: listing @ 04/15/24 09:31:54.507
  STEP: watching @ 04/15/24 09:31:54.513
  Apr 15 09:31:54.513: INFO: starting watch
  STEP: cluster-wide listing @ 04/15/24 09:31:54.516
  STEP: cluster-wide watching @ 04/15/24 09:31:54.521
  Apr 15 09:31:54.521: INFO: starting watch
  STEP: patching @ 04/15/24 09:31:54.523
  STEP: updating @ 04/15/24 09:31:54.542
  Apr 15 09:31:54.573: INFO: waiting for watch events with expected annotations
  Apr 15 09:31:54.574: INFO: saw patched and updated annotations
  STEP: patching /status @ 04/15/24 09:31:54.574
  STEP: updating /status @ 04/15/24 09:31:54.592
  STEP: get /status @ 04/15/24 09:31:54.613
  STEP: deleting @ 04/15/24 09:31:54.621
  STEP: deleting a collection @ 04/15/24 09:31:54.653
  Apr 15 09:31:54.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-1845" for this suite. @ 04/15/24 09:31:54.701
• [0.406 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 04/15/24 09:31:54.735
  Apr 15 09:31:54.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename runtimeclass @ 04/15/24 09:31:54.738
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:31:54.792
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:31:54.797
  STEP: getting /apis @ 04/15/24 09:31:54.801
  STEP: getting /apis/node.k8s.io @ 04/15/24 09:31:54.808
  STEP: getting /apis/node.k8s.io/v1 @ 04/15/24 09:31:54.81
  STEP: creating @ 04/15/24 09:31:54.812
  STEP: watching @ 04/15/24 09:31:54.868
  Apr 15 09:31:54.868: INFO: starting watch
  STEP: getting @ 04/15/24 09:31:54.976
  STEP: listing @ 04/15/24 09:31:55.018
  STEP: patching @ 04/15/24 09:31:55.037
  STEP: updating @ 04/15/24 09:31:55.049
  Apr 15 09:31:55.083: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 04/15/24 09:31:55.084
  STEP: deleting a collection @ 04/15/24 09:31:55.193
  Apr 15 09:31:55.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-4341" for this suite. @ 04/15/24 09:31:55.323
• [0.600 seconds]
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 04/15/24 09:31:55.337
  Apr 15 09:31:55.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 04/15/24 09:31:55.34
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:31:55.388
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:31:55.392
  STEP: create the container to handle the HTTPGet hook request. @ 04/15/24 09:31:55.41
  STEP: create the pod with lifecycle hook @ 04/15/24 09:32:13.562
  STEP: delete the pod with lifecycle hook @ 04/15/24 09:32:15.593
  STEP: check prestop hook @ 04/15/24 09:32:17.632
  Apr 15 09:32:17.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-296" for this suite. @ 04/15/24 09:32:17.676
• [22.360 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 04/15/24 09:32:17.704
  Apr 15 09:32:17.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 09:32:17.706
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:32:17.744
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:32:17.751
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 04/15/24 09:32:17.756
  STEP: Saw pod success @ 04/15/24 09:32:21.817
  Apr 15 09:32:21.826: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-0a5d900e-0fce-47cb-b505-3ce0c7dd0d92 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 09:32:21.845
  Apr 15 09:32:21.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1820" for this suite. @ 04/15/24 09:32:21.942
• [4.256 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 04/15/24 09:32:21.964
  Apr 15 09:32:21.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename pods @ 04/15/24 09:32:21.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:32:22.006
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:32:22.013
  STEP: creating the pod @ 04/15/24 09:32:22.026
  STEP: setting up watch @ 04/15/24 09:32:22.026
  STEP: submitting the pod to kubernetes @ 04/15/24 09:32:22.141
  STEP: verifying the pod is in kubernetes @ 04/15/24 09:32:22.171
  STEP: verifying pod creation was observed @ 04/15/24 09:32:22.181
  STEP: deleting the pod gracefully @ 04/15/24 09:32:24.214
  STEP: verifying pod deletion was observed @ 04/15/24 09:32:24.226
  Apr 15 09:32:26.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5226" for this suite. @ 04/15/24 09:32:26.484
• [4.536 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 04/15/24 09:32:26.508
  Apr 15 09:32:26.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/15/24 09:32:26.511
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:32:26.545
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:32:26.551
  STEP: fetching the /apis discovery document @ 04/15/24 09:32:26.558
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 04/15/24 09:32:26.562
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 04/15/24 09:32:26.562
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 04/15/24 09:32:26.563
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 04/15/24 09:32:26.565
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 04/15/24 09:32:26.565
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 04/15/24 09:32:26.567
  Apr 15 09:32:26.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2801" for this suite. @ 04/15/24 09:32:26.587
• [0.096 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 04/15/24 09:32:26.612
  Apr 15 09:32:26.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename configmap @ 04/15/24 09:32:26.615
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:32:26.651
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:32:26.656
  STEP: Creating configMap configmap-758/configmap-test-ac9ccf14-a0df-401c-bf79-5ff9583140ce @ 04/15/24 09:32:26.661
  STEP: Creating a pod to test consume configMaps @ 04/15/24 09:32:26.671
  STEP: Saw pod success @ 04/15/24 09:32:28.71
  Apr 15 09:32:28.717: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-configmaps-92169bad-8571-4a31-b93d-1e48401c94b1 container env-test: <nil>
  STEP: delete the pod @ 04/15/24 09:32:28.753
  Apr 15 09:32:28.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-758" for this suite. @ 04/15/24 09:32:28.797
• [2.208 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 04/15/24 09:32:28.825
  Apr 15 09:32:28.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename downward-api @ 04/15/24 09:32:28.83
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:32:28.883
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:32:28.889
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 09:32:28.896
  STEP: Saw pod success @ 04/15/24 09:32:32.956
  Apr 15 09:32:32.962: INFO: Trying to get logs from node ahz3daisheng-3 pod downwardapi-volume-211bb744-2720-45dd-8d9e-d91004f21df8 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 09:32:32.977
  Apr 15 09:32:33.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-318" for this suite. @ 04/15/24 09:32:33.042
• [4.236 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 04/15/24 09:32:33.062
  Apr 15 09:32:33.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename runtimeclass @ 04/15/24 09:32:33.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:32:33.132
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:32:33.139
  Apr 15 09:32:33.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6758" for this suite. @ 04/15/24 09:32:33.18
• [0.150 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 04/15/24 09:32:33.212
  Apr 15 09:32:33.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename secrets @ 04/15/24 09:32:33.215
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:32:33.262
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:32:33.268
  STEP: Creating secret with name secret-test-e6ab1517-1370-47c6-b53e-76a40293ce18 @ 04/15/24 09:32:33.273
  STEP: Creating a pod to test consume secrets @ 04/15/24 09:32:33.284
  STEP: Saw pod success @ 04/15/24 09:32:37.339
  Apr 15 09:32:37.347: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-secrets-ee97be3b-cf80-4733-823d-5a9315e4292c container secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 09:32:37.368
  Apr 15 09:32:37.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9090" for this suite. @ 04/15/24 09:32:37.407
• [4.208 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 04/15/24 09:32:37.426
  Apr 15 09:32:37.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename namespaces @ 04/15/24 09:32:37.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:32:37.474
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:32:37.48
  STEP: Creating namespace "e2e-ns-7sfnd" @ 04/15/24 09:32:37.488
  Apr 15 09:32:37.524: INFO: Namespace "e2e-ns-7sfnd-945" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-7sfnd-945" @ 04/15/24 09:32:37.525
  Apr 15 09:32:37.545: INFO: Namespace "e2e-ns-7sfnd-945" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-7sfnd-945" @ 04/15/24 09:32:37.545
  Apr 15 09:32:37.561: INFO: Namespace "e2e-ns-7sfnd-945" has []v1.FinalizerName{"kubernetes"}
  Apr 15 09:32:37.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9027" for this suite. @ 04/15/24 09:32:37.571
  STEP: Destroying namespace "e2e-ns-7sfnd-945" for this suite. @ 04/15/24 09:32:37.587
• [0.178 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 04/15/24 09:32:37.608
  Apr 15 09:32:37.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename job @ 04/15/24 09:32:37.61
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:32:37.65
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:32:37.657
  STEP: Creating a job @ 04/15/24 09:32:37.663
  STEP: Ensuring active pods == parallelism @ 04/15/24 09:32:37.674
  STEP: delete a job @ 04/15/24 09:32:39.684
  STEP: deleting Job.batch foo in namespace job-3478, will wait for the garbage collector to delete the pods @ 04/15/24 09:32:39.684
  Apr 15 09:32:39.759: INFO: Deleting Job.batch foo took: 17.665902ms
  Apr 15 09:32:39.861: INFO: Terminating Job.batch foo pods took: 102.174476ms
  STEP: Ensuring job was deleted @ 04/15/24 09:33:13.061
  Apr 15 09:33:13.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3478" for this suite. @ 04/15/24 09:33:13.086
• [35.493 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 04/15/24 09:33:13.115
  Apr 15 09:33:13.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 09:33:13.118
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:33:13.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:33:13.172
  STEP: Creating pod liveness-0a284c2d-1add-4f4f-a8b6-1859ded3a76a in namespace container-probe-409 @ 04/15/24 09:33:13.178
  Apr 15 09:33:15.221: INFO: Started pod liveness-0a284c2d-1add-4f4f-a8b6-1859ded3a76a in namespace container-probe-409
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/15/24 09:33:15.221
  Apr 15 09:33:15.230: INFO: Initial restart count of pod liveness-0a284c2d-1add-4f4f-a8b6-1859ded3a76a is 0
  Apr 15 09:37:16.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 09:37:16.641
  STEP: Destroying namespace "container-probe-409" for this suite. @ 04/15/24 09:37:16.7
• [243.627 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 04/15/24 09:37:16.743
  Apr 15 09:37:16.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 09:37:16.756
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:37:16.824
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:37:16.832
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 04/15/24 09:37:16.839
  STEP: Saw pod success @ 04/15/24 09:37:20.921
  Apr 15 09:37:20.932: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-6208c565-d9f0-41f6-9617-2acb56cf0923 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 09:37:20.977
  Apr 15 09:37:21.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4984" for this suite. @ 04/15/24 09:37:21.021
• [4.300 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 04/15/24 09:37:21.056
  Apr 15 09:37:21.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubelet-test @ 04/15/24 09:37:21.059
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:37:21.108
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:37:21.116
  Apr 15 09:37:23.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4536" for this suite. @ 04/15/24 09:37:23.213
• [2.173 seconds]
------------------------------
S
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 04/15/24 09:37:23.229
  Apr 15 09:37:23.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename dns @ 04/15/24 09:37:23.232
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:37:23.275
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:37:23.283
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 04/15/24 09:37:23.288
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 04/15/24 09:37:23.288
  STEP: creating a pod to probe DNS @ 04/15/24 09:37:23.288
  STEP: submitting the pod to kubernetes @ 04/15/24 09:37:23.288
  STEP: retrieving the pod @ 04/15/24 09:37:25.34
  STEP: looking for the results for each expected name from probers @ 04/15/24 09:37:25.346
  Apr 15 09:37:25.386: INFO: DNS probes using dns-7890/dns-test-07ca2aa8-b787-49b4-b721-f568f8bff30a succeeded

  Apr 15 09:37:25.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 09:37:25.397
  STEP: Destroying namespace "dns-7890" for this suite. @ 04/15/24 09:37:25.424
• [2.210 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 04/15/24 09:37:25.44
  Apr 15 09:37:25.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename services @ 04/15/24 09:37:25.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:37:25.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:37:25.499
  STEP: creating service in namespace services-3718 @ 04/15/24 09:37:25.516
  STEP: creating service affinity-nodeport in namespace services-3718 @ 04/15/24 09:37:25.517
  STEP: creating replication controller affinity-nodeport in namespace services-3718 @ 04/15/24 09:37:25.561
  I0415 09:37:25.608569      14 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-3718, replica count: 3
  I0415 09:37:28.660110      14 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 15 09:37:28.743: INFO: Creating new exec pod
  Apr 15 09:37:31.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-3718 exec execpod-affinitygwc2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Apr 15 09:37:32.163: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Apr 15 09:37:32.163: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 09:37:32.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-3718 exec execpod-affinitygwc2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.51.182 80'
  Apr 15 09:37:32.479: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.51.182 80\nConnection to 10.233.51.182 80 port [tcp/http] succeeded!\n"
  Apr 15 09:37:32.479: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 09:37:32.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-3718 exec execpod-affinitygwc2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.120 31893'
  Apr 15 09:37:32.775: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.120 31893\nConnection to 192.168.121.120 31893 port [tcp/*] succeeded!\n"
  Apr 15 09:37:32.775: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 09:37:32.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-3718 exec execpod-affinitygwc2s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.199 31893'
  Apr 15 09:37:33.045: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.199 31893\nConnection to 192.168.121.199 31893 port [tcp/*] succeeded!\n"
  Apr 15 09:37:33.045: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Apr 15 09:37:33.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-3718 exec execpod-affinitygwc2s -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.96:31893/ ; done'
  Apr 15 09:37:33.585: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:31893/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:31893/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:31893/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:31893/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:31893/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:31893/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:31893/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:31893/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:31893/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:31893/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:31893/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:31893/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:31893/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:31893/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:31893/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.96:31893/\n"
  Apr 15 09:37:33.585: INFO: stdout: "\naffinity-nodeport-4km8l\naffinity-nodeport-4km8l\naffinity-nodeport-4km8l\naffinity-nodeport-4km8l\naffinity-nodeport-4km8l\naffinity-nodeport-4km8l\naffinity-nodeport-4km8l\naffinity-nodeport-4km8l\naffinity-nodeport-4km8l\naffinity-nodeport-4km8l\naffinity-nodeport-4km8l\naffinity-nodeport-4km8l\naffinity-nodeport-4km8l\naffinity-nodeport-4km8l\naffinity-nodeport-4km8l\naffinity-nodeport-4km8l"
  Apr 15 09:37:33.585: INFO: Received response from host: affinity-nodeport-4km8l
  Apr 15 09:37:33.585: INFO: Received response from host: affinity-nodeport-4km8l
  Apr 15 09:37:33.585: INFO: Received response from host: affinity-nodeport-4km8l
  Apr 15 09:37:33.585: INFO: Received response from host: affinity-nodeport-4km8l
  Apr 15 09:37:33.585: INFO: Received response from host: affinity-nodeport-4km8l
  Apr 15 09:37:33.585: INFO: Received response from host: affinity-nodeport-4km8l
  Apr 15 09:37:33.585: INFO: Received response from host: affinity-nodeport-4km8l
  Apr 15 09:37:33.585: INFO: Received response from host: affinity-nodeport-4km8l
  Apr 15 09:37:33.585: INFO: Received response from host: affinity-nodeport-4km8l
  Apr 15 09:37:33.585: INFO: Received response from host: affinity-nodeport-4km8l
  Apr 15 09:37:33.585: INFO: Received response from host: affinity-nodeport-4km8l
  Apr 15 09:37:33.585: INFO: Received response from host: affinity-nodeport-4km8l
  Apr 15 09:37:33.585: INFO: Received response from host: affinity-nodeport-4km8l
  Apr 15 09:37:33.585: INFO: Received response from host: affinity-nodeport-4km8l
  Apr 15 09:37:33.585: INFO: Received response from host: affinity-nodeport-4km8l
  Apr 15 09:37:33.585: INFO: Received response from host: affinity-nodeport-4km8l
  Apr 15 09:37:33.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 09:37:33.609: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-3718, will wait for the garbage collector to delete the pods @ 04/15/24 09:37:33.645
  Apr 15 09:37:33.721: INFO: Deleting ReplicationController affinity-nodeport took: 18.138903ms
  Apr 15 09:37:33.822: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.985363ms
  STEP: Destroying namespace "services-3718" for this suite. @ 04/15/24 09:37:36.188
• [10.765 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 04/15/24 09:37:36.212
  Apr 15 09:37:36.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 09:37:36.215
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:37:36.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:37:36.265
  STEP: Starting the proxy @ 04/15/24 09:37:36.272
  Apr 15 09:37:36.274: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-1139 proxy --unix-socket=/tmp/kubectl-proxy-unix2187183786/test'
  STEP: retrieving proxy /api/ output @ 04/15/24 09:37:36.393
  Apr 15 09:37:36.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1139" for this suite. @ 04/15/24 09:37:36.408
• [0.213 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 04/15/24 09:37:36.43
  Apr 15 09:37:36.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename secrets @ 04/15/24 09:37:36.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:37:36.474
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:37:36.482
  STEP: creating secret secrets-8991/secret-test-5c7ea7fb-f657-416f-bfbc-b42aa6f53e4e @ 04/15/24 09:37:36.49
  STEP: Creating a pod to test consume secrets @ 04/15/24 09:37:36.502
  STEP: Saw pod success @ 04/15/24 09:37:40.565
  Apr 15 09:37:40.574: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-configmaps-4c2b3dee-26c1-445e-bb56-1de6f4951040 container env-test: <nil>
  STEP: delete the pod @ 04/15/24 09:37:40.6
  Apr 15 09:37:40.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8991" for this suite. @ 04/15/24 09:37:40.677
• [4.293 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 04/15/24 09:37:40.734
  Apr 15 09:37:40.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename deployment @ 04/15/24 09:37:40.738
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:37:40.786
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:37:40.793
  Apr 15 09:37:40.799: INFO: Creating deployment "test-recreate-deployment"
  Apr 15 09:37:40.814: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Apr 15 09:37:40.838: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
  Apr 15 09:37:42.889: INFO: Waiting deployment "test-recreate-deployment" to complete
  Apr 15 09:37:42.905: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Apr 15 09:37:42.931: INFO: Updating deployment test-recreate-deployment
  Apr 15 09:37:42.932: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Apr 15 09:37:43.187: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-7500  16759a3d-2d41-44a9-95da-1e85034dda29 208826 2 2024-04-15 09:37:40 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-04-15 09:37:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 09:37:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003564448 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-04-15 09:37:43 +0000 UTC,LastTransitionTime:2024-04-15 09:37:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2024-04-15 09:37:43 +0000 UTC,LastTransitionTime:2024-04-15 09:37:40 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Apr 15 09:37:43.198: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-7500  27285925-fd23-4dc5-b253-8d4b1203a928 208825 1 2024-04-15 09:37:43 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 16759a3d-2d41-44a9-95da-1e85034dda29 0xc0044a8ad7 0xc0044a8ad8}] [] [{kube-controller-manager Update apps/v1 2024-04-15 09:37:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16759a3d-2d41-44a9-95da-1e85034dda29\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 09:37:43 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044a8b78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 09:37:43.198: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Apr 15 09:37:43.198: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6fcf456ccb  deployment-7500  e373ec7f-67b6-4bcd-8f4a-8395fda7a358 208815 2 2024-04-15 09:37:40 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6fcf456ccb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 16759a3d-2d41-44a9-95da-1e85034dda29 0xc0044a8be7 0xc0044a8be8}] [] [{kube-controller-manager Update apps/v1 2024-04-15 09:37:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"16759a3d-2d41-44a9-95da-1e85034dda29\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 09:37:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6fcf456ccb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6fcf456ccb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.47 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044a8c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 09:37:43.210: INFO: Pod "test-recreate-deployment-54757ffd6c-9d6jx" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-9d6jx test-recreate-deployment-54757ffd6c- deployment-7500  df09446b-ac8f-4dc5-aac6-aa17f9deb82d 208824 0 2024-04-15 09:37:43 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 27285925-fd23-4dc5-b253-8d4b1203a928 0xc0044a9107 0xc0044a9108}] [] [{kube-controller-manager Update v1 2024-04-15 09:37:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27285925-fd23-4dc5-b253-8d4b1203a928\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 09:37:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j96lr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j96lr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 09:37:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 09:37:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 09:37:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 09:37:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.199,PodIP:,StartTime:2024-04-15 09:37:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 09:37:43.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7500" for this suite. @ 04/15/24 09:37:43.228
• [2.513 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 04/15/24 09:37:43.249
  Apr 15 09:37:43.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:37:43.253
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:37:43.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:37:43.299
  STEP: Creating configMap with name configmap-projected-all-test-volume-dbb57d91-a03a-43af-ad77-971518eb5876 @ 04/15/24 09:37:43.306
  STEP: Creating secret with name secret-projected-all-test-volume-825e123b-5060-4f6e-90e5-b7411dfc50e3 @ 04/15/24 09:37:43.318
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 04/15/24 09:37:43.33
  STEP: Saw pod success @ 04/15/24 09:37:47.403
  Apr 15 09:37:47.413: INFO: Trying to get logs from node ahz3daisheng-3 pod projected-volume-cce47c49-5a41-47c2-8ad6-f5984e090373 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 09:37:47.428
  Apr 15 09:37:47.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2245" for this suite. @ 04/15/24 09:37:47.476
• [4.243 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 04/15/24 09:37:47.498
  Apr 15 09:37:47.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename subpath @ 04/15/24 09:37:47.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:37:47.548
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:37:47.556
  STEP: Setting up data @ 04/15/24 09:37:47.564
  STEP: Creating pod pod-subpath-test-downwardapi-v6wp @ 04/15/24 09:37:47.589
  STEP: Creating a pod to test atomic-volume-subpath @ 04/15/24 09:37:47.589
  STEP: Saw pod success @ 04/15/24 09:38:11.753
  Apr 15 09:38:11.763: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-subpath-test-downwardapi-v6wp container test-container-subpath-downwardapi-v6wp: <nil>
  STEP: delete the pod @ 04/15/24 09:38:11.782
  STEP: Deleting pod pod-subpath-test-downwardapi-v6wp @ 04/15/24 09:38:11.823
  Apr 15 09:38:11.823: INFO: Deleting pod "pod-subpath-test-downwardapi-v6wp" in namespace "subpath-9029"
  Apr 15 09:38:11.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-9029" for this suite. @ 04/15/24 09:38:11.849
• [24.364 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 04/15/24 09:38:11.893
  Apr 15 09:38:11.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename replication-controller @ 04/15/24 09:38:11.899
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:38:11.944
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:38:11.951
  STEP: creating a ReplicationController @ 04/15/24 09:38:11.966
  STEP: waiting for RC to be added @ 04/15/24 09:38:11.979
  STEP: waiting for available Replicas @ 04/15/24 09:38:11.979
  STEP: patching ReplicationController @ 04/15/24 09:38:13.161
  STEP: waiting for RC to be modified @ 04/15/24 09:38:13.192
  STEP: patching ReplicationController status @ 04/15/24 09:38:13.192
  STEP: waiting for RC to be modified @ 04/15/24 09:38:13.205
  STEP: waiting for available Replicas @ 04/15/24 09:38:13.206
  STEP: fetching ReplicationController status @ 04/15/24 09:38:13.219
  STEP: patching ReplicationController scale @ 04/15/24 09:38:13.227
  STEP: waiting for RC to be modified @ 04/15/24 09:38:13.239
  STEP: waiting for ReplicationController's scale to be the max amount @ 04/15/24 09:38:13.243
  STEP: fetching ReplicationController; ensuring that it's patched @ 04/15/24 09:38:15.819
  STEP: updating ReplicationController status @ 04/15/24 09:38:15.829
  STEP: waiting for RC to be modified @ 04/15/24 09:38:15.849
  STEP: listing all ReplicationControllers @ 04/15/24 09:38:15.852
  STEP: checking that ReplicationController has expected values @ 04/15/24 09:38:15.862
  STEP: deleting ReplicationControllers by collection @ 04/15/24 09:38:15.863
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 04/15/24 09:38:15.882
  Apr 15 09:38:16.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0415 09:38:16.002647      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-181" for this suite. @ 04/15/24 09:38:16.012
• [4.135 seconds]
------------------------------
S
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 04/15/24 09:38:16.029
  Apr 15 09:38:16.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename subjectreview @ 04/15/24 09:38:16.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:38:16.071
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:38:16.076
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-4475" @ 04/15/24 09:38:16.082
  Apr 15 09:38:16.091: INFO: saUsername: "system:serviceaccount:subjectreview-4475:e2e"
  Apr 15 09:38:16.092: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-4475"}
  Apr 15 09:38:16.092: INFO: saUID: "03ad3af1-e47e-4a23-8e97-54d4b5adebd1"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-4475:e2e" @ 04/15/24 09:38:16.093
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-4475:e2e" @ 04/15/24 09:38:16.094
  Apr 15 09:38:16.101: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-4475:e2e" api 'list' configmaps in "subjectreview-4475" namespace @ 04/15/24 09:38:16.101
  Apr 15 09:38:16.104: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-4475:e2e" @ 04/15/24 09:38:16.104
  Apr 15 09:38:16.119: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Apr 15 09:38:16.119: INFO: LocalSubjectAccessReview has been verified
  Apr 15 09:38:16.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-4475" for this suite. @ 04/15/24 09:38:16.134
• [0.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 04/15/24 09:38:16.159
  Apr 15 09:38:16.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 09:38:16.162
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:38:16.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:38:16.203
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 04/15/24 09:38:16.211
  E0415 09:38:17.002770      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:18.003081      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:19.003707      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:20.004562      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 09:38:20.27
  Apr 15 09:38:20.276: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-334d80a7-aa06-44f2-ab2b-3995766205c9 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 09:38:20.29
  Apr 15 09:38:20.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8379" for this suite. @ 04/15/24 09:38:20.332
• [4.186 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 04/15/24 09:38:20.351
  Apr 15 09:38:20.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename conformance-tests @ 04/15/24 09:38:20.355
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:38:20.39
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:38:20.395
  STEP: Getting node addresses @ 04/15/24 09:38:20.402
  Apr 15 09:38:20.402: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Apr 15 09:38:20.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-854" for this suite. @ 04/15/24 09:38:20.429
• [0.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 04/15/24 09:38:20.45
  Apr 15 09:38:20.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename field-validation @ 04/15/24 09:38:20.452
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:38:20.48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:38:20.487
  STEP: apply creating a deployment @ 04/15/24 09:38:20.495
  Apr 15 09:38:20.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7656" for this suite. @ 04/15/24 09:38:20.528
• [0.088 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 04/15/24 09:38:20.543
  Apr 15 09:38:20.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 09:38:20.546
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:38:20.58
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:38:20.586
  STEP: Creating pod liveness-75d9d5ec-b581-41e4-97c1-46f2bc2d2c4c in namespace container-probe-1731 @ 04/15/24 09:38:20.591
  E0415 09:38:21.004470      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:22.005047      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:38:22.673: INFO: Started pod liveness-75d9d5ec-b581-41e4-97c1-46f2bc2d2c4c in namespace container-probe-1731
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/15/24 09:38:22.673
  Apr 15 09:38:22.678: INFO: Initial restart count of pod liveness-75d9d5ec-b581-41e4-97c1-46f2bc2d2c4c is 0
  E0415 09:38:23.005854      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:24.006015      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:25.006371      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:26.007478      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:27.007247      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:28.007736      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:29.008769      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:30.008537      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:31.009003      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:32.009571      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:33.009688      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:34.010258      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:35.010630      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:36.010703      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:37.012013      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:38.012485      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:39.012692      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:40.013462      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:41.014416      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:42.015059      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:38:42.794: INFO: Restart count of pod container-probe-1731/liveness-75d9d5ec-b581-41e4-97c1-46f2bc2d2c4c is now 1 (20.116062315s elapsed)
  E0415 09:38:43.016025      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:44.016576      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:45.017261      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:46.017790      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:47.018825      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:48.019100      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:49.019890      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:50.020203      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:51.021020      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:52.021100      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:53.021597      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:54.021923      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:55.022710      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:56.023196      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:57.023967      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:58.024759      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:38:59.025279      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:00.025526      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:01.028850      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:02.028463      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:39:02.944: INFO: Restart count of pod container-probe-1731/liveness-75d9d5ec-b581-41e4-97c1-46f2bc2d2c4c is now 2 (40.265475043s elapsed)
  E0415 09:39:03.029163      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:04.030017      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:05.030621      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:06.030960      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:07.031595      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:08.031535      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:09.032098      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:10.032704      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:11.033428      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:12.033768      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:13.034115      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:14.034448      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:15.034986      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:16.035386      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:17.037319      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:18.036508      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:19.037780      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:20.038643      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:21.039874      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:22.038485      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:23.039039      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:39:23.079: INFO: Restart count of pod container-probe-1731/liveness-75d9d5ec-b581-41e4-97c1-46f2bc2d2c4c is now 3 (1m0.400660411s elapsed)
  E0415 09:39:24.039253      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:25.039476      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:26.039593      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:27.039566      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:28.039990      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:29.040205      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:30.040538      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:31.041197      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:32.041351      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:33.041916      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:34.042235      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:35.042469      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:36.042622      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:37.042603      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:38.042910      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:39.043341      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:40.043648      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:41.044070      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:42.043967      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:43.044851      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:39:43.195: INFO: Restart count of pod container-probe-1731/liveness-75d9d5ec-b581-41e4-97c1-46f2bc2d2c4c is now 4 (1m20.517127562s elapsed)
  E0415 09:39:44.044993      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:45.045201      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:46.045611      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:47.047402      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:48.047145      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:49.047138      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:50.047363      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:51.047746      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:52.048809      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:53.049009      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:54.049161      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:55.049916      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:56.049907      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:57.050783      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:58.051057      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:39:59.051370      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:00.051615      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:01.052588      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:02.052942      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:03.053102      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:04.054384      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:05.054622      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:06.055388      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:07.055543      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:08.056309      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:09.056646      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:10.057219      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:11.057829      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:12.057851      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:13.058165      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:14.058756      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:15.059980      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:16.060221      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:17.060982      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:18.060947      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:19.061900      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:20.062285      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:21.063254      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:22.064201      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:23.064474      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:24.064728      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:25.065783      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:26.065694      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:27.066633      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:28.066969      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:29.067356      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:30.068049      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:31.068664      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:32.069619      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:33.070075      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:34.070789      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:35.071167      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:36.071585      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:37.071856      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:38.072120      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:39.072655      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:40.073057      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:41.073782      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:42.077239      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:43.077746      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:40:43.537: INFO: Restart count of pod container-probe-1731/liveness-75d9d5ec-b581-41e4-97c1-46f2bc2d2c4c is now 5 (2m20.85910826s elapsed)
  Apr 15 09:40:43.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 09:40:43.55
  STEP: Destroying namespace "container-probe-1731" for this suite. @ 04/15/24 09:40:43.572
• [143.044 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 04/15/24 09:40:43.594
  Apr 15 09:40:43.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 04/15/24 09:40:43.598
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:40:43.64
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:40:43.647
  STEP: Setting up the test @ 04/15/24 09:40:43.656
  STEP: Creating hostNetwork=false pod @ 04/15/24 09:40:43.656
  E0415 09:40:44.077734      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:45.078511      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:46.078904      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:47.079511      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 04/15/24 09:40:47.73
  E0415 09:40:48.082021      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:49.081589      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 04/15/24 09:40:49.789
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 04/15/24 09:40:49.789
  Apr 15 09:40:49.789: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7488 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:40:49.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:40:49.792: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:40:49.793: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7488/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 15 09:40:49.935: INFO: Exec stderr: ""
  Apr 15 09:40:49.935: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7488 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:40:49.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:40:49.938: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:40:49.938: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7488/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 15 09:40:50.045: INFO: Exec stderr: ""
  Apr 15 09:40:50.046: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7488 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:40:50.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:40:50.047: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:40:50.047: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7488/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  E0415 09:40:50.082289      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:40:50.175: INFO: Exec stderr: ""
  Apr 15 09:40:50.175: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7488 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:40:50.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:40:50.178: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:40:50.178: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7488/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 15 09:40:50.296: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 04/15/24 09:40:50.297
  Apr 15 09:40:50.297: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7488 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:40:50.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:40:50.298: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:40:50.298: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7488/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Apr 15 09:40:50.413: INFO: Exec stderr: ""
  Apr 15 09:40:50.413: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7488 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:40:50.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:40:50.415: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:40:50.415: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7488/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Apr 15 09:40:50.519: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 04/15/24 09:40:50.519
  Apr 15 09:40:50.519: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7488 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:40:50.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:40:50.521: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:40:50.521: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7488/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 15 09:40:50.645: INFO: Exec stderr: ""
  Apr 15 09:40:50.645: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7488 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:40:50.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:40:50.647: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:40:50.647: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7488/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Apr 15 09:40:50.751: INFO: Exec stderr: ""
  Apr 15 09:40:50.751: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7488 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:40:50.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:40:50.754: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:40:50.754: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7488/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 15 09:40:50.883: INFO: Exec stderr: ""
  Apr 15 09:40:50.884: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7488 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:40:50.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:40:50.889: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:40:50.891: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7488/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Apr 15 09:40:50.983: INFO: Exec stderr: ""
  Apr 15 09:40:50.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-7488" for this suite. @ 04/15/24 09:40:50.998
• [7.419 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 04/15/24 09:40:51.018
  Apr 15 09:40:51.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename webhook @ 04/15/24 09:40:51.02
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:40:51.053
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:40:51.061
  E0415 09:40:51.082347      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 04/15/24 09:40:51.111
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 09:40:51.811
  STEP: Deploying the webhook pod @ 04/15/24 09:40:51.833
  STEP: Wait for the deployment to be ready @ 04/15/24 09:40:51.879
  Apr 15 09:40:51.903: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0415 09:40:52.082590      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:53.082878      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:40:53.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.April, 15, 9, 40, 51, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 40, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.April, 15, 9, 40, 51, 0, time.Local), LastTransitionTime:time.Date(2024, time.April, 15, 9, 40, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6d58c8c59c\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0415 09:40:54.083130      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:55.083348      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/15/24 09:40:55.942
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 09:40:55.974
  E0415 09:40:56.084445      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:40:56.974: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 04/15/24 09:40:56.985
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 04/15/24 09:40:57.043
  STEP: Creating a dummy validating-webhook-configuration object @ 04/15/24 09:40:57.07
  E0415 09:40:57.085105      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 04/15/24 09:40:57.1
  STEP: Creating a dummy mutating-webhook-configuration object @ 04/15/24 09:40:57.119
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 04/15/24 09:40:57.138
  Apr 15 09:40:57.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8249" for this suite. @ 04/15/24 09:40:57.324
  STEP: Destroying namespace "webhook-markers-7177" for this suite. @ 04/15/24 09:40:57.354
• [6.350 seconds]
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 04/15/24 09:40:57.368
  Apr 15 09:40:57.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:40:57.372
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:40:57.409
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:40:57.416
  STEP: Creating the pod @ 04/15/24 09:40:57.423
  E0415 09:40:58.085694      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:40:59.086077      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:41:00.043: INFO: Successfully updated pod "annotationupdatee40c6b1c-aa83-4991-9a92-6638b67eabb0"
  E0415 09:41:00.086494      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:01.087248      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:41:02.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0415 09:41:02.088001      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "projected-1013" for this suite. @ 04/15/24 09:41:02.098
• [4.754 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 04/15/24 09:41:02.124
  Apr 15 09:41:02.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename replicaset @ 04/15/24 09:41:02.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:41:02.173
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:41:02.18
  STEP: Create a ReplicaSet @ 04/15/24 09:41:02.185
  STEP: Verify that the required pods have come up @ 04/15/24 09:41:02.203
  Apr 15 09:41:02.214: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0415 09:41:03.088100      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:04.088449      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:05.088661      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:06.088759      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:07.089740      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:41:07.227: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 04/15/24 09:41:07.227
  Apr 15 09:41:07.233: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 04/15/24 09:41:07.234
  STEP: DeleteCollection of the ReplicaSets @ 04/15/24 09:41:07.244
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 04/15/24 09:41:07.263
  Apr 15 09:41:07.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4416" for this suite. @ 04/15/24 09:41:07.296
• [5.229 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 04/15/24 09:41:07.355
  Apr 15 09:41:07.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename webhook @ 04/15/24 09:41:07.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:41:07.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:41:07.425
  STEP: Setting up server cert @ 04/15/24 09:41:07.523
  E0415 09:41:08.090209      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 09:41:08.694
  STEP: Deploying the webhook pod @ 04/15/24 09:41:08.711
  STEP: Wait for the deployment to be ready @ 04/15/24 09:41:08.747
  Apr 15 09:41:08.764: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0415 09:41:09.091052      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:10.091346      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/15/24 09:41:10.786
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 09:41:10.811
  E0415 09:41:11.092323      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:41:11.812: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 04/15/24 09:41:11.821
  STEP: create a pod @ 04/15/24 09:41:11.877
  E0415 09:41:12.092785      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:13.093913      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 04/15/24 09:41:13.921
  Apr 15 09:41:13.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=webhook-417 attach --namespace=webhook-417 to-be-attached-pod -i -c=container1'
  E0415 09:41:14.093998      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:41:14.151: INFO: rc: 1
  Apr 15 09:41:14.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-417" for this suite. @ 04/15/24 09:41:14.257
  STEP: Destroying namespace "webhook-markers-6385" for this suite. @ 04/15/24 09:41:14.272
• [6.938 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 04/15/24 09:41:14.295
  Apr 15 09:41:14.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename services @ 04/15/24 09:41:14.298
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:41:14.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:41:14.338
  STEP: creating an Endpoint @ 04/15/24 09:41:14.354
  STEP: waiting for available Endpoint @ 04/15/24 09:41:14.365
  STEP: listing all Endpoints @ 04/15/24 09:41:14.368
  STEP: updating the Endpoint @ 04/15/24 09:41:14.375
  STEP: fetching the Endpoint @ 04/15/24 09:41:14.389
  STEP: patching the Endpoint @ 04/15/24 09:41:14.397
  STEP: fetching the Endpoint @ 04/15/24 09:41:14.421
  STEP: deleting the Endpoint by Collection @ 04/15/24 09:41:14.429
  STEP: waiting for Endpoint deletion @ 04/15/24 09:41:14.453
  STEP: fetching the Endpoint @ 04/15/24 09:41:14.457
  Apr 15 09:41:14.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8345" for this suite. @ 04/15/24 09:41:14.478
• [0.202 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 04/15/24 09:41:14.502
  Apr 15 09:41:14.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/15/24 09:41:14.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:41:14.551
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:41:14.562
  Apr 15 09:41:14.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  E0415 09:41:15.094286      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:16.108305      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 04/15/24 09:41:17.01
  Apr 15 09:41:17.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4493 --namespace=crd-publish-openapi-4493 create -f -'
  E0415 09:41:17.109251      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:18.109442      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:41:18.662: INFO: stderr: ""
  Apr 15 09:41:18.663: INFO: stdout: "e2e-test-crd-publish-openapi-3125-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Apr 15 09:41:18.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4493 --namespace=crd-publish-openapi-4493 delete e2e-test-crd-publish-openapi-3125-crds test-foo'
  Apr 15 09:41:18.833: INFO: stderr: ""
  Apr 15 09:41:18.833: INFO: stdout: "e2e-test-crd-publish-openapi-3125-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Apr 15 09:41:18.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4493 --namespace=crd-publish-openapi-4493 apply -f -'
  E0415 09:41:19.109569      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:41:19.345: INFO: stderr: ""
  Apr 15 09:41:19.345: INFO: stdout: "e2e-test-crd-publish-openapi-3125-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Apr 15 09:41:19.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4493 --namespace=crd-publish-openapi-4493 delete e2e-test-crd-publish-openapi-3125-crds test-foo'
  Apr 15 09:41:19.536: INFO: stderr: ""
  Apr 15 09:41:19.536: INFO: stdout: "e2e-test-crd-publish-openapi-3125-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 04/15/24 09:41:19.536
  Apr 15 09:41:19.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4493 --namespace=crd-publish-openapi-4493 create -f -'
  E0415 09:41:20.110318      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:41:20.126: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 04/15/24 09:41:20.127
  Apr 15 09:41:20.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4493 --namespace=crd-publish-openapi-4493 create -f -'
  Apr 15 09:41:20.699: INFO: rc: 1
  Apr 15 09:41:20.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4493 --namespace=crd-publish-openapi-4493 apply -f -'
  E0415 09:41:21.110449      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:41:21.288: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 04/15/24 09:41:21.288
  Apr 15 09:41:21.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4493 --namespace=crd-publish-openapi-4493 create -f -'
  Apr 15 09:41:21.793: INFO: rc: 1
  Apr 15 09:41:21.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4493 --namespace=crd-publish-openapi-4493 apply -f -'
  E0415 09:41:22.111512      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:41:22.321: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 04/15/24 09:41:22.321
  Apr 15 09:41:22.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4493 explain e2e-test-crd-publish-openapi-3125-crds'
  Apr 15 09:41:22.786: INFO: stderr: ""
  Apr 15 09:41:22.786: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3125-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 04/15/24 09:41:22.787
  Apr 15 09:41:22.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4493 explain e2e-test-crd-publish-openapi-3125-crds.metadata'
  E0415 09:41:23.112311      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:41:23.250: INFO: stderr: ""
  Apr 15 09:41:23.250: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3125-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Apr 15 09:41:23.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4493 explain e2e-test-crd-publish-openapi-3125-crds.spec'
  Apr 15 09:41:23.790: INFO: stderr: ""
  Apr 15 09:41:23.791: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3125-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Apr 15 09:41:23.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4493 explain e2e-test-crd-publish-openapi-3125-crds.spec.bars'
  E0415 09:41:24.113090      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:41:24.290: INFO: stderr: ""
  Apr 15 09:41:24.290: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3125-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 04/15/24 09:41:24.291
  Apr 15 09:41:24.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-4493 explain e2e-test-crd-publish-openapi-3125-crds.spec.bars2'
  Apr 15 09:41:24.784: INFO: rc: 1
  E0415 09:41:25.113915      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:26.130117      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:27.130675      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:41:27.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4493" for this suite. @ 04/15/24 09:41:27.324
• [12.837 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 04/15/24 09:41:27.341
  Apr 15 09:41:27.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename secrets @ 04/15/24 09:41:27.344
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:41:27.395
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:41:27.4
  STEP: Creating secret with name secret-test-map-e171963b-c804-458e-b688-ee2a29ddf6d0 @ 04/15/24 09:41:27.409
  STEP: Creating a pod to test consume secrets @ 04/15/24 09:41:27.433
  E0415 09:41:28.130960      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:29.131201      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:30.131489      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:31.132189      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 09:41:31.495
  Apr 15 09:41:31.505: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-secrets-d7d0ff2b-de5e-4007-b58b-1722b057b6d7 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 09:41:31.544
  Apr 15 09:41:31.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-796" for this suite. @ 04/15/24 09:41:31.585
• [4.256 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 04/15/24 09:41:31.599
  Apr 15 09:41:31.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:41:31.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:41:31.627
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:41:31.638
  STEP: Creating configMap with name projected-configmap-test-volume-map-a08addce-53c7-47e1-a7b7-41e1aaecbb6f @ 04/15/24 09:41:31.644
  STEP: Creating a pod to test consume configMaps @ 04/15/24 09:41:31.655
  E0415 09:41:32.132702      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:33.133013      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:34.133500      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:35.133671      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 09:41:35.698
  Apr 15 09:41:35.705: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-projected-configmaps-01e6c5f7-bfdd-4152-8a11-28676dd3067a container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 09:41:35.718
  Apr 15 09:41:35.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8915" for this suite. @ 04/15/24 09:41:35.758
• [4.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 04/15/24 09:41:35.789
  Apr 15 09:41:35.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 09:41:35.792
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:41:35.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:41:35.825
  STEP: Creating pod test-grpc-d0cb3618-691f-4c1f-95f7-f4880340edab in namespace container-probe-3413 @ 04/15/24 09:41:35.832
  E0415 09:41:36.134151      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:37.134578      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:41:37.880: INFO: Started pod test-grpc-d0cb3618-691f-4c1f-95f7-f4880340edab in namespace container-probe-3413
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/15/24 09:41:37.88
  Apr 15 09:41:37.889: INFO: Initial restart count of pod test-grpc-d0cb3618-691f-4c1f-95f7-f4880340edab is 0
  E0415 09:41:38.135782      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:39.135883      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:40.137218      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:41.137683      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:42.137492      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:43.137672      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:44.137645      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:45.137969      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:46.139022      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:47.139318      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:48.140056      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:49.140504      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:50.140470      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:51.140853      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:52.141525      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:53.142190      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:54.143522      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:55.143119      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:56.143679      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:57.143699      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:58.144194      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:41:59.144897      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:00.145032      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:01.145798      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:02.147141      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:03.147583      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:04.148443      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:05.148813      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:06.149818      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:07.152144      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:08.152088      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:09.152507      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:10.152710      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:11.153298      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:12.153771      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:13.154315      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:14.155039      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:15.155830      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:16.156100      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:17.156632      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:18.157406      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:19.158228      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:20.159625      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:21.161157      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:22.161337      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:23.161419      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:24.161635      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:25.162176      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:26.162366      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:27.162593      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:28.162777      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:29.163070      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:30.163499      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:31.163487      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:32.163701      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:33.165940      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:34.165198      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:35.165278      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:36.166045      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:37.166654      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:38.168399      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:39.167137      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:40.167253      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:41.168041      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:42.168197      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:43.168500      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:44.168989      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:42:44.295: INFO: Restart count of pod container-probe-3413/test-grpc-d0cb3618-691f-4c1f-95f7-f4880340edab is now 1 (1m6.40609555s elapsed)
  Apr 15 09:42:44.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 09:42:44.312
  STEP: Destroying namespace "container-probe-3413" for this suite. @ 04/15/24 09:42:44.339
• [68.568 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 04/15/24 09:42:44.36
  Apr 15 09:42:44.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename configmap @ 04/15/24 09:42:44.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:42:44.409
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:42:44.424
  STEP: Creating configMap with name configmap-test-volume-93793fdb-24f8-4471-b686-59c313e6b116 @ 04/15/24 09:42:44.435
  STEP: Creating a pod to test consume configMaps @ 04/15/24 09:42:44.447
  E0415 09:42:45.168926      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:46.169198      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:47.169376      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:48.170668      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 09:42:48.511
  Apr 15 09:42:48.518: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-configmaps-e138ce80-75ad-4127-9313-f487a0568697 container configmap-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 09:42:48.54
  Apr 15 09:42:48.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8500" for this suite. @ 04/15/24 09:42:48.588
• [4.240 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 04/15/24 09:42:48.604
  Apr 15 09:42:48.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename csistoragecapacity @ 04/15/24 09:42:48.609
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:42:48.647
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:42:48.658
  STEP: getting /apis @ 04/15/24 09:42:48.663
  STEP: getting /apis/storage.k8s.io @ 04/15/24 09:42:48.678
  STEP: getting /apis/storage.k8s.io/v1 @ 04/15/24 09:42:48.681
  STEP: creating @ 04/15/24 09:42:48.684
  STEP: watching @ 04/15/24 09:42:48.773
  Apr 15 09:42:48.774: INFO: starting watch
  STEP: getting @ 04/15/24 09:42:48.801
  STEP: listing in namespace @ 04/15/24 09:42:48.809
  STEP: listing across namespaces @ 04/15/24 09:42:48.817
  STEP: patching @ 04/15/24 09:42:48.826
  STEP: updating @ 04/15/24 09:42:48.839
  Apr 15 09:42:48.851: INFO: waiting for watch events with expected annotations in namespace
  Apr 15 09:42:48.851: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 04/15/24 09:42:48.851
  STEP: deleting a collection @ 04/15/24 09:42:48.879
  Apr 15 09:42:48.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-9484" for this suite. @ 04/15/24 09:42:48.926
• [0.333 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 04/15/24 09:42:48.94
  Apr 15 09:42:48.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename webhook @ 04/15/24 09:42:48.945
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:42:48.979
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:42:48.984
  STEP: Setting up server cert @ 04/15/24 09:42:49.029
  E0415 09:42:49.171358      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 09:42:49.627
  STEP: Deploying the webhook pod @ 04/15/24 09:42:49.644
  STEP: Wait for the deployment to be ready @ 04/15/24 09:42:49.671
  Apr 15 09:42:49.697: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0415 09:42:50.172053      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:51.180470      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/15/24 09:42:51.72
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 09:42:51.76
  E0415 09:42:52.177029      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:42:52.767: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 04/15/24 09:42:52.882
  STEP: Creating a configMap that should be mutated @ 04/15/24 09:42:52.92
  STEP: Deleting the collection of validation webhooks @ 04/15/24 09:42:52.975
  STEP: Creating a configMap that should not be mutated @ 04/15/24 09:42:53.078
  Apr 15 09:42:53.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0415 09:42:53.186758      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-2911" for this suite. @ 04/15/24 09:42:53.247
  STEP: Destroying namespace "webhook-markers-1802" for this suite. @ 04/15/24 09:42:53.279
• [4.365 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 04/15/24 09:42:53.354
  Apr 15 09:42:53.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename replication-controller @ 04/15/24 09:42:53.359
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:42:53.393
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:42:53.398
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 04/15/24 09:42:53.404
  E0415 09:42:54.185105      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:55.185312      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replication controller with a matching selector is created @ 04/15/24 09:42:55.454
  STEP: Then the orphan pod is adopted @ 04/15/24 09:42:55.463
  E0415 09:42:56.185534      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:42:56.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-5307" for this suite. @ 04/15/24 09:42:56.494
• [3.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 04/15/24 09:42:56.543
  Apr 15 09:42:56.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename proxy @ 04/15/24 09:42:56.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:42:56.573
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:42:56.578
  Apr 15 09:42:56.583: INFO: Creating pod...
  E0415 09:42:57.185479      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:42:58.185848      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:42:58.617: INFO: Creating service...
  Apr 15 09:42:58.648: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6964/pods/agnhost/proxy?method=DELETE
  Apr 15 09:42:58.682: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 15 09:42:58.682: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6964/pods/agnhost/proxy?method=OPTIONS
  Apr 15 09:42:58.691: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 15 09:42:58.692: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6964/pods/agnhost/proxy?method=PATCH
  Apr 15 09:42:58.700: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 15 09:42:58.700: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6964/pods/agnhost/proxy?method=POST
  Apr 15 09:42:58.731: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 15 09:42:58.731: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6964/pods/agnhost/proxy?method=PUT
  Apr 15 09:42:58.752: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 15 09:42:58.752: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6964/services/e2e-proxy-test-service/proxy?method=DELETE
  Apr 15 09:42:58.770: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Apr 15 09:42:58.770: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6964/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Apr 15 09:42:58.787: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Apr 15 09:42:58.787: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6964/services/e2e-proxy-test-service/proxy?method=PATCH
  Apr 15 09:42:58.800: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Apr 15 09:42:58.800: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6964/services/e2e-proxy-test-service/proxy?method=POST
  Apr 15 09:42:58.828: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Apr 15 09:42:58.828: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6964/services/e2e-proxy-test-service/proxy?method=PUT
  Apr 15 09:42:58.960: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Apr 15 09:42:58.960: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6964/pods/agnhost/proxy?method=GET
  Apr 15 09:42:58.979: INFO: http.Client request:GET StatusCode:301
  Apr 15 09:42:58.979: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6964/services/e2e-proxy-test-service/proxy?method=GET
  Apr 15 09:42:58.995: INFO: http.Client request:GET StatusCode:301
  Apr 15 09:42:58.995: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6964/pods/agnhost/proxy?method=HEAD
  Apr 15 09:42:59.006: INFO: http.Client request:HEAD StatusCode:301
  Apr 15 09:42:59.006: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6964/services/e2e-proxy-test-service/proxy?method=HEAD
  Apr 15 09:42:59.022: INFO: http.Client request:HEAD StatusCode:301
  Apr 15 09:42:59.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-6964" for this suite. @ 04/15/24 09:42:59.034
• [2.514 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 04/15/24 09:42:59.061
  Apr 15 09:42:59.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename replicaset @ 04/15/24 09:42:59.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:42:59.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:42:59.108
  Apr 15 09:42:59.113: INFO: Creating ReplicaSet my-hostname-basic-38cb2d60-3c8d-405d-8efa-5d9db7f27771
  Apr 15 09:42:59.133: INFO: Pod name my-hostname-basic-38cb2d60-3c8d-405d-8efa-5d9db7f27771: Found 0 pods out of 1
  E0415 09:42:59.185932      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:00.186192      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:01.186260      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:02.186952      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:03.187300      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:43:04.145: INFO: Pod name my-hostname-basic-38cb2d60-3c8d-405d-8efa-5d9db7f27771: Found 1 pods out of 1
  Apr 15 09:43:04.152: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-38cb2d60-3c8d-405d-8efa-5d9db7f27771" is running
  Apr 15 09:43:04.167: INFO: Pod "my-hostname-basic-38cb2d60-3c8d-405d-8efa-5d9db7f27771-kqknj" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-04-15 09:42:59 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-04-15 09:43:00 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-04-15 09:43:00 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-04-15 09:42:59 +0000 UTC Reason: Message:}])
  Apr 15 09:43:04.172: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 04/15/24 09:43:04.172
  E0415 09:43:04.189640      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:43:04.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4047" for this suite. @ 04/15/24 09:43:04.233
• [5.190 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:790
  STEP: Creating a kubernetes client @ 04/15/24 09:43:04.253
  Apr 15 09:43:04.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename statefulset @ 04/15/24 09:43:04.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:43:04.289
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:43:04.296
  STEP: Creating service test in namespace statefulset-2896 @ 04/15/24 09:43:04.303
  STEP: Looking for a node to schedule stateful set and pod @ 04/15/24 09:43:04.327
  STEP: Creating pod with conflicting port in namespace statefulset-2896 @ 04/15/24 09:43:04.353
  STEP: Waiting until pod test-pod will start running in namespace statefulset-2896 @ 04/15/24 09:43:04.385
  E0415 09:43:05.188943      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:06.188895      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-2896 @ 04/15/24 09:43:06.404
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2896 @ 04/15/24 09:43:06.418
  Apr 15 09:43:06.464: INFO: Observed stateful pod in namespace: statefulset-2896, name: ss-0, uid: a4015af6-0f15-4abf-b496-5f529e585b5c, status phase: Pending. Waiting for statefulset controller to delete.
  Apr 15 09:43:06.491: INFO: Observed stateful pod in namespace: statefulset-2896, name: ss-0, uid: a4015af6-0f15-4abf-b496-5f529e585b5c, status phase: Failed. Waiting for statefulset controller to delete.
  Apr 15 09:43:06.520: INFO: Observed stateful pod in namespace: statefulset-2896, name: ss-0, uid: a4015af6-0f15-4abf-b496-5f529e585b5c, status phase: Failed. Waiting for statefulset controller to delete.
  Apr 15 09:43:06.530: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2896
  STEP: Removing pod with conflicting port in namespace statefulset-2896 @ 04/15/24 09:43:06.531
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2896 and will be in running state @ 04/15/24 09:43:06.579
  E0415 09:43:07.189360      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:08.189564      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:43:08.599: INFO: Deleting all statefulset in ns statefulset-2896
  Apr 15 09:43:08.608: INFO: Scaling statefulset ss to 0
  E0415 09:43:09.190145      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:10.190314      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:11.190286      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:12.190500      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:13.190784      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:14.191163      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:15.191586      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:16.192535      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:17.193588      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:18.193875      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:43:18.652: INFO: Waiting for statefulset status.replicas updated to 0
  Apr 15 09:43:18.661: INFO: Deleting statefulset ss
  Apr 15 09:43:18.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2896" for this suite. @ 04/15/24 09:43:18.715
• [14.475 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 04/15/24 09:43:18.73
  Apr 15 09:43:18.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename pods @ 04/15/24 09:43:18.733
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:43:18.768
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:43:18.774
  STEP: creating the pod @ 04/15/24 09:43:18.781
  STEP: submitting the pod to kubernetes @ 04/15/24 09:43:18.781
  STEP: verifying QOS class is set on the pod @ 04/15/24 09:43:18.799
  Apr 15 09:43:18.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6750" for this suite. @ 04/15/24 09:43:18.825
• [0.143 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 04/15/24 09:43:18.873
  Apr 15 09:43:18.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename pods @ 04/15/24 09:43:18.875
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:43:18.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:43:18.964
  STEP: Create set of pods @ 04/15/24 09:43:18.974
  Apr 15 09:43:18.992: INFO: created test-pod-1
  Apr 15 09:43:19.008: INFO: created test-pod-2
  Apr 15 09:43:19.022: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 04/15/24 09:43:19.022
  E0415 09:43:19.195244      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:20.194885      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 04/15/24 09:43:21.153
  Apr 15 09:43:21.164: INFO: Pod quantity 3 is different from expected quantity 0
  E0415 09:43:21.195763      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:43:22.173: INFO: Pod quantity 3 is different from expected quantity 0
  E0415 09:43:22.196587      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:43:23.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2076" for this suite. @ 04/15/24 09:43:23.185
  E0415 09:43:23.197641      14 retrywatcher.go:130] "Watch failed" err="context canceled"
• [4.331 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 04/15/24 09:43:23.207
  Apr 15 09:43:23.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename sched-pred @ 04/15/24 09:43:23.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:43:23.244
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:43:23.249
  Apr 15 09:43:23.256: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Apr 15 09:43:23.275: INFO: Waiting for terminating namespaces to be deleted...
  Apr 15 09:43:23.283: INFO: 
  Logging pods the apiserver thinks is on node ahz3daisheng-1 before test
  Apr 15 09:43:23.298: INFO: coredns-5d78c9869d-hwtz8 from kube-system started at 2024-04-15 08:21:15 +0000 UTC (1 container statuses recorded)
  Apr 15 09:43:23.299: INFO: 	Container coredns ready: true, restart count 0
  Apr 15 09:43:23.299: INFO: kube-addon-manager-ahz3daisheng-1 from kube-system started at 2024-04-15 06:17:06 +0000 UTC (1 container statuses recorded)
  Apr 15 09:43:23.300: INFO: 	Container kube-addon-manager ready: true, restart count 5
  Apr 15 09:43:23.300: INFO: kube-apiserver-ahz3daisheng-1 from kube-system started at 2024-04-15 06:17:06 +0000 UTC (1 container statuses recorded)
  Apr 15 09:43:23.300: INFO: 	Container kube-apiserver ready: true, restart count 5
  Apr 15 09:43:23.300: INFO: kube-controller-manager-ahz3daisheng-1 from kube-system started at 2024-04-15 06:17:06 +0000 UTC (1 container statuses recorded)
  Apr 15 09:43:23.301: INFO: 	Container kube-controller-manager ready: true, restart count 5
  Apr 15 09:43:23.301: INFO: kube-flannel-ds-k86c4 from kube-system started at 2024-04-15 08:21:18 +0000 UTC (1 container statuses recorded)
  Apr 15 09:43:23.301: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 09:43:23.301: INFO: kube-proxy-8g55q from kube-system started at 2024-04-15 08:21:17 +0000 UTC (1 container statuses recorded)
  Apr 15 09:43:23.302: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 09:43:23.302: INFO: kube-scheduler-ahz3daisheng-1 from kube-system started at 2024-04-15 06:17:06 +0000 UTC (1 container statuses recorded)
  Apr 15 09:43:23.302: INFO: 	Container kube-scheduler ready: true, restart count 5
  Apr 15 09:43:23.303: INFO: sonobuoy-systemd-logs-daemon-set-10fc9ca8ef6747d1-9lf7d from sonobuoy started at 2024-04-15 08:22:14 +0000 UTC (2 container statuses recorded)
  Apr 15 09:43:23.303: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 09:43:23.303: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 15 09:43:23.304: INFO: 
  Logging pods the apiserver thinks is on node ahz3daisheng-2 before test
  Apr 15 09:43:23.320: INFO: coredns-5d78c9869d-zfc7x from kube-system started at 2024-04-15 09:24:04 +0000 UTC (1 container statuses recorded)
  Apr 15 09:43:23.320: INFO: 	Container coredns ready: true, restart count 0
  Apr 15 09:43:23.321: INFO: kube-addon-manager-ahz3daisheng-2 from kube-system started at 2024-04-15 06:17:33 +0000 UTC (1 container statuses recorded)
  Apr 15 09:43:23.321: INFO: 	Container kube-addon-manager ready: true, restart count 5
  Apr 15 09:43:23.322: INFO: kube-apiserver-ahz3daisheng-2 from kube-system started at 2024-04-15 06:17:33 +0000 UTC (1 container statuses recorded)
  Apr 15 09:43:23.322: INFO: 	Container kube-apiserver ready: true, restart count 5
  Apr 15 09:43:23.323: INFO: kube-controller-manager-ahz3daisheng-2 from kube-system started at 2024-04-15 06:17:33 +0000 UTC (1 container statuses recorded)
  Apr 15 09:43:23.323: INFO: 	Container kube-controller-manager ready: true, restart count 5
  Apr 15 09:43:23.324: INFO: kube-flannel-ds-5c2gp from kube-system started at 2024-04-15 08:21:17 +0000 UTC (1 container statuses recorded)
  Apr 15 09:43:23.324: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 09:43:23.324: INFO: kube-proxy-llrhn from kube-system started at 2024-04-15 08:21:16 +0000 UTC (1 container statuses recorded)
  Apr 15 09:43:23.324: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 09:43:23.325: INFO: kube-scheduler-ahz3daisheng-2 from kube-system started at 2024-04-15 06:17:33 +0000 UTC (1 container statuses recorded)
  Apr 15 09:43:23.325: INFO: 	Container kube-scheduler ready: true, restart count 5
  Apr 15 09:43:23.325: INFO: sonobuoy-systemd-logs-daemon-set-10fc9ca8ef6747d1-r768m from sonobuoy started at 2024-04-15 08:22:14 +0000 UTC (2 container statuses recorded)
  Apr 15 09:43:23.326: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 09:43:23.326: INFO: 	Container systemd-logs ready: true, restart count 0
  Apr 15 09:43:23.326: INFO: 
  Logging pods the apiserver thinks is on node ahz3daisheng-3 before test
  Apr 15 09:43:23.342: INFO: kube-flannel-ds-jdqzd from kube-system started at 2024-04-15 09:24:05 +0000 UTC (1 container statuses recorded)
  Apr 15 09:43:23.342: INFO: 	Container kube-flannel ready: true, restart count 0
  Apr 15 09:43:23.342: INFO: kube-proxy-66n59 from kube-system started at 2024-04-15 08:21:16 +0000 UTC (1 container statuses recorded)
  Apr 15 09:43:23.342: INFO: 	Container kube-proxy ready: true, restart count 0
  Apr 15 09:43:23.342: INFO: pod-qos-class-42abb7ae-d8ce-4dee-bfdb-80cb07f1760e from pods-6750 started at 2024-04-15 09:43:18 +0000 UTC (1 container statuses recorded)
  Apr 15 09:43:23.342: INFO: 	Container agnhost ready: false, restart count 0
  Apr 15 09:43:23.342: INFO: sonobuoy from sonobuoy started at 2024-04-15 08:22:13 +0000 UTC (1 container statuses recorded)
  Apr 15 09:43:23.342: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Apr 15 09:43:23.342: INFO: sonobuoy-e2e-job-dd62b7a77ff54dbf from sonobuoy started at 2024-04-15 08:22:14 +0000 UTC (2 container statuses recorded)
  Apr 15 09:43:23.342: INFO: 	Container e2e ready: true, restart count 0
  Apr 15 09:43:23.342: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 09:43:23.342: INFO: sonobuoy-systemd-logs-daemon-set-10fc9ca8ef6747d1-qhkjw from sonobuoy started at 2024-04-15 08:22:14 +0000 UTC (2 container statuses recorded)
  Apr 15 09:43:23.342: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Apr 15 09:43:23.342: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 04/15/24 09:43:23.342
  E0415 09:43:24.198401      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:25.198675      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 04/15/24 09:43:25.378
  STEP: Trying to apply a random label on the found node. @ 04/15/24 09:43:25.415
  STEP: verifying the node has the label kubernetes.io/e2e-00e09785-7669-4322-9c93-44cbd2f0cf57 95 @ 04/15/24 09:43:25.434
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 04/15/24 09:43:25.445
  E0415 09:43:26.199362      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:27.199490      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.199 on the node which pod4 resides and expect not scheduled @ 04/15/24 09:43:27.487
  E0415 09:43:28.199689      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:29.200324      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:30.201095      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:31.202083      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:32.202323      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:33.203421      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:34.203891      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:35.204722      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:36.204937      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:37.205880      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:38.206798      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:39.207115      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:40.208097      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:41.208234      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:42.209332      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:43.210350      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:44.210639      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:45.211428      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:46.212446      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:47.213431      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:48.213601      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:49.213776      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:50.214162      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:51.214273      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:52.214371      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:53.214617      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:54.214759      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:55.215112      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:56.215363      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:57.215574      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:58.215660      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:43:59.216446      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:00.216636      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:01.217697      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:02.218168      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:03.219967      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:04.220512      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:05.220846      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:06.221228      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:07.221679      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:08.221911      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:09.222448      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:10.222640      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:11.223278      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:12.223720      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:13.224288      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:14.225184      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:15.225887      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:16.231230      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:17.229192      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:18.229473      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:19.229737      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:20.229793      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:21.230410      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:22.230793      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:23.230783      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:24.231119      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:25.232052      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:26.233387      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:27.233675      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:28.234618      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:29.235431      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:30.235690      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:31.236267      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:32.236537      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:33.237115      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:34.237959      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:35.238209      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:36.239034      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:37.240196      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:38.241436      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:39.242466      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:40.244019      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:41.244528      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:42.244607      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:43.245330      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:44.245574      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:45.249356      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:46.250377      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:47.251097      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:48.251272      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:49.251453      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:50.252081      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:51.252254      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:52.252759      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:53.252940      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:54.254533      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:55.254680      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:56.255048      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:57.255913      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:58.256332      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:44:59.258017      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:00.257488      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:01.257673      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:02.258120      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:03.259181      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:04.259375      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:05.259592      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:06.259699      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:07.260272      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:08.261104      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:09.261887      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:10.262206      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:11.262733      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:12.263573      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:13.263521      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:14.264331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:15.264602      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:16.265351      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:17.265898      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:18.265962      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:19.266634      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:20.267497      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:21.268193      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:22.268311      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:23.269049      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:24.270530      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:25.272818      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:26.274274      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:27.274954      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:28.275404      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:29.275601      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:30.276567      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:31.276772      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:32.276817      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:33.277049      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:34.278327      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:35.278415      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:36.278605      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:37.279106      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:38.280047      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:39.281126      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:40.281797      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:41.282191      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:42.283326      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:43.283913      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:44.284752      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:45.285779      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:46.285470      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:47.286455      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:48.287126      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:49.288252      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:50.288451      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:51.288676      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:52.288861      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:53.289609      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:54.290233      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:55.290682      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:56.291564      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:57.291583      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:58.292241      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:45:59.293119      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:00.293592      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:01.293740      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:02.294977      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:03.295403      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:04.296029      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:05.298763      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:06.297574      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:07.298383      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:08.298648      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:09.298820      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:10.299154      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:11.299346      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:12.299724      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:13.300619      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:14.300687      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:15.301196      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:16.301486      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:17.302062      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:18.302915      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:19.303095      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:20.303343      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:21.303578      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:22.304320      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:23.304966      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:24.305214      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:25.305290      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:26.305482      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:27.306639      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:28.306801      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:29.307016      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:30.307232      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:31.307464      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:32.308698      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:33.309499      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:34.310031      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:35.310472      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:36.311303      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:37.311323      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:38.312014      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:39.312036      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:40.312204      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:41.312498      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:42.312791      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:43.313272      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:44.314097      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:45.314274      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:46.316998      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:47.317937      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:48.319084      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:49.319340      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:50.319737      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:51.319930      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:52.320647      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:53.320870      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:54.321592      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:55.322770      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:56.323149      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:57.323595      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:58.323817      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:46:59.324449      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:00.324608      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:01.325154      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:02.325806      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:03.326204      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:04.326493      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:05.326789      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:06.327628      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:07.328850      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:08.328812      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:09.329854      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:10.330324      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:11.330763      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:12.330681      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:13.331252      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:14.331617      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:15.332666      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:16.333456      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:17.333125      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:18.333343      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:19.334613      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:20.335057      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:21.335394      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:22.336225      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:23.336957      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:24.337369      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:25.338429      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:26.338843      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:27.339200      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:28.339979      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:29.340075      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:30.340319      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:31.341311      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:32.341915      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:33.342085      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:34.342318      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:35.343425      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:36.344549      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:37.344558      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:38.345265      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:39.346068      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:40.346462      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:41.346598      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:42.346772      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:43.347256      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:44.348057      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:45.348653      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:46.349298      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:47.350012      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:48.350619      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:49.350720      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:50.351491      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:51.351649      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:52.352216      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:53.352249      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:54.352641      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:55.352798      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:56.353142      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:57.353768      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:58.354774      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:47:59.355232      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:00.355432      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:01.356465      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:02.357219      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:03.357973      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:04.364554      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:05.361460      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:06.361789      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:07.363423      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:08.363009      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:09.363303      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:10.363658      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:11.364219      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:12.364804      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:13.365543      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:14.366099      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:15.366309      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:16.366817      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:17.368950      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:18.369504      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:19.370074      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:20.370743      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:21.370956      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:22.371874      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:23.372676      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:24.373067      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:25.373438      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:26.374354      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:27.375061      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-00e09785-7669-4322-9c93-44cbd2f0cf57 off the node ahz3daisheng-3 @ 04/15/24 09:48:27.507
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-00e09785-7669-4322-9c93-44cbd2f0cf57 @ 04/15/24 09:48:27.543
  Apr 15 09:48:27.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9054" for this suite. @ 04/15/24 09:48:27.576
• [304.383 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 04/15/24 09:48:27.593
  Apr 15 09:48:27.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename pod-network-test @ 04/15/24 09:48:27.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:48:27.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:48:27.641
  STEP: Performing setup for networking test in namespace pod-network-test-5586 @ 04/15/24 09:48:27.649
  STEP: creating a selector @ 04/15/24 09:48:27.649
  STEP: Creating the service pods in kubernetes @ 04/15/24 09:48:27.65
  Apr 15 09:48:27.650: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0415 09:48:28.375614      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:29.375748      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:30.376195      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:31.376893      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:32.376825      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:33.376964      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:34.377257      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:35.377524      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:36.377908      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:37.379338      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:38.380906      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:39.381119      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:40.382308      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:41.382733      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:42.382696      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:43.383218      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:44.383162      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:45.383650      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:46.384464      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:47.385355      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:48.386160      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:49.386478      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/15/24 09:48:49.924
  E0415 09:48:50.386722      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:51.387645      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:48:51.969: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 15 09:48:51.970: INFO: Breadth first check of 10.233.64.131 on host 192.168.121.96...
  Apr 15 09:48:51.979: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.52:9080/dial?request=hostname&protocol=http&host=10.233.64.131&port=8083&tries=1'] Namespace:pod-network-test-5586 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:48:51.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:48:51.984: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:48:51.985: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5586/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.52%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.131%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 15 09:48:52.211: INFO: Waiting for responses: map[]
  Apr 15 09:48:52.211: INFO: reached 10.233.64.131 after 0/1 tries
  Apr 15 09:48:52.211: INFO: Breadth first check of 10.233.65.171 on host 192.168.121.120...
  Apr 15 09:48:52.218: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.52:9080/dial?request=hostname&protocol=http&host=10.233.65.171&port=8083&tries=1'] Namespace:pod-network-test-5586 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:48:52.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:48:52.219: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:48:52.219: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5586/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.52%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.65.171%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 15 09:48:52.335: INFO: Waiting for responses: map[]
  Apr 15 09:48:52.335: INFO: reached 10.233.65.171 after 0/1 tries
  Apr 15 09:48:52.335: INFO: Breadth first check of 10.233.66.51 on host 192.168.121.199...
  Apr 15 09:48:52.342: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.52:9080/dial?request=hostname&protocol=http&host=10.233.66.51&port=8083&tries=1'] Namespace:pod-network-test-5586 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:48:52.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:48:52.344: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:48:52.344: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5586/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.52%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.51%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  E0415 09:48:52.387282      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:48:52.458: INFO: Waiting for responses: map[]
  Apr 15 09:48:52.458: INFO: reached 10.233.66.51 after 0/1 tries
  Apr 15 09:48:52.458: INFO: Going to retry 0 out of 3 pods....
  Apr 15 09:48:52.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-5586" for this suite. @ 04/15/24 09:48:52.469
• [24.895 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 04/15/24 09:48:52.492
  Apr 15 09:48:52.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename replication-controller @ 04/15/24 09:48:52.494
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:48:52.524
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:48:52.532
  STEP: Creating ReplicationController "e2e-rc-lbqfs" @ 04/15/24 09:48:52.539
  Apr 15 09:48:52.556: INFO: Get Replication Controller "e2e-rc-lbqfs" to confirm replicas
  E0415 09:48:53.387889      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:48:53.564: INFO: Get Replication Controller "e2e-rc-lbqfs" to confirm replicas
  Apr 15 09:48:53.581: INFO: Found 1 replicas for "e2e-rc-lbqfs" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-lbqfs" @ 04/15/24 09:48:53.583
  STEP: Updating a scale subresource @ 04/15/24 09:48:53.593
  STEP: Verifying replicas where modified for replication controller "e2e-rc-lbqfs" @ 04/15/24 09:48:53.609
  Apr 15 09:48:53.609: INFO: Get Replication Controller "e2e-rc-lbqfs" to confirm replicas
  E0415 09:48:54.388682      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:48:54.622: INFO: Get Replication Controller "e2e-rc-lbqfs" to confirm replicas
  Apr 15 09:48:54.631: INFO: Found 2 replicas for "e2e-rc-lbqfs" replication controller
  Apr 15 09:48:54.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8963" for this suite. @ 04/15/24 09:48:54.639
• [2.160 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 04/15/24 09:48:54.658
  Apr 15 09:48:54.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/15/24 09:48:54.661
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:48:54.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:48:54.699
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 04/15/24 09:48:54.704
  Apr 15 09:48:54.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  E0415 09:48:55.389515      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:56.389679      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:48:56.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  E0415 09:48:57.390635      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:58.392670      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:48:59.393309      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:00.394853      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:01.394721      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:02.394877      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:03.395712      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:04.395943      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:05.397825      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:49:05.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-211" for this suite. @ 04/15/24 09:49:05.468
• [10.824 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 04/15/24 09:49:05.483
  Apr 15 09:49:05.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename configmap @ 04/15/24 09:49:05.486
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:49:05.518
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:49:05.524
  STEP: Creating configMap with name configmap-test-volume-5178d466-1bcb-45c4-b175-817471d405c3 @ 04/15/24 09:49:05.531
  STEP: Creating a pod to test consume configMaps @ 04/15/24 09:49:05.55
  E0415 09:49:06.398283      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:07.399048      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:08.399468      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:09.399495      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 09:49:09.626
  Apr 15 09:49:09.632: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-configmaps-0d2235b0-6eb7-4968-a90f-598c11715187 container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 09:49:09.667
  Apr 15 09:49:09.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7228" for this suite. @ 04/15/24 09:49:09.706
• [4.236 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 04/15/24 09:49:09.722
  Apr 15 09:49:09.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 09:49:09.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:49:09.756
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:49:09.76
  STEP: Creating pod test-webserver-f09c6e66-41a6-4a5a-9a6b-1f0411b0f6d5 in namespace container-probe-6117 @ 04/15/24 09:49:09.766
  E0415 09:49:10.399549      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:11.399988      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:49:11.800: INFO: Started pod test-webserver-f09c6e66-41a6-4a5a-9a6b-1f0411b0f6d5 in namespace container-probe-6117
  STEP: checking the pod's current state and verifying that restartCount is present @ 04/15/24 09:49:11.801
  Apr 15 09:49:11.809: INFO: Initial restart count of pod test-webserver-f09c6e66-41a6-4a5a-9a6b-1f0411b0f6d5 is 0
  E0415 09:49:12.400777      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:13.401487      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:14.402717      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:15.402994      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:16.403607      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:17.403927      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:18.404236      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:19.405121      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:20.405239      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:21.405801      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:22.406711      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:23.407331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:24.407620      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:25.407828      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:26.408801      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:27.409590      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:28.410515      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:29.411324      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:30.411987      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:31.412660      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:32.412867      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:33.413112      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:34.413633      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:35.413555      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:36.414510      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:37.415380      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:38.416180      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:39.417304      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:40.418321      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:41.418506      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:42.418642      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:43.418822      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:44.419658      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:45.420485      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:46.421405      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:47.421686      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:48.422618      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:49.423305      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:50.424033      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:51.424836      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:52.425670      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:53.425856      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:54.426390      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:55.426475      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:56.427187      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:57.427602      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:58.428525      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:49:59.429475      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:00.430277      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:01.431331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:02.431500      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:03.431620      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:04.432619      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:05.433098      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:06.434083      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:07.434430      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:08.435663      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:09.435883      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:10.435977      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:11.436433      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:12.436691      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:13.437615      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:14.438511      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:15.439257      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:16.440075      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:17.440761      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:18.441458      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:19.442195      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:20.443051      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:21.444115      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:22.444587      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:23.444892      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:24.445010      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:25.446012      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:26.446847      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:27.447840      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:28.448051      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:29.448813      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:30.449493      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:31.449837      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:32.450524      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:33.451191      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:34.451563      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:35.451750      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:36.452567      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:37.453228      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:38.454336      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:39.454276      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:40.454456      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:41.455230      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:42.455792      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:43.456400      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:44.457180      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:45.457668      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:46.457939      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:47.458957      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:48.460646      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:49.460504      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:50.460486      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:51.461542      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:52.461854      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:53.462247      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:54.464241      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:55.464282      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:56.465919      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:57.466437      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:58.467709      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:50:59.468028      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:00.468008      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:01.468223      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:02.469119      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:03.469230      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:04.470021      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:05.476870      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:06.470678      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:07.470569      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:08.470683      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:09.470874      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:10.471330      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:11.471515      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:12.472412      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:13.472561      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:14.473020      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:15.473548      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:16.473781      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:17.474643      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:18.475295      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:19.476299      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:20.476475      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:21.477330      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:22.477620      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:23.478159      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:24.479147      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:25.479712      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:26.479903      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:27.480235      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:28.480880      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:29.481233      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:30.481972      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:31.482330      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:32.483164      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:33.483455      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:34.483573      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:35.483926      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:36.484716      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:37.485408      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:38.486282      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:39.486463      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:40.486711      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:41.487322      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:42.488034      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:43.488431      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:44.489123      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:45.489547      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:46.490367      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:47.490913      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:48.491022      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:49.491261      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:50.491554      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:51.491737      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:52.491964      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:53.492125      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:54.492292      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:55.492539      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:56.492753      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:57.493142      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:58.493399      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:51:59.493616      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:00.494432      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:01.494312      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:02.494853      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:03.495069      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:04.495265      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:05.495515      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:06.495771      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:07.496701      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:08.496889      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:09.496940      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:10.497321      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:11.497491      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:12.498202      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:13.498851      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:14.498801      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:15.499875      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:16.500060      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:17.501105      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:18.501014      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:19.501174      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:20.501420      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:21.501617      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:22.501824      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:23.502352      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:24.503020      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:25.503242      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:26.504129      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:27.505187      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:28.505432      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:29.506468      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:30.506700      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:31.507237      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:32.508222      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:33.508611      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:34.508696      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:35.508666      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:36.508924      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:37.509017      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:38.509790      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:39.510047      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:40.510812      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:41.512140      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:42.512460      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:43.512722      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:44.513344      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:45.513205      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:46.513500      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:47.514163      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:48.515067      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:49.514923      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:50.515114      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:51.516426      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:52.516704      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:53.517741      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:54.517689      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:55.518930      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:56.519770      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:57.520609      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:58.521323      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:52:59.521674      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:00.523084      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:01.523020      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:02.523289      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:03.523672      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:04.524716      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:05.525851      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:06.527153      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:07.527102      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:08.527470      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:09.528217      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:10.528672      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:11.529876      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:12.530406      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:53:13.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 09:53:13.092
  STEP: Destroying namespace "container-probe-6117" for this suite. @ 04/15/24 09:53:13.126
• [243.422 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 04/15/24 09:53:13.15
  Apr 15 09:53:13.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename cronjob @ 04/15/24 09:53:13.157
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:53:13.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:53:13.197
  STEP: Creating a ReplaceConcurrent cronjob @ 04/15/24 09:53:13.203
  STEP: Ensuring a job is scheduled @ 04/15/24 09:53:13.23
  E0415 09:53:13.531798      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:14.531691      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:15.532579      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:16.532741      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:17.533437      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:18.534247      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:19.534139      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:20.534454      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:21.535430      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:22.536102      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:23.536443      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:24.536819      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:25.537915      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:26.538828      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:27.539325      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:28.540525      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:29.542037      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:30.542428      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:31.543763      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:32.544055      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:33.544430      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:34.544636      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:35.545611      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:36.545883      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:37.546724      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:38.546874      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:39.547362      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:40.548238      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:41.548659      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:42.549624      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:43.550168      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:44.550222      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:45.550464      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:46.551553      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:47.552403      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:48.552586      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:49.552706      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:50.553278      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:51.553890      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:52.554263      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:53.554803      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:54.555609      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:55.556537      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:56.556862      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:57.557105      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:58.557387      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:53:59.558220      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:00.558792      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 04/15/24 09:54:01.244
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 04/15/24 09:54:01.254
  STEP: Ensuring the job is replaced with a new one @ 04/15/24 09:54:01.265
  E0415 09:54:01.558784      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:02.558909      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:03.559798      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:04.560049      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:05.560870      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:06.561032      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:07.561765      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:08.561977      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:09.562709      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:10.563364      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:11.563791      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:12.564254      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:13.565297      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:14.566080      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:15.566474      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:16.566523      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:17.567458      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:18.568922      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:19.569450      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:20.570334      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:21.571783      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:22.572539      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:23.572938      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:24.574014      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:25.574298      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:26.575890      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:27.575646      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:28.576732      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:29.577482      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:30.577534      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:31.578313      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:32.579589      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:33.580403      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:34.580918      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:35.581936      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:36.582525      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:37.583724      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:38.583293      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:39.584244      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:40.584835      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:41.585645      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:42.585910      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:43.586514      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:44.586693      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:45.587779      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:46.588867      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:47.589307      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:48.589844      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:49.590844      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:50.591075      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:51.591603      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:52.591829      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:53.592225      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:54.592912      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:55.593039      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:56.594056      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:57.594295      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:58.594528      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:54:59.595827      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:00.595951      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 04/15/24 09:55:01.272
  Apr 15 09:55:01.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-939" for this suite. @ 04/15/24 09:55:01.306
• [108.180 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 04/15/24 09:55:01.342
  Apr 15 09:55:01.342: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename disruption @ 04/15/24 09:55:01.349
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:55:01.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:55:01.394
  STEP: creating the pdb @ 04/15/24 09:55:01.404
  STEP: Waiting for the pdb to be processed @ 04/15/24 09:55:01.419
  E0415 09:55:01.597010      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:02.597018      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 04/15/24 09:55:03.44
  STEP: Waiting for the pdb to be processed @ 04/15/24 09:55:03.462
  STEP: patching the pdb @ 04/15/24 09:55:03.472
  STEP: Waiting for the pdb to be processed @ 04/15/24 09:55:03.497
  E0415 09:55:03.598129      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:04.598844      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be deleted @ 04/15/24 09:55:05.532
  Apr 15 09:55:05.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-8944" for this suite. @ 04/15/24 09:55:05.552
• [4.226 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 04/15/24 09:55:05.574
  Apr 15 09:55:05.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename secrets @ 04/15/24 09:55:05.576
  E0415 09:55:05.598281      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:55:05.607
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:55:05.612
  STEP: Creating secret with name secret-test-2a305618-82d7-4bb9-b03c-b67798ee0eab @ 04/15/24 09:55:05.618
  STEP: Creating a pod to test consume secrets @ 04/15/24 09:55:05.632
  E0415 09:55:06.598813      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:07.598915      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:08.599121      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:09.599195      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 09:55:09.693
  Apr 15 09:55:09.700: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-secrets-5750d44c-b6bb-4b45-811c-284492bb5e8c container secret-env-test: <nil>
  STEP: delete the pod @ 04/15/24 09:55:09.74
  Apr 15 09:55:09.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3128" for this suite. @ 04/15/24 09:55:09.78
• [4.221 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 04/15/24 09:55:09.8
  Apr 15 09:55:09.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename replication-controller @ 04/15/24 09:55:09.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:55:09.83
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:55:09.837
  STEP: Creating replication controller my-hostname-basic-74701507-759d-4928-9bd0-4cbbf594bb46 @ 04/15/24 09:55:09.842
  Apr 15 09:55:09.862: INFO: Pod name my-hostname-basic-74701507-759d-4928-9bd0-4cbbf594bb46: Found 0 pods out of 1
  E0415 09:55:10.600004      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:11.601629      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:12.602320      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:13.602552      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:14.602736      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:55:14.877: INFO: Pod name my-hostname-basic-74701507-759d-4928-9bd0-4cbbf594bb46: Found 1 pods out of 1
  Apr 15 09:55:14.877: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-74701507-759d-4928-9bd0-4cbbf594bb46" are running
  Apr 15 09:55:14.889: INFO: Pod "my-hostname-basic-74701507-759d-4928-9bd0-4cbbf594bb46-9h6zb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-04-15 09:55:09 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-04-15 09:55:10 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-04-15 09:55:10 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-04-15 09:55:09 +0000 UTC Reason: Message:}])
  Apr 15 09:55:14.889: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 04/15/24 09:55:14.89
  Apr 15 09:55:14.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-5032" for this suite. @ 04/15/24 09:55:14.936
• [5.152 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 04/15/24 09:55:14.966
  Apr 15 09:55:14.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-runtime @ 04/15/24 09:55:14.968
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:55:15
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:55:15.006
  STEP: create the container @ 04/15/24 09:55:15.015
  W0415 09:55:15.037591      14 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 04/15/24 09:55:15.038
  E0415 09:55:15.603159      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:16.603337      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:17.603626      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 04/15/24 09:55:18.076
  STEP: the container should be terminated @ 04/15/24 09:55:18.082
  STEP: the termination message should be set @ 04/15/24 09:55:18.083
  Apr 15 09:55:18.083: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 04/15/24 09:55:18.084
  Apr 15 09:55:18.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5345" for this suite. @ 04/15/24 09:55:18.147
• [3.198 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 04/15/24 09:55:18.172
  Apr 15 09:55:18.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename watch @ 04/15/24 09:55:18.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:55:18.208
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:55:18.216
  STEP: creating a watch on configmaps @ 04/15/24 09:55:18.223
  STEP: creating a new configmap @ 04/15/24 09:55:18.226
  STEP: modifying the configmap once @ 04/15/24 09:55:18.239
  STEP: closing the watch once it receives two notifications @ 04/15/24 09:55:18.259
  Apr 15 09:55:18.259: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9779  bcd04908-4aa7-4bbc-82ee-331a183dfef2 212084 0 2024-04-15 09:55:18 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-04-15 09:55:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 09:55:18.260: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9779  bcd04908-4aa7-4bbc-82ee-331a183dfef2 212085 0 2024-04-15 09:55:18 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-04-15 09:55:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 04/15/24 09:55:18.261
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 04/15/24 09:55:18.285
  STEP: deleting the configmap @ 04/15/24 09:55:18.291
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 04/15/24 09:55:18.309
  Apr 15 09:55:18.309: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9779  bcd04908-4aa7-4bbc-82ee-331a183dfef2 212086 0 2024-04-15 09:55:18 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-04-15 09:55:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 09:55:18.310: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9779  bcd04908-4aa7-4bbc-82ee-331a183dfef2 212087 0 2024-04-15 09:55:18 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-04-15 09:55:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 09:55:18.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9779" for this suite. @ 04/15/24 09:55:18.323
• [0.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 04/15/24 09:55:18.355
  Apr 15 09:55:18.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:55:18.36
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:55:18.406
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:55:18.417
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 09:55:18.426
  E0415 09:55:18.603877      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:19.604654      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:20.605587      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:21.606653      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 09:55:22.493
  Apr 15 09:55:22.501: INFO: Trying to get logs from node ahz3daisheng-3 pod downwardapi-volume-bb4dba75-bc21-47fd-b65f-9f58488a5e0c container client-container: <nil>
  STEP: delete the pod @ 04/15/24 09:55:22.519
  Apr 15 09:55:22.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1535" for this suite. @ 04/15/24 09:55:22.564
• [4.222 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 04/15/24 09:55:22.58
  Apr 15 09:55:22.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename sysctl @ 04/15/24 09:55:22.584
  E0415 09:55:22.606898      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:55:22.614
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:55:22.622
  STEP: Creating a pod with one valid and two invalid sysctls @ 04/15/24 09:55:22.627
  Apr 15 09:55:22.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-2014" for this suite. @ 04/15/24 09:55:22.649
• [0.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 04/15/24 09:55:22.693
  Apr 15 09:55:22.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename configmap @ 04/15/24 09:55:22.695
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:55:22.726
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:55:22.731
  STEP: creating a ConfigMap @ 04/15/24 09:55:22.74
  STEP: fetching the ConfigMap @ 04/15/24 09:55:22.749
  STEP: patching the ConfigMap @ 04/15/24 09:55:22.754
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 04/15/24 09:55:22.765
  STEP: deleting the ConfigMap by collection with a label selector @ 04/15/24 09:55:22.776
  STEP: listing all ConfigMaps in test namespace @ 04/15/24 09:55:22.795
  Apr 15 09:55:22.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1199" for this suite. @ 04/15/24 09:55:22.82
• [0.142 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 04/15/24 09:55:22.836
  Apr 15 09:55:22.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename configmap @ 04/15/24 09:55:22.839
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:55:22.927
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:55:22.94
  STEP: Creating configMap with name configmap-test-volume-1347c68b-41ea-48a5-bc08-91ccae925def @ 04/15/24 09:55:22.946
  STEP: Creating a pod to test consume configMaps @ 04/15/24 09:55:22.958
  E0415 09:55:23.610863      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:24.610716      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:25.610988      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:26.611244      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 09:55:27.014
  Apr 15 09:55:27.021: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-configmaps-cd6a827d-b48c-40d9-af39-cc3fe7e8a027 container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 09:55:27.034
  Apr 15 09:55:27.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5822" for this suite. @ 04/15/24 09:55:27.08
• [4.257 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 04/15/24 09:55:27.102
  Apr 15 09:55:27.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubelet-test @ 04/15/24 09:55:27.105
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:55:27.137
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:55:27.147
  Apr 15 09:55:27.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-7358" for this suite. @ 04/15/24 09:55:27.238
• [0.154 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 04/15/24 09:55:27.267
  Apr 15 09:55:27.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename services @ 04/15/24 09:55:27.271
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:55:27.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:55:27.34
  STEP: creating service nodeport-test with type=NodePort in namespace services-6528 @ 04/15/24 09:55:27.346
  STEP: creating replication controller nodeport-test in namespace services-6528 @ 04/15/24 09:55:27.381
  I0415 09:55:27.407530      14 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-6528, replica count: 2
  E0415 09:55:27.612032      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:28.612882      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:29.613202      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0415 09:55:30.458036      14 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 15 09:55:30.458: INFO: Creating new exec pod
  E0415 09:55:30.614120      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:31.614472      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:32.614957      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:55:33.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-6528 exec execpodv6j82 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  E0415 09:55:33.614958      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:55:33.887: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Apr 15 09:55:33.887: INFO: stdout: "nodeport-test-jlgc4"
  Apr 15 09:55:33.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-6528 exec execpodv6j82 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.19.170 80'
  Apr 15 09:55:34.187: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.19.170 80\nConnection to 10.233.19.170 80 port [tcp/http] succeeded!\n"
  Apr 15 09:55:34.187: INFO: stdout: "nodeport-test-jlgc4"
  Apr 15 09:55:34.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-6528 exec execpodv6j82 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.199 31922'
  Apr 15 09:55:34.443: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.199 31922\nConnection to 192.168.121.199 31922 port [tcp/*] succeeded!\n"
  Apr 15 09:55:34.443: INFO: stdout: "nodeport-test-4mg99"
  Apr 15 09:55:34.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-6528 exec execpodv6j82 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.120 31922'
  E0415 09:55:34.615633      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:55:34.709: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.120 31922\nConnection to 192.168.121.120 31922 port [tcp/*] succeeded!\n"
  Apr 15 09:55:34.709: INFO: stdout: "nodeport-test-jlgc4"
  Apr 15 09:55:34.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6528" for this suite. @ 04/15/24 09:55:34.722
• [7.472 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 04/15/24 09:55:34.739
  Apr 15 09:55:34.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename deployment @ 04/15/24 09:55:34.744
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:55:34.77
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:55:34.779
  Apr 15 09:55:34.786: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Apr 15 09:55:34.805: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0415 09:55:35.616804      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:36.617097      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:37.617480      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:38.617892      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:39.618942      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:55:39.816: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/15/24 09:55:39.816
  Apr 15 09:55:39.817: INFO: Creating deployment "test-rolling-update-deployment"
  Apr 15 09:55:39.829: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Apr 15 09:55:39.861: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0415 09:55:40.619981      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:41.620802      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:55:41.873: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Apr 15 09:55:41.879: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Apr 15 09:55:41.903: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3632  c12408e1-6880-4a39-8db1-04cf1716f024 212370 1 2024-04-15 09:55:39 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2024-04-15 09:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 09:55:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.47 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005c5db38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-04-15 09:55:39 +0000 UTC,LastTransitionTime:2024-04-15 09:55:39 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67b9f48bb4" has successfully progressed.,LastUpdateTime:2024-04-15 09:55:41 +0000 UTC,LastTransitionTime:2024-04-15 09:55:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 15 09:55:41.925: INFO: New ReplicaSet "test-rolling-update-deployment-67b9f48bb4" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67b9f48bb4  deployment-3632  6903bad1-9479-409e-bdf7-f0a5a9a547f6 212360 1 2024-04-15 09:55:39 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67b9f48bb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment c12408e1-6880-4a39-8db1-04cf1716f024 0xc0041c2e77 0xc0041c2e78}] [] [{kube-controller-manager Update apps/v1 2024-04-15 09:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c12408e1-6880-4a39-8db1-04cf1716f024\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 09:55:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67b9f48bb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67b9f48bb4] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.47 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041c2f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 09:55:41.925: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Apr 15 09:55:41.926: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3632  681fe044-9d13-465e-a81c-e53cd86c7369 212368 2 2024-04-15 09:55:34 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment c12408e1-6880-4a39-8db1-04cf1716f024 0xc0041c2d47 0xc0041c2d48}] [] [{e2e.test Update apps/v1 2024-04-15 09:55:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 09:55:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c12408e1-6880-4a39-8db1-04cf1716f024\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2024-04-15 09:55:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0041c2e08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 09:55:41.941: INFO: Pod "test-rolling-update-deployment-67b9f48bb4-2tndw" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-67b9f48bb4-2tndw test-rolling-update-deployment-67b9f48bb4- deployment-3632  c5988a80-f1b4-4634-9958-4412d841bbb1 212358 0 2024-04-15 09:55:39 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67b9f48bb4] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-67b9f48bb4 6903bad1-9479-409e-bdf7-f0a5a9a547f6 0xc005c5df47 0xc005c5df48}] [] [{kube-controller-manager Update v1 2024-04-15 09:55:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6903bad1-9479-409e-bdf7-f0a5a9a547f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 09:55:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.67\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-52dtr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.47,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52dtr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 09:55:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 09:55:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 09:55:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 09:55:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.199,PodIP:10.233.66.67,StartTime:2024-04-15 09:55:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 09:55:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.47,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:c9997bf8d2e223d7d2a0078dcfb11a653e9b16cf09418829ec03e1d57ca9628a,ContainerID:cri-o://b6f34a9b998482341074b7af75fa6a765b1852a54cac97380517733a47297348,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.67,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 09:55:41.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3632" for this suite. @ 04/15/24 09:55:41.956
• [7.232 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 04/15/24 09:55:41.977
  Apr 15 09:55:41.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename services @ 04/15/24 09:55:41.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:55:42.023
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:55:42.032
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8496 @ 04/15/24 09:55:42.037
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 04/15/24 09:55:42.08
  STEP: creating service externalsvc in namespace services-8496 @ 04/15/24 09:55:42.081
  STEP: creating replication controller externalsvc in namespace services-8496 @ 04/15/24 09:55:42.129
  I0415 09:55:42.148231      14 runners.go:194] Created replication controller with name: externalsvc, namespace: services-8496, replica count: 2
  E0415 09:55:42.621786      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:43.622837      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:44.623309      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0415 09:55:45.203055      14 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 04/15/24 09:55:45.209
  Apr 15 09:55:45.246: INFO: Creating new exec pod
  E0415 09:55:45.623942      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:46.624233      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:55:47.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=services-8496 exec execpoddqjd2 -- /bin/sh -x -c nslookup clusterip-service.services-8496.svc.cluster.local'
  E0415 09:55:47.624742      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:55:47.850: INFO: stderr: "+ nslookup clusterip-service.services-8496.svc.cluster.local\n"
  Apr 15 09:55:47.850: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-8496.svc.cluster.local\tcanonical name = externalsvc.services-8496.svc.cluster.local.\nName:\texternalsvc.services-8496.svc.cluster.local\nAddress: 10.233.50.72\n\n"
  Apr 15 09:55:47.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-8496, will wait for the garbage collector to delete the pods @ 04/15/24 09:55:47.863
  Apr 15 09:55:47.937: INFO: Deleting ReplicationController externalsvc took: 15.249045ms
  Apr 15 09:55:48.038: INFO: Terminating ReplicationController externalsvc pods took: 101.013656ms
  E0415 09:55:48.625156      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:49.626268      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:55:50.215: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-8496" for this suite. @ 04/15/24 09:55:50.252
• [8.291 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 04/15/24 09:55:50.282
  Apr 15 09:55:50.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename job @ 04/15/24 09:55:50.288
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:55:50.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:55:50.332
  STEP: Creating a job @ 04/15/24 09:55:50.344
  STEP: Ensure pods equal to parallelism count is attached to the job @ 04/15/24 09:55:50.358
  E0415 09:55:50.627202      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:51.628259      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 04/15/24 09:55:52.375
  STEP: updating /status @ 04/15/24 09:55:52.397
  STEP: get /status @ 04/15/24 09:55:52.45
  Apr 15 09:55:52.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1087" for this suite. @ 04/15/24 09:55:52.479
• [2.215 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 04/15/24 09:55:52.51
  Apr 15 09:55:52.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename pods @ 04/15/24 09:55:52.515
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:55:52.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:55:52.548
  STEP: creating the pod @ 04/15/24 09:55:52.554
  STEP: submitting the pod to kubernetes @ 04/15/24 09:55:52.555
  E0415 09:55:52.629759      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:53.628869      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 04/15/24 09:55:54.6
  STEP: updating the pod @ 04/15/24 09:55:54.611
  E0415 09:55:54.630043      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:55:55.140: INFO: Successfully updated pod "pod-update-2aea4d2b-3969-4caa-896f-a28c4429a741"
  STEP: verifying the updated pod is in kubernetes @ 04/15/24 09:55:55.148
  Apr 15 09:55:55.155: INFO: Pod update OK
  Apr 15 09:55:55.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6175" for this suite. @ 04/15/24 09:55:55.166
• [2.668 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 04/15/24 09:55:55.179
  Apr 15 09:55:55.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename container-probe @ 04/15/24 09:55:55.184
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:55:55.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:55:55.224
  E0415 09:55:55.630705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:56.634704      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:57.633141      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:58.633491      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:55:59.633634      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:00.634041      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:01.634193      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:02.636850      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:03.636646      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:04.636815      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:05.637469      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:06.637474      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:07.638341      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:08.638622      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:09.638915      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:10.639782      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:11.640448      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:12.641454      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:13.641647      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:14.641698      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:15.642513      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:16.643375      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:56:17.415: INFO: Container started at 2024-04-15 09:55:55 +0000 UTC, pod became ready at 2024-04-15 09:56:15 +0000 UTC
  Apr 15 09:56:17.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-1995" for this suite. @ 04/15/24 09:56:17.44
• [22.283 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 04/15/24 09:56:17.469
  Apr 15 09:56:17.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename init-container @ 04/15/24 09:56:17.477
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:56:17.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:56:17.537
  STEP: creating the pod @ 04/15/24 09:56:17.549
  Apr 15 09:56:17.550: INFO: PodSpec: initContainers in spec.initContainers
  E0415 09:56:17.643903      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:18.644055      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:19.644833      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:20.644853      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:21.645331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:56:22.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-4450" for this suite. @ 04/15/24 09:56:22.361
• [4.908 seconds]
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 04/15/24 09:56:22.378
  Apr 15 09:56:22.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename subpath @ 04/15/24 09:56:22.383
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:56:22.421
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:56:22.432
  STEP: Setting up data @ 04/15/24 09:56:22.439
  STEP: Creating pod pod-subpath-test-secret-frnz @ 04/15/24 09:56:22.469
  STEP: Creating a pod to test atomic-volume-subpath @ 04/15/24 09:56:22.471
  E0415 09:56:22.645898      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:23.646292      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:24.646730      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:25.647065      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:26.646915      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:27.647357      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:28.648334      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:29.648747      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:30.649165      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:31.650017      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:32.650895      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:33.650944      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:34.651401      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:35.652545      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:36.652417      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:37.653091      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:38.653217      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:39.653995      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:40.655144      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:41.655480      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:42.655554      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:43.656307      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:44.657170      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:45.657577      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:46.658354      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 09:56:46.693
  Apr 15 09:56:46.700: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-subpath-test-secret-frnz container test-container-subpath-secret-frnz: <nil>
  STEP: delete the pod @ 04/15/24 09:56:46.724
  STEP: Deleting pod pod-subpath-test-secret-frnz @ 04/15/24 09:56:46.778
  Apr 15 09:56:46.779: INFO: Deleting pod "pod-subpath-test-secret-frnz" in namespace "subpath-5913"
  Apr 15 09:56:46.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5913" for this suite. @ 04/15/24 09:56:46.81
• [24.450 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 04/15/24 09:56:46.837
  Apr 15 09:56:46.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename namespaces @ 04/15/24 09:56:46.841
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:56:46.905
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:56:46.913
  STEP: Updating Namespace "namespaces-7251" @ 04/15/24 09:56:46.924
  Apr 15 09:56:46.948: INFO: Namespace "namespaces-7251" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"b296a868-5d29-45ef-ab0f-9de0eeb55529", "kubernetes.io/metadata.name":"namespaces-7251", "namespaces-7251":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Apr 15 09:56:46.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7251" for this suite. @ 04/15/24 09:56:46.961
• [0.140 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 04/15/24 09:56:46.978
  Apr 15 09:56:46.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename pod-network-test @ 04/15/24 09:56:46.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:56:47.006
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:56:47.013
  STEP: Performing setup for networking test in namespace pod-network-test-9594 @ 04/15/24 09:56:47.018
  STEP: creating a selector @ 04/15/24 09:56:47.019
  STEP: Creating the service pods in kubernetes @ 04/15/24 09:56:47.019
  Apr 15 09:56:47.019: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0415 09:56:47.658745      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:48.659184      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:49.659311      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:50.659366      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:51.659715      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:52.659992      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:53.660549      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:54.661037      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:55.661891      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:56.662082      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:57.662467      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:56:58.662975      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 04/15/24 09:56:59.335
  E0415 09:56:59.663780      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:00.664288      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:57:01.376: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Apr 15 09:57:01.376: INFO: Breadth first check of 10.233.64.134 on host 192.168.121.96...
  Apr 15 09:57:01.382: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.77:9080/dial?request=hostname&protocol=udp&host=10.233.64.134&port=8081&tries=1'] Namespace:pod-network-test-9594 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:57:01.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:57:01.384: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:57:01.385: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9594/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.77%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.134%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 15 09:57:01.579: INFO: Waiting for responses: map[]
  Apr 15 09:57:01.579: INFO: reached 10.233.64.134 after 0/1 tries
  Apr 15 09:57:01.579: INFO: Breadth first check of 10.233.65.173 on host 192.168.121.120...
  Apr 15 09:57:01.587: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.77:9080/dial?request=hostname&protocol=udp&host=10.233.65.173&port=8081&tries=1'] Namespace:pod-network-test-9594 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:57:01.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:57:01.589: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:57:01.589: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9594/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.77%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.65.173%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  E0415 09:57:01.665118      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:57:01.723: INFO: Waiting for responses: map[]
  Apr 15 09:57:01.723: INFO: reached 10.233.65.173 after 0/1 tries
  Apr 15 09:57:01.723: INFO: Breadth first check of 10.233.66.76 on host 192.168.121.199...
  Apr 15 09:57:01.731: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.77:9080/dial?request=hostname&protocol=udp&host=10.233.66.76&port=8081&tries=1'] Namespace:pod-network-test-9594 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:57:01.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:57:01.735: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:57:01.736: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9594/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.77%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.76%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Apr 15 09:57:01.850: INFO: Waiting for responses: map[]
  Apr 15 09:57:01.851: INFO: reached 10.233.66.76 after 0/1 tries
  Apr 15 09:57:01.851: INFO: Going to retry 0 out of 3 pods....
  Apr 15 09:57:01.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-9594" for this suite. @ 04/15/24 09:57:01.879
• [14.919 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 04/15/24 09:57:01.898
  Apr 15 09:57:01.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:57:01.901
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:57:01.938
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:57:01.946
  STEP: Creating the pod @ 04/15/24 09:57:01.951
  E0415 09:57:02.665999      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:03.666968      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:57:04.568: INFO: Successfully updated pod "labelsupdateaae1a20d-60e3-4a5b-9b0e-a832c87fd20d"
  E0415 09:57:04.673665      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:05.674008      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:57:06.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4618" for this suite. @ 04/15/24 09:57:06.607
• [4.724 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 04/15/24 09:57:06.63
  Apr 15 09:57:06.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:57:06.635
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:57:06.668
  E0415 09:57:06.673830      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:57:06.674
  STEP: Creating secret with name projected-secret-test-64ba5b16-e74a-4ebd-b08f-7872b01ce338 @ 04/15/24 09:57:06.681
  STEP: Creating a pod to test consume secrets @ 04/15/24 09:57:06.69
  E0415 09:57:07.674500      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:08.675258      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:09.675842      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:10.676594      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 09:57:10.735
  Apr 15 09:57:10.744: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-projected-secrets-be9937fb-dbe6-4585-b41d-c44d1bbe9bfc container secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 09:57:10.762
  Apr 15 09:57:10.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4097" for this suite. @ 04/15/24 09:57:10.821
• [4.205 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:622
  STEP: Creating a kubernetes client @ 04/15/24 09:57:10.836
  Apr 15 09:57:10.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename field-validation @ 04/15/24 09:57:10.841
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:57:10.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:57:10.908
  Apr 15 09:57:10.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  E0415 09:57:11.677245      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:12.677762      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:13.677956      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0415 09:57:13.707933      14 warnings.go:70] unknown field "alpha"
  W0415 09:57:13.707998      14 warnings.go:70] unknown field "beta"
  W0415 09:57:13.708011      14 warnings.go:70] unknown field "delta"
  W0415 09:57:13.708033      14 warnings.go:70] unknown field "epsilon"
  W0415 09:57:13.708047      14 warnings.go:70] unknown field "gamma"
  Apr 15 09:57:14.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4982" for this suite. @ 04/15/24 09:57:14.392
• [3.573 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 04/15/24 09:57:14.412
  Apr 15 09:57:14.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 09:57:14.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:57:14.453
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:57:14.461
  STEP: Creating Pod @ 04/15/24 09:57:14.466
  E0415 09:57:14.678681      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:15.678999      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 04/15/24 09:57:16.504
  Apr 15 09:57:16.504: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5938 PodName:pod-sharedvolume-9383717e-0ce1-49f7-b17f-bed236af8af4 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:57:16.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:57:16.507: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:57:16.507: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-5938/pods/pod-sharedvolume-9383717e-0ce1-49f7-b17f-bed236af8af4/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Apr 15 09:57:16.638: INFO: Exec stderr: ""
  Apr 15 09:57:16.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0415 09:57:16.679848      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "emptydir-5938" for this suite. @ 04/15/24 09:57:16.683
• [2.313 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 04/15/24 09:57:16.727
  Apr 15 09:57:16.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename deployment @ 04/15/24 09:57:16.729
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:57:16.777
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:57:16.783
  Apr 15 09:57:16.831: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0415 09:57:17.680532      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:18.680849      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:19.681769      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:20.681950      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:21.681929      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:57:21.851: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/15/24 09:57:21.852
  Apr 15 09:57:21.852: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 04/15/24 09:57:21.886
  E0415 09:57:22.681952      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:23.682919      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:57:23.960: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-402  2344b86d-b7b7-4602-a2e7-5879ef6e3d6c 213087 1 2024-04-15 09:57:21 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2024-04-15 09:57:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 09:57:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.47 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004700ce8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-04-15 09:57:21 +0000 UTC,LastTransitionTime:2024-04-15 09:57:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-7c495bfbdb" has successfully progressed.,LastUpdateTime:2024-04-15 09:57:22 +0000 UTC,LastTransitionTime:2024-04-15 09:57:21 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Apr 15 09:57:23.968: INFO: New ReplicaSet "test-cleanup-deployment-7c495bfbdb" of Deployment "test-cleanup-deployment":
  &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7c495bfbdb  deployment-402  ffd511fd-5bb2-42e4-bb93-154374292eb2 213077 1 2024-04-15 09:57:21 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7c495bfbdb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 2344b86d-b7b7-4602-a2e7-5879ef6e3d6c 0xc006221927 0xc006221928}] [] [{kube-controller-manager Update apps/v1 2024-04-15 09:57:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2344b86d-b7b7-4602-a2e7-5879ef6e3d6c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-04-15 09:57:22 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7c495bfbdb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7c495bfbdb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.47 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006221b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Apr 15 09:57:23.978: INFO: Pod "test-cleanup-deployment-7c495bfbdb-n6hbm" is available:
  &Pod{ObjectMeta:{test-cleanup-deployment-7c495bfbdb-n6hbm test-cleanup-deployment-7c495bfbdb- deployment-402  af8a529b-33cb-4c09-bd22-fc659de86328 213076 0 2024-04-15 09:57:21 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7c495bfbdb] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7c495bfbdb ffd511fd-5bb2-42e4-bb93-154374292eb2 0xc006221f27 0xc006221f28}] [] [{kube-controller-manager Update v1 2024-04-15 09:57:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ffd511fd-5bb2-42e4-bb93-154374292eb2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-04-15 09:57:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lsts4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.47,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lsts4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ahz3daisheng-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 09:57:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 09:57:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 09:57:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-04-15 09:57:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.199,PodIP:10.233.66.82,StartTime:2024-04-15 09:57:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-04-15 09:57:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.47,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:c9997bf8d2e223d7d2a0078dcfb11a653e9b16cf09418829ec03e1d57ca9628a,ContainerID:cri-o://cce2473a4145bee481b2a2f0c3d67c51eccbc118b12573e2c9e607a60c91cece,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.82,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Apr 15 09:57:23.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-402" for this suite. @ 04/15/24 09:57:23.988
• [7.276 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 04/15/24 09:57:24.006
  Apr 15 09:57:24.006: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename discovery @ 04/15/24 09:57:24.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:57:24.039
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:57:24.045
  STEP: Setting up server cert @ 04/15/24 09:57:24.052
  Apr 15 09:57:24.625: INFO: Checking APIGroup: apiregistration.k8s.io
  Apr 15 09:57:24.628: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Apr 15 09:57:24.628: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Apr 15 09:57:24.628: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Apr 15 09:57:24.628: INFO: Checking APIGroup: apps
  Apr 15 09:57:24.630: INFO: PreferredVersion.GroupVersion: apps/v1
  Apr 15 09:57:24.631: INFO: Versions found [{apps/v1 v1}]
  Apr 15 09:57:24.631: INFO: apps/v1 matches apps/v1
  Apr 15 09:57:24.631: INFO: Checking APIGroup: events.k8s.io
  Apr 15 09:57:24.633: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Apr 15 09:57:24.634: INFO: Versions found [{events.k8s.io/v1 v1}]
  Apr 15 09:57:24.634: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Apr 15 09:57:24.634: INFO: Checking APIGroup: authentication.k8s.io
  Apr 15 09:57:24.636: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Apr 15 09:57:24.636: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Apr 15 09:57:24.636: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Apr 15 09:57:24.636: INFO: Checking APIGroup: authorization.k8s.io
  Apr 15 09:57:24.638: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Apr 15 09:57:24.638: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Apr 15 09:57:24.638: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Apr 15 09:57:24.638: INFO: Checking APIGroup: autoscaling
  Apr 15 09:57:24.641: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Apr 15 09:57:24.641: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Apr 15 09:57:24.641: INFO: autoscaling/v2 matches autoscaling/v2
  Apr 15 09:57:24.642: INFO: Checking APIGroup: batch
  Apr 15 09:57:24.644: INFO: PreferredVersion.GroupVersion: batch/v1
  Apr 15 09:57:24.645: INFO: Versions found [{batch/v1 v1}]
  Apr 15 09:57:24.645: INFO: batch/v1 matches batch/v1
  Apr 15 09:57:24.645: INFO: Checking APIGroup: certificates.k8s.io
  Apr 15 09:57:24.647: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Apr 15 09:57:24.647: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Apr 15 09:57:24.647: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Apr 15 09:57:24.647: INFO: Checking APIGroup: networking.k8s.io
  Apr 15 09:57:24.649: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Apr 15 09:57:24.649: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Apr 15 09:57:24.649: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Apr 15 09:57:24.649: INFO: Checking APIGroup: policy
  Apr 15 09:57:24.651: INFO: PreferredVersion.GroupVersion: policy/v1
  Apr 15 09:57:24.652: INFO: Versions found [{policy/v1 v1}]
  Apr 15 09:57:24.652: INFO: policy/v1 matches policy/v1
  Apr 15 09:57:24.653: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Apr 15 09:57:24.655: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Apr 15 09:57:24.655: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Apr 15 09:57:24.656: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Apr 15 09:57:24.656: INFO: Checking APIGroup: storage.k8s.io
  Apr 15 09:57:24.660: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Apr 15 09:57:24.660: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Apr 15 09:57:24.660: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Apr 15 09:57:24.660: INFO: Checking APIGroup: admissionregistration.k8s.io
  Apr 15 09:57:24.663: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Apr 15 09:57:24.663: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Apr 15 09:57:24.663: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Apr 15 09:57:24.663: INFO: Checking APIGroup: apiextensions.k8s.io
  Apr 15 09:57:24.666: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Apr 15 09:57:24.666: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Apr 15 09:57:24.667: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Apr 15 09:57:24.667: INFO: Checking APIGroup: scheduling.k8s.io
  Apr 15 09:57:24.670: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Apr 15 09:57:24.670: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Apr 15 09:57:24.670: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Apr 15 09:57:24.670: INFO: Checking APIGroup: coordination.k8s.io
  Apr 15 09:57:24.672: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Apr 15 09:57:24.672: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Apr 15 09:57:24.672: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Apr 15 09:57:24.672: INFO: Checking APIGroup: node.k8s.io
  Apr 15 09:57:24.675: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Apr 15 09:57:24.675: INFO: Versions found [{node.k8s.io/v1 v1}]
  Apr 15 09:57:24.675: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Apr 15 09:57:24.675: INFO: Checking APIGroup: discovery.k8s.io
  Apr 15 09:57:24.677: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Apr 15 09:57:24.677: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Apr 15 09:57:24.677: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Apr 15 09:57:24.677: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Apr 15 09:57:24.679: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Apr 15 09:57:24.679: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Apr 15 09:57:24.679: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Apr 15 09:57:24.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0415 09:57:24.683409      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "discovery-7011" for this suite. @ 04/15/24 09:57:24.688
• [0.698 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 04/15/24 09:57:24.707
  Apr 15 09:57:24.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename replicaset @ 04/15/24 09:57:24.71
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:57:24.768
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:57:24.773
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 04/15/24 09:57:24.779
  Apr 15 09:57:24.800: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0415 09:57:25.684435      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:26.684825      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:27.685788      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:28.685281      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:29.685621      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:57:29.810: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 04/15/24 09:57:29.811
  STEP: getting scale subresource @ 04/15/24 09:57:29.811
  STEP: updating a scale subresource @ 04/15/24 09:57:29.82
  STEP: verifying the replicaset Spec.Replicas was modified @ 04/15/24 09:57:29.838
  STEP: Patch a scale subresource @ 04/15/24 09:57:29.848
  Apr 15 09:57:29.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1610" for this suite. @ 04/15/24 09:57:29.919
• [5.229 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 04/15/24 09:57:29.939
  Apr 15 09:57:29.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename webhook @ 04/15/24 09:57:29.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:57:29.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:57:29.975
  STEP: Setting up server cert @ 04/15/24 09:57:30.016
  E0415 09:57:30.688138      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 04/15/24 09:57:30.701
  STEP: Deploying the webhook pod @ 04/15/24 09:57:30.725
  STEP: Wait for the deployment to be ready @ 04/15/24 09:57:30.767
  Apr 15 09:57:30.818: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0415 09:57:31.687684      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:32.687659      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 04/15/24 09:57:32.842
  STEP: Verifying the service has paired with the endpoint @ 04/15/24 09:57:32.866
  E0415 09:57:33.688094      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:57:33.867: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 04/15/24 09:57:33.876
  STEP: create a pod that should be updated by the webhook @ 04/15/24 09:57:33.92
  Apr 15 09:57:33.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2007" for this suite. @ 04/15/24 09:57:34.082
  STEP: Destroying namespace "webhook-markers-1194" for this suite. @ 04/15/24 09:57:34.109
• [4.189 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 04/15/24 09:57:34.132
  Apr 15 09:57:34.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename field-validation @ 04/15/24 09:57:34.136
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:57:34.162
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:57:34.17
  Apr 15 09:57:34.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  E0415 09:57:34.688770      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:35.688423      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:36.689099      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0415 09:57:36.992129      14 warnings.go:70] unknown field "alpha"
  W0415 09:57:36.992213      14 warnings.go:70] unknown field "beta"
  W0415 09:57:36.992312      14 warnings.go:70] unknown field "delta"
  W0415 09:57:36.992400      14 warnings.go:70] unknown field "epsilon"
  W0415 09:57:36.992465      14 warnings.go:70] unknown field "gamma"
  Apr 15 09:57:37.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8147" for this suite. @ 04/15/24 09:57:37.615
• [3.497 seconds]
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 04/15/24 09:57:37.63
  Apr 15 09:57:37.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename dns @ 04/15/24 09:57:37.633
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:57:37.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:57:37.677
  STEP: Creating a test headless service @ 04/15/24 09:57:37.683
  E0415 09:57:37.690103      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9473.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9473.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 04/15/24 09:57:37.696
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9473.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9473.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 04/15/24 09:57:37.696
  STEP: creating a pod to probe DNS @ 04/15/24 09:57:37.697
  STEP: submitting the pod to kubernetes @ 04/15/24 09:57:37.697
  E0415 09:57:38.691282      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:39.691539      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/15/24 09:57:39.746
  STEP: looking for the results for each expected name from probers @ 04/15/24 09:57:39.762
  Apr 15 09:57:39.820: INFO: Unable to read jessie_hosts@dns-querier-2 from pod dns-9473/dns-test-27eab349-ac06-4300-930e-6ddb0953cf21: the server could not find the requested resource (get pods dns-test-27eab349-ac06-4300-930e-6ddb0953cf21)
  Apr 15 09:57:39.820: INFO: Lookups using dns-9473/dns-test-27eab349-ac06-4300-930e-6ddb0953cf21 failed for: [jessie_hosts@dns-querier-2]

  E0415 09:57:40.691873      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:41.692286      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:42.693039      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:43.692730      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:44.693335      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:57:44.875: INFO: DNS probes using dns-9473/dns-test-27eab349-ac06-4300-930e-6ddb0953cf21 succeeded

  Apr 15 09:57:44.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 09:57:44.897
  STEP: deleting the test headless service @ 04/15/24 09:57:44.946
  STEP: Destroying namespace "dns-9473" for this suite. @ 04/15/24 09:57:44.991
• [7.384 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 04/15/24 09:57:45.019
  Apr 15 09:57:45.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename crd-watch @ 04/15/24 09:57:45.025
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:57:45.071
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:57:45.077
  Apr 15 09:57:45.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  E0415 09:57:45.693649      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:46.694568      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:47.694507      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 04/15/24 09:57:47.757
  Apr 15 09:57:47.770: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-04-15T09:57:47Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-04-15T09:57:47Z]] name:name1 resourceVersion:213341 uid:efd8cb86-7f8c-4fd4-828b-3fc7c1b8ac06] num:map[num1:9223372036854775807 num2:1000000]]}
  E0415 09:57:48.695149      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:49.695343      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:50.696176      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:51.696706      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:52.696888      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:53.697903      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:54.698289      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:55.698450      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:56.698697      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:57.699253      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 04/15/24 09:57:57.771
  Apr 15 09:57:57.784: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-04-15T09:57:57Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-04-15T09:57:57Z]] name:name2 resourceVersion:213375 uid:7e908768-15ab-4441-bc1e-9e282985a93e] num:map[num1:9223372036854775807 num2:1000000]]}
  E0415 09:57:58.699985      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:57:59.700753      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:00.700178      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:01.702872      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:02.702772      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:03.703059      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:04.703826      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:05.704107      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:06.704301      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:07.705341      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 04/15/24 09:58:07.785
  Apr 15 09:58:07.805: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-04-15T09:57:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-04-15T09:58:07Z]] name:name1 resourceVersion:213392 uid:efd8cb86-7f8c-4fd4-828b-3fc7c1b8ac06] num:map[num1:9223372036854775807 num2:1000000]]}
  E0415 09:58:08.705714      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:09.706357      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:10.706680      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:11.707383      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:12.707683      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:13.708142      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:14.708262      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:15.708468      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:16.708967      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:17.709199      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 04/15/24 09:58:17.806
  Apr 15 09:58:17.826: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-04-15T09:57:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-04-15T09:58:17Z]] name:name2 resourceVersion:213410 uid:7e908768-15ab-4441-bc1e-9e282985a93e] num:map[num1:9223372036854775807 num2:1000000]]}
  E0415 09:58:18.709395      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:19.710418      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:20.710639      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:21.710746      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:22.711016      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:23.711270      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:24.712142      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:25.712771      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:26.717373      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:27.717958      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 04/15/24 09:58:27.827
  Apr 15 09:58:27.848: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-04-15T09:57:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-04-15T09:58:07Z]] name:name1 resourceVersion:213429 uid:efd8cb86-7f8c-4fd4-828b-3fc7c1b8ac06] num:map[num1:9223372036854775807 num2:1000000]]}
  E0415 09:58:28.718184      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:29.718380      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:30.719434      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:31.719562      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:32.719723      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:33.720242      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:34.720493      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:35.721006      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:36.721191      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:37.722123      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 04/15/24 09:58:37.849
  Apr 15 09:58:37.866: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-04-15T09:57:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-04-15T09:58:17Z]] name:name2 resourceVersion:213447 uid:7e908768-15ab-4441-bc1e-9e282985a93e] num:map[num1:9223372036854775807 num2:1000000]]}
  E0415 09:58:38.722322      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:39.722787      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:40.723820      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:41.724032      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:42.724197      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:43.724784      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:44.725013      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:45.725400      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:46.725915      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:47.726723      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:58:48.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-3010" for this suite. @ 04/15/24 09:58:48.431
• [63.430 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 04/15/24 09:58:48.455
  Apr 15 09:58:48.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename dns @ 04/15/24 09:58:48.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:58:48.5
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:58:48.506
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 04/15/24 09:58:48.513
  Apr 15 09:58:48.530: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3142  2a8b7a3a-836b-496b-bf3d-f9104ac32975 213475 0 2024-04-15 09:58:48 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2024-04-15 09:58:48 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c27n8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.47,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c27n8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0415 09:58:48.729830      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:49.728559      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 04/15/24 09:58:50.55
  Apr 15 09:58:50.550: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3142 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:58:50.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:58:50.554: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:58:50.555: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-3142/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0415 09:58:50.732226      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS server is configured on pod... @ 04/15/24 09:58:50.784
  Apr 15 09:58:50.785: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3142 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Apr 15 09:58:50.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 09:58:50.787: INFO: ExecWithOptions: Clientset creation
  Apr 15 09:58:50.788: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-3142/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Apr 15 09:58:50.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 09:58:50.972: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-3142" for this suite. @ 04/15/24 09:58:50.994
• [2.554 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 04/15/24 09:58:51.011
  Apr 15 09:58:51.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/15/24 09:58:51.014
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:58:51.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:58:51.054
  Apr 15 09:58:51.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  E0415 09:58:51.733836      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:52.734096      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 04/15/24 09:58:53.345
  Apr 15 09:58:53.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-5659 --namespace=crd-publish-openapi-5659 create -f -'
  E0415 09:58:53.734934      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:54.735527      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:58:55.070: INFO: stderr: ""
  Apr 15 09:58:55.071: INFO: stdout: "e2e-test-crd-publish-openapi-8464-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Apr 15 09:58:55.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-5659 --namespace=crd-publish-openapi-5659 delete e2e-test-crd-publish-openapi-8464-crds test-cr'
  Apr 15 09:58:55.275: INFO: stderr: ""
  Apr 15 09:58:55.275: INFO: stdout: "e2e-test-crd-publish-openapi-8464-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Apr 15 09:58:55.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-5659 --namespace=crd-publish-openapi-5659 apply -f -'
  E0415 09:58:55.735507      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:58:55.890: INFO: stderr: ""
  Apr 15 09:58:55.890: INFO: stdout: "e2e-test-crd-publish-openapi-8464-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Apr 15 09:58:55.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-5659 --namespace=crd-publish-openapi-5659 delete e2e-test-crd-publish-openapi-8464-crds test-cr'
  Apr 15 09:58:56.058: INFO: stderr: ""
  Apr 15 09:58:56.058: INFO: stdout: "e2e-test-crd-publish-openapi-8464-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 04/15/24 09:58:56.058
  Apr 15 09:58:56.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=crd-publish-openapi-5659 explain e2e-test-crd-publish-openapi-8464-crds'
  E0415 09:58:56.735636      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:58:57.528: INFO: stderr: ""
  Apr 15 09:58:57.528: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-8464-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0415 09:58:57.736676      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:58:58.737403      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:58:59.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5659" for this suite. @ 04/15/24 09:58:59.431
• [8.443 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 04/15/24 09:58:59.472
  Apr 15 09:58:59.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/15/24 09:58:59.477
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:58:59.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:58:59.519
  Apr 15 09:58:59.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  E0415 09:58:59.738636      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:00.738579      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:01.739897      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:02.740665      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:59:03.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8763" for this suite. @ 04/15/24 09:59:03.276
• [3.818 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 04/15/24 09:59:03.298
  Apr 15 09:59:03.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename containers @ 04/15/24 09:59:03.301
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:59:03.33
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:59:03.336
  STEP: Creating a pod to test override command @ 04/15/24 09:59:03.34
  E0415 09:59:03.741460      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:04.741760      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:05.742369      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:06.742866      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 09:59:07.384
  Apr 15 09:59:07.391: INFO: Trying to get logs from node ahz3daisheng-3 pod client-containers-7050d4ad-f0ad-46cf-b372-19aa856d6b60 container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 09:59:07.422
  Apr 15 09:59:07.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-9776" for this suite. @ 04/15/24 09:59:07.465
• [4.181 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 04/15/24 09:59:07.48
  Apr 15 09:59:07.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 09:59:07.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:59:07.514
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:59:07.519
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/15/24 09:59:07.526
  Apr 15 09:59:07.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-4671 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Apr 15 09:59:07.735: INFO: stderr: ""
  Apr 15 09:59:07.735: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 04/15/24 09:59:07.735
  E0415 09:59:07.743134      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:08.743413      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:09.743640      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:10.744550      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:11.745235      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:12.745412      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 04/15/24 09:59:12.787
  Apr 15 09:59:12.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-4671 get pod e2e-test-httpd-pod -o json'
  Apr 15 09:59:12.998: INFO: stderr: ""
  Apr 15 09:59:12.998: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2024-04-15T09:59:07Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4671\",\n        \"resourceVersion\": \"213604\",\n        \"uid\": \"fb4b40fe-31c3-4f62-a87a-92091e88112f\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-2wlww\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ahz3daisheng-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-2wlww\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-04-15T09:59:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-04-15T09:59:09Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-04-15T09:59:09Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-04-15T09:59:07Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://7c4d685188090189676fffaf5e0968f6b1c64ecf432bc7a5ed908963dfcefafe\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2024-04-15T09:59:08Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.199\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.66.88\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.66.88\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2024-04-15T09:59:07Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 04/15/24 09:59:12.999
  Apr 15 09:59:13.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-4671 replace -f -'
  E0415 09:59:13.746508      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:59:14.652: INFO: stderr: ""
  Apr 15 09:59:14.652: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 04/15/24 09:59:14.652
  Apr 15 09:59:14.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-4671 delete pods e2e-test-httpd-pod'
  E0415 09:59:14.747185      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:15.747231      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:59:16.323: INFO: stderr: ""
  Apr 15 09:59:16.323: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 15 09:59:16.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4671" for this suite. @ 04/15/24 09:59:16.343
• [8.882 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 04/15/24 09:59:16.366
  Apr 15 09:59:16.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename events @ 04/15/24 09:59:16.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:59:16.402
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:59:16.41
  STEP: creating a test event @ 04/15/24 09:59:16.416
  STEP: listing all events in all namespaces @ 04/15/24 09:59:16.432
  STEP: patching the test event @ 04/15/24 09:59:16.443
  STEP: fetching the test event @ 04/15/24 09:59:16.457
  STEP: updating the test event @ 04/15/24 09:59:16.466
  STEP: getting the test event @ 04/15/24 09:59:16.495
  STEP: deleting the test event @ 04/15/24 09:59:16.507
  STEP: listing all events in all namespaces @ 04/15/24 09:59:16.552
  Apr 15 09:59:16.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-438" for this suite. @ 04/15/24 09:59:16.586
• [0.255 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 04/15/24 09:59:16.635
  Apr 15 09:59:16.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 09:59:16.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:59:16.732
  E0415 09:59:16.749667      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:59:16.754
  STEP: Counting existing ResourceQuota @ 04/15/24 09:59:16.766
  E0415 09:59:17.749800      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:18.750140      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:19.750426      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:20.750297      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:21.750789      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 04/15/24 09:59:21.774
  STEP: Ensuring resource quota status is calculated @ 04/15/24 09:59:21.79
  E0415 09:59:22.751102      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:23.751745      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:59:23.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3789" for this suite. @ 04/15/24 09:59:23.809
• [7.187 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 04/15/24 09:59:23.825
  Apr 15 09:59:23.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename field-validation @ 04/15/24 09:59:23.827
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:59:23.858
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:59:23.863
  STEP: apply creating a deployment @ 04/15/24 09:59:23.869
  Apr 15 09:59:23.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-3339" for this suite. @ 04/15/24 09:59:23.913
• [0.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 04/15/24 09:59:23.941
  Apr 15 09:59:23.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 09:59:23.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:59:23.972
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:59:23.979
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 04/15/24 09:59:23.985
  E0415 09:59:24.751991      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:25.752822      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:26.752892      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:27.753993      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 09:59:28.03
  Apr 15 09:59:28.037: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-3dd5cddc-d7d4-45f2-8364-9b70357fb3e6 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 09:59:28.053
  Apr 15 09:59:28.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-100" for this suite. @ 04/15/24 09:59:28.097
• [4.174 seconds]
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:205
  STEP: Creating a kubernetes client @ 04/15/24 09:59:28.117
  Apr 15 09:59:28.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename daemonsets @ 04/15/24 09:59:28.121
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:59:28.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:59:28.164
  Apr 15 09:59:28.215: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 04/15/24 09:59:28.228
  Apr 15 09:59:28.235: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 09:59:28.235: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 04/15/24 09:59:28.235
  Apr 15 09:59:28.293: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 09:59:28.294: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  E0415 09:59:28.754248      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:59:29.309: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 09:59:29.309: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  E0415 09:59:29.755080      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:59:30.309: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 09:59:30.309: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  E0415 09:59:30.755421      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:59:31.306: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 15 09:59:31.306: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 04/15/24 09:59:31.316
  Apr 15 09:59:31.358: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 15 09:59:31.358: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E0415 09:59:31.755604      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:59:32.367: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 09:59:32.367: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 04/15/24 09:59:32.368
  Apr 15 09:59:32.393: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 09:59:32.394: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  E0415 09:59:32.756679      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:59:33.403: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 09:59:33.404: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  E0415 09:59:33.756731      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:59:34.403: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 09:59:34.403: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  E0415 09:59:34.757565      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:59:35.403: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 15 09:59:35.404: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/15/24 09:59:35.422
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4694, will wait for the garbage collector to delete the pods @ 04/15/24 09:59:35.422
  Apr 15 09:59:35.494: INFO: Deleting DaemonSet.extensions daemon-set took: 15.157751ms
  Apr 15 09:59:35.596: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.110151ms
  E0415 09:59:35.758618      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:36.759324      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:37.760249      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:59:37.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 09:59:37.906: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 15 09:59:37.913: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"213789"},"items":null}

  Apr 15 09:59:37.920: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"213789"},"items":null}

  Apr 15 09:59:37.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4694" for this suite. @ 04/15/24 09:59:37.99
• [9.888 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:836
  STEP: Creating a kubernetes client @ 04/15/24 09:59:38.008
  Apr 15 09:59:38.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename daemonsets @ 04/15/24 09:59:38.01
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:59:38.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:59:38.055
  STEP: Creating simple DaemonSet "daemon-set" @ 04/15/24 09:59:38.126
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/15/24 09:59:38.138
  Apr 15 09:59:38.154: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 09:59:38.154: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  E0415 09:59:38.761078      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:59:39.175: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 09:59:39.175: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  E0415 09:59:39.761831      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:59:40.179: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 15 09:59:40.180: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  E0415 09:59:40.762059      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:59:41.183: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 15 09:59:41.183: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 04/15/24 09:59:41.191
  STEP: DeleteCollection of the DaemonSets @ 04/15/24 09:59:41.2
  STEP: Verify that ReplicaSets have been deleted @ 04/15/24 09:59:41.219
  Apr 15 09:59:41.252: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"213838"},"items":null}

  Apr 15 09:59:41.285: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"213839"},"items":[{"metadata":{"name":"daemon-set-226lz","generateName":"daemon-set-","namespace":"daemonsets-8464","uid":"ee03eff5-e800-4f1c-ad68-d83c3da9f410","resourceVersion":"213826","creationTimestamp":"2024-04-15T09:59:38Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"fe564369-e097-46f9-b0cb-42a26e912749","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-04-15T09:59:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe564369-e097-46f9-b0cb-42a26e912749\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-04-15T09:59:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9rhxn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9rhxn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ahz3daisheng-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ahz3daisheng-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T09:59:38Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T09:59:39Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T09:59:39Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T09:59:38Z"}],"hostIP":"192.168.121.199","podIP":"10.233.66.90","podIPs":[{"ip":"10.233.66.90"}],"startTime":"2024-04-15T09:59:38Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-04-15T09:59:38Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://d804dfb3d7d97b6d7c26ef7e09bf55a37ecab5909a601b38af163457d4035532","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-lzzhw","generateName":"daemon-set-","namespace":"daemonsets-8464","uid":"83ec7787-bbbf-468c-a83f-a79af209576e","resourceVersion":"213829","creationTimestamp":"2024-04-15T09:59:38Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"fe564369-e097-46f9-b0cb-42a26e912749","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-04-15T09:59:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe564369-e097-46f9-b0cb-42a26e912749\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-04-15T09:59:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9tkt8","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9tkt8","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ahz3daisheng-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ahz3daisheng-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T09:59:38Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T09:59:39Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T09:59:39Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T09:59:38Z"}],"hostIP":"192.168.121.120","podIP":"10.233.65.174","podIPs":[{"ip":"10.233.65.174"}],"startTime":"2024-04-15T09:59:38Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-04-15T09:59:39Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://7c6b4948396eeec6ea2747495bd34f81ecc49848535ae4e70dd7e5d4b2df89f3","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-trld7","generateName":"daemon-set-","namespace":"daemonsets-8464","uid":"e2db3fa0-0222-4eb8-9294-2666c438ad49","resourceVersion":"213835","creationTimestamp":"2024-04-15T09:59:38Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"fe564369-e097-46f9-b0cb-42a26e912749","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-04-15T09:59:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe564369-e097-46f9-b0cb-42a26e912749\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-04-15T09:59:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.137\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9znjv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9znjv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ahz3daisheng-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ahz3daisheng-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T09:59:38Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T09:59:40Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T09:59:40Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-04-15T09:59:38Z"}],"hostIP":"192.168.121.96","podIP":"10.233.64.137","podIPs":[{"ip":"10.233.64.137"}],"startTime":"2024-04-15T09:59:38Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-04-15T09:59:40Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://271b896ba6f2340059b789aebfca0ac3f4741613980d8127063f77b5524a0966","started":true}],"qosClass":"BestEffort"}}]}

  Apr 15 09:59:41.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8464" for this suite. @ 04/15/24 09:59:41.391
• [3.421 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 04/15/24 09:59:41.434
  Apr 15 09:59:41.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename init-container @ 04/15/24 09:59:41.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:59:41.472
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:59:41.48
  STEP: creating the pod @ 04/15/24 09:59:41.49
  Apr 15 09:59:41.491: INFO: PodSpec: initContainers in spec.initContainers
  E0415 09:59:41.763017      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:42.762999      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:43.763183      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:59:44.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3815" for this suite. @ 04/15/24 09:59:44.518
• [3.098 seconds]
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 04/15/24 09:59:44.532
  Apr 15 09:59:44.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 09:59:44.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:59:44.571
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:59:44.577
  STEP: Creating a pod to test downward API volume plugin @ 04/15/24 09:59:44.585
  E0415 09:59:44.764212      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:45.764710      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:46.765870      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:47.766113      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 09:59:48.642
  Apr 15 09:59:48.676: INFO: Trying to get logs from node ahz3daisheng-3 pod downwardapi-volume-1dba6010-04d1-4dd4-81ba-dbb0ef548a14 container client-container: <nil>
  STEP: delete the pod @ 04/15/24 09:59:48.739
  E0415 09:59:48.766475      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 09:59:48.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-94" for this suite. @ 04/15/24 09:59:48.813
• [4.310 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 04/15/24 09:59:48.843
  Apr 15 09:59:48.843: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename watch @ 04/15/24 09:59:48.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 09:59:48.894
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 09:59:48.899
  STEP: creating a watch on configmaps with label A @ 04/15/24 09:59:48.906
  STEP: creating a watch on configmaps with label B @ 04/15/24 09:59:48.908
  STEP: creating a watch on configmaps with label A or B @ 04/15/24 09:59:48.911
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 04/15/24 09:59:48.916
  Apr 15 09:59:48.926: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5085  3eb217ad-3cf2-4a3b-8971-502541de6669 213949 0 2024-04-15 09:59:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-04-15 09:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 09:59:48.926: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5085  3eb217ad-3cf2-4a3b-8971-502541de6669 213949 0 2024-04-15 09:59:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-04-15 09:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 04/15/24 09:59:48.927
  Apr 15 09:59:48.944: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5085  3eb217ad-3cf2-4a3b-8971-502541de6669 213950 0 2024-04-15 09:59:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-04-15 09:59:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 09:59:48.945: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5085  3eb217ad-3cf2-4a3b-8971-502541de6669 213950 0 2024-04-15 09:59:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-04-15 09:59:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 04/15/24 09:59:48.945
  Apr 15 09:59:48.965: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5085  3eb217ad-3cf2-4a3b-8971-502541de6669 213951 0 2024-04-15 09:59:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-04-15 09:59:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 09:59:48.966: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5085  3eb217ad-3cf2-4a3b-8971-502541de6669 213951 0 2024-04-15 09:59:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-04-15 09:59:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 04/15/24 09:59:48.966
  Apr 15 09:59:48.978: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5085  3eb217ad-3cf2-4a3b-8971-502541de6669 213952 0 2024-04-15 09:59:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-04-15 09:59:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 09:59:48.978: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5085  3eb217ad-3cf2-4a3b-8971-502541de6669 213952 0 2024-04-15 09:59:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-04-15 09:59:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 04/15/24 09:59:48.978
  Apr 15 09:59:48.988: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5085  8148a36f-03e0-4ee9-8373-1c913bb28aa7 213953 0 2024-04-15 09:59:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-04-15 09:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 09:59:48.989: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5085  8148a36f-03e0-4ee9-8373-1c913bb28aa7 213953 0 2024-04-15 09:59:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-04-15 09:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0415 09:59:49.766867      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:50.767123      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:51.767311      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:52.768525      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:53.768734      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:54.769126      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:55.769805      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:56.770175      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:57.771283      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 09:59:58.771944      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 04/15/24 09:59:58.99
  Apr 15 09:59:59.011: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5085  8148a36f-03e0-4ee9-8373-1c913bb28aa7 213992 0 2024-04-15 09:59:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-04-15 09:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Apr 15 09:59:59.011: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5085  8148a36f-03e0-4ee9-8373-1c913bb28aa7 213992 0 2024-04-15 09:59:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-04-15 09:59:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0415 09:59:59.772065      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:00.772781      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:01.773248      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:02.772989      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:03.773135      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:04.773582      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:05.774151      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:06.774483      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:07.778437      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:08.776280      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:00:09.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5085" for this suite. @ 04/15/24 10:00:09.029
• [20.206 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 04/15/24 10:00:09.057
  Apr 15 10:00:09.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 10:00:09.061
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:00:09.093
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:00:09.098
  STEP: starting the proxy server @ 04/15/24 10:00:09.104
  Apr 15 10:00:09.104: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-9514 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 04/15/24 10:00:09.237
  Apr 15 10:00:09.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9514" for this suite. @ 04/15/24 10:00:09.285
• [0.243 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 04/15/24 10:00:09.307
  Apr 15 10:00:09.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename crd-publish-openapi @ 04/15/24 10:00:09.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:00:09.352
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:00:09.358
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 04/15/24 10:00:09.363
  Apr 15 10:00:09.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  E0415 10:00:09.777235      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:10.782851      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:00:11.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  E0415 10:00:11.783779      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:12.805344      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:13.805820      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:14.806711      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:15.809075      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:16.808996      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:17.809206      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:18.820829      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:19.820907      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:20.821707      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:00:21.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5391" for this suite. @ 04/15/24 10:00:21.12
• [11.834 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 04/15/24 10:00:21.143
  Apr 15 10:00:21.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename disruption @ 04/15/24 10:00:21.15
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:00:21.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:00:21.208
  STEP: Creating a kubernetes client @ 04/15/24 10:00:21.218
  Apr 15 10:00:21.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename disruption-2 @ 04/15/24 10:00:21.224
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:00:21.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:00:21.279
  STEP: Waiting for the pdb to be processed @ 04/15/24 10:00:21.3
  E0415 10:00:21.821890      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:22.822462      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 04/15/24 10:00:23.333
  STEP: Waiting for the pdb to be processed @ 04/15/24 10:00:23.355
  E0415 10:00:23.823234      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:24.825804      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 04/15/24 10:00:25.374
  STEP: listing a collection of PDBs in namespace disruption-8734 @ 04/15/24 10:00:25.383
  STEP: deleting a collection of PDBs @ 04/15/24 10:00:25.391
  STEP: Waiting for the PDB collection to be deleted @ 04/15/24 10:00:25.425
  Apr 15 10:00:25.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 10:00:25.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-4678" for this suite. @ 04/15/24 10:00:25.464
  STEP: Destroying namespace "disruption-8734" for this suite. @ 04/15/24 10:00:25.483
• [4.357 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 04/15/24 10:00:25.511
  Apr 15 10:00:25.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename taint-multiple-pods @ 04/15/24 10:00:25.514
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:00:25.56
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:00:25.572
  Apr 15 10:00:25.584: INFO: Waiting up to 1m0s for all nodes to be ready
  E0415 10:00:25.828795      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:26.829875      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:27.830336      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:28.830563      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:29.830751      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:30.831543      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:31.832454      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:32.833339      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:33.834799      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:34.835281      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:35.836153      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:36.836606      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:37.836718      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:38.836933      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:39.837646      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:40.837989      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:41.838278      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:42.838837      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:43.839141      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:44.839303      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:45.839740      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:46.839645      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:47.840208      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:48.840567      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:49.841183      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:50.841444      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:51.841615      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:52.842539      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:53.843104      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:54.843050      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:55.843162      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:56.843390      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:57.844203      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:58.844656      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:00:59.845029      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:00.845229      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:01.846296      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:02.846528      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:03.847389      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:04.848242      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:05.848469      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:06.848822      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:07.849622      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:08.849908      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:09.850833      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:10.851202      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:11.851852      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:12.851997      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:13.852256      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:14.852676      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:15.853727      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:16.854114      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:17.854038      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:18.854406      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:19.854908      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:20.855326      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:21.855524      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:22.856823      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:23.857423      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:24.857674      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:01:25.646: INFO: Waiting for terminating namespaces to be deleted...
  Apr 15 10:01:25.658: INFO: Starting informer...
  STEP: Starting pods... @ 04/15/24 10:01:25.659
  E0415 10:01:25.857952      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:01:25.908: INFO: Pod1 is running on ahz3daisheng-3. Tainting Node
  E0415 10:01:26.859348      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:27.860122      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:01:28.164: INFO: Pod2 is running on ahz3daisheng-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 04/15/24 10:01:28.164
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/15/24 10:01:28.206
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 04/15/24 10:01:28.219
  E0415 10:01:28.860331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:29.860945      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:30.860952      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:31.861831      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:32.862776      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:33.864868      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:01:34.029: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0415 10:01:34.863850      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:35.864617      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:36.865566      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:37.866014      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:38.866499      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:39.866952      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:40.867342      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:41.867896      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:42.868234      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:43.868969      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:44.869927      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:45.870699      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:46.872098      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:47.872268      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:48.872712      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:49.873036      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:50.872944      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:51.873540      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:52.874835      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:53.876146      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:01:54.104: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Apr 15 10:01:54.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 04/15/24 10:01:54.148
  STEP: Destroying namespace "taint-multiple-pods-5529" for this suite. @ 04/15/24 10:01:54.16
• [88.664 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 04/15/24 10:01:54.178
  Apr 15 10:01:54.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename pods @ 04/15/24 10:01:54.185
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:01:54.242
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:01:54.25
  Apr 15 10:01:54.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: creating the pod @ 04/15/24 10:01:54.262
  STEP: submitting the pod to kubernetes @ 04/15/24 10:01:54.263
  E0415 10:01:54.876185      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:55.877367      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:01:56.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-71" for this suite. @ 04/15/24 10:01:56.382
• [2.221 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 04/15/24 10:01:56.411
  Apr 15 10:01:56.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 10:01:56.418
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:01:56.455
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:01:56.471
  STEP: Creating projection with secret that has name projected-secret-test-c65aaecb-8732-4fd3-80a0-4d3c03325a93 @ 04/15/24 10:01:56.484
  STEP: Creating a pod to test consume secrets @ 04/15/24 10:01:56.503
  E0415 10:01:56.878379      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:57.878190      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:58.878344      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:01:59.879013      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 10:02:00.577
  Apr 15 10:02:00.588: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-projected-secrets-b417b662-5d9e-4947-9d05-cccd063af136 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 10:02:00.615
  Apr 15 10:02:00.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6148" for this suite. @ 04/15/24 10:02:00.668
• [4.293 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 04/15/24 10:02:00.71
  Apr 15 10:02:00.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename secrets @ 04/15/24 10:02:00.717
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:02:00.769
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:02:00.775
  STEP: Creating secret with name secret-test-a3af3ffa-ecc0-409d-84af-55c0f98b7211 @ 04/15/24 10:02:00.782
  STEP: Creating a pod to test consume secrets @ 04/15/24 10:02:00.793
  E0415 10:02:00.878993      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:01.879277      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:02.880541      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:03.880631      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 10:02:04.846
  Apr 15 10:02:04.853: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-secrets-44a15286-7f58-4a75-8a01-93e1a9f3e193 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 10:02:04.869
  E0415 10:02:04.881685      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:02:04.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6231" for this suite. @ 04/15/24 10:02:04.913
• [4.216 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 04/15/24 10:02:04.928
  Apr 15 10:02:04.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename services @ 04/15/24 10:02:04.931
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:02:04.969
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:02:04.977
  STEP: creating a collection of services @ 04/15/24 10:02:04.985
  Apr 15 10:02:04.986: INFO: Creating e2e-svc-a-856fr
  Apr 15 10:02:05.015: INFO: Creating e2e-svc-b-nk8g7
  Apr 15 10:02:05.049: INFO: Creating e2e-svc-c-vpk4g
  STEP: deleting service collection @ 04/15/24 10:02:05.094
  Apr 15 10:02:05.172: INFO: Collection of services has been deleted
  Apr 15 10:02:05.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6878" for this suite. @ 04/15/24 10:02:05.186
• [0.272 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 04/15/24 10:02:05.202
  Apr 15 10:02:05.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename namespaces @ 04/15/24 10:02:05.206
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:02:05.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:02:05.248
  STEP: Creating a test namespace @ 04/15/24 10:02:05.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:02:05.29
  STEP: Creating a pod in the namespace @ 04/15/24 10:02:05.298
  STEP: Waiting for the pod to have running status @ 04/15/24 10:02:05.318
  E0415 10:02:05.882705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:06.883897      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 04/15/24 10:02:07.338
  STEP: Waiting for the namespace to be removed. @ 04/15/24 10:02:07.35
  E0415 10:02:07.885353      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:08.885447      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:09.886049      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:10.886144      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:11.886935      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:12.893004      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:13.889253      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:14.889127      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:15.889840      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:16.890198      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:17.890462      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:18.890634      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 04/15/24 10:02:19.358
  STEP: Verifying there are no pods in the namespace @ 04/15/24 10:02:19.39
  Apr 15 10:02:19.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9105" for this suite. @ 04/15/24 10:02:19.409
  STEP: Destroying namespace "nsdeletetest-4622" for this suite. @ 04/15/24 10:02:19.426
  Apr 15 10:02:19.432: INFO: Namespace nsdeletetest-4622 was already deleted
  STEP: Destroying namespace "nsdeletetest-415" for this suite. @ 04/15/24 10:02:19.432
• [14.244 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 04/15/24 10:02:19.463
  Apr 15 10:02:19.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename namespaces @ 04/15/24 10:02:19.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:02:19.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:02:19.505
  STEP: creating a Namespace @ 04/15/24 10:02:19.51
  STEP: patching the Namespace @ 04/15/24 10:02:19.544
  STEP: get the Namespace and ensuring it has the label @ 04/15/24 10:02:19.555
  Apr 15 10:02:19.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5184" for this suite. @ 04/15/24 10:02:19.571
  STEP: Destroying namespace "nspatchtest-5792bb46-5650-4876-b636-ddd960497882-5526" for this suite. @ 04/15/24 10:02:19.584
• [0.137 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 04/15/24 10:02:19.608
  Apr 15 10:02:19.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename projected @ 04/15/24 10:02:19.611
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:02:19.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:02:19.65
  STEP: Creating configMap with name projected-configmap-test-volume-2746f28f-8cf7-465a-a3fb-afad52b58c58 @ 04/15/24 10:02:19.66
  STEP: Creating a pod to test consume configMaps @ 04/15/24 10:02:19.669
  E0415 10:02:19.891138      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:20.891491      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:21.891925      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:22.892120      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 10:02:23.714
  Apr 15 10:02:23.722: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-projected-configmaps-3ff867f8-7e1c-425d-a0f8-0790d26038be container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 10:02:23.743
  Apr 15 10:02:23.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8412" for this suite. @ 04/15/24 10:02:23.793
• [4.199 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 04/15/24 10:02:23.811
  Apr 15 10:02:23.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename resourcequota @ 04/15/24 10:02:23.815
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:02:23.851
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:02:23.858
  STEP: Creating a ResourceQuota with best effort scope @ 04/15/24 10:02:23.868
  STEP: Ensuring ResourceQuota status is calculated @ 04/15/24 10:02:23.879
  E0415 10:02:23.891915      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:24.892226      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not best effort scope @ 04/15/24 10:02:25.887
  E0415 10:02:25.892669      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring ResourceQuota status is calculated @ 04/15/24 10:02:25.91
  E0415 10:02:26.893189      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:27.893264      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a best-effort pod @ 04/15/24 10:02:27.92
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 04/15/24 10:02:27.954
  E0415 10:02:28.894003      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:29.895044      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 04/15/24 10:02:29.966
  E0415 10:02:30.895433      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:31.896319      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/15/24 10:02:31.976
  STEP: Ensuring resource quota status released the pod usage @ 04/15/24 10:02:32.005
  E0415 10:02:32.896927      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:33.896701      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a not best-effort pod @ 04/15/24 10:02:34.013
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 04/15/24 10:02:34.037
  E0415 10:02:34.897136      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:35.897228      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 04/15/24 10:02:36.047
  E0415 10:02:36.897565      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:37.897672      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 04/15/24 10:02:38.058
  STEP: Ensuring resource quota status released the pod usage @ 04/15/24 10:02:38.103
  E0415 10:02:38.897911      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:39.898089      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:02:40.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-497" for this suite. @ 04/15/24 10:02:40.124
• [16.332 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 04/15/24 10:02:40.156
  Apr 15 10:02:40.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename dns @ 04/15/24 10:02:40.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:02:40.198
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:02:40.21
  STEP: Creating a test headless service @ 04/15/24 10:02:40.215
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7779.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7779.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7779.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7779.svc.cluster.local;sleep 1; done
   @ 04/15/24 10:02:40.231
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7779.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7779.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7779.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7779.svc.cluster.local;sleep 1; done
   @ 04/15/24 10:02:40.231
  STEP: creating a pod to probe DNS @ 04/15/24 10:02:40.231
  STEP: submitting the pod to kubernetes @ 04/15/24 10:02:40.231
  E0415 10:02:40.898378      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:41.899336      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:42.899345      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:43.899979      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/15/24 10:02:44.287
  STEP: looking for the results for each expected name from probers @ 04/15/24 10:02:44.297
  Apr 15 10:02:44.312: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:44.322: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:44.333: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:44.342: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:44.350: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:44.358: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:44.365: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:44.373: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:44.373: INFO: Lookups using dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7779.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7779.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local jessie_udp@dns-test-service-2.dns-7779.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7779.svc.cluster.local]

  E0415 10:02:44.899965      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:45.903948      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:46.902225      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:47.902289      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:48.903604      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:02:49.391: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:49.402: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:49.411: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:49.418: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:49.425: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:49.432: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:49.437: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:49.443: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:49.444: INFO: Lookups using dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7779.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7779.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local jessie_udp@dns-test-service-2.dns-7779.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7779.svc.cluster.local]

  E0415 10:02:49.904709      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:50.905042      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:51.907474      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:52.906119      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:53.906399      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:02:54.386: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:54.398: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:54.413: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:54.423: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:54.433: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:54.442: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:54.466: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:54.480: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:54.480: INFO: Lookups using dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7779.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7779.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local jessie_udp@dns-test-service-2.dns-7779.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7779.svc.cluster.local]

  E0415 10:02:54.906834      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:55.908298      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:56.907732      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:57.907929      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:02:58.908899      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:02:59.391: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:59.403: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:59.414: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:59.426: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:59.436: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:59.444: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:59.454: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:59.462: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:02:59.463: INFO: Lookups using dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7779.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7779.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local jessie_udp@dns-test-service-2.dns-7779.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7779.svc.cluster.local]

  E0415 10:02:59.908603      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:00.909112      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:01.909746      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:02.910184      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:03.910271      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:03:04.384: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:03:04.393: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:03:04.402: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:03:04.416: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:03:04.426: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:03:04.439: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:03:04.447: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:03:04.456: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:03:04.456: INFO: Lookups using dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7779.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7779.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local jessie_udp@dns-test-service-2.dns-7779.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7779.svc.cluster.local]

  E0415 10:03:04.911310      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:05.911146      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:06.912032      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:07.912271      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:08.913033      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:03:09.387: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:03:09.396: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:03:09.409: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:03:09.420: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:03:09.429: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:03:09.439: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:03:09.449: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:03:09.458: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7779.svc.cluster.local from pod dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f: the server could not find the requested resource (get pods dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f)
  Apr 15 10:03:09.459: INFO: Lookups using dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7779.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7779.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7779.svc.cluster.local jessie_udp@dns-test-service-2.dns-7779.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7779.svc.cluster.local]

  E0415 10:03:09.913019      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:10.913486      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:11.913635      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:12.914339      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:13.914176      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:03:14.454: INFO: DNS probes using dns-7779/dns-test-7b423a83-f770-48a7-ab82-ec5aa45e245f succeeded

  Apr 15 10:03:14.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 10:03:14.477
  STEP: deleting the test headless service @ 04/15/24 10:03:14.511
  STEP: Destroying namespace "dns-7779" for this suite. @ 04/15/24 10:03:14.561
• [34.418 seconds]
------------------------------
SSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 04/15/24 10:03:14.577
  Apr 15 10:03:14.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename dns @ 04/15/24 10:03:14.584
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:03:14.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:03:14.631
  STEP: Creating a test headless service @ 04/15/24 10:03:14.648
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2570.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2570.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2570.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2570.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2570.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2570.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2570.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2570.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2570.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2570.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 239.42.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.42.239_udp@PTR;check="$$(dig +tcp +noall +answer +search 239.42.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.42.239_tcp@PTR;sleep 1; done
   @ 04/15/24 10:03:14.686
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2570.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2570.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2570.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2570.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2570.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2570.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2570.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2570.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2570.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2570.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 239.42.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.42.239_udp@PTR;check="$$(dig +tcp +noall +answer +search 239.42.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.42.239_tcp@PTR;sleep 1; done
   @ 04/15/24 10:03:14.686
  STEP: creating a pod to probe DNS @ 04/15/24 10:03:14.686
  STEP: submitting the pod to kubernetes @ 04/15/24 10:03:14.687
  E0415 10:03:14.914876      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:15.914957      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 04/15/24 10:03:16.756
  STEP: looking for the results for each expected name from probers @ 04/15/24 10:03:16.767
  Apr 15 10:03:16.785: INFO: Unable to read wheezy_udp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:16.797: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:16.806: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:16.817: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:16.862: INFO: Unable to read jessie_udp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:16.871: INFO: Unable to read jessie_tcp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:16.890: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:16.901: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  E0415 10:03:16.916153      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:03:16.941: INFO: Lookups using dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05 failed for: [wheezy_udp@dns-test-service.dns-2570.svc.cluster.local wheezy_tcp@dns-test-service.dns-2570.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local jessie_udp@dns-test-service.dns-2570.svc.cluster.local jessie_tcp@dns-test-service.dns-2570.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local]

  E0415 10:03:17.916033      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:18.916687      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:19.917218      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:20.917336      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:21.918086      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:03:21.956: INFO: Unable to read wheezy_udp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:21.964: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:21.974: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:21.988: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:22.048: INFO: Unable to read jessie_udp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:22.059: INFO: Unable to read jessie_tcp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:22.076: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:22.087: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:22.123: INFO: Lookups using dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05 failed for: [wheezy_udp@dns-test-service.dns-2570.svc.cluster.local wheezy_tcp@dns-test-service.dns-2570.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local jessie_udp@dns-test-service.dns-2570.svc.cluster.local jessie_tcp@dns-test-service.dns-2570.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local]

  E0415 10:03:22.918979      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:23.919631      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:24.920202      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:25.920538      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:26.920448      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:03:26.951: INFO: Unable to read wheezy_udp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:26.959: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:26.967: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:26.975: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:27.010: INFO: Unable to read jessie_udp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:27.022: INFO: Unable to read jessie_tcp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:27.030: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:27.040: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:27.067: INFO: Lookups using dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05 failed for: [wheezy_udp@dns-test-service.dns-2570.svc.cluster.local wheezy_tcp@dns-test-service.dns-2570.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local jessie_udp@dns-test-service.dns-2570.svc.cluster.local jessie_tcp@dns-test-service.dns-2570.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local]

  E0415 10:03:27.921242      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:28.921290      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:29.922305      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:30.922834      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:31.923384      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:03:31.956: INFO: Unable to read wheezy_udp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:31.969: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:31.981: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:31.994: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:32.043: INFO: Unable to read jessie_udp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:32.051: INFO: Unable to read jessie_tcp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:32.060: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:32.072: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:32.120: INFO: Lookups using dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05 failed for: [wheezy_udp@dns-test-service.dns-2570.svc.cluster.local wheezy_tcp@dns-test-service.dns-2570.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local jessie_udp@dns-test-service.dns-2570.svc.cluster.local jessie_tcp@dns-test-service.dns-2570.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local]

  E0415 10:03:32.923457      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:33.924137      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:34.924683      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:35.924878      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:36.925059      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:03:36.952: INFO: Unable to read wheezy_udp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:36.964: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:36.972: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:36.983: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:37.016: INFO: Unable to read jessie_udp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:37.024: INFO: Unable to read jessie_tcp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:37.034: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:37.043: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:37.075: INFO: Lookups using dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05 failed for: [wheezy_udp@dns-test-service.dns-2570.svc.cluster.local wheezy_tcp@dns-test-service.dns-2570.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local jessie_udp@dns-test-service.dns-2570.svc.cluster.local jessie_tcp@dns-test-service.dns-2570.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local]

  E0415 10:03:37.925467      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:38.925709      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:39.928652      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:40.929390      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:41.929816      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:03:41.952: INFO: Unable to read wheezy_udp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:41.960: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:41.971: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:41.978: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:42.021: INFO: Unable to read jessie_udp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:42.029: INFO: Unable to read jessie_tcp@dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:42.036: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:42.045: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local from pod dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05: the server could not find the requested resource (get pods dns-test-007ef504-2146-47c3-b428-d8e9dba13a05)
  Apr 15 10:03:42.076: INFO: Lookups using dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05 failed for: [wheezy_udp@dns-test-service.dns-2570.svc.cluster.local wheezy_tcp@dns-test-service.dns-2570.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local jessie_udp@dns-test-service.dns-2570.svc.cluster.local jessie_tcp@dns-test-service.dns-2570.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2570.svc.cluster.local]

  E0415 10:03:42.930363      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:43.930379      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:44.930676      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:45.930970      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:46.931731      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:03:47.080: INFO: DNS probes using dns-2570/dns-test-007ef504-2146-47c3-b428-d8e9dba13a05 succeeded

  Apr 15 10:03:47.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 04/15/24 10:03:47.096
  STEP: deleting the test service @ 04/15/24 10:03:47.137
  STEP: deleting the test headless service @ 04/15/24 10:03:47.202
  STEP: Destroying namespace "dns-2570" for this suite. @ 04/15/24 10:03:47.236
• [32.682 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 04/15/24 10:03:47.26
  Apr 15 10:03:47.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 10:03:47.265
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:03:47.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:03:47.312
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 04/15/24 10:03:47.321
  E0415 10:03:47.934653      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:48.935545      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:49.936832      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:50.936737      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 10:03:51.399
  Apr 15 10:03:51.407: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-00043cca-c8b8-4200-9d9e-1059868da558 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 10:03:51.424
  Apr 15 10:03:51.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-465" for this suite. @ 04/15/24 10:03:51.467
• [4.221 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 04/15/24 10:03:51.509
  Apr 15 10:03:51.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename pods @ 04/15/24 10:03:51.514
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:03:51.563
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:03:51.569
  Apr 15 10:03:51.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: creating the pod @ 04/15/24 10:03:51.578
  STEP: submitting the pod to kubernetes @ 04/15/24 10:03:51.579
  E0415 10:03:51.937659      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:52.938513      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:03:53.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5106" for this suite. @ 04/15/24 10:03:53.73
• [2.236 seconds]
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 04/15/24 10:03:53.745
  Apr 15 10:03:53.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename proxy @ 04/15/24 10:03:53.747
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:03:53.808
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:03:53.817
  STEP: starting an echo server on multiple ports @ 04/15/24 10:03:53.869
  STEP: creating replication controller proxy-service-fgbsn in namespace proxy-6418 @ 04/15/24 10:03:53.87
  I0415 10:03:53.890245      14 runners.go:194] Created replication controller with name: proxy-service-fgbsn, namespace: proxy-6418, replica count: 1
  E0415 10:03:53.939132      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:54.940259      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0415 10:03:54.940713      14 runners.go:194] proxy-service-fgbsn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  I0415 10:03:55.941332      14 runners.go:194] proxy-service-fgbsn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0415 10:03:55.941834      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0415 10:03:56.941780      14 runners.go:194] proxy-service-fgbsn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0415 10:03:56.942708      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:03:57.942591      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0415 10:03:57.943154      14 runners.go:194] proxy-service-fgbsn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0415 10:03:58.942604      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0415 10:03:58.943745      14 runners.go:194] proxy-service-fgbsn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0415 10:03:59.943670      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0415 10:03:59.944850      14 runners.go:194] proxy-service-fgbsn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0415 10:04:00.943740      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0415 10:04:00.945438      14 runners.go:194] proxy-service-fgbsn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0415 10:04:01.944717      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0415 10:04:01.946251      14 runners.go:194] proxy-service-fgbsn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0415 10:04:02.944890      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0415 10:04:02.946620      14 runners.go:194] proxy-service-fgbsn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0415 10:04:03.945918      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0415 10:04:03.947615      14 runners.go:194] proxy-service-fgbsn Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0415 10:04:04.947371      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0415 10:04:04.948116      14 runners.go:194] proxy-service-fgbsn Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Apr 15 10:04:04.962: INFO: setup took 11.135520472s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 04/15/24 10:04:04.962
  Apr 15 10:04:04.998: INFO: (0) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 34.128627ms)
  Apr 15 10:04:04.999: INFO: (0) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 33.686746ms)
  Apr 15 10:04:05.019: INFO: (0) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 55.18034ms)
  Apr 15 10:04:05.040: INFO: (0) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 74.816345ms)
  Apr 15 10:04:05.040: INFO: (0) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 77.128448ms)
  Apr 15 10:04:05.040: INFO: (0) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 74.816124ms)
  Apr 15 10:04:05.040: INFO: (0) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 76.917553ms)
  Apr 15 10:04:05.040: INFO: (0) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 77.363303ms)
  Apr 15 10:04:05.041: INFO: (0) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 74.854709ms)
  Apr 15 10:04:05.041: INFO: (0) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 75.981988ms)
  Apr 15 10:04:05.041: INFO: (0) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 76.916482ms)
  Apr 15 10:04:05.041: INFO: (0) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 76.533356ms)
  Apr 15 10:04:05.041: INFO: (0) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 77.324318ms)
  Apr 15 10:04:05.041: INFO: (0) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 76.371908ms)
  Apr 15 10:04:05.042: INFO: (0) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 75.682199ms)
  Apr 15 10:04:05.042: INFO: (0) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 76.224636ms)
  Apr 15 10:04:05.084: INFO: (1) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 35.374687ms)
  Apr 15 10:04:05.087: INFO: (1) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 38.815616ms)
  Apr 15 10:04:05.090: INFO: (1) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 41.694654ms)
  Apr 15 10:04:05.098: INFO: (1) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 47.776247ms)
  Apr 15 10:04:05.098: INFO: (1) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 50.170558ms)
  Apr 15 10:04:05.098: INFO: (1) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 49.686611ms)
  Apr 15 10:04:05.099: INFO: (1) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 48.388486ms)
  Apr 15 10:04:05.098: INFO: (1) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 50.599812ms)
  Apr 15 10:04:05.101: INFO: (1) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 51.811937ms)
  Apr 15 10:04:05.104: INFO: (1) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 53.990084ms)
  Apr 15 10:04:05.104: INFO: (1) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 55.008254ms)
  Apr 15 10:04:05.105: INFO: (1) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 55.448126ms)
  Apr 15 10:04:05.106: INFO: (1) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 55.883762ms)
  Apr 15 10:04:05.106: INFO: (1) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 56.916633ms)
  Apr 15 10:04:05.107: INFO: (1) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 56.736243ms)
  Apr 15 10:04:05.108: INFO: (1) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 58.318399ms)
  Apr 15 10:04:05.130: INFO: (2) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 21.434468ms)
  Apr 15 10:04:05.133: INFO: (2) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 24.208801ms)
  Apr 15 10:04:05.134: INFO: (2) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 25.674969ms)
  Apr 15 10:04:05.137: INFO: (2) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 28.050751ms)
  Apr 15 10:04:05.138: INFO: (2) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 28.896324ms)
  Apr 15 10:04:05.140: INFO: (2) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 29.334209ms)
  Apr 15 10:04:05.140: INFO: (2) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 30.120786ms)
  Apr 15 10:04:05.144: INFO: (2) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 33.657193ms)
  Apr 15 10:04:05.144: INFO: (2) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 33.839229ms)
  Apr 15 10:04:05.144: INFO: (2) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 33.376964ms)
  Apr 15 10:04:05.144: INFO: (2) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 33.911418ms)
  Apr 15 10:04:05.145: INFO: (2) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 35.532439ms)
  Apr 15 10:04:05.145: INFO: (2) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 35.023788ms)
  Apr 15 10:04:05.146: INFO: (2) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 36.495154ms)
  Apr 15 10:04:05.146: INFO: (2) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 36.466523ms)
  Apr 15 10:04:05.147: INFO: (2) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 36.333124ms)
  Apr 15 10:04:05.168: INFO: (3) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 17.630569ms)
  Apr 15 10:04:05.168: INFO: (3) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 18.596911ms)
  Apr 15 10:04:05.176: INFO: (3) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 26.056423ms)
  Apr 15 10:04:05.176: INFO: (3) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 26.171801ms)
  Apr 15 10:04:05.179: INFO: (3) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 28.071963ms)
  Apr 15 10:04:05.179: INFO: (3) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 29.085914ms)
  Apr 15 10:04:05.179: INFO: (3) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 28.474607ms)
  Apr 15 10:04:05.181: INFO: (3) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 29.542827ms)
  Apr 15 10:04:05.181: INFO: (3) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 31.67442ms)
  Apr 15 10:04:05.181: INFO: (3) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 30.226997ms)
  Apr 15 10:04:05.181: INFO: (3) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 30.778801ms)
  Apr 15 10:04:05.184: INFO: (3) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 33.079637ms)
  Apr 15 10:04:05.185: INFO: (3) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 35.473083ms)
  Apr 15 10:04:05.185: INFO: (3) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 35.682983ms)
  Apr 15 10:04:05.186: INFO: (3) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 36.065675ms)
  Apr 15 10:04:05.186: INFO: (3) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 35.261316ms)
  Apr 15 10:04:05.210: INFO: (4) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 21.555541ms)
  Apr 15 10:04:05.215: INFO: (4) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 26.201452ms)
  Apr 15 10:04:05.217: INFO: (4) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 27.359338ms)
  Apr 15 10:04:05.217: INFO: (4) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 27.207081ms)
  Apr 15 10:04:05.218: INFO: (4) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 28.65626ms)
  Apr 15 10:04:05.218: INFO: (4) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 28.903209ms)
  Apr 15 10:04:05.219: INFO: (4) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 29.091658ms)
  Apr 15 10:04:05.219: INFO: (4) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 30.205212ms)
  Apr 15 10:04:05.221: INFO: (4) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 31.523892ms)
  Apr 15 10:04:05.221: INFO: (4) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 31.890584ms)
  Apr 15 10:04:05.222: INFO: (4) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 31.96955ms)
  Apr 15 10:04:05.223: INFO: (4) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 32.769145ms)
  Apr 15 10:04:05.223: INFO: (4) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 33.603056ms)
  Apr 15 10:04:05.223: INFO: (4) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 33.034636ms)
  Apr 15 10:04:05.226: INFO: (4) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 36.470807ms)
  Apr 15 10:04:05.226: INFO: (4) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 36.599153ms)
  Apr 15 10:04:05.253: INFO: (5) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 25.810842ms)
  Apr 15 10:04:05.264: INFO: (5) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 35.872546ms)
  Apr 15 10:04:05.269: INFO: (5) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 40.303024ms)
  Apr 15 10:04:05.269: INFO: (5) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 41.758654ms)
  Apr 15 10:04:05.272: INFO: (5) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 43.268972ms)
  Apr 15 10:04:05.272: INFO: (5) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 44.064452ms)
  Apr 15 10:04:05.272: INFO: (5) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 44.335215ms)
  Apr 15 10:04:05.272: INFO: (5) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 44.402208ms)
  Apr 15 10:04:05.272: INFO: (5) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 45.396123ms)
  Apr 15 10:04:05.273: INFO: (5) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 44.273788ms)
  Apr 15 10:04:05.278: INFO: (5) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 50.562247ms)
  Apr 15 10:04:05.278: INFO: (5) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 49.488579ms)
  Apr 15 10:04:05.278: INFO: (5) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 50.567627ms)
  Apr 15 10:04:05.278: INFO: (5) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 50.113236ms)
  Apr 15 10:04:05.280: INFO: (5) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 52.191227ms)
  Apr 15 10:04:05.281: INFO: (5) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 52.588906ms)
  Apr 15 10:04:05.307: INFO: (6) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 25.961619ms)
  Apr 15 10:04:05.309: INFO: (6) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 27.835327ms)
  Apr 15 10:04:05.309: INFO: (6) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 28.113133ms)
  Apr 15 10:04:05.319: INFO: (6) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 37.590261ms)
  Apr 15 10:04:05.319: INFO: (6) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 37.178263ms)
  Apr 15 10:04:05.319: INFO: (6) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 37.831413ms)
  Apr 15 10:04:05.319: INFO: (6) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 36.882371ms)
  Apr 15 10:04:05.319: INFO: (6) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 37.234852ms)
  Apr 15 10:04:05.320: INFO: (6) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 36.042334ms)
  Apr 15 10:04:05.322: INFO: (6) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 39.219178ms)
  Apr 15 10:04:05.325: INFO: (6) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 43.455405ms)
  Apr 15 10:04:05.325: INFO: (6) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 43.139806ms)
  Apr 15 10:04:05.325: INFO: (6) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 44.043161ms)
  Apr 15 10:04:05.330: INFO: (6) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 47.349518ms)
  Apr 15 10:04:05.331: INFO: (6) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 48.210575ms)
  Apr 15 10:04:05.332: INFO: (6) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 50.086909ms)
  Apr 15 10:04:05.370: INFO: (7) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 33.382976ms)
  Apr 15 10:04:05.370: INFO: (7) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 32.814907ms)
  Apr 15 10:04:05.372: INFO: (7) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 33.318583ms)
  Apr 15 10:04:05.376: INFO: (7) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 37.185485ms)
  Apr 15 10:04:05.376: INFO: (7) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 38.307476ms)
  Apr 15 10:04:05.376: INFO: (7) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 37.744638ms)
  Apr 15 10:04:05.376: INFO: (7) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 37.564937ms)
  Apr 15 10:04:05.377: INFO: (7) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 39.268498ms)
  Apr 15 10:04:05.378: INFO: (7) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 38.200325ms)
  Apr 15 10:04:05.378: INFO: (7) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 38.326972ms)
  Apr 15 10:04:05.378: INFO: (7) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 39.560045ms)
  Apr 15 10:04:05.378: INFO: (7) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 40.715436ms)
  Apr 15 10:04:05.380: INFO: (7) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 42.082829ms)
  Apr 15 10:04:05.381: INFO: (7) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 40.410148ms)
  Apr 15 10:04:05.387: INFO: (7) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 49.682859ms)
  Apr 15 10:04:05.388: INFO: (7) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 49.480046ms)
  Apr 15 10:04:05.406: INFO: (8) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 18.290433ms)
  Apr 15 10:04:05.414: INFO: (8) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 24.930106ms)
  Apr 15 10:04:05.417: INFO: (8) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 29.203544ms)
  Apr 15 10:04:05.418: INFO: (8) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 28.66788ms)
  Apr 15 10:04:05.421: INFO: (8) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 31.687678ms)
  Apr 15 10:04:05.423: INFO: (8) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 29.792312ms)
  Apr 15 10:04:05.424: INFO: (8) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 30.460863ms)
  Apr 15 10:04:05.429: INFO: (8) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 34.844162ms)
  Apr 15 10:04:05.430: INFO: (8) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 40.979063ms)
  Apr 15 10:04:05.430: INFO: (8) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 36.068652ms)
  Apr 15 10:04:05.432: INFO: (8) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 38.676545ms)
  Apr 15 10:04:05.433: INFO: (8) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 45.035877ms)
  Apr 15 10:04:05.435: INFO: (8) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 40.581414ms)
  Apr 15 10:04:05.435: INFO: (8) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 40.881914ms)
  Apr 15 10:04:05.435: INFO: (8) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 40.692946ms)
  Apr 15 10:04:05.436: INFO: (8) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 41.965546ms)
  Apr 15 10:04:05.463: INFO: (9) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 25.149475ms)
  Apr 15 10:04:05.466: INFO: (9) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 28.909597ms)
  Apr 15 10:04:05.468: INFO: (9) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 29.079692ms)
  Apr 15 10:04:05.468: INFO: (9) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 31.117102ms)
  Apr 15 10:04:05.472: INFO: (9) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 33.109541ms)
  Apr 15 10:04:05.472: INFO: (9) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 33.809495ms)
  Apr 15 10:04:05.472: INFO: (9) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 34.200969ms)
  Apr 15 10:04:05.477: INFO: (9) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 38.182365ms)
  Apr 15 10:04:05.477: INFO: (9) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 39.921982ms)
  Apr 15 10:04:05.478: INFO: (9) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 39.314395ms)
  Apr 15 10:04:05.478: INFO: (9) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 39.225876ms)
  Apr 15 10:04:05.482: INFO: (9) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 43.577229ms)
  Apr 15 10:04:05.488: INFO: (9) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 49.181937ms)
  Apr 15 10:04:05.489: INFO: (9) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 51.986627ms)
  Apr 15 10:04:05.489: INFO: (9) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 50.0189ms)
  Apr 15 10:04:05.491: INFO: (9) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 53.409476ms)
  Apr 15 10:04:05.516: INFO: (10) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 22.973361ms)
  Apr 15 10:04:05.516: INFO: (10) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 23.377294ms)
  Apr 15 10:04:05.517: INFO: (10) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 23.74425ms)
  Apr 15 10:04:05.522: INFO: (10) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 30.457401ms)
  Apr 15 10:04:05.523: INFO: (10) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 30.363891ms)
  Apr 15 10:04:05.524: INFO: (10) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 30.867357ms)
  Apr 15 10:04:05.524: INFO: (10) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 30.065338ms)
  Apr 15 10:04:05.524: INFO: (10) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 29.971787ms)
  Apr 15 10:04:05.528: INFO: (10) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 35.505169ms)
  Apr 15 10:04:05.528: INFO: (10) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 34.669639ms)
  Apr 15 10:04:05.529: INFO: (10) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 34.920433ms)
  Apr 15 10:04:05.529: INFO: (10) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 35.531194ms)
  Apr 15 10:04:05.533: INFO: (10) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 38.870948ms)
  Apr 15 10:04:05.534: INFO: (10) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 40.000281ms)
  Apr 15 10:04:05.534: INFO: (10) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 39.449853ms)
  Apr 15 10:04:05.536: INFO: (10) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 42.170309ms)
  Apr 15 10:04:05.558: INFO: (11) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 21.135855ms)
  Apr 15 10:04:05.566: INFO: (11) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 28.001168ms)
  Apr 15 10:04:05.566: INFO: (11) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 27.933471ms)
  Apr 15 10:04:05.567: INFO: (11) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 26.533985ms)
  Apr 15 10:04:05.567: INFO: (11) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 28.971602ms)
  Apr 15 10:04:05.567: INFO: (11) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 26.90167ms)
  Apr 15 10:04:05.568: INFO: (11) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 30.295518ms)
  Apr 15 10:04:05.568: INFO: (11) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 29.992095ms)
  Apr 15 10:04:05.568: INFO: (11) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 29.464517ms)
  Apr 15 10:04:05.570: INFO: (11) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 31.190138ms)
  Apr 15 10:04:05.576: INFO: (11) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 36.496532ms)
  Apr 15 10:04:05.577: INFO: (11) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 38.065649ms)
  Apr 15 10:04:05.579: INFO: (11) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 38.014876ms)
  Apr 15 10:04:05.579: INFO: (11) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 41.461014ms)
  Apr 15 10:04:05.580: INFO: (11) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 41.559066ms)
  Apr 15 10:04:05.581: INFO: (11) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 44.013665ms)
  Apr 15 10:04:05.605: INFO: (12) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 21.691381ms)
  Apr 15 10:04:05.607: INFO: (12) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 24.442027ms)
  Apr 15 10:04:05.609: INFO: (12) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 26.987128ms)
  Apr 15 10:04:05.610: INFO: (12) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 26.85348ms)
  Apr 15 10:04:05.610: INFO: (12) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 27.880539ms)
  Apr 15 10:04:05.613: INFO: (12) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 30.693745ms)
  Apr 15 10:04:05.613: INFO: (12) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 29.676437ms)
  Apr 15 10:04:05.613: INFO: (12) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 29.661267ms)
  Apr 15 10:04:05.613: INFO: (12) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 29.987407ms)
  Apr 15 10:04:05.614: INFO: (12) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 29.620949ms)
  Apr 15 10:04:05.614: INFO: (12) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 30.912014ms)
  Apr 15 10:04:05.614: INFO: (12) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 30.38792ms)
  Apr 15 10:04:05.614: INFO: (12) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 32.19195ms)
  Apr 15 10:04:05.617: INFO: (12) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 34.030541ms)
  Apr 15 10:04:05.618: INFO: (12) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 34.46043ms)
  Apr 15 10:04:05.619: INFO: (12) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 35.975127ms)
  Apr 15 10:04:05.634: INFO: (13) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 13.748642ms)
  Apr 15 10:04:05.634: INFO: (13) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 15.184856ms)
  Apr 15 10:04:05.635: INFO: (13) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 16.101724ms)
  Apr 15 10:04:05.652: INFO: (13) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 31.263652ms)
  Apr 15 10:04:05.653: INFO: (13) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 34.045933ms)
  Apr 15 10:04:05.654: INFO: (13) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 34.525455ms)
  Apr 15 10:04:05.656: INFO: (13) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 35.01868ms)
  Apr 15 10:04:05.657: INFO: (13) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 34.482982ms)
  Apr 15 10:04:05.660: INFO: (13) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 37.372303ms)
  Apr 15 10:04:05.660: INFO: (13) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 39.207348ms)
  Apr 15 10:04:05.660: INFO: (13) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 37.224212ms)
  Apr 15 10:04:05.661: INFO: (13) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 38.55617ms)
  Apr 15 10:04:05.661: INFO: (13) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 37.446036ms)
  Apr 15 10:04:05.661: INFO: (13) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 36.895868ms)
  Apr 15 10:04:05.661: INFO: (13) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 39.357905ms)
  Apr 15 10:04:05.661: INFO: (13) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 39.871858ms)
  Apr 15 10:04:05.680: INFO: (14) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 16.820303ms)
  Apr 15 10:04:05.685: INFO: (14) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 20.724789ms)
  Apr 15 10:04:05.685: INFO: (14) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 19.813414ms)
  Apr 15 10:04:05.689: INFO: (14) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 25.349997ms)
  Apr 15 10:04:05.689: INFO: (14) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 24.283993ms)
  Apr 15 10:04:05.692: INFO: (14) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 27.804071ms)
  Apr 15 10:04:05.694: INFO: (14) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 29.614631ms)
  Apr 15 10:04:05.694: INFO: (14) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 29.018172ms)
  Apr 15 10:04:05.695: INFO: (14) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 29.973974ms)
  Apr 15 10:04:05.696: INFO: (14) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 30.060782ms)
  Apr 15 10:04:05.696: INFO: (14) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 31.345116ms)
  Apr 15 10:04:05.696: INFO: (14) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 31.254267ms)
  Apr 15 10:04:05.696: INFO: (14) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 31.22616ms)
  Apr 15 10:04:05.696: INFO: (14) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 31.109526ms)
  Apr 15 10:04:05.695: INFO: (14) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 30.359258ms)
  Apr 15 10:04:05.700: INFO: (14) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 34.554356ms)
  Apr 15 10:04:05.715: INFO: (15) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 15.32896ms)
  Apr 15 10:04:05.722: INFO: (15) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 20.924049ms)
  Apr 15 10:04:05.724: INFO: (15) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 23.003849ms)
  Apr 15 10:04:05.727: INFO: (15) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 25.732016ms)
  Apr 15 10:04:05.730: INFO: (15) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 29.028642ms)
  Apr 15 10:04:05.731: INFO: (15) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 29.602494ms)
  Apr 15 10:04:05.735: INFO: (15) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 33.841679ms)
  Apr 15 10:04:05.735: INFO: (15) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 33.777705ms)
  Apr 15 10:04:05.735: INFO: (15) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 33.957221ms)
  Apr 15 10:04:05.738: INFO: (15) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 36.869477ms)
  Apr 15 10:04:05.739: INFO: (15) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 38.301803ms)
  Apr 15 10:04:05.740: INFO: (15) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 38.44579ms)
  Apr 15 10:04:05.740: INFO: (15) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 38.624665ms)
  Apr 15 10:04:05.740: INFO: (15) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 39.385267ms)
  Apr 15 10:04:05.740: INFO: (15) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 38.797835ms)
  Apr 15 10:04:05.740: INFO: (15) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 39.650474ms)
  Apr 15 10:04:05.754: INFO: (16) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 12.27126ms)
  Apr 15 10:04:05.757: INFO: (16) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 14.789035ms)
  Apr 15 10:04:05.757: INFO: (16) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 15.440922ms)
  Apr 15 10:04:05.757: INFO: (16) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 15.212016ms)
  Apr 15 10:04:05.765: INFO: (16) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 22.7133ms)
  Apr 15 10:04:05.765: INFO: (16) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 22.995966ms)
  Apr 15 10:04:05.766: INFO: (16) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 22.996099ms)
  Apr 15 10:04:05.766: INFO: (16) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 23.658708ms)
  Apr 15 10:04:05.766: INFO: (16) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 24.714396ms)
  Apr 15 10:04:05.767: INFO: (16) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 24.115062ms)
  Apr 15 10:04:05.767: INFO: (16) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 24.856765ms)
  Apr 15 10:04:05.767: INFO: (16) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 24.020665ms)
  Apr 15 10:04:05.768: INFO: (16) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 25.04757ms)
  Apr 15 10:04:05.772: INFO: (16) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 30.551932ms)
  Apr 15 10:04:05.774: INFO: (16) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 30.937907ms)
  Apr 15 10:04:05.777: INFO: (16) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 34.145337ms)
  Apr 15 10:04:05.805: INFO: (17) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 27.960636ms)
  Apr 15 10:04:05.807: INFO: (17) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 28.819747ms)
  Apr 15 10:04:05.807: INFO: (17) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 29.088998ms)
  Apr 15 10:04:05.807: INFO: (17) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 28.955262ms)
  Apr 15 10:04:05.808: INFO: (17) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 30.254931ms)
  Apr 15 10:04:05.809: INFO: (17) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 30.956465ms)
  Apr 15 10:04:05.810: INFO: (17) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 31.737667ms)
  Apr 15 10:04:05.810: INFO: (17) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 31.678851ms)
  Apr 15 10:04:05.810: INFO: (17) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 32.052256ms)
  Apr 15 10:04:05.811: INFO: (17) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 32.613941ms)
  Apr 15 10:04:05.812: INFO: (17) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 33.355925ms)
  Apr 15 10:04:05.814: INFO: (17) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 35.491288ms)
  Apr 15 10:04:05.815: INFO: (17) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 38.311823ms)
  Apr 15 10:04:05.817: INFO: (17) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 38.622953ms)
  Apr 15 10:04:05.817: INFO: (17) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 38.819672ms)
  Apr 15 10:04:05.820: INFO: (17) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 42.670403ms)
  Apr 15 10:04:05.839: INFO: (18) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 19.132149ms)
  Apr 15 10:04:05.843: INFO: (18) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 23.129939ms)
  Apr 15 10:04:05.843: INFO: (18) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 23.433237ms)
  Apr 15 10:04:05.844: INFO: (18) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 22.830572ms)
  Apr 15 10:04:05.858: INFO: (18) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 37.35574ms)
  Apr 15 10:04:05.858: INFO: (18) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 37.608536ms)
  Apr 15 10:04:05.859: INFO: (18) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 38.241722ms)
  Apr 15 10:04:05.859: INFO: (18) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 38.435694ms)
  Apr 15 10:04:05.860: INFO: (18) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 38.835224ms)
  Apr 15 10:04:05.861: INFO: (18) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 40.585288ms)
  Apr 15 10:04:05.862: INFO: (18) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 41.017435ms)
  Apr 15 10:04:05.862: INFO: (18) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 41.719269ms)
  Apr 15 10:04:05.862: INFO: (18) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 41.591411ms)
  Apr 15 10:04:05.863: INFO: (18) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 41.510368ms)
  Apr 15 10:04:05.866: INFO: (18) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 44.842814ms)
  Apr 15 10:04:05.866: INFO: (18) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 44.87385ms)
  Apr 15 10:04:05.882: INFO: (19) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">... (200; 15.692914ms)
  Apr 15 10:04:05.887: INFO: (19) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:1080/proxy/rewriteme">test<... (200; 18.576266ms)
  Apr 15 10:04:05.890: INFO: (19) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 21.569747ms)
  Apr 15 10:04:05.891: INFO: (19) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 22.745451ms)
  Apr 15 10:04:05.891: INFO: (19) /api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/proxy-service-fgbsn-wcnhx/proxy/rewriteme">test</a> (200; 23.232033ms)
  Apr 15 10:04:05.894: INFO: (19) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:162/proxy/: bar (200; 24.91608ms)
  Apr 15 10:04:05.894: INFO: (19) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname1/proxy/: foo (200; 27.918118ms)
  Apr 15 10:04:05.895: INFO: (19) /api/v1/namespaces/proxy-6418/pods/http:proxy-service-fgbsn-wcnhx:160/proxy/: foo (200; 27.439929ms)
  Apr 15 10:04:05.895: INFO: (19) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/: <a href="/api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:443/proxy/tlsrewritem... (200; 26.372643ms)
  Apr 15 10:04:05.895: INFO: (19) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:462/proxy/: tls qux (200; 26.434653ms)
  Apr 15 10:04:05.895: INFO: (19) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname2/proxy/: bar (200; 28.570926ms)
  Apr 15 10:04:05.895: INFO: (19) /api/v1/namespaces/proxy-6418/pods/https:proxy-service-fgbsn-wcnhx:460/proxy/: tls baz (200; 26.509904ms)
  Apr 15 10:04:05.898: INFO: (19) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname1/proxy/: tls baz (200; 29.917976ms)
  Apr 15 10:04:05.898: INFO: (19) /api/v1/namespaces/proxy-6418/services/https:proxy-service-fgbsn:tlsportname2/proxy/: tls qux (200; 29.444036ms)
  Apr 15 10:04:05.903: INFO: (19) /api/v1/namespaces/proxy-6418/services/http:proxy-service-fgbsn:portname1/proxy/: foo (200; 36.127089ms)
  Apr 15 10:04:05.904: INFO: (19) /api/v1/namespaces/proxy-6418/services/proxy-service-fgbsn:portname2/proxy/: bar (200; 35.258406ms)
  Apr 15 10:04:05.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-fgbsn in namespace proxy-6418, will wait for the garbage collector to delete the pods @ 04/15/24 10:04:05.921
  E0415 10:04:05.947700      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:04:06.000: INFO: Deleting ReplicationController proxy-service-fgbsn took: 19.519139ms
  Apr 15 10:04:06.101: INFO: Terminating ReplicationController proxy-service-fgbsn pods took: 101.076915ms
  STEP: Destroying namespace "proxy-6418" for this suite. @ 04/15/24 10:04:06.905
• [13.179 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 04/15/24 10:04:06.927
  Apr 15 10:04:06.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename emptydir @ 04/15/24 10:04:06.931
  E0415 10:04:06.947848      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:04:06.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:04:06.984
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 04/15/24 10:04:06.996
  E0415 10:04:07.949831      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:04:08.948811      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:04:09.948879      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:04:10.949574      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 10:04:11.052
  Apr 15 10:04:11.062: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-10937417-91b7-42d2-82c9-0b64ce2320d3 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 10:04:11.079
  Apr 15 10:04:11.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-105" for this suite. @ 04/15/24 10:04:11.136
• [4.222 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 04/15/24 10:04:11.153
  Apr 15 10:04:11.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename kubectl @ 04/15/24 10:04:11.157
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:04:11.203
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:04:11.21
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/15/24 10:04:11.219
  Apr 15 10:04:11.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-2563 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Apr 15 10:04:11.481: INFO: stderr: ""
  Apr 15 10:04:11.484: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 04/15/24 10:04:11.486
  Apr 15 10:04:11.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-2563 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Apr 15 10:04:11.712: INFO: stderr: ""
  Apr 15 10:04:11.712: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 04/15/24 10:04:11.712
  Apr 15 10:04:11.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3789019340 --namespace=kubectl-2563 delete pods e2e-test-httpd-pod'
  E0415 10:04:11.950309      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:04:12.950413      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:04:13.888: INFO: stderr: ""
  Apr 15 10:04:13.888: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Apr 15 10:04:13.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2563" for this suite. @ 04/15/24 10:04:13.907
• [2.778 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 04/15/24 10:04:13.94
  Apr 15 10:04:13.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename secrets @ 04/15/24 10:04:13.946
  E0415 10:04:13.951287      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:04:13.999
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:04:14.012
  STEP: Creating secret with name secret-test-70e250f9-fa44-4736-aa23-1b6e86159ada @ 04/15/24 10:04:14.022
  STEP: Creating a pod to test consume secrets @ 04/15/24 10:04:14.036
  E0415 10:04:14.951691      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:04:15.952085      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:04:16.952383      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:04:17.953022      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 10:04:18.088
  Apr 15 10:04:18.101: INFO: Trying to get logs from node ahz3daisheng-3 pod pod-secrets-84587191-6d24-4b18-9d32-afb4e35ec8b2 container secret-volume-test: <nil>
  STEP: delete the pod @ 04/15/24 10:04:18.12
  Apr 15 10:04:18.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1715" for this suite. @ 04/15/24 10:04:18.184
• [4.258 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 04/15/24 10:04:18.216
  Apr 15 10:04:18.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename custom-resource-definition @ 04/15/24 10:04:18.219
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:04:18.262
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:04:18.27
  Apr 15 10:04:18.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  Apr 15 10:04:18.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-7120" for this suite. @ 04/15/24 10:04:18.943
  E0415 10:04:18.965070      14 retrywatcher.go:130] "Watch failed" err="context canceled"
• [0.749 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:385
  STEP: Creating a kubernetes client @ 04/15/24 10:04:18.974
  Apr 15 10:04:18.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename daemonsets @ 04/15/24 10:04:18.976
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:04:19.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:04:19.018
  Apr 15 10:04:19.079: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 04/15/24 10:04:19.094
  Apr 15 10:04:19.130: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 10:04:19.130: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  E0415 10:04:19.965269      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:04:20.150: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Apr 15 10:04:20.151: INFO: Node ahz3daisheng-1 is running 0 daemon pod, expected 1
  E0415 10:04:20.966231      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:04:21.151: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 15 10:04:21.152: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 04/15/24 10:04:21.187
  STEP: Check that daemon pods images are updated. @ 04/15/24 10:04:21.207
  Apr 15 10:04:21.220: INFO: Wrong image for pod: daemon-set-dwgwt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.47, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 15 10:04:21.220: INFO: Wrong image for pod: daemon-set-g8xzp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.47, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 15 10:04:21.221: INFO: Wrong image for pod: daemon-set-tzg9p. Expected: registry.k8s.io/e2e-test-images/agnhost:2.47, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0415 10:04:21.966692      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:04:22.248: INFO: Wrong image for pod: daemon-set-g8xzp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.47, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 15 10:04:22.248: INFO: Wrong image for pod: daemon-set-tzg9p. Expected: registry.k8s.io/e2e-test-images/agnhost:2.47, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Apr 15 10:04:22.248: INFO: Pod daemon-set-zjsm5 is not available
  E0415 10:04:22.966852      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:04:23.258: INFO: Wrong image for pod: daemon-set-tzg9p. Expected: registry.k8s.io/e2e-test-images/agnhost:2.47, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0415 10:04:23.967223      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:04:24.253: INFO: Pod daemon-set-dc4g6 is not available
  Apr 15 10:04:24.253: INFO: Wrong image for pod: daemon-set-tzg9p. Expected: registry.k8s.io/e2e-test-images/agnhost:2.47, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0415 10:04:24.967615      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:04:25.968570      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:04:26.256: INFO: Pod daemon-set-65zks is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 04/15/24 10:04:26.269
  Apr 15 10:04:26.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Apr 15 10:04:26.297: INFO: Node ahz3daisheng-2 is running 0 daemon pod, expected 1
  E0415 10:04:26.968962      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:04:27.317: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Apr 15 10:04:27.317: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 04/15/24 10:04:27.36
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5876, will wait for the garbage collector to delete the pods @ 04/15/24 10:04:27.36
  Apr 15 10:04:27.432: INFO: Deleting DaemonSet.extensions daemon-set took: 14.70423ms
  Apr 15 10:04:27.533: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.938176ms
  E0415 10:04:27.969651      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:04:28.844: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Apr 15 10:04:28.844: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Apr 15 10:04:28.850: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"215241"},"items":null}

  Apr 15 10:04:28.858: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"215241"},"items":null}

  Apr 15 10:04:28.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5876" for this suite. @ 04/15/24 10:04:28.903
• [9.945 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 04/15/24 10:04:28.928
  Apr 15 10:04:28.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename var-expansion @ 04/15/24 10:04:28.931
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:04:28.964
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:04:28.97
  E0415 10:04:28.970777      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:04:29.971519      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:04:30.971974      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Apr 15 10:04:31.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Apr 15 10:04:31.021: INFO: Deleting pod "var-expansion-b8ef6143-b6b3-4511-8ca9-3bae38445ef8" in namespace "var-expansion-8322"
  Apr 15 10:04:31.034: INFO: Wait up to 5m0s for pod "var-expansion-b8ef6143-b6b3-4511-8ca9-3bae38445ef8" to be fully deleted
  E0415 10:04:31.972399      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:04:32.972995      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-8322" for this suite. @ 04/15/24 10:04:33.052
• [4.139 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 04/15/24 10:04:33.076
  Apr 15 10:04:33.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename svcaccounts @ 04/15/24 10:04:33.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:04:33.115
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:04:33.122
  STEP: Creating a pod to test service account token:  @ 04/15/24 10:04:33.129
  E0415 10:04:33.973584      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:04:34.974441      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:04:35.974753      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:04:36.975346      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 10:04:37.196
  Apr 15 10:04:37.205: INFO: Trying to get logs from node ahz3daisheng-3 pod test-pod-e1c8179b-5ce3-4cdc-9350-e84b19ff1c59 container agnhost-container: <nil>
  STEP: delete the pod @ 04/15/24 10:04:37.221
  Apr 15 10:04:37.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6300" for this suite. @ 04/15/24 10:04:37.271
• [4.211 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 04/15/24 10:04:37.299
  Apr 15 10:04:37.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3789019340
  STEP: Building a namespace api object, basename security-context @ 04/15/24 10:04:37.302
  STEP: Waiting for a default service account to be provisioned in namespace @ 04/15/24 10:04:37.345
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 04/15/24 10:04:37.352
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 04/15/24 10:04:37.359
  E0415 10:04:37.976071      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:04:38.976237      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:04:39.976630      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0415 10:04:40.977076      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 04/15/24 10:04:41.409
  Apr 15 10:04:41.416: INFO: Trying to get logs from node ahz3daisheng-3 pod security-context-d83e4e8b-e797-4cc0-9c43-33b9b46ee105 container test-container: <nil>
  STEP: delete the pod @ 04/15/24 10:04:41.441
  Apr 15 10:04:41.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-9780" for this suite. @ 04/15/24 10:04:41.488
• [4.213 seconds]
------------------------------
SSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Apr 15 10:04:41.555: INFO: Running AfterSuite actions on node 1
  Apr 15 10:04:41.555: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.001 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.154 seconds]
------------------------------

Ran 378 of 7209 Specs in 6143.677 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6831 Skipped
PASS

Ginkgo ran 1 suite in 1h42m25.53700545s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

