  I1219 10:01:57.239009      13 e2e.go:117] Starting e2e run "7c3c8437-b26f-4a1e-8f35-74d4cc7f162a" on Ginkgo node 1
  Dec 19 10:01:57.303: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1702980116 - will randomize all specs

Will run 378 of 7209 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Dec 19 10:01:57.724: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:01:57.733: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Dec 19 10:01:57.848: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Dec 19 10:01:57.859: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
  Dec 19 10:01:57.859: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  Dec 19 10:01:57.860: INFO: e2e test version: v1.27.8
  Dec 19 10:01:57.862: INFO: kube-apiserver version: v1.27.8
  Dec 19 10:01:57.863: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:01:57.872: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.149 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 12/19/23 10:01:58.431
  Dec 19 10:01:58.431: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 10:01:58.435
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:01:58.483
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:01:58.489
  STEP: Creating a pod to test downward API volume plugin @ 12/19/23 10:01:58.494
  STEP: Saw pod success @ 12/19/23 10:02:32.778
  Dec 19 10:02:32.785: INFO: Trying to get logs from node cahyeife7pae-3 pod downwardapi-volume-4d3f1f1b-8df7-443d-b906-22a94941efd7 container client-container: <nil>
  STEP: delete the pod @ 12/19/23 10:02:32.837
  Dec 19 10:02:32.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6338" for this suite. @ 12/19/23 10:02:32.881
• [34.465 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 12/19/23 10:02:32.899
  Dec 19 10:02:32.899: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename svcaccounts @ 12/19/23 10:02:32.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:02:32.946
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:02:32.951
  Dec 19 10:02:33.004: INFO: created pod pod-service-account-defaultsa
  Dec 19 10:02:33.004: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Dec 19 10:02:33.016: INFO: created pod pod-service-account-mountsa
  Dec 19 10:02:33.016: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Dec 19 10:02:33.047: INFO: created pod pod-service-account-nomountsa
  Dec 19 10:02:33.047: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Dec 19 10:02:33.064: INFO: created pod pod-service-account-defaultsa-mountspec
  Dec 19 10:02:33.064: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Dec 19 10:02:33.074: INFO: created pod pod-service-account-mountsa-mountspec
  Dec 19 10:02:33.074: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Dec 19 10:02:33.123: INFO: created pod pod-service-account-nomountsa-mountspec
  Dec 19 10:02:33.124: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Dec 19 10:02:33.144: INFO: created pod pod-service-account-defaultsa-nomountspec
  Dec 19 10:02:33.144: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Dec 19 10:02:33.155: INFO: created pod pod-service-account-mountsa-nomountspec
  Dec 19 10:02:33.155: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Dec 19 10:02:33.230: INFO: created pod pod-service-account-nomountsa-nomountspec
  Dec 19 10:02:33.231: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Dec 19 10:02:33.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-849" for this suite. @ 12/19/23 10:02:33.248
• [0.391 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:205
  STEP: Creating a kubernetes client @ 12/19/23 10:02:33.294
  Dec 19 10:02:33.294: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename daemonsets @ 12/19/23 10:02:33.298
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:02:33.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:02:33.358
  Dec 19 10:02:33.413: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 12/19/23 10:02:33.453
  Dec 19 10:02:33.461: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:33.461: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 12/19/23 10:02:33.461
  Dec 19 10:02:33.529: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:33.529: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:34.538: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:34.539: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:35.537: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:35.537: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:36.538: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:36.538: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:37.536: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:37.536: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:38.541: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:38.541: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:39.537: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:39.537: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:40.541: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:40.541: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:41.576: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:41.576: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:42.538: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:42.538: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:43.537: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:43.537: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:44.539: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:44.539: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:45.551: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:45.551: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:46.537: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:46.537: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:47.541: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:47.541: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:48.539: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:48.540: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:49.539: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:49.539: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:50.541: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:50.543: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:51.537: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:51.537: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:52.538: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:52.538: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:53.543: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:53.543: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:54.539: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:54.540: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:55.537: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:55.537: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:56.538: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:56.538: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:57.602: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:57.602: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:58.540: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:58.540: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:02:59.547: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:02:59.547: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:03:00.539: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:03:00.539: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:03:01.539: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 19 10:03:01.539: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 12/19/23 10:03:01.546
  Dec 19 10:03:01.588: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 19 10:03:01.589: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  Dec 19 10:03:02.607: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:03:02.607: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 12/19/23 10:03:02.607
  Dec 19 10:03:02.638: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:03:02.638: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:03:03.648: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:03:03.648: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:03:04.646: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:03:04.646: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:03:05.651: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 19 10:03:05.651: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 12/19/23 10:03:05.665
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1169, will wait for the garbage collector to delete the pods @ 12/19/23 10:03:05.665
  Dec 19 10:03:05.738: INFO: Deleting DaemonSet.extensions daemon-set took: 15.498328ms
  Dec 19 10:03:05.839: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.786116ms
  Dec 19 10:03:07.053: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:03:07.053: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Dec 19 10:03:07.067: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"4186"},"items":null}

  Dec 19 10:03:07.074: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"4186"},"items":null}

  Dec 19 10:03:07.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1169" for this suite. @ 12/19/23 10:03:07.142
• [33.867 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 12/19/23 10:03:07.169
  Dec 19 10:03:07.169: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename svcaccounts @ 12/19/23 10:03:07.174
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:03:07.219
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:03:07.225
  STEP: creating a ServiceAccount @ 12/19/23 10:03:07.233
  STEP: watching for the ServiceAccount to be added @ 12/19/23 10:03:07.247
  STEP: patching the ServiceAccount @ 12/19/23 10:03:07.253
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 12/19/23 10:03:07.264
  STEP: deleting the ServiceAccount @ 12/19/23 10:03:07.269
  Dec 19 10:03:07.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9337" for this suite. @ 12/19/23 10:03:07.324
• [0.192 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 12/19/23 10:03:07.375
  Dec 19 10:03:07.375: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename job @ 12/19/23 10:03:07.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:03:07.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:03:07.42
  STEP: Creating a job @ 12/19/23 10:03:07.425
  STEP: Ensuring active pods == parallelism @ 12/19/23 10:03:07.437
  STEP: delete a job @ 12/19/23 10:03:11.452
  STEP: deleting Job.batch foo in namespace job-1433, will wait for the garbage collector to delete the pods @ 12/19/23 10:03:11.452
  Dec 19 10:03:11.525: INFO: Deleting Job.batch foo took: 14.359176ms
  Dec 19 10:03:11.625: INFO: Terminating Job.batch foo pods took: 100.379566ms
  STEP: Ensuring job was deleted @ 12/19/23 10:03:43.526
  Dec 19 10:03:43.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1433" for this suite. @ 12/19/23 10:03:43.543
• [36.186 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:959
  STEP: Creating a kubernetes client @ 12/19/23 10:03:43.582
  Dec 19 10:03:43.582: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename statefulset @ 12/19/23 10:03:43.584
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:03:43.619
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:03:43.624
  STEP: Creating service test in namespace statefulset-8943 @ 12/19/23 10:03:43.629
  Dec 19 10:03:43.661: INFO: Found 0 stateful pods, waiting for 1
  Dec 19 10:03:53.671: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Pending - Ready=false
  Dec 19 10:04:03.678: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Pending - Ready=false
  Dec 19 10:04:13.675: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 12/19/23 10:04:13.693
  W1219 10:04:13.713222      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Dec 19 10:04:13.727: INFO: Found 1 stateful pods, waiting for 2
  Dec 19 10:04:23.737: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Dec 19 10:04:23.737: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
  Dec 19 10:04:33.738: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Dec 19 10:04:33.738: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 12/19/23 10:04:33.75
  STEP: Delete all of the StatefulSets @ 12/19/23 10:04:33.756
  STEP: Verify that StatefulSets have been deleted @ 12/19/23 10:04:33.776
  Dec 19 10:04:33.785: INFO: Deleting all statefulset in ns statefulset-8943
  Dec 19 10:04:33.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8943" for this suite. @ 12/19/23 10:04:33.825
• [50.322 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 12/19/23 10:04:33.912
  Dec 19 10:04:33.912: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename secrets @ 12/19/23 10:04:33.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:04:33.952
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:04:33.961
  STEP: Creating projection with secret that has name secret-emptykey-test-96e1644e-5a31-4b45-b519-1d33bc9c0341 @ 12/19/23 10:04:33.968
  Dec 19 10:04:33.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2678" for this suite. @ 12/19/23 10:04:33.982
• [0.093 seconds]
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 12/19/23 10:04:34.007
  Dec 19 10:04:34.007: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename configmap @ 12/19/23 10:04:34.01
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:04:34.047
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:04:34.051
  STEP: Creating configMap that has name configmap-test-emptyKey-1b9e33d3-499e-4a38-ad72-ff0cb28979d1 @ 12/19/23 10:04:34.056
  Dec 19 10:04:34.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7817" for this suite. @ 12/19/23 10:04:34.072
• [0.079 seconds]
------------------------------
S
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 12/19/23 10:04:34.087
  Dec 19 10:04:34.087: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename dns @ 12/19/23 10:04:34.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:04:34.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:04:34.139
  STEP: Creating a test headless service @ 12/19/23 10:04:34.151
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7450.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7450.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 12/19/23 10:04:34.16
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7450.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7450.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 12/19/23 10:04:34.16
  STEP: creating a pod to probe DNS @ 12/19/23 10:04:34.16
  STEP: submitting the pod to kubernetes @ 12/19/23 10:04:34.161
  STEP: retrieving the pod @ 12/19/23 10:05:12.401
  STEP: looking for the results for each expected name from probers @ 12/19/23 10:05:12.408
  Dec 19 10:05:12.446: INFO: Unable to read jessie_hosts@dns-querier-2 from pod dns-7450/dns-test-2f55b9f0-4db9-4f17-91cd-1efc16e215f7: the server could not find the requested resource (get pods dns-test-2f55b9f0-4db9-4f17-91cd-1efc16e215f7)
  Dec 19 10:05:12.447: INFO: Lookups using dns-7450/dns-test-2f55b9f0-4db9-4f17-91cd-1efc16e215f7 failed for: [jessie_hosts@dns-querier-2]

  Dec 19 10:05:17.498: INFO: DNS probes using dns-7450/dns-test-2f55b9f0-4db9-4f17-91cd-1efc16e215f7 succeeded

  Dec 19 10:05:17.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/19/23 10:05:17.563
  STEP: deleting the test headless service @ 12/19/23 10:05:17.612
  STEP: Destroying namespace "dns-7450" for this suite. @ 12/19/23 10:05:17.657
• [43.588 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 12/19/23 10:05:17.678
  Dec 19 10:05:17.678: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename webhook @ 12/19/23 10:05:17.683
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:05:17.723
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:05:17.729
  STEP: Setting up server cert @ 12/19/23 10:05:17.776
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/19/23 10:05:19.226
  STEP: Deploying the webhook pod @ 12/19/23 10:05:19.247
  STEP: Wait for the deployment to be ready @ 12/19/23 10:05:19.349
  Dec 19 10:05:19.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 10, 5, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 5, 19, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-7497495989\""}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 12/19/23 10:05:21.4
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 10:05:21.442
  Dec 19 10:05:22.442: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Dec 19 10:05:22.449: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 12/19/23 10:05:22.978
  STEP: Creating a custom resource that should be denied by the webhook @ 12/19/23 10:05:23.021
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 12/19/23 10:05:25.153
  STEP: Updating the custom resource with disallowed data should be denied @ 12/19/23 10:05:25.168
  STEP: Deleting the custom resource should be denied @ 12/19/23 10:05:25.191
  STEP: Remove the offending key and value from the custom resource data @ 12/19/23 10:05:25.213
  STEP: Deleting the updated custom resource should be successful @ 12/19/23 10:05:25.241
  Dec 19 10:05:25.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1726" for this suite. @ 12/19/23 10:05:25.981
  STEP: Destroying namespace "webhook-markers-5125" for this suite. @ 12/19/23 10:05:26.005
• [8.342 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:875
  STEP: Creating a kubernetes client @ 12/19/23 10:05:26.024
  Dec 19 10:05:26.024: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename daemonsets @ 12/19/23 10:05:26.026
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:05:26.064
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:05:26.071
  STEP: Creating simple DaemonSet "daemon-set" @ 12/19/23 10:05:26.132
  STEP: Check that daemon pods launch on every node of the cluster. @ 12/19/23 10:05:26.146
  Dec 19 10:05:26.170: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:05:26.170: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:05:27.189: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 19 10:05:27.190: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:05:28.195: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Dec 19 10:05:28.195: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 12/19/23 10:05:28.201
  Dec 19 10:05:28.209: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 12/19/23 10:05:28.209
  Dec 19 10:05:28.228: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 12/19/23 10:05:28.228
  Dec 19 10:05:28.233: INFO: Observed &DaemonSet event: ADDED
  Dec 19 10:05:28.234: INFO: Observed &DaemonSet event: MODIFIED
  Dec 19 10:05:28.234: INFO: Observed &DaemonSet event: MODIFIED
  Dec 19 10:05:28.235: INFO: Observed &DaemonSet event: MODIFIED
  Dec 19 10:05:28.236: INFO: Observed &DaemonSet event: MODIFIED
  Dec 19 10:05:28.236: INFO: Found daemon set daemon-set in namespace daemonsets-1171 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Dec 19 10:05:28.237: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 12/19/23 10:05:28.237
  STEP: watching for the daemon set status to be patched @ 12/19/23 10:05:28.25
  Dec 19 10:05:28.255: INFO: Observed &DaemonSet event: ADDED
  Dec 19 10:05:28.255: INFO: Observed &DaemonSet event: MODIFIED
  Dec 19 10:05:28.255: INFO: Observed &DaemonSet event: MODIFIED
  Dec 19 10:05:28.256: INFO: Observed &DaemonSet event: MODIFIED
  Dec 19 10:05:28.256: INFO: Observed &DaemonSet event: MODIFIED
  Dec 19 10:05:28.256: INFO: Observed daemon set daemon-set in namespace daemonsets-1171 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Dec 19 10:05:28.257: INFO: Observed &DaemonSet event: MODIFIED
  Dec 19 10:05:28.257: INFO: Found daemon set daemon-set in namespace daemonsets-1171 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Dec 19 10:05:28.257: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 12/19/23 10:05:28.265
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1171, will wait for the garbage collector to delete the pods @ 12/19/23 10:05:28.266
  Dec 19 10:05:28.336: INFO: Deleting DaemonSet.extensions daemon-set took: 12.554923ms
  Dec 19 10:05:28.436: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.465702ms
  Dec 19 10:05:31.043: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:05:31.043: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Dec 19 10:05:31.050: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"4852"},"items":null}

  Dec 19 10:05:31.057: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"4852"},"items":null}

  Dec 19 10:05:31.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1171" for this suite. @ 12/19/23 10:05:31.098
• [5.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 12/19/23 10:05:31.116
  Dec 19 10:05:31.116: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename crd-publish-openapi @ 12/19/23 10:05:31.118
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:05:31.168
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:05:31.172
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 12/19/23 10:05:31.176
  Dec 19 10:05:31.177: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:05:32.936: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:05:40.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5614" for this suite. @ 12/19/23 10:05:40.1
• [8.996 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 12/19/23 10:05:40.115
  Dec 19 10:05:40.115: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename resourcequota @ 12/19/23 10:05:40.118
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:05:40.152
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:05:40.157
  STEP: Counting existing ResourceQuota @ 12/19/23 10:05:40.163
  STEP: Creating a ResourceQuota @ 12/19/23 10:05:45.171
  STEP: Ensuring resource quota status is calculated @ 12/19/23 10:05:45.182
  STEP: Creating a ReplicationController @ 12/19/23 10:05:47.191
  STEP: Ensuring resource quota status captures replication controller creation @ 12/19/23 10:05:47.215
  STEP: Deleting a ReplicationController @ 12/19/23 10:05:49.224
  STEP: Ensuring resource quota status released usage @ 12/19/23 10:05:49.238
  Dec 19 10:05:51.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-498" for this suite. @ 12/19/23 10:05:51.255
• [11.151 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 12/19/23 10:05:51.28
  Dec 19 10:05:51.280: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename services @ 12/19/23 10:05:51.285
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:05:51.324
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:05:51.337
  Dec 19 10:05:51.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2825" for this suite. @ 12/19/23 10:05:51.381
• [0.118 seconds]
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 12/19/23 10:05:51.399
  Dec 19 10:05:51.399: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename subpath @ 12/19/23 10:05:51.401
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:05:51.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:05:51.446
  STEP: Setting up data @ 12/19/23 10:05:51.451
  STEP: Creating pod pod-subpath-test-configmap-8rrd @ 12/19/23 10:05:51.476
  STEP: Creating a pod to test atomic-volume-subpath @ 12/19/23 10:05:51.477
  STEP: Saw pod success @ 12/19/23 10:06:15.645
  Dec 19 10:06:15.653: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-subpath-test-configmap-8rrd container test-container-subpath-configmap-8rrd: <nil>
  STEP: delete the pod @ 12/19/23 10:06:15.688
  STEP: Deleting pod pod-subpath-test-configmap-8rrd @ 12/19/23 10:06:15.729
  Dec 19 10:06:15.729: INFO: Deleting pod "pod-subpath-test-configmap-8rrd" in namespace "subpath-9473"
  Dec 19 10:06:15.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-9473" for this suite. @ 12/19/23 10:06:15.752
• [24.369 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 12/19/23 10:06:15.777
  Dec 19 10:06:15.777: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename downward-api @ 12/19/23 10:06:15.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:06:15.842
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:06:15.85
  STEP: Creating a pod to test downward API volume plugin @ 12/19/23 10:06:15.859
  STEP: Saw pod success @ 12/19/23 10:06:19.919
  Dec 19 10:06:19.927: INFO: Trying to get logs from node cahyeife7pae-3 pod downwardapi-volume-b409ad15-1946-43a1-b376-4fe581b01136 container client-container: <nil>
  STEP: delete the pod @ 12/19/23 10:06:19.944
  Dec 19 10:06:19.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9228" for this suite. @ 12/19/23 10:06:19.99
• [4.229 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 12/19/23 10:06:20.01
  Dec 19 10:06:20.010: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename proxy @ 12/19/23 10:06:20.012
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:06:20.05
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:06:20.055
  STEP: starting an echo server on multiple ports @ 12/19/23 10:06:20.083
  STEP: creating replication controller proxy-service-d848q in namespace proxy-7705 @ 12/19/23 10:06:20.084
  I1219 10:06:20.105342      13 runners.go:194] Created replication controller with name: proxy-service-d848q, namespace: proxy-7705, replica count: 1
  I1219 10:06:21.157261      13 runners.go:194] proxy-service-d848q Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Dec 19 10:06:21.166: INFO: Endpoint proxy-7705/proxy-service-d848q is not ready yet
  Dec 19 10:06:23.173: INFO: setup took 3.112105127s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 12/19/23 10:06:23.173
  Dec 19 10:06:23.194: INFO: (0) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 20.167299ms)
  Dec 19 10:06:23.194: INFO: (0) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 20.249724ms)
  Dec 19 10:06:23.194: INFO: (0) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 20.27793ms)
  Dec 19 10:06:23.215: INFO: (0) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 40.5738ms)
  Dec 19 10:06:23.215: INFO: (0) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 40.392693ms)
  Dec 19 10:06:23.215: INFO: (0) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 40.318727ms)
  Dec 19 10:06:23.215: INFO: (0) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 41.107248ms)
  Dec 19 10:06:23.216: INFO: (0) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 41.358669ms)
  Dec 19 10:06:23.216: INFO: (0) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 42.600365ms)
  Dec 19 10:06:23.217: INFO: (0) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 42.495126ms)
  Dec 19 10:06:23.217: INFO: (0) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 42.57265ms)
  Dec 19 10:06:23.217: INFO: (0) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 42.834189ms)
  Dec 19 10:06:23.217: INFO: (0) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 43.082515ms)
  Dec 19 10:06:23.217: INFO: (0) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 42.673569ms)
  Dec 19 10:06:23.217: INFO: (0) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 42.55957ms)
  Dec 19 10:06:23.217: INFO: (0) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 43.944168ms)
  Dec 19 10:06:23.232: INFO: (1) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 13.114415ms)
  Dec 19 10:06:23.237: INFO: (1) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 18.251859ms)
  Dec 19 10:06:23.237: INFO: (1) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 17.799206ms)
  Dec 19 10:06:23.240: INFO: (1) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 20.874274ms)
  Dec 19 10:06:23.240: INFO: (1) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 21.106746ms)
  Dec 19 10:06:23.242: INFO: (1) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 22.757553ms)
  Dec 19 10:06:23.243: INFO: (1) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 22.944272ms)
  Dec 19 10:06:23.243: INFO: (1) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 22.940573ms)
  Dec 19 10:06:23.244: INFO: (1) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 23.930853ms)
  Dec 19 10:06:23.244: INFO: (1) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 23.900236ms)
  Dec 19 10:06:23.244: INFO: (1) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 24.545023ms)
  Dec 19 10:06:23.244: INFO: (1) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 24.160771ms)
  Dec 19 10:06:23.244: INFO: (1) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 24.286724ms)
  Dec 19 10:06:23.244: INFO: (1) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 24.021265ms)
  Dec 19 10:06:23.246: INFO: (1) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 25.686549ms)
  Dec 19 10:06:23.246: INFO: (1) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 25.735075ms)
  Dec 19 10:06:23.261: INFO: (2) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 13.963661ms)
  Dec 19 10:06:23.261: INFO: (2) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 13.781824ms)
  Dec 19 10:06:23.262: INFO: (2) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 14.910624ms)
  Dec 19 10:06:23.262: INFO: (2) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 15.938665ms)
  Dec 19 10:06:23.264: INFO: (2) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 16.862624ms)
  Dec 19 10:06:23.264: INFO: (2) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 16.955834ms)
  Dec 19 10:06:23.264: INFO: (2) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 17.894284ms)
  Dec 19 10:06:23.264: INFO: (2) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 16.455232ms)
  Dec 19 10:06:23.266: INFO: (2) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 19.534009ms)
  Dec 19 10:06:23.266: INFO: (2) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 19.155606ms)
  Dec 19 10:06:23.269: INFO: (2) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 22.05416ms)
  Dec 19 10:06:23.270: INFO: (2) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 23.298094ms)
  Dec 19 10:06:23.270: INFO: (2) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 23.230459ms)
  Dec 19 10:06:23.270: INFO: (2) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 23.018202ms)
  Dec 19 10:06:23.271: INFO: (2) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 23.887776ms)
  Dec 19 10:06:23.289: INFO: (2) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 42.440815ms)
  Dec 19 10:06:23.307: INFO: (3) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 17.630024ms)
  Dec 19 10:06:23.312: INFO: (3) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 22.588391ms)
  Dec 19 10:06:23.314: INFO: (3) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 23.693335ms)
  Dec 19 10:06:23.316: INFO: (3) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 26.108192ms)
  Dec 19 10:06:23.317: INFO: (3) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 26.387187ms)
  Dec 19 10:06:23.317: INFO: (3) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 25.839316ms)
  Dec 19 10:06:23.317: INFO: (3) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 26.019563ms)
  Dec 19 10:06:23.320: INFO: (3) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 29.260235ms)
  Dec 19 10:06:23.320: INFO: (3) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 30.419131ms)
  Dec 19 10:06:23.330: INFO: (3) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 39.956606ms)
  Dec 19 10:06:23.331: INFO: (3) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 40.745752ms)
  Dec 19 10:06:23.331: INFO: (3) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 40.400104ms)
  Dec 19 10:06:23.332: INFO: (3) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 41.377771ms)
  Dec 19 10:06:23.332: INFO: (3) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 41.125995ms)
  Dec 19 10:06:23.332: INFO: (3) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 41.470387ms)
  Dec 19 10:06:23.332: INFO: (3) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 42.224997ms)
  Dec 19 10:06:23.347: INFO: (4) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 12.374471ms)
  Dec 19 10:06:23.347: INFO: (4) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 14.480251ms)
  Dec 19 10:06:23.347: INFO: (4) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 13.731669ms)
  Dec 19 10:06:23.348: INFO: (4) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 10.64124ms)
  Dec 19 10:06:23.359: INFO: (4) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 24.785163ms)
  Dec 19 10:06:23.359: INFO: (4) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 26.214723ms)
  Dec 19 10:06:23.359: INFO: (4) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 24.784309ms)
  Dec 19 10:06:23.359: INFO: (4) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 22.429831ms)
  Dec 19 10:06:23.359: INFO: (4) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 26.221605ms)
  Dec 19 10:06:23.360: INFO: (4) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 25.751929ms)
  Dec 19 10:06:23.396: INFO: (4) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 62.233415ms)
  Dec 19 10:06:23.406: INFO: (4) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 69.21473ms)
  Dec 19 10:06:23.406: INFO: (4) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 74.159469ms)
  Dec 19 10:06:23.410: INFO: (4) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 75.596898ms)
  Dec 19 10:06:23.411: INFO: (4) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 78.427532ms)
  Dec 19 10:06:23.411: INFO: (4) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 76.639006ms)
  Dec 19 10:06:23.422: INFO: (5) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 9.913188ms)
  Dec 19 10:06:23.425: INFO: (5) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 11.156284ms)
  Dec 19 10:06:23.426: INFO: (5) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 13.277254ms)
  Dec 19 10:06:23.427: INFO: (5) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 13.375906ms)
  Dec 19 10:06:23.428: INFO: (5) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 16.036342ms)
  Dec 19 10:06:23.428: INFO: (5) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 15.536015ms)
  Dec 19 10:06:23.428: INFO: (5) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 14.561403ms)
  Dec 19 10:06:23.428: INFO: (5) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 15.469253ms)
  Dec 19 10:06:23.430: INFO: (5) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 16.866043ms)
  Dec 19 10:06:23.436: INFO: (5) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 22.021463ms)
  Dec 19 10:06:23.436: INFO: (5) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 23.537553ms)
  Dec 19 10:06:23.437: INFO: (5) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 24.433456ms)
  Dec 19 10:06:23.439: INFO: (5) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 26.925819ms)
  Dec 19 10:06:23.439: INFO: (5) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 27.071267ms)
  Dec 19 10:06:23.439: INFO: (5) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 25.943877ms)
  Dec 19 10:06:23.440: INFO: (5) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 27.551304ms)
  Dec 19 10:06:23.453: INFO: (6) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 12.236509ms)
  Dec 19 10:06:23.455: INFO: (6) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 14.639704ms)
  Dec 19 10:06:23.458: INFO: (6) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 16.840283ms)
  Dec 19 10:06:23.459: INFO: (6) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 17.238092ms)
  Dec 19 10:06:23.459: INFO: (6) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 17.613413ms)
  Dec 19 10:06:23.459: INFO: (6) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 17.572889ms)
  Dec 19 10:06:23.460: INFO: (6) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 18.04029ms)
  Dec 19 10:06:23.460: INFO: (6) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 18.733791ms)
  Dec 19 10:06:23.460: INFO: (6) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 18.537158ms)
  Dec 19 10:06:23.461: INFO: (6) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 19.647191ms)
  Dec 19 10:06:23.462: INFO: (6) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 20.327346ms)
  Dec 19 10:06:23.464: INFO: (6) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 22.329304ms)
  Dec 19 10:06:23.464: INFO: (6) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 22.285379ms)
  Dec 19 10:06:23.466: INFO: (6) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 24.766879ms)
  Dec 19 10:06:23.467: INFO: (6) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 25.33908ms)
  Dec 19 10:06:23.469: INFO: (6) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 27.508961ms)
  Dec 19 10:06:23.479: INFO: (7) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 9.903006ms)
  Dec 19 10:06:23.480: INFO: (7) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 10.103736ms)
  Dec 19 10:06:23.481: INFO: (7) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 11.556775ms)
  Dec 19 10:06:23.482: INFO: (7) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 12.635903ms)
  Dec 19 10:06:23.488: INFO: (7) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 16.660342ms)
  Dec 19 10:06:23.489: INFO: (7) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 17.0074ms)
  Dec 19 10:06:23.489: INFO: (7) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 18.65128ms)
  Dec 19 10:06:23.489: INFO: (7) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 19.041744ms)
  Dec 19 10:06:23.490: INFO: (7) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 19.32413ms)
  Dec 19 10:06:23.491: INFO: (7) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 19.965529ms)
  Dec 19 10:06:23.491: INFO: (7) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 22.096557ms)
  Dec 19 10:06:23.492: INFO: (7) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 21.575427ms)
  Dec 19 10:06:23.492: INFO: (7) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 21.439639ms)
  Dec 19 10:06:23.492: INFO: (7) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 21.036319ms)
  Dec 19 10:06:23.495: INFO: (7) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 23.219498ms)
  Dec 19 10:06:23.495: INFO: (7) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 24.05704ms)
  Dec 19 10:06:23.506: INFO: (8) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 10.645751ms)
  Dec 19 10:06:23.507: INFO: (8) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 11.853742ms)
  Dec 19 10:06:23.508: INFO: (8) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 12.18261ms)
  Dec 19 10:06:23.514: INFO: (8) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 18.406917ms)
  Dec 19 10:06:23.515: INFO: (8) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 19.248151ms)
  Dec 19 10:06:23.516: INFO: (8) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 19.483616ms)
  Dec 19 10:06:23.517: INFO: (8) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 20.37917ms)
  Dec 19 10:06:23.519: INFO: (8) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 23.577669ms)
  Dec 19 10:06:23.520: INFO: (8) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 24.353398ms)
  Dec 19 10:06:23.520: INFO: (8) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 24.395207ms)
  Dec 19 10:06:23.521: INFO: (8) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 24.548545ms)
  Dec 19 10:06:23.521: INFO: (8) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 24.586498ms)
  Dec 19 10:06:23.521: INFO: (8) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 25.027696ms)
  Dec 19 10:06:23.521: INFO: (8) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 24.747592ms)
  Dec 19 10:06:23.522: INFO: (8) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 26.281667ms)
  Dec 19 10:06:23.526: INFO: (8) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 29.499888ms)
  Dec 19 10:06:23.539: INFO: (9) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 11.71641ms)
  Dec 19 10:06:23.540: INFO: (9) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 13.153387ms)
  Dec 19 10:06:23.540: INFO: (9) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 14.011408ms)
  Dec 19 10:06:23.542: INFO: (9) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 15.156219ms)
  Dec 19 10:06:23.542: INFO: (9) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 15.078907ms)
  Dec 19 10:06:23.543: INFO: (9) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 15.18053ms)
  Dec 19 10:06:23.543: INFO: (9) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 14.887005ms)
  Dec 19 10:06:23.545: INFO: (9) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 16.546711ms)
  Dec 19 10:06:23.548: INFO: (9) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 19.39599ms)
  Dec 19 10:06:23.549: INFO: (9) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 21.791558ms)
  Dec 19 10:06:23.549: INFO: (9) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 22.741151ms)
  Dec 19 10:06:23.550: INFO: (9) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 22.30397ms)
  Dec 19 10:06:23.550: INFO: (9) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 21.717941ms)
  Dec 19 10:06:23.551: INFO: (9) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 22.616951ms)
  Dec 19 10:06:23.551: INFO: (9) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 23.685324ms)
  Dec 19 10:06:23.554: INFO: (9) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 25.45066ms)
  Dec 19 10:06:23.566: INFO: (10) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 12.676405ms)
  Dec 19 10:06:23.575: INFO: (10) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 18.453537ms)
  Dec 19 10:06:23.578: INFO: (10) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 23.530885ms)
  Dec 19 10:06:23.578: INFO: (10) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 23.018019ms)
  Dec 19 10:06:23.578: INFO: (10) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 23.76184ms)
  Dec 19 10:06:23.578: INFO: (10) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 22.162026ms)
  Dec 19 10:06:23.580: INFO: (10) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 24.355272ms)
  Dec 19 10:06:23.580: INFO: (10) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 24.228473ms)
  Dec 19 10:06:23.580: INFO: (10) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 24.72734ms)
  Dec 19 10:06:23.580: INFO: (10) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 24.111961ms)
  Dec 19 10:06:23.580: INFO: (10) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 24.905843ms)
  Dec 19 10:06:23.580: INFO: (10) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 25.073999ms)
  Dec 19 10:06:23.580: INFO: (10) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 26.106108ms)
  Dec 19 10:06:23.580: INFO: (10) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 25.828999ms)
  Dec 19 10:06:23.580: INFO: (10) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 24.266363ms)
  Dec 19 10:06:23.580: INFO: (10) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 24.132256ms)
  Dec 19 10:06:23.599: INFO: (11) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 14.133949ms)
  Dec 19 10:06:23.599: INFO: (11) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 15.357729ms)
  Dec 19 10:06:23.599: INFO: (11) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 14.517587ms)
  Dec 19 10:06:23.600: INFO: (11) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 16.176318ms)
  Dec 19 10:06:23.600: INFO: (11) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 16.340537ms)
  Dec 19 10:06:23.605: INFO: (11) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 20.772564ms)
  Dec 19 10:06:23.605: INFO: (11) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 21.314521ms)
  Dec 19 10:06:23.606: INFO: (11) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 22.151352ms)
  Dec 19 10:06:23.607: INFO: (11) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 22.550934ms)
  Dec 19 10:06:23.607: INFO: (11) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 23.443215ms)
  Dec 19 10:06:23.607: INFO: (11) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 23.433776ms)
  Dec 19 10:06:23.607: INFO: (11) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 23.309856ms)
  Dec 19 10:06:23.607: INFO: (11) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 23.133667ms)
  Dec 19 10:06:23.607: INFO: (11) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 22.962865ms)
  Dec 19 10:06:23.607: INFO: (11) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 22.878583ms)
  Dec 19 10:06:23.608: INFO: (11) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 23.310352ms)
  Dec 19 10:06:23.620: INFO: (12) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 11.82545ms)
  Dec 19 10:06:23.621: INFO: (12) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 12.715861ms)
  Dec 19 10:06:23.621: INFO: (12) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 13.707498ms)
  Dec 19 10:06:23.622: INFO: (12) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 14.030957ms)
  Dec 19 10:06:23.623: INFO: (12) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 14.880715ms)
  Dec 19 10:06:23.624: INFO: (12) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 15.375822ms)
  Dec 19 10:06:23.625: INFO: (12) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 16.526406ms)
  Dec 19 10:06:23.625: INFO: (12) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 16.144754ms)
  Dec 19 10:06:23.626: INFO: (12) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 16.883921ms)
  Dec 19 10:06:23.626: INFO: (12) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 17.17052ms)
  Dec 19 10:06:23.626: INFO: (12) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 17.312389ms)
  Dec 19 10:06:23.628: INFO: (12) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 19.512695ms)
  Dec 19 10:06:23.629: INFO: (12) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 19.680665ms)
  Dec 19 10:06:23.635: INFO: (12) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 27.350802ms)
  Dec 19 10:06:23.636: INFO: (12) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 27.446466ms)
  Dec 19 10:06:23.638: INFO: (12) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 29.137426ms)
  Dec 19 10:06:23.649: INFO: (13) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 9.129477ms)
  Dec 19 10:06:23.649: INFO: (13) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 9.595603ms)
  Dec 19 10:06:23.650: INFO: (13) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 11.394608ms)
  Dec 19 10:06:23.656: INFO: (13) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 17.315765ms)
  Dec 19 10:06:23.659: INFO: (13) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 17.791204ms)
  Dec 19 10:06:23.659: INFO: (13) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 16.915115ms)
  Dec 19 10:06:23.661: INFO: (13) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 21.557848ms)
  Dec 19 10:06:23.666: INFO: (13) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 25.229648ms)
  Dec 19 10:06:23.666: INFO: (13) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 26.903958ms)
  Dec 19 10:06:23.667: INFO: (13) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 27.170465ms)
  Dec 19 10:06:23.668: INFO: (13) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 26.70315ms)
  Dec 19 10:06:23.668: INFO: (13) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 27.642869ms)
  Dec 19 10:06:23.668: INFO: (13) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 26.61667ms)
  Dec 19 10:06:23.672: INFO: (13) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 30.383788ms)
  Dec 19 10:06:23.672: INFO: (13) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 29.923047ms)
  Dec 19 10:06:23.673: INFO: (13) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 32.615237ms)
  Dec 19 10:06:23.687: INFO: (14) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 12.364569ms)
  Dec 19 10:06:23.687: INFO: (14) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 12.468662ms)
  Dec 19 10:06:23.687: INFO: (14) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 13.261002ms)
  Dec 19 10:06:23.689: INFO: (14) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 15.252511ms)
  Dec 19 10:06:23.689: INFO: (14) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 14.897833ms)
  Dec 19 10:06:23.690: INFO: (14) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 17.017563ms)
  Dec 19 10:06:23.690: INFO: (14) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 16.07456ms)
  Dec 19 10:06:23.691: INFO: (14) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 16.187151ms)
  Dec 19 10:06:23.692: INFO: (14) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 18.467417ms)
  Dec 19 10:06:23.693: INFO: (14) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 19.420386ms)
  Dec 19 10:06:23.699: INFO: (14) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 25.118981ms)
  Dec 19 10:06:23.700: INFO: (14) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 25.717384ms)
  Dec 19 10:06:23.700: INFO: (14) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 25.265049ms)
  Dec 19 10:06:23.703: INFO: (14) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 29.185277ms)
  Dec 19 10:06:23.703: INFO: (14) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 29.416979ms)
  Dec 19 10:06:23.703: INFO: (14) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 28.502821ms)
  Dec 19 10:06:23.714: INFO: (15) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 11.123141ms)
  Dec 19 10:06:23.714: INFO: (15) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 11.039625ms)
  Dec 19 10:06:23.716: INFO: (15) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 11.54179ms)
  Dec 19 10:06:23.717: INFO: (15) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 12.876508ms)
  Dec 19 10:06:23.721: INFO: (15) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 16.681106ms)
  Dec 19 10:06:23.721: INFO: (15) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 17.194833ms)
  Dec 19 10:06:23.723: INFO: (15) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 17.795178ms)
  Dec 19 10:06:23.724: INFO: (15) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 18.746857ms)
  Dec 19 10:06:23.725: INFO: (15) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 19.973217ms)
  Dec 19 10:06:23.725: INFO: (15) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 19.630216ms)
  Dec 19 10:06:23.725: INFO: (15) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 20.7302ms)
  Dec 19 10:06:23.726: INFO: (15) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 21.966811ms)
  Dec 19 10:06:23.726: INFO: (15) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 22.511062ms)
  Dec 19 10:06:23.727: INFO: (15) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 22.867909ms)
  Dec 19 10:06:23.729: INFO: (15) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 24.346432ms)
  Dec 19 10:06:23.731: INFO: (15) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 27.982831ms)
  Dec 19 10:06:23.744: INFO: (16) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 12.982906ms)
  Dec 19 10:06:23.745: INFO: (16) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 12.867396ms)
  Dec 19 10:06:23.750: INFO: (16) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 18.363351ms)
  Dec 19 10:06:23.751: INFO: (16) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 19.029503ms)
  Dec 19 10:06:23.753: INFO: (16) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 21.283604ms)
  Dec 19 10:06:23.753: INFO: (16) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 21.33732ms)
  Dec 19 10:06:23.754: INFO: (16) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 21.810829ms)
  Dec 19 10:06:23.754: INFO: (16) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 22.300807ms)
  Dec 19 10:06:23.754: INFO: (16) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 22.08302ms)
  Dec 19 10:06:23.754: INFO: (16) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 22.574419ms)
  Dec 19 10:06:23.754: INFO: (16) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 22.048955ms)
  Dec 19 10:06:23.754: INFO: (16) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 22.086934ms)
  Dec 19 10:06:23.756: INFO: (16) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 24.477559ms)
  Dec 19 10:06:23.757: INFO: (16) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 24.539022ms)
  Dec 19 10:06:23.759: INFO: (16) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 27.825871ms)
  Dec 19 10:06:23.760: INFO: (16) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 28.315029ms)
  Dec 19 10:06:23.769: INFO: (17) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 8.300823ms)
  Dec 19 10:06:23.771: INFO: (17) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 10.001201ms)
  Dec 19 10:06:23.776: INFO: (17) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 15.455298ms)
  Dec 19 10:06:23.779: INFO: (17) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 18.081442ms)
  Dec 19 10:06:23.779: INFO: (17) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 18.348626ms)
  Dec 19 10:06:23.779: INFO: (17) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 17.952321ms)
  Dec 19 10:06:23.779: INFO: (17) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 18.68293ms)
  Dec 19 10:06:23.780: INFO: (17) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 19.242963ms)
  Dec 19 10:06:23.780: INFO: (17) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 19.072782ms)
  Dec 19 10:06:23.780: INFO: (17) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 19.53217ms)
  Dec 19 10:06:23.783: INFO: (17) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 22.186271ms)
  Dec 19 10:06:23.784: INFO: (17) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 23.603229ms)
  Dec 19 10:06:23.784: INFO: (17) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 23.009603ms)
  Dec 19 10:06:23.784: INFO: (17) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 23.409365ms)
  Dec 19 10:06:23.784: INFO: (17) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 23.036308ms)
  Dec 19 10:06:23.785: INFO: (17) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 23.662891ms)
  Dec 19 10:06:23.796: INFO: (18) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 10.765131ms)
  Dec 19 10:06:23.796: INFO: (18) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 10.892099ms)
  Dec 19 10:06:23.796: INFO: (18) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 11.100056ms)
  Dec 19 10:06:23.802: INFO: (18) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 15.750227ms)
  Dec 19 10:06:23.802: INFO: (18) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 16.215041ms)
  Dec 19 10:06:23.802: INFO: (18) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 16.498081ms)
  Dec 19 10:06:23.802: INFO: (18) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 16.120716ms)
  Dec 19 10:06:23.802: INFO: (18) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 16.734427ms)
  Dec 19 10:06:23.802: INFO: (18) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 16.838361ms)
  Dec 19 10:06:23.804: INFO: (18) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 17.375324ms)
  Dec 19 10:06:23.804: INFO: (18) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 17.329538ms)
  Dec 19 10:06:23.804: INFO: (18) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 17.872497ms)
  Dec 19 10:06:23.804: INFO: (18) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 18.403252ms)
  Dec 19 10:06:23.805: INFO: (18) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 18.932447ms)
  Dec 19 10:06:23.808: INFO: (18) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 22.235871ms)
  Dec 19 10:06:23.810: INFO: (18) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 23.899335ms)
  Dec 19 10:06:23.818: INFO: (19) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:162/proxy/: bar (200; 8.32896ms)
  Dec 19 10:06:23.820: INFO: (19) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn/proxy/rewriteme">test</a> (200; 9.370869ms)
  Dec 19 10:06:23.822: INFO: (19) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:162/proxy/: bar (200; 11.428746ms)
  Dec 19 10:06:23.822: INFO: (19) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:443/proxy/tlsrewritem... (200; 11.374271ms)
  Dec 19 10:06:23.822: INFO: (19) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:160/proxy/: foo (200; 11.747638ms)
  Dec 19 10:06:23.824: INFO: (19) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:160/proxy/: foo (200; 13.593654ms)
  Dec 19 10:06:23.830: INFO: (19) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname1/proxy/: foo (200; 19.592441ms)
  Dec 19 10:06:23.831: INFO: (19) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname1/proxy/: tls baz (200; 20.985776ms)
  Dec 19 10:06:23.831: INFO: (19) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:460/proxy/: tls baz (200; 20.539956ms)
  Dec 19 10:06:23.832: INFO: (19) /api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/http:proxy-service-d848q-g76dn:1080/proxy/rewriteme">... (200; 21.613496ms)
  Dec 19 10:06:23.832: INFO: (19) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname1/proxy/: foo (200; 21.272754ms)
  Dec 19 10:06:23.832: INFO: (19) /api/v1/namespaces/proxy-7705/services/http:proxy-service-d848q:portname2/proxy/: bar (200; 21.767457ms)
  Dec 19 10:06:23.832: INFO: (19) /api/v1/namespaces/proxy-7705/pods/https:proxy-service-d848q-g76dn:462/proxy/: tls qux (200; 21.834413ms)
  Dec 19 10:06:23.835: INFO: (19) /api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/: <a href="/api/v1/namespaces/proxy-7705/pods/proxy-service-d848q-g76dn:1080/proxy/rewriteme">test<... (200; 24.107804ms)
  Dec 19 10:06:23.837: INFO: (19) /api/v1/namespaces/proxy-7705/services/proxy-service-d848q:portname2/proxy/: bar (200; 26.613265ms)
  Dec 19 10:06:23.838: INFO: (19) /api/v1/namespaces/proxy-7705/services/https:proxy-service-d848q:tlsportname2/proxy/: tls qux (200; 27.396309ms)
  Dec 19 10:06:23.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-d848q in namespace proxy-7705, will wait for the garbage collector to delete the pods @ 12/19/23 10:06:23.845
  Dec 19 10:06:23.914: INFO: Deleting ReplicationController proxy-service-d848q took: 12.326048ms
  Dec 19 10:06:24.016: INFO: Terminating ReplicationController proxy-service-d848q pods took: 101.387869ms
  STEP: Destroying namespace "proxy-7705" for this suite. @ 12/19/23 10:06:25.218
• [5.222 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 12/19/23 10:06:25.234
  Dec 19 10:06:25.234: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename gc @ 12/19/23 10:06:25.235
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:06:25.274
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:06:25.278
  STEP: create the rc @ 12/19/23 10:06:25.293
  W1219 10:06:25.301550      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 12/19/23 10:06:31.384
  STEP: wait for the rc to be deleted @ 12/19/23 10:06:31.577
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 12/19/23 10:06:36.587
  STEP: Gathering metrics @ 12/19/23 10:07:06.627
  Dec 19 10:07:06.842: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Dec 19 10:07:06.845: INFO: Deleting pod "simpletest.rc-298f8" in namespace "gc-8470"
  Dec 19 10:07:06.880: INFO: Deleting pod "simpletest.rc-2cbrb" in namespace "gc-8470"
  Dec 19 10:07:06.929: INFO: Deleting pod "simpletest.rc-2kpr5" in namespace "gc-8470"
  Dec 19 10:07:07.031: INFO: Deleting pod "simpletest.rc-2np78" in namespace "gc-8470"
  Dec 19 10:07:07.091: INFO: Deleting pod "simpletest.rc-45rsr" in namespace "gc-8470"
  Dec 19 10:07:07.157: INFO: Deleting pod "simpletest.rc-48s2s" in namespace "gc-8470"
  Dec 19 10:07:07.252: INFO: Deleting pod "simpletest.rc-4khzd" in namespace "gc-8470"
  Dec 19 10:07:07.306: INFO: Deleting pod "simpletest.rc-56t8l" in namespace "gc-8470"
  Dec 19 10:07:07.383: INFO: Deleting pod "simpletest.rc-5cxmw" in namespace "gc-8470"
  Dec 19 10:07:07.479: INFO: Deleting pod "simpletest.rc-5ndrh" in namespace "gc-8470"
  Dec 19 10:07:07.546: INFO: Deleting pod "simpletest.rc-65pqs" in namespace "gc-8470"
  Dec 19 10:07:07.596: INFO: Deleting pod "simpletest.rc-6qbp8" in namespace "gc-8470"
  Dec 19 10:07:07.625: INFO: Deleting pod "simpletest.rc-6qcd4" in namespace "gc-8470"
  Dec 19 10:07:07.681: INFO: Deleting pod "simpletest.rc-7jn78" in namespace "gc-8470"
  Dec 19 10:07:07.730: INFO: Deleting pod "simpletest.rc-7ldzd" in namespace "gc-8470"
  Dec 19 10:07:07.767: INFO: Deleting pod "simpletest.rc-7tl49" in namespace "gc-8470"
  Dec 19 10:07:07.861: INFO: Deleting pod "simpletest.rc-8gkmf" in namespace "gc-8470"
  Dec 19 10:07:07.994: INFO: Deleting pod "simpletest.rc-8j5p7" in namespace "gc-8470"
  Dec 19 10:07:08.108: INFO: Deleting pod "simpletest.rc-8n6dj" in namespace "gc-8470"
  Dec 19 10:07:08.175: INFO: Deleting pod "simpletest.rc-8rzvp" in namespace "gc-8470"
  Dec 19 10:07:08.246: INFO: Deleting pod "simpletest.rc-96mdt" in namespace "gc-8470"
  Dec 19 10:07:08.338: INFO: Deleting pod "simpletest.rc-99856" in namespace "gc-8470"
  Dec 19 10:07:08.404: INFO: Deleting pod "simpletest.rc-9ncv9" in namespace "gc-8470"
  Dec 19 10:07:08.467: INFO: Deleting pod "simpletest.rc-bbrs6" in namespace "gc-8470"
  Dec 19 10:07:08.574: INFO: Deleting pod "simpletest.rc-bgt6l" in namespace "gc-8470"
  Dec 19 10:07:08.698: INFO: Deleting pod "simpletest.rc-bqp4p" in namespace "gc-8470"
  Dec 19 10:07:08.774: INFO: Deleting pod "simpletest.rc-brtm8" in namespace "gc-8470"
  Dec 19 10:07:08.828: INFO: Deleting pod "simpletest.rc-c4rj9" in namespace "gc-8470"
  Dec 19 10:07:08.910: INFO: Deleting pod "simpletest.rc-ccmp8" in namespace "gc-8470"
  Dec 19 10:07:09.020: INFO: Deleting pod "simpletest.rc-cj5sd" in namespace "gc-8470"
  Dec 19 10:07:09.142: INFO: Deleting pod "simpletest.rc-cnsr8" in namespace "gc-8470"
  Dec 19 10:07:09.252: INFO: Deleting pod "simpletest.rc-dc4sn" in namespace "gc-8470"
  Dec 19 10:07:09.309: INFO: Deleting pod "simpletest.rc-dgsz8" in namespace "gc-8470"
  Dec 19 10:07:09.353: INFO: Deleting pod "simpletest.rc-dkb95" in namespace "gc-8470"
  Dec 19 10:07:09.428: INFO: Deleting pod "simpletest.rc-dnrqc" in namespace "gc-8470"
  Dec 19 10:07:09.480: INFO: Deleting pod "simpletest.rc-f2npw" in namespace "gc-8470"
  Dec 19 10:07:09.594: INFO: Deleting pod "simpletest.rc-f2xz6" in namespace "gc-8470"
  Dec 19 10:07:09.639: INFO: Deleting pod "simpletest.rc-fvt4t" in namespace "gc-8470"
  Dec 19 10:07:09.719: INFO: Deleting pod "simpletest.rc-glj6f" in namespace "gc-8470"
  Dec 19 10:07:09.786: INFO: Deleting pod "simpletest.rc-gr5mr" in namespace "gc-8470"
  Dec 19 10:07:09.864: INFO: Deleting pod "simpletest.rc-gsb2n" in namespace "gc-8470"
  Dec 19 10:07:09.947: INFO: Deleting pod "simpletest.rc-gxbcg" in namespace "gc-8470"
  Dec 19 10:07:10.046: INFO: Deleting pod "simpletest.rc-gxzdw" in namespace "gc-8470"
  Dec 19 10:07:10.127: INFO: Deleting pod "simpletest.rc-h487q" in namespace "gc-8470"
  Dec 19 10:07:10.156: INFO: Deleting pod "simpletest.rc-h5nh2" in namespace "gc-8470"
  Dec 19 10:07:10.262: INFO: Deleting pod "simpletest.rc-h8j47" in namespace "gc-8470"
  Dec 19 10:07:10.331: INFO: Deleting pod "simpletest.rc-hbhvl" in namespace "gc-8470"
  Dec 19 10:07:10.512: INFO: Deleting pod "simpletest.rc-hg8rj" in namespace "gc-8470"
  Dec 19 10:07:10.646: INFO: Deleting pod "simpletest.rc-hhq2g" in namespace "gc-8470"
  Dec 19 10:07:10.725: INFO: Deleting pod "simpletest.rc-hn6wx" in namespace "gc-8470"
  Dec 19 10:07:10.890: INFO: Deleting pod "simpletest.rc-hrqwd" in namespace "gc-8470"
  Dec 19 10:07:10.987: INFO: Deleting pod "simpletest.rc-j4jbj" in namespace "gc-8470"
  Dec 19 10:07:11.117: INFO: Deleting pod "simpletest.rc-j6hbg" in namespace "gc-8470"
  Dec 19 10:07:11.172: INFO: Deleting pod "simpletest.rc-jkmvw" in namespace "gc-8470"
  Dec 19 10:07:11.260: INFO: Deleting pod "simpletest.rc-jpw7j" in namespace "gc-8470"
  Dec 19 10:07:11.382: INFO: Deleting pod "simpletest.rc-jxpdj" in namespace "gc-8470"
  Dec 19 10:07:11.645: INFO: Deleting pod "simpletest.rc-k6xrf" in namespace "gc-8470"
  Dec 19 10:07:11.762: INFO: Deleting pod "simpletest.rc-k85lp" in namespace "gc-8470"
  Dec 19 10:07:11.864: INFO: Deleting pod "simpletest.rc-kl8lv" in namespace "gc-8470"
  Dec 19 10:07:11.995: INFO: Deleting pod "simpletest.rc-kpkpf" in namespace "gc-8470"
  Dec 19 10:07:12.055: INFO: Deleting pod "simpletest.rc-kvwl5" in namespace "gc-8470"
  Dec 19 10:07:12.210: INFO: Deleting pod "simpletest.rc-kxgfr" in namespace "gc-8470"
  Dec 19 10:07:12.291: INFO: Deleting pod "simpletest.rc-l4s8b" in namespace "gc-8470"
  Dec 19 10:07:12.413: INFO: Deleting pod "simpletest.rc-l5frm" in namespace "gc-8470"
  Dec 19 10:07:12.491: INFO: Deleting pod "simpletest.rc-l6shx" in namespace "gc-8470"
  Dec 19 10:07:12.674: INFO: Deleting pod "simpletest.rc-lwgfr" in namespace "gc-8470"
  Dec 19 10:07:12.764: INFO: Deleting pod "simpletest.rc-m68j6" in namespace "gc-8470"
  Dec 19 10:07:12.857: INFO: Deleting pod "simpletest.rc-mbg8w" in namespace "gc-8470"
  Dec 19 10:07:13.053: INFO: Deleting pod "simpletest.rc-mnnvr" in namespace "gc-8470"
  Dec 19 10:07:13.204: INFO: Deleting pod "simpletest.rc-n2962" in namespace "gc-8470"
  Dec 19 10:07:13.319: INFO: Deleting pod "simpletest.rc-n96jv" in namespace "gc-8470"
  Dec 19 10:07:13.395: INFO: Deleting pod "simpletest.rc-nfvms" in namespace "gc-8470"
  Dec 19 10:07:13.570: INFO: Deleting pod "simpletest.rc-nq5n7" in namespace "gc-8470"
  Dec 19 10:07:13.640: INFO: Deleting pod "simpletest.rc-pts4h" in namespace "gc-8470"
  Dec 19 10:07:13.693: INFO: Deleting pod "simpletest.rc-pzhgq" in namespace "gc-8470"
  Dec 19 10:07:13.801: INFO: Deleting pod "simpletest.rc-qnvbz" in namespace "gc-8470"
  Dec 19 10:07:13.884: INFO: Deleting pod "simpletest.rc-r69pp" in namespace "gc-8470"
  Dec 19 10:07:13.991: INFO: Deleting pod "simpletest.rc-r69zb" in namespace "gc-8470"
  Dec 19 10:07:14.079: INFO: Deleting pod "simpletest.rc-rkhrp" in namespace "gc-8470"
  Dec 19 10:07:14.215: INFO: Deleting pod "simpletest.rc-rlkdw" in namespace "gc-8470"
  Dec 19 10:07:14.293: INFO: Deleting pod "simpletest.rc-rpc62" in namespace "gc-8470"
  Dec 19 10:07:14.424: INFO: Deleting pod "simpletest.rc-sftrg" in namespace "gc-8470"
  Dec 19 10:07:14.725: INFO: Deleting pod "simpletest.rc-swt2g" in namespace "gc-8470"
  Dec 19 10:07:14.841: INFO: Deleting pod "simpletest.rc-sxvjl" in namespace "gc-8470"
  Dec 19 10:07:15.056: INFO: Deleting pod "simpletest.rc-tgb6t" in namespace "gc-8470"
  Dec 19 10:07:15.248: INFO: Deleting pod "simpletest.rc-tgs8v" in namespace "gc-8470"
  Dec 19 10:07:15.365: INFO: Deleting pod "simpletest.rc-vbcq7" in namespace "gc-8470"
  Dec 19 10:07:15.601: INFO: Deleting pod "simpletest.rc-vh6mr" in namespace "gc-8470"
  Dec 19 10:07:15.702: INFO: Deleting pod "simpletest.rc-vnbqb" in namespace "gc-8470"
  Dec 19 10:07:15.776: INFO: Deleting pod "simpletest.rc-vpz2q" in namespace "gc-8470"
  Dec 19 10:07:15.924: INFO: Deleting pod "simpletest.rc-vwd4n" in namespace "gc-8470"
  Dec 19 10:07:16.093: INFO: Deleting pod "simpletest.rc-w5jbn" in namespace "gc-8470"
  Dec 19 10:07:16.192: INFO: Deleting pod "simpletest.rc-w5pn8" in namespace "gc-8470"
  Dec 19 10:07:16.262: INFO: Deleting pod "simpletest.rc-x2qtt" in namespace "gc-8470"
  Dec 19 10:07:16.315: INFO: Deleting pod "simpletest.rc-xp78p" in namespace "gc-8470"
  Dec 19 10:07:16.405: INFO: Deleting pod "simpletest.rc-xz7c8" in namespace "gc-8470"
  Dec 19 10:07:16.463: INFO: Deleting pod "simpletest.rc-zjmxg" in namespace "gc-8470"
  Dec 19 10:07:16.505: INFO: Deleting pod "simpletest.rc-zjqgn" in namespace "gc-8470"
  Dec 19 10:07:16.553: INFO: Deleting pod "simpletest.rc-zqrr7" in namespace "gc-8470"
  Dec 19 10:07:16.623: INFO: Deleting pod "simpletest.rc-zxqgj" in namespace "gc-8470"
  Dec 19 10:07:16.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8470" for this suite. @ 12/19/23 10:07:16.739
• [51.539 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 12/19/23 10:07:16.783
  Dec 19 10:07:16.783: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename cronjob @ 12/19/23 10:07:16.79
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:07:16.954
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:07:16.96
  STEP: Creating a ReplaceConcurrent cronjob @ 12/19/23 10:07:16.967
  STEP: Ensuring a job is scheduled @ 12/19/23 10:07:17.01
  STEP: Ensuring exactly one is scheduled @ 12/19/23 10:08:01.021
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 12/19/23 10:08:01.03
  STEP: Ensuring the job is replaced with a new one @ 12/19/23 10:08:01.039
  STEP: Removing cronjob @ 12/19/23 10:09:01.058
  Dec 19 10:09:01.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3230" for this suite. @ 12/19/23 10:09:01.084
• [104.311 seconds]
------------------------------
SS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 12/19/23 10:09:01.095
  Dec 19 10:09:01.095: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename ingressclass @ 12/19/23 10:09:01.099
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:09:01.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:09:01.172
  STEP: getting /apis @ 12/19/23 10:09:01.179
  STEP: getting /apis/networking.k8s.io @ 12/19/23 10:09:01.191
  STEP: getting /apis/networking.k8s.iov1 @ 12/19/23 10:09:01.194
  STEP: creating @ 12/19/23 10:09:01.198
  STEP: getting @ 12/19/23 10:09:01.234
  STEP: listing @ 12/19/23 10:09:01.241
  STEP: watching @ 12/19/23 10:09:01.249
  Dec 19 10:09:01.249: INFO: starting watch
  STEP: patching @ 12/19/23 10:09:01.252
  STEP: updating @ 12/19/23 10:09:01.262
  Dec 19 10:09:01.274: INFO: waiting for watch events with expected annotations
  Dec 19 10:09:01.274: INFO: saw patched and updated annotations
  STEP: deleting @ 12/19/23 10:09:01.275
  STEP: deleting a collection @ 12/19/23 10:09:01.296
  Dec 19 10:09:01.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-3404" for this suite. @ 12/19/23 10:09:01.338
• [0.256 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 12/19/23 10:09:01.354
  Dec 19 10:09:01.354: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-probe @ 12/19/23 10:09:01.356
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:09:01.388
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:09:01.392
  STEP: Creating pod liveness-55b46fc4-0591-4c3e-8374-3d93c39f0eb5 in namespace container-probe-7767 @ 12/19/23 10:09:01.398
  Dec 19 10:09:03.427: INFO: Started pod liveness-55b46fc4-0591-4c3e-8374-3d93c39f0eb5 in namespace container-probe-7767
  STEP: checking the pod's current state and verifying that restartCount is present @ 12/19/23 10:09:03.428
  Dec 19 10:09:03.435: INFO: Initial restart count of pod liveness-55b46fc4-0591-4c3e-8374-3d93c39f0eb5 is 0
  Dec 19 10:13:04.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/19/23 10:13:04.742
  STEP: Destroying namespace "container-probe-7767" for this suite. @ 12/19/23 10:13:04.775
• [243.441 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 12/19/23 10:13:04.796
  Dec 19 10:13:04.797: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename pods @ 12/19/23 10:13:04.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:13:04.845
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:13:04.854
  Dec 19 10:13:04.861: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: creating the pod @ 12/19/23 10:13:04.862
  STEP: submitting the pod to kubernetes @ 12/19/23 10:13:04.863
  Dec 19 10:13:07.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4238" for this suite. @ 12/19/23 10:13:07.032
• [2.249 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 12/19/23 10:13:07.048
  Dec 19 10:13:07.048: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename taint-single-pod @ 12/19/23 10:13:07.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:13:07.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:13:07.084
  Dec 19 10:13:07.088: INFO: Waiting up to 1m0s for all nodes to be ready
  Dec 19 10:14:07.123: INFO: Waiting for terminating namespaces to be deleted...
  Dec 19 10:14:07.131: INFO: Starting informer...
  STEP: Starting pod... @ 12/19/23 10:14:07.131
  Dec 19 10:14:07.359: INFO: Pod is running on cahyeife7pae-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 12/19/23 10:14:07.359
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 12/19/23 10:14:07.386
  STEP: Waiting short time to make sure Pod is queued for deletion @ 12/19/23 10:14:07.396
  Dec 19 10:14:07.397: INFO: Pod wasn't evicted. Proceeding
  Dec 19 10:14:07.399: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 12/19/23 10:14:07.435
  STEP: Waiting some time to make sure that toleration time passed. @ 12/19/23 10:14:07.448
  Dec 19 10:15:22.449: INFO: Pod wasn't evicted. Test successful
  Dec 19 10:15:22.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-9750" for this suite. @ 12/19/23 10:15:22.482
• [135.459 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 12/19/23 10:15:22.511
  Dec 19 10:15:22.511: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename services @ 12/19/23 10:15:22.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:15:22.589
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:15:22.596
  STEP: creating service endpoint-test2 in namespace services-7285 @ 12/19/23 10:15:22.602
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7285 to expose endpoints map[] @ 12/19/23 10:15:22.639
  Dec 19 10:15:22.647: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  Dec 19 10:15:23.664: INFO: successfully validated that service endpoint-test2 in namespace services-7285 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-7285 @ 12/19/23 10:15:23.664
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7285 to expose endpoints map[pod1:[80]] @ 12/19/23 10:15:25.727
  Dec 19 10:15:25.749: INFO: successfully validated that service endpoint-test2 in namespace services-7285 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 12/19/23 10:15:25.75
  Dec 19 10:15:25.750: INFO: Creating new exec pod
  Dec 19 10:15:28.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7285 exec execpod5thk9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Dec 19 10:15:29.286: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Dec 19 10:15:29.286: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 19 10:15:29.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7285 exec execpod5thk9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.1.118 80'
  Dec 19 10:15:29.609: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.1.118 80\nConnection to 10.233.1.118 80 port [tcp/http] succeeded!\n"
  Dec 19 10:15:29.609: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-7285 @ 12/19/23 10:15:29.609
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7285 to expose endpoints map[pod1:[80] pod2:[80]] @ 12/19/23 10:15:31.647
  Dec 19 10:15:31.670: INFO: successfully validated that service endpoint-test2 in namespace services-7285 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 12/19/23 10:15:31.67
  Dec 19 10:15:32.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7285 exec execpod5thk9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Dec 19 10:15:32.963: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Dec 19 10:15:32.963: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 19 10:15:32.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7285 exec execpod5thk9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.1.118 80'
  Dec 19 10:15:33.215: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.1.118 80\nConnection to 10.233.1.118 80 port [tcp/http] succeeded!\n"
  Dec 19 10:15:33.215: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-7285 @ 12/19/23 10:15:33.215
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7285 to expose endpoints map[pod2:[80]] @ 12/19/23 10:15:33.262
  Dec 19 10:15:33.301: INFO: successfully validated that service endpoint-test2 in namespace services-7285 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 12/19/23 10:15:33.301
  Dec 19 10:15:34.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7285 exec execpod5thk9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Dec 19 10:15:34.638: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Dec 19 10:15:34.638: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 19 10:15:34.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7285 exec execpod5thk9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.1.118 80'
  Dec 19 10:15:34.912: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.1.118 80\nConnection to 10.233.1.118 80 port [tcp/http] succeeded!\n"
  Dec 19 10:15:34.912: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-7285 @ 12/19/23 10:15:34.912
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7285 to expose endpoints map[] @ 12/19/23 10:15:34.985
  Dec 19 10:15:35.018: INFO: successfully validated that service endpoint-test2 in namespace services-7285 exposes endpoints map[]
  Dec 19 10:15:35.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7285" for this suite. @ 12/19/23 10:15:35.079
• [12.590 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 12/19/23 10:15:35.133
  Dec 19 10:15:35.133: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename namespaces @ 12/19/23 10:15:35.136
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:15:35.193
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:15:35.203
  STEP: Updating Namespace "namespaces-3144" @ 12/19/23 10:15:35.209
  Dec 19 10:15:35.234: INFO: Namespace "namespaces-3144" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"7c3c8437-b26f-4a1e-8f35-74d4cc7f162a", "kubernetes.io/metadata.name":"namespaces-3144", "namespaces-3144":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Dec 19 10:15:35.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3144" for this suite. @ 12/19/23 10:15:35.247
• [0.132 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 12/19/23 10:15:35.267
  Dec 19 10:15:35.267: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 10:15:35.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:15:35.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:15:35.335
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-56363727-4d21-4900-abfd-ceeaf5a6d143 @ 12/19/23 10:15:35.356
  STEP: Creating the pod @ 12/19/23 10:15:35.368
  STEP: Updating configmap projected-configmap-test-upd-56363727-4d21-4900-abfd-ceeaf5a6d143 @ 12/19/23 10:15:37.458
  STEP: waiting to observe update in volume @ 12/19/23 10:15:37.478
  Dec 19 10:16:48.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8938" for this suite. @ 12/19/23 10:16:48.327
• [73.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 12/19/23 10:16:48.36
  Dec 19 10:16:48.361: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename downward-api @ 12/19/23 10:16:48.366
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:16:48.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:16:48.434
  STEP: Creating a pod to test downward api env vars @ 12/19/23 10:16:48.443
  STEP: Saw pod success @ 12/19/23 10:16:52.508
  Dec 19 10:16:52.514: INFO: Trying to get logs from node cahyeife7pae-3 pod downward-api-3a584c1e-df04-417a-b8d8-03729f2b1713 container dapi-container: <nil>
  STEP: delete the pod @ 12/19/23 10:16:52.543
  Dec 19 10:16:52.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1085" for this suite. @ 12/19/23 10:16:52.596
• [4.258 seconds]
------------------------------
S
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 12/19/23 10:16:52.621
  Dec 19 10:16:52.621: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename services @ 12/19/23 10:16:52.626
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:16:52.664
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:16:52.677
  STEP: creating a Service @ 12/19/23 10:16:52.697
  STEP: watching for the Service to be added @ 12/19/23 10:16:52.732
  Dec 19 10:16:52.740: INFO: Found Service test-service-5xrrw in namespace services-2541 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Dec 19 10:16:52.742: INFO: Service test-service-5xrrw created
  STEP: Getting /status @ 12/19/23 10:16:52.743
  Dec 19 10:16:52.760: INFO: Service test-service-5xrrw has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 12/19/23 10:16:52.762
  STEP: watching for the Service to be patched @ 12/19/23 10:16:52.777
  Dec 19 10:16:52.783: INFO: observed Service test-service-5xrrw in namespace services-2541 with annotations: map[] & LoadBalancer: {[]}
  Dec 19 10:16:52.784: INFO: Found Service test-service-5xrrw in namespace services-2541 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Dec 19 10:16:52.784: INFO: Service test-service-5xrrw has service status patched
  STEP: updating the ServiceStatus @ 12/19/23 10:16:52.785
  Dec 19 10:16:52.811: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 12/19/23 10:16:52.811
  Dec 19 10:16:52.815: INFO: Observed Service test-service-5xrrw in namespace services-2541 with annotations: map[] & Conditions: {[]}
  Dec 19 10:16:52.815: INFO: Observed event: &Service{ObjectMeta:{test-service-5xrrw  services-2541  841aacc5-9bce-4b41-8e7c-3fbe5c1e7619 8459 0 2023-12-19 10:16:52 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-12-19 10:16:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-12-19 10:16:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.58.82,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.58.82],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Dec 19 10:16:52.815: INFO: Found Service test-service-5xrrw in namespace services-2541 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Dec 19 10:16:52.815: INFO: Service test-service-5xrrw has service status updated
  STEP: patching the service @ 12/19/23 10:16:52.815
  STEP: watching for the Service to be patched @ 12/19/23 10:16:52.847
  Dec 19 10:16:52.853: INFO: observed Service test-service-5xrrw in namespace services-2541 with labels: map[test-service-static:true]
  Dec 19 10:16:52.854: INFO: observed Service test-service-5xrrw in namespace services-2541 with labels: map[test-service-static:true]
  Dec 19 10:16:52.854: INFO: observed Service test-service-5xrrw in namespace services-2541 with labels: map[test-service-static:true]
  Dec 19 10:16:52.855: INFO: Found Service test-service-5xrrw in namespace services-2541 with labels: map[test-service:patched test-service-static:true]
  Dec 19 10:16:52.855: INFO: Service test-service-5xrrw patched
  STEP: deleting the service @ 12/19/23 10:16:52.855
  STEP: watching for the Service to be deleted @ 12/19/23 10:16:52.888
  Dec 19 10:16:52.893: INFO: Observed event: ADDED
  Dec 19 10:16:52.893: INFO: Observed event: MODIFIED
  Dec 19 10:16:52.893: INFO: Observed event: MODIFIED
  Dec 19 10:16:52.893: INFO: Observed event: MODIFIED
  Dec 19 10:16:52.893: INFO: Found Service test-service-5xrrw in namespace services-2541 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Dec 19 10:16:52.893: INFO: Service test-service-5xrrw deleted
  Dec 19 10:16:52.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2541" for this suite. @ 12/19/23 10:16:52.912
• [0.310 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 12/19/23 10:16:52.937
  Dec 19 10:16:52.938: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename services @ 12/19/23 10:16:52.942
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:16:52.995
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:16:53.001
  STEP: creating service in namespace services-585 @ 12/19/23 10:16:53.009
  STEP: creating service affinity-clusterip-transition in namespace services-585 @ 12/19/23 10:16:53.01
  STEP: creating replication controller affinity-clusterip-transition in namespace services-585 @ 12/19/23 10:16:53.04
  I1219 10:16:53.055893      13 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-585, replica count: 3
  I1219 10:16:56.108712      13 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I1219 10:16:59.110755      13 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I1219 10:17:02.111668      13 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I1219 10:17:05.112417      13 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I1219 10:17:08.113149      13 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I1219 10:17:11.113959      13 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Dec 19 10:17:11.137: INFO: Creating new exec pod
  Dec 19 10:17:14.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-585 exec execpod-affinityhdqgw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Dec 19 10:17:14.571: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Dec 19 10:17:14.571: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 19 10:17:14.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-585 exec execpod-affinityhdqgw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.25.197 80'
  Dec 19 10:17:14.902: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.25.197 80\nConnection to 10.233.25.197 80 port [tcp/http] succeeded!\n"
  Dec 19 10:17:14.902: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 19 10:17:14.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-585 exec execpod-affinityhdqgw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.25.197:80/ ; done'
  Dec 19 10:17:15.471: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n"
  Dec 19 10:17:15.471: INFO: stdout: "\naffinity-clusterip-transition-2fbdg\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-2fbdg\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-8kgkh\naffinity-clusterip-transition-8kgkh\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-2fbdg\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-8kgkh\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-8kgkh\naffinity-clusterip-transition-2fbdg\naffinity-clusterip-transition-8kgkh\naffinity-clusterip-transition-95fkv"
  Dec 19 10:17:15.471: INFO: Received response from host: affinity-clusterip-transition-2fbdg
  Dec 19 10:17:15.471: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:15.471: INFO: Received response from host: affinity-clusterip-transition-2fbdg
  Dec 19 10:17:15.471: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:15.471: INFO: Received response from host: affinity-clusterip-transition-8kgkh
  Dec 19 10:17:15.471: INFO: Received response from host: affinity-clusterip-transition-8kgkh
  Dec 19 10:17:15.472: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:15.472: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:15.472: INFO: Received response from host: affinity-clusterip-transition-2fbdg
  Dec 19 10:17:15.472: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:15.472: INFO: Received response from host: affinity-clusterip-transition-8kgkh
  Dec 19 10:17:15.472: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:15.472: INFO: Received response from host: affinity-clusterip-transition-8kgkh
  Dec 19 10:17:15.472: INFO: Received response from host: affinity-clusterip-transition-2fbdg
  Dec 19 10:17:15.472: INFO: Received response from host: affinity-clusterip-transition-8kgkh
  Dec 19 10:17:15.472: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:15.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-585 exec execpod-affinityhdqgw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.25.197:80/ ; done'
  Dec 19 10:17:16.120: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.25.197:80/\n"
  Dec 19 10:17:16.120: INFO: stdout: "\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-95fkv\naffinity-clusterip-transition-95fkv"
  Dec 19 10:17:16.120: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:16.120: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:16.120: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:16.120: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:16.120: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:16.120: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:16.120: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:16.120: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:16.120: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:16.120: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:16.120: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:16.121: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:16.121: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:16.121: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:16.121: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:16.121: INFO: Received response from host: affinity-clusterip-transition-95fkv
  Dec 19 10:17:16.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 19 10:17:16.144: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-585, will wait for the garbage collector to delete the pods @ 12/19/23 10:17:16.179
  Dec 19 10:17:16.265: INFO: Deleting ReplicationController affinity-clusterip-transition took: 23.295465ms
  Dec 19 10:17:16.366: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.135844ms
  STEP: Destroying namespace "services-585" for this suite. @ 12/19/23 10:17:18.83
• [25.920 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 12/19/23 10:17:18.867
  Dec 19 10:17:18.867: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename init-container @ 12/19/23 10:17:18.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:17:18.915
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:17:18.926
  STEP: creating the pod @ 12/19/23 10:17:18.934
  Dec 19 10:17:18.934: INFO: PodSpec: initContainers in spec.initContainers
  Dec 19 10:17:23.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-9092" for this suite. @ 12/19/23 10:17:23.273
• [4.425 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 12/19/23 10:17:23.295
  Dec 19 10:17:23.295: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename containers @ 12/19/23 10:17:23.297
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:17:23.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:17:23.345
  STEP: Creating a pod to test override command @ 12/19/23 10:17:23.352
  STEP: Saw pod success @ 12/19/23 10:17:27.443
  Dec 19 10:17:27.453: INFO: Trying to get logs from node cahyeife7pae-3 pod client-containers-4617a059-320b-4fc2-9866-6f9d234d36a3 container agnhost-container: <nil>
  STEP: delete the pod @ 12/19/23 10:17:27.466
  Dec 19 10:17:27.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-6142" for this suite. @ 12/19/23 10:17:27.503
• [4.220 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 12/19/23 10:17:27.516
  Dec 19 10:17:27.517: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename emptydir @ 12/19/23 10:17:27.519
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:17:27.56
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:17:27.571
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 12/19/23 10:17:27.578
  STEP: Saw pod success @ 12/19/23 10:17:31.618
  Dec 19 10:17:31.625: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-33963c7f-a5dc-4bbc-885f-a7f9f5e4ff09 container test-container: <nil>
  STEP: delete the pod @ 12/19/23 10:17:31.642
  Dec 19 10:17:31.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9636" for this suite. @ 12/19/23 10:17:31.685
• [4.184 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:899
  STEP: Creating a kubernetes client @ 12/19/23 10:17:31.704
  Dec 19 10:17:31.704: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename statefulset @ 12/19/23 10:17:31.707
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:17:31.751
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:17:31.756
  STEP: Creating service test in namespace statefulset-7622 @ 12/19/23 10:17:31.762
  STEP: Creating statefulset ss in namespace statefulset-7622 @ 12/19/23 10:17:31.775
  Dec 19 10:17:31.800: INFO: Found 0 stateful pods, waiting for 1
  Dec 19 10:17:41.810: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 12/19/23 10:17:41.82
  STEP: updating a scale subresource @ 12/19/23 10:17:41.827
  STEP: verifying the statefulset Spec.Replicas was modified @ 12/19/23 10:17:41.841
  STEP: Patch a scale subresource @ 12/19/23 10:17:41.848
  STEP: verifying the statefulset Spec.Replicas was modified @ 12/19/23 10:17:41.881
  Dec 19 10:17:41.893: INFO: Deleting all statefulset in ns statefulset-7622
  Dec 19 10:17:41.907: INFO: Scaling statefulset ss to 0
  Dec 19 10:17:51.982: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 19 10:17:51.994: INFO: Deleting statefulset ss
  Dec 19 10:17:52.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7622" for this suite. @ 12/19/23 10:17:52.051
• [20.361 seconds]
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 12/19/23 10:17:52.066
  Dec 19 10:17:52.066: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename services @ 12/19/23 10:17:52.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:17:52.142
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:17:52.151
  STEP: creating service multi-endpoint-test in namespace services-7389 @ 12/19/23 10:17:52.158
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7389 to expose endpoints map[] @ 12/19/23 10:17:52.184
  Dec 19 10:17:52.195: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  Dec 19 10:17:53.220: INFO: successfully validated that service multi-endpoint-test in namespace services-7389 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-7389 @ 12/19/23 10:17:53.221
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7389 to expose endpoints map[pod1:[100]] @ 12/19/23 10:17:55.266
  Dec 19 10:17:55.297: INFO: successfully validated that service multi-endpoint-test in namespace services-7389 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-7389 @ 12/19/23 10:17:55.297
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7389 to expose endpoints map[pod1:[100] pod2:[101]] @ 12/19/23 10:17:57.389
  Dec 19 10:17:57.438: INFO: successfully validated that service multi-endpoint-test in namespace services-7389 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 12/19/23 10:17:57.439
  Dec 19 10:17:57.439: INFO: Creating new exec pod
  Dec 19 10:18:00.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7389 exec execpodmzkqj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Dec 19 10:18:00.868: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Dec 19 10:18:00.868: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 19 10:18:00.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7389 exec execpodmzkqj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.61.128 80'
  Dec 19 10:18:01.128: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.61.128 80\nConnection to 10.233.61.128 80 port [tcp/http] succeeded!\n"
  Dec 19 10:18:01.128: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 19 10:18:01.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7389 exec execpodmzkqj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Dec 19 10:18:01.409: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Dec 19 10:18:01.409: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 19 10:18:01.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7389 exec execpodmzkqj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.61.128 81'
  Dec 19 10:18:01.704: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.61.128 81\nConnection to 10.233.61.128 81 port [tcp/*] succeeded!\n"
  Dec 19 10:18:01.704: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-7389 @ 12/19/23 10:18:01.704
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7389 to expose endpoints map[pod2:[101]] @ 12/19/23 10:18:01.741
  Dec 19 10:18:01.778: INFO: successfully validated that service multi-endpoint-test in namespace services-7389 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-7389 @ 12/19/23 10:18:01.779
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7389 to expose endpoints map[] @ 12/19/23 10:18:01.807
  Dec 19 10:18:02.886: INFO: successfully validated that service multi-endpoint-test in namespace services-7389 exposes endpoints map[]
  Dec 19 10:18:02.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7389" for this suite. @ 12/19/23 10:18:02.948
• [10.896 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 12/19/23 10:18:02.963
  Dec 19 10:18:02.963: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename services @ 12/19/23 10:18:02.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:18:03.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:18:03.025
  STEP: creating service in namespace services-6540 @ 12/19/23 10:18:03.031
  STEP: creating service affinity-clusterip in namespace services-6540 @ 12/19/23 10:18:03.031
  STEP: creating replication controller affinity-clusterip in namespace services-6540 @ 12/19/23 10:18:03.049
  I1219 10:18:03.091902      13 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-6540, replica count: 3
  I1219 10:18:06.143186      13 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Dec 19 10:18:06.158: INFO: Creating new exec pod
  Dec 19 10:18:09.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-6540 exec execpod-affinitykdqbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Dec 19 10:18:09.539: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Dec 19 10:18:09.539: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 19 10:18:09.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-6540 exec execpod-affinitykdqbt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.55.56 80'
  Dec 19 10:18:09.835: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.55.56 80\nConnection to 10.233.55.56 80 port [tcp/http] succeeded!\n"
  Dec 19 10:18:09.835: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 19 10:18:09.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-6540 exec execpod-affinitykdqbt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.55.56:80/ ; done'
  Dec 19 10:18:10.478: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.56:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.55.56:80/\n"
  Dec 19 10:18:10.478: INFO: stdout: "\naffinity-clusterip-4jnzt\naffinity-clusterip-4jnzt\naffinity-clusterip-4jnzt\naffinity-clusterip-4jnzt\naffinity-clusterip-4jnzt\naffinity-clusterip-4jnzt\naffinity-clusterip-4jnzt\naffinity-clusterip-4jnzt\naffinity-clusterip-4jnzt\naffinity-clusterip-4jnzt\naffinity-clusterip-4jnzt\naffinity-clusterip-4jnzt\naffinity-clusterip-4jnzt\naffinity-clusterip-4jnzt\naffinity-clusterip-4jnzt\naffinity-clusterip-4jnzt"
  Dec 19 10:18:10.479: INFO: Received response from host: affinity-clusterip-4jnzt
  Dec 19 10:18:10.479: INFO: Received response from host: affinity-clusterip-4jnzt
  Dec 19 10:18:10.479: INFO: Received response from host: affinity-clusterip-4jnzt
  Dec 19 10:18:10.479: INFO: Received response from host: affinity-clusterip-4jnzt
  Dec 19 10:18:10.479: INFO: Received response from host: affinity-clusterip-4jnzt
  Dec 19 10:18:10.479: INFO: Received response from host: affinity-clusterip-4jnzt
  Dec 19 10:18:10.479: INFO: Received response from host: affinity-clusterip-4jnzt
  Dec 19 10:18:10.479: INFO: Received response from host: affinity-clusterip-4jnzt
  Dec 19 10:18:10.479: INFO: Received response from host: affinity-clusterip-4jnzt
  Dec 19 10:18:10.479: INFO: Received response from host: affinity-clusterip-4jnzt
  Dec 19 10:18:10.479: INFO: Received response from host: affinity-clusterip-4jnzt
  Dec 19 10:18:10.479: INFO: Received response from host: affinity-clusterip-4jnzt
  Dec 19 10:18:10.479: INFO: Received response from host: affinity-clusterip-4jnzt
  Dec 19 10:18:10.479: INFO: Received response from host: affinity-clusterip-4jnzt
  Dec 19 10:18:10.479: INFO: Received response from host: affinity-clusterip-4jnzt
  Dec 19 10:18:10.479: INFO: Received response from host: affinity-clusterip-4jnzt
  Dec 19 10:18:10.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 19 10:18:10.489: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-6540, will wait for the garbage collector to delete the pods @ 12/19/23 10:18:10.521
  Dec 19 10:18:10.601: INFO: Deleting ReplicationController affinity-clusterip took: 15.815586ms
  Dec 19 10:18:10.702: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.798853ms
  STEP: Destroying namespace "services-6540" for this suite. @ 12/19/23 10:18:13.233
• [10.283 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 12/19/23 10:18:13.248
  Dec 19 10:18:13.248: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 10:18:13.25
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:18:13.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:18:13.295
  STEP: Creating projection with secret that has name projected-secret-test-3f5b6aaa-2b7a-405d-ad9f-992ae5d3c80d @ 12/19/23 10:18:13.301
  STEP: Creating a pod to test consume secrets @ 12/19/23 10:18:13.311
  STEP: Saw pod success @ 12/19/23 10:18:17.358
  Dec 19 10:18:17.366: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-projected-secrets-951862ee-3cae-4747-bd53-525cc3d2beae container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 12/19/23 10:18:17.388
  Dec 19 10:18:17.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1059" for this suite. @ 12/19/23 10:18:17.438
• [4.203 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 12/19/23 10:18:17.465
  Dec 19 10:18:17.465: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubelet-test @ 12/19/23 10:18:17.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:18:17.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:18:17.506
  Dec 19 10:18:21.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-7458" for this suite. @ 12/19/23 10:18:21.584
• [4.136 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 12/19/23 10:18:21.602
  Dec 19 10:18:21.602: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-probe @ 12/19/23 10:18:21.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:18:21.649
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:18:21.655
  STEP: Creating pod test-grpc-2361bb03-afd3-4d09-90f2-7a700decc821 in namespace container-probe-7902 @ 12/19/23 10:18:21.662
  Dec 19 10:18:23.721: INFO: Started pod test-grpc-2361bb03-afd3-4d09-90f2-7a700decc821 in namespace container-probe-7902
  STEP: checking the pod's current state and verifying that restartCount is present @ 12/19/23 10:18:23.721
  Dec 19 10:18:23.731: INFO: Initial restart count of pod test-grpc-2361bb03-afd3-4d09-90f2-7a700decc821 is 0
  Dec 19 10:22:25.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/19/23 10:22:25.218
  STEP: Destroying namespace "container-probe-7902" for this suite. @ 12/19/23 10:22:25.271
• [243.711 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 12/19/23 10:22:25.328
  Dec 19 10:22:25.328: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename watch @ 12/19/23 10:22:25.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:22:25.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:22:25.392
  STEP: creating a new configmap @ 12/19/23 10:22:25.402
  STEP: modifying the configmap once @ 12/19/23 10:22:25.419
  STEP: modifying the configmap a second time @ 12/19/23 10:22:25.443
  STEP: deleting the configmap @ 12/19/23 10:22:25.468
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 12/19/23 10:22:25.49
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 12/19/23 10:22:25.497
  Dec 19 10:22:25.498: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6546  d74298f8-2aa1-405c-b45c-78f514f0f44c 9617 0 2023-12-19 10:22:25 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-12-19 10:22:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 19 10:22:25.498: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6546  d74298f8-2aa1-405c-b45c-78f514f0f44c 9618 0 2023-12-19 10:22:25 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-12-19 10:22:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 19 10:22:25.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-6546" for this suite. @ 12/19/23 10:22:25.561
• [0.251 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 12/19/23 10:22:25.579
  Dec 19 10:22:25.579: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename crd-publish-openapi @ 12/19/23 10:22:25.581
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:22:25.627
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:22:25.634
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 12/19/23 10:22:25.643
  Dec 19 10:22:25.645: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:22:27.904: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:22:36.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1555" for this suite. @ 12/19/23 10:22:36.8
• [11.242 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 12/19/23 10:22:36.823
  Dec 19 10:22:36.823: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename configmap @ 12/19/23 10:22:36.826
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:22:36.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:22:36.891
  STEP: Creating configMap with name configmap-test-volume-map-c809e4d3-89b3-43ce-984e-4ab07ffe0e59 @ 12/19/23 10:22:36.897
  STEP: Creating a pod to test consume configMaps @ 12/19/23 10:22:36.92
  STEP: Saw pod success @ 12/19/23 10:22:41.033
  Dec 19 10:22:41.040: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-configmaps-87920d78-b74d-45fb-86b5-1557ac30b675 container agnhost-container: <nil>
  STEP: delete the pod @ 12/19/23 10:22:41.074
  Dec 19 10:22:41.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2769" for this suite. @ 12/19/23 10:22:41.122
• [4.314 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 12/19/23 10:22:41.143
  Dec 19 10:22:41.143: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename pod-network-test @ 12/19/23 10:22:41.147
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:22:41.199
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:22:41.206
  STEP: Performing setup for networking test in namespace pod-network-test-3340 @ 12/19/23 10:22:41.213
  STEP: creating a selector @ 12/19/23 10:22:41.213
  STEP: Creating the service pods in kubernetes @ 12/19/23 10:22:41.214
  Dec 19 10:22:41.214: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 12/19/23 10:23:03.591
  Dec 19 10:23:05.629: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Dec 19 10:23:05.629: INFO: Breadth first check of 10.233.64.41 on host 192.168.121.236...
  Dec 19 10:23:05.637: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.79:9080/dial?request=hostname&protocol=http&host=10.233.64.41&port=8083&tries=1'] Namespace:pod-network-test-3340 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 10:23:05.637: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:23:05.639: INFO: ExecWithOptions: Clientset creation
  Dec 19 10:23:05.639: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3340/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.79%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.41%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Dec 19 10:23:05.868: INFO: Waiting for responses: map[]
  Dec 19 10:23:05.869: INFO: reached 10.233.64.41 after 0/1 tries
  Dec 19 10:23:05.869: INFO: Breadth first check of 10.233.65.42 on host 192.168.121.223...
  Dec 19 10:23:05.879: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.79:9080/dial?request=hostname&protocol=http&host=10.233.65.42&port=8083&tries=1'] Namespace:pod-network-test-3340 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 10:23:05.879: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:23:05.881: INFO: ExecWithOptions: Clientset creation
  Dec 19 10:23:05.882: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3340/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.79%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.65.42%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Dec 19 10:23:06.020: INFO: Waiting for responses: map[]
  Dec 19 10:23:06.021: INFO: reached 10.233.65.42 after 0/1 tries
  Dec 19 10:23:06.021: INFO: Breadth first check of 10.233.66.78 on host 192.168.121.61...
  Dec 19 10:23:06.029: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.79:9080/dial?request=hostname&protocol=http&host=10.233.66.78&port=8083&tries=1'] Namespace:pod-network-test-3340 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 10:23:06.029: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:23:06.030: INFO: ExecWithOptions: Clientset creation
  Dec 19 10:23:06.031: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3340/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.79%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.78%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Dec 19 10:23:06.126: INFO: Waiting for responses: map[]
  Dec 19 10:23:06.126: INFO: reached 10.233.66.78 after 0/1 tries
  Dec 19 10:23:06.126: INFO: Going to retry 0 out of 3 pods....
  Dec 19 10:23:06.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-3340" for this suite. @ 12/19/23 10:23:06.135
• [25.008 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 12/19/23 10:23:06.164
  Dec 19 10:23:06.164: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename subpath @ 12/19/23 10:23:06.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:23:06.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:23:06.206
  STEP: Setting up data @ 12/19/23 10:23:06.21
  STEP: Creating pod pod-subpath-test-secret-lp2x @ 12/19/23 10:23:06.231
  STEP: Creating a pod to test atomic-volume-subpath @ 12/19/23 10:23:06.231
  STEP: Saw pod success @ 12/19/23 10:23:30.438
  Dec 19 10:23:30.450: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-subpath-test-secret-lp2x container test-container-subpath-secret-lp2x: <nil>
  STEP: delete the pod @ 12/19/23 10:23:30.476
  STEP: Deleting pod pod-subpath-test-secret-lp2x @ 12/19/23 10:23:30.516
  Dec 19 10:23:30.516: INFO: Deleting pod "pod-subpath-test-secret-lp2x" in namespace "subpath-7612"
  Dec 19 10:23:30.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7612" for this suite. @ 12/19/23 10:23:30.541
• [24.400 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 12/19/23 10:23:30.565
  Dec 19 10:23:30.566: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-probe @ 12/19/23 10:23:30.57
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:23:30.608
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:23:30.616
  STEP: Creating pod liveness-34f0e920-ae75-4b7e-ac50-2fa57bfeba58 in namespace container-probe-492 @ 12/19/23 10:23:30.627
  Dec 19 10:23:32.673: INFO: Started pod liveness-34f0e920-ae75-4b7e-ac50-2fa57bfeba58 in namespace container-probe-492
  STEP: checking the pod's current state and verifying that restartCount is present @ 12/19/23 10:23:32.673
  Dec 19 10:23:32.683: INFO: Initial restart count of pod liveness-34f0e920-ae75-4b7e-ac50-2fa57bfeba58 is 0
  Dec 19 10:23:52.817: INFO: Restart count of pod container-probe-492/liveness-34f0e920-ae75-4b7e-ac50-2fa57bfeba58 is now 1 (20.132976524s elapsed)
  Dec 19 10:24:12.933: INFO: Restart count of pod container-probe-492/liveness-34f0e920-ae75-4b7e-ac50-2fa57bfeba58 is now 2 (40.249245562s elapsed)
  Dec 19 10:24:33.081: INFO: Restart count of pod container-probe-492/liveness-34f0e920-ae75-4b7e-ac50-2fa57bfeba58 is now 3 (1m0.397411845s elapsed)
  Dec 19 10:24:53.201: INFO: Restart count of pod container-probe-492/liveness-34f0e920-ae75-4b7e-ac50-2fa57bfeba58 is now 4 (1m20.517654551s elapsed)
  Dec 19 10:26:03.705: INFO: Restart count of pod container-probe-492/liveness-34f0e920-ae75-4b7e-ac50-2fa57bfeba58 is now 5 (2m31.021362867s elapsed)
  Dec 19 10:26:03.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/19/23 10:26:03.72
  STEP: Destroying namespace "container-probe-492" for this suite. @ 12/19/23 10:26:03.747
• [153.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 12/19/23 10:26:03.775
  Dec 19 10:26:03.775: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename disruption @ 12/19/23 10:26:03.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:26:03.833
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:26:03.841
  STEP: Creating a pdb that targets all three pods in a test replica set @ 12/19/23 10:26:03.846
  STEP: Waiting for the pdb to be processed @ 12/19/23 10:26:03.865
  STEP: First trying to evict a pod which shouldn't be evictable @ 12/19/23 10:26:05.907
  STEP: Waiting for all pods to be running @ 12/19/23 10:26:05.908
  Dec 19 10:26:05.920: INFO: pods: 0 < 3
  Dec 19 10:26:07.936: INFO: running pods: 2 < 3
  STEP: locating a running pod @ 12/19/23 10:26:09.939
  STEP: Updating the pdb to allow a pod to be evicted @ 12/19/23 10:26:09.962
  STEP: Waiting for the pdb to be processed @ 12/19/23 10:26:09.981
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 12/19/23 10:26:12.003
  STEP: Waiting for all pods to be running @ 12/19/23 10:26:12.003
  STEP: Waiting for the pdb to observed all healthy pods @ 12/19/23 10:26:12.013
  STEP: Patching the pdb to disallow a pod to be evicted @ 12/19/23 10:26:12.081
  STEP: Waiting for the pdb to be processed @ 12/19/23 10:26:12.156
  STEP: Waiting for all pods to be running @ 12/19/23 10:26:14.19
  STEP: locating a running pod @ 12/19/23 10:26:14.199
  STEP: Deleting the pdb to allow a pod to be evicted @ 12/19/23 10:26:14.239
  STEP: Waiting for the pdb to be deleted @ 12/19/23 10:26:14.266
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 12/19/23 10:26:14.274
  STEP: Waiting for all pods to be running @ 12/19/23 10:26:14.274
  Dec 19 10:26:14.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-6001" for this suite. @ 12/19/23 10:26:14.484
• [10.762 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 12/19/23 10:26:14.539
  Dec 19 10:26:14.539: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename secrets @ 12/19/23 10:26:14.541
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:26:14.591
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:26:14.602
  STEP: creating secret secrets-9060/secret-test-4fde0e3a-b1f2-474f-9e03-bd0688bdbd99 @ 12/19/23 10:26:14.61
  STEP: Creating a pod to test consume secrets @ 12/19/23 10:26:14.624
  STEP: Saw pod success @ 12/19/23 10:26:18.677
  Dec 19 10:26:18.685: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-configmaps-f4e240b7-0d7a-4a85-afe6-30c72153630a container env-test: <nil>
  STEP: delete the pod @ 12/19/23 10:26:18.721
  Dec 19 10:26:18.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9060" for this suite. @ 12/19/23 10:26:18.761
• [4.234 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 12/19/23 10:26:18.774
  Dec 19 10:26:18.774: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename tables @ 12/19/23 10:26:18.776
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:26:18.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:26:18.809
  Dec 19 10:26:18.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-3234" for this suite. @ 12/19/23 10:26:18.829
• [0.068 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 12/19/23 10:26:18.845
  Dec 19 10:26:18.845: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename secrets @ 12/19/23 10:26:18.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:26:18.882
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:26:18.889
  STEP: Creating secret with name secret-test-map-d4aea388-587e-4e4d-aaea-8fe84ef3b5a2 @ 12/19/23 10:26:18.896
  STEP: Creating a pod to test consume secrets @ 12/19/23 10:26:18.906
  STEP: Saw pod success @ 12/19/23 10:26:22.961
  Dec 19 10:26:22.968: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-secrets-8c6bbc25-cacf-4701-95be-16cae6d5308a container secret-volume-test: <nil>
  STEP: delete the pod @ 12/19/23 10:26:22.983
  Dec 19 10:26:23.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5587" for this suite. @ 12/19/23 10:26:23.029
• [4.203 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:836
  STEP: Creating a kubernetes client @ 12/19/23 10:26:23.061
  Dec 19 10:26:23.061: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename daemonsets @ 12/19/23 10:26:23.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:26:23.099
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:26:23.109
  STEP: Creating simple DaemonSet "daemon-set" @ 12/19/23 10:26:23.161
  STEP: Check that daemon pods launch on every node of the cluster. @ 12/19/23 10:26:23.179
  Dec 19 10:26:23.202: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:26:23.202: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:26:24.227: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 19 10:26:24.227: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:26:25.238: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 19 10:26:25.238: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:26:26.225: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 19 10:26:26.225: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:26:27.221: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Dec 19 10:26:27.222: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 12/19/23 10:26:27.228
  STEP: DeleteCollection of the DaemonSets @ 12/19/23 10:26:27.239
  STEP: Verify that ReplicaSets have been deleted @ 12/19/23 10:26:27.258
  Dec 19 10:26:27.306: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10472"},"items":null}

  Dec 19 10:26:27.319: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10472"},"items":[{"metadata":{"name":"daemon-set-6sfjq","generateName":"daemon-set-","namespace":"daemonsets-2797","uid":"7af18e84-877b-4ab8-a3c6-cad0df81dc97","resourceVersion":"10458","creationTimestamp":"2023-12-19T10:26:23Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d17b710a-6210-468c-8f04-2cb9965d3407","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-12-19T10:26:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d17b710a-6210-468c-8f04-2cb9965d3407\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-12-19T10:26:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pgdm4","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pgdm4","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cahyeife7pae-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cahyeife7pae-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-19T10:26:23Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-19T10:26:24Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-19T10:26:24Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-19T10:26:23Z"}],"hostIP":"192.168.121.61","podIP":"10.233.66.85","podIPs":[{"ip":"10.233.66.85"}],"startTime":"2023-12-19T10:26:23Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-12-19T10:26:23Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://bbcf63c1701fdad49ebad0e1013b2ea9965dcc503d590682914a5e95a2d3cb03","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-dzsk7","generateName":"daemon-set-","namespace":"daemonsets-2797","uid":"23dd4f00-05c0-4dc0-9011-ea1cbf812930","resourceVersion":"10472","creationTimestamp":"2023-12-19T10:26:23Z","deletionTimestamp":"2023-12-19T10:26:57Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d17b710a-6210-468c-8f04-2cb9965d3407","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-12-19T10:26:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d17b710a-6210-468c-8f04-2cb9965d3407\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-12-19T10:26:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4jplc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4jplc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cahyeife7pae-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cahyeife7pae-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-19T10:26:23Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-19T10:26:24Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-19T10:26:24Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-19T10:26:23Z"}],"hostIP":"192.168.121.223","podIP":"10.233.65.45","podIPs":[{"ip":"10.233.65.45"}],"startTime":"2023-12-19T10:26:23Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-12-19T10:26:23Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://bf667c4da0afe02ef0270019c4a8b046cdeb8ae24e4136cdd6e828bc8010deff","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-t4t9d","generateName":"daemon-set-","namespace":"daemonsets-2797","uid":"b2632ec2-a3ec-4f42-9885-cb3a578a57eb","resourceVersion":"10467","creationTimestamp":"2023-12-19T10:26:23Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d17b710a-6210-468c-8f04-2cb9965d3407","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-12-19T10:26:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d17b710a-6210-468c-8f04-2cb9965d3407\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-12-19T10:26:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-n5cc2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-n5cc2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cahyeife7pae-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cahyeife7pae-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-19T10:26:23Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-19T10:26:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-19T10:26:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-12-19T10:26:23Z"}],"hostIP":"192.168.121.236","podIP":"10.233.64.43","podIPs":[{"ip":"10.233.64.43"}],"startTime":"2023-12-19T10:26:23Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-12-19T10:26:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://2ccfc5d4325c8c04054de4613dbfa4a2c6f51879a4909f99e4126be6182a1203","started":true}],"qosClass":"BestEffort"}}]}

  Dec 19 10:26:27.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2797" for this suite. @ 12/19/23 10:26:27.516
• [4.482 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 12/19/23 10:26:27.544
  Dec 19 10:26:27.544: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename webhook @ 12/19/23 10:26:27.546
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:26:27.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:26:27.629
  STEP: Setting up server cert @ 12/19/23 10:26:27.682
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/19/23 10:26:29.113
  STEP: Deploying the webhook pod @ 12/19/23 10:26:29.133
  STEP: Wait for the deployment to be ready @ 12/19/23 10:26:29.153
  Dec 19 10:26:29.168: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 12/19/23 10:26:31.202
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 10:26:31.235
  Dec 19 10:26:32.236: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Dec 19 10:26:32.252: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8558-crds.webhook.example.com via the AdmissionRegistration API @ 12/19/23 10:26:32.783
  STEP: Creating a custom resource that should be mutated by the webhook @ 12/19/23 10:26:32.841
  Dec 19 10:26:35.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1078" for this suite. @ 12/19/23 10:26:35.923
  STEP: Destroying namespace "webhook-markers-8840" for this suite. @ 12/19/23 10:26:35.937
• [8.409 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 12/19/23 10:26:35.955
  Dec 19 10:26:35.955: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename replicaset @ 12/19/23 10:26:35.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:26:36.004
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:26:36.015
  STEP: Create a ReplicaSet @ 12/19/23 10:26:36.024
  STEP: Verify that the required pods have come up @ 12/19/23 10:26:36.037
  Dec 19 10:26:36.051: INFO: Pod name sample-pod: Found 0 pods out of 3
  Dec 19 10:26:41.067: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 12/19/23 10:26:41.068
  Dec 19 10:26:41.073: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 12/19/23 10:26:41.074
  STEP: DeleteCollection of the ReplicaSets @ 12/19/23 10:26:41.081
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 12/19/23 10:26:41.096
  Dec 19 10:26:41.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7661" for this suite. @ 12/19/23 10:26:41.13
• [5.233 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 12/19/23 10:26:41.199
  Dec 19 10:26:41.199: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename secrets @ 12/19/23 10:26:41.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:26:41.32
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:26:41.329
  STEP: Creating secret with name secret-test-506a1bcb-f31d-42b6-917d-326b1c8cc80b @ 12/19/23 10:26:41.337
  STEP: Creating a pod to test consume secrets @ 12/19/23 10:26:41.349
  STEP: Saw pod success @ 12/19/23 10:26:45.42
  Dec 19 10:26:45.427: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-secrets-e05ac5c8-0c4d-42a6-b194-5cfc2a0333b1 container secret-volume-test: <nil>
  STEP: delete the pod @ 12/19/23 10:26:45.451
  Dec 19 10:26:45.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-995" for this suite. @ 12/19/23 10:26:45.569
• [4.389 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 12/19/23 10:26:45.605
  Dec 19 10:26:45.606: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename services @ 12/19/23 10:26:45.609
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:26:45.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:26:45.648
  STEP: creating service in namespace services-7251 @ 12/19/23 10:26:45.654
  STEP: creating service affinity-nodeport in namespace services-7251 @ 12/19/23 10:26:45.655
  STEP: creating replication controller affinity-nodeport in namespace services-7251 @ 12/19/23 10:26:45.694
  I1219 10:26:45.707707      13 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-7251, replica count: 3
  I1219 10:26:48.760140      13 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Dec 19 10:26:48.798: INFO: Creating new exec pod
  Dec 19 10:26:51.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7251 exec execpod-affinitykwq8l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Dec 19 10:26:52.212: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Dec 19 10:26:52.212: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 19 10:26:52.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7251 exec execpod-affinitykwq8l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.63.44 80'
  Dec 19 10:26:52.539: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.63.44 80\nConnection to 10.233.63.44 80 port [tcp/http] succeeded!\n"
  Dec 19 10:26:52.539: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 19 10:26:52.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7251 exec execpod-affinitykwq8l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.223 32394'
  Dec 19 10:26:52.815: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.223 32394\nConnection to 192.168.121.223 32394 port [tcp/*] succeeded!\n"
  Dec 19 10:26:52.815: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 19 10:26:52.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7251 exec execpod-affinitykwq8l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.236 32394'
  Dec 19 10:26:53.113: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.236 32394\nConnection to 192.168.121.236 32394 port [tcp/*] succeeded!\n"
  Dec 19 10:26:53.113: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 19 10:26:53.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7251 exec execpod-affinitykwq8l -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.236:32394/ ; done'
  Dec 19 10:26:53.658: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:32394/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:32394/\n"
  Dec 19 10:26:53.658: INFO: stdout: "\naffinity-nodeport-bx5gf\naffinity-nodeport-bx5gf\naffinity-nodeport-bx5gf\naffinity-nodeport-bx5gf\naffinity-nodeport-bx5gf\naffinity-nodeport-bx5gf\naffinity-nodeport-bx5gf\naffinity-nodeport-bx5gf\naffinity-nodeport-bx5gf\naffinity-nodeport-bx5gf\naffinity-nodeport-bx5gf\naffinity-nodeport-bx5gf\naffinity-nodeport-bx5gf\naffinity-nodeport-bx5gf\naffinity-nodeport-bx5gf\naffinity-nodeport-bx5gf"
  Dec 19 10:26:53.658: INFO: Received response from host: affinity-nodeport-bx5gf
  Dec 19 10:26:53.658: INFO: Received response from host: affinity-nodeport-bx5gf
  Dec 19 10:26:53.658: INFO: Received response from host: affinity-nodeport-bx5gf
  Dec 19 10:26:53.658: INFO: Received response from host: affinity-nodeport-bx5gf
  Dec 19 10:26:53.658: INFO: Received response from host: affinity-nodeport-bx5gf
  Dec 19 10:26:53.658: INFO: Received response from host: affinity-nodeport-bx5gf
  Dec 19 10:26:53.658: INFO: Received response from host: affinity-nodeport-bx5gf
  Dec 19 10:26:53.658: INFO: Received response from host: affinity-nodeport-bx5gf
  Dec 19 10:26:53.658: INFO: Received response from host: affinity-nodeport-bx5gf
  Dec 19 10:26:53.658: INFO: Received response from host: affinity-nodeport-bx5gf
  Dec 19 10:26:53.658: INFO: Received response from host: affinity-nodeport-bx5gf
  Dec 19 10:26:53.658: INFO: Received response from host: affinity-nodeport-bx5gf
  Dec 19 10:26:53.658: INFO: Received response from host: affinity-nodeport-bx5gf
  Dec 19 10:26:53.658: INFO: Received response from host: affinity-nodeport-bx5gf
  Dec 19 10:26:53.658: INFO: Received response from host: affinity-nodeport-bx5gf
  Dec 19 10:26:53.658: INFO: Received response from host: affinity-nodeport-bx5gf
  Dec 19 10:26:53.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 19 10:26:53.670: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-7251, will wait for the garbage collector to delete the pods @ 12/19/23 10:26:53.693
  Dec 19 10:26:53.769: INFO: Deleting ReplicationController affinity-nodeport took: 15.327037ms
  Dec 19 10:26:53.871: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.831307ms
  STEP: Destroying namespace "services-7251" for this suite. @ 12/19/23 10:26:56.03
• [10.440 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 12/19/23 10:26:56.049
  Dec 19 10:26:56.049: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename sched-pred @ 12/19/23 10:26:56.052
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:26:56.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:26:56.096
  Dec 19 10:26:56.102: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Dec 19 10:26:56.122: INFO: Waiting for terminating namespaces to be deleted...
  Dec 19 10:26:56.129: INFO: 
  Logging pods the apiserver thinks is on node cahyeife7pae-1 before test
  Dec 19 10:26:56.146: INFO: rs-zb2dp from disruption-6001 started at 2023-12-19 10:26:14 +0000 UTC (1 container statuses recorded)
  Dec 19 10:26:56.146: INFO: 	Container donothing ready: false, restart count 0
  Dec 19 10:26:56.146: INFO: coredns-5d78c9869d-b55f2 from kube-system started at 2023-12-19 09:34:24 +0000 UTC (1 container statuses recorded)
  Dec 19 10:26:56.146: INFO: 	Container coredns ready: true, restart count 1
  Dec 19 10:26:56.146: INFO: coredns-5d78c9869d-pvg7c from kube-system started at 2023-12-19 09:34:24 +0000 UTC (1 container statuses recorded)
  Dec 19 10:26:56.146: INFO: 	Container coredns ready: true, restart count 1
  Dec 19 10:26:56.146: INFO: kube-addon-manager-cahyeife7pae-1 from kube-system started at 2023-12-19 09:46:25 +0000 UTC (1 container statuses recorded)
  Dec 19 10:26:56.146: INFO: 	Container kube-addon-manager ready: true, restart count 1
  Dec 19 10:26:56.146: INFO: kube-apiserver-cahyeife7pae-1 from kube-system started at 2023-12-19 09:46:25 +0000 UTC (1 container statuses recorded)
  Dec 19 10:26:56.146: INFO: 	Container kube-apiserver ready: true, restart count 1
  Dec 19 10:26:56.146: INFO: kube-controller-manager-cahyeife7pae-1 from kube-system started at 2023-12-19 09:46:25 +0000 UTC (1 container statuses recorded)
  Dec 19 10:26:56.146: INFO: 	Container kube-controller-manager ready: true, restart count 1
  Dec 19 10:26:56.146: INFO: kube-flannel-ds-84xmx from kube-system started at 2023-12-19 09:35:44 +0000 UTC (1 container statuses recorded)
  Dec 19 10:26:56.146: INFO: 	Container kube-flannel ready: true, restart count 1
  Dec 19 10:26:56.146: INFO: kube-proxy-xmh99 from kube-system started at 2023-12-19 09:34:23 +0000 UTC (1 container statuses recorded)
  Dec 19 10:26:56.146: INFO: 	Container kube-proxy ready: true, restart count 1
  Dec 19 10:26:56.146: INFO: kube-scheduler-cahyeife7pae-1 from kube-system started at 2023-12-19 09:46:25 +0000 UTC (1 container statuses recorded)
  Dec 19 10:26:56.146: INFO: 	Container kube-scheduler ready: true, restart count 1
  Dec 19 10:26:56.147: INFO: sonobuoy-systemd-logs-daemon-set-a1a4b21fb49145dd-557wq from sonobuoy started at 2023-12-19 10:01:11 +0000 UTC (2 container statuses recorded)
  Dec 19 10:26:56.147: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 19 10:26:56.147: INFO: 	Container systemd-logs ready: true, restart count 0
  Dec 19 10:26:56.147: INFO: 
  Logging pods the apiserver thinks is on node cahyeife7pae-2 before test
  Dec 19 10:26:56.164: INFO: kube-addon-manager-cahyeife7pae-2 from kube-system started at 2023-12-19 09:46:48 +0000 UTC (1 container statuses recorded)
  Dec 19 10:26:56.164: INFO: 	Container kube-addon-manager ready: true, restart count 1
  Dec 19 10:26:56.164: INFO: kube-apiserver-cahyeife7pae-2 from kube-system started at 2023-12-19 09:46:48 +0000 UTC (1 container statuses recorded)
  Dec 19 10:26:56.164: INFO: 	Container kube-apiserver ready: true, restart count 1
  Dec 19 10:26:56.164: INFO: kube-controller-manager-cahyeife7pae-2 from kube-system started at 2023-12-19 09:46:48 +0000 UTC (1 container statuses recorded)
  Dec 19 10:26:56.165: INFO: 	Container kube-controller-manager ready: true, restart count 1
  Dec 19 10:26:56.165: INFO: kube-flannel-ds-zfl5g from kube-system started at 2023-12-19 09:35:44 +0000 UTC (1 container statuses recorded)
  Dec 19 10:26:56.165: INFO: 	Container kube-flannel ready: true, restart count 1
  Dec 19 10:26:56.165: INFO: kube-proxy-qhj8b from kube-system started at 2023-12-19 09:34:50 +0000 UTC (1 container statuses recorded)
  Dec 19 10:26:56.165: INFO: 	Container kube-proxy ready: true, restart count 1
  Dec 19 10:26:56.165: INFO: kube-scheduler-cahyeife7pae-2 from kube-system started at 2023-12-19 09:46:48 +0000 UTC (1 container statuses recorded)
  Dec 19 10:26:56.165: INFO: 	Container kube-scheduler ready: true, restart count 1
  Dec 19 10:26:56.165: INFO: sonobuoy-systemd-logs-daemon-set-a1a4b21fb49145dd-rfjdj from sonobuoy started at 2023-12-19 10:01:11 +0000 UTC (2 container statuses recorded)
  Dec 19 10:26:56.165: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 19 10:26:56.165: INFO: 	Container systemd-logs ready: true, restart count 0
  Dec 19 10:26:56.165: INFO: 
  Logging pods the apiserver thinks is on node cahyeife7pae-3 before test
  Dec 19 10:26:56.179: INFO: kube-flannel-ds-knpnq from kube-system started at 2023-12-19 10:14:07 +0000 UTC (1 container statuses recorded)
  Dec 19 10:26:56.179: INFO: 	Container kube-flannel ready: true, restart count 0
  Dec 19 10:26:56.179: INFO: kube-proxy-v2qp9 from kube-system started at 2023-12-19 09:35:14 +0000 UTC (1 container statuses recorded)
  Dec 19 10:26:56.179: INFO: 	Container kube-proxy ready: true, restart count 1
  Dec 19 10:26:56.179: INFO: sonobuoy from sonobuoy started at 2023-12-19 10:01:00 +0000 UTC (1 container statuses recorded)
  Dec 19 10:26:56.179: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Dec 19 10:26:56.179: INFO: sonobuoy-e2e-job-b6558dadff6840cc from sonobuoy started at 2023-12-19 10:01:11 +0000 UTC (2 container statuses recorded)
  Dec 19 10:26:56.179: INFO: 	Container e2e ready: true, restart count 0
  Dec 19 10:26:56.179: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 19 10:26:56.179: INFO: sonobuoy-systemd-logs-daemon-set-a1a4b21fb49145dd-xsfr4 from sonobuoy started at 2023-12-19 10:01:11 +0000 UTC (2 container statuses recorded)
  Dec 19 10:26:56.179: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 19 10:26:56.179: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 12/19/23 10:26:56.18
  STEP: Explicitly delete pod here to free the resource it takes. @ 12/19/23 10:26:58.219
  STEP: Trying to apply a random label on the found node. @ 12/19/23 10:26:58.27
  STEP: verifying the node has the label kubernetes.io/e2e-74e41817-bc79-4cd0-995f-c353dbd07189 95 @ 12/19/23 10:26:58.3
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 12/19/23 10:26:58.314
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.61 on the node which pod4 resides and expect not scheduled @ 12/19/23 10:27:00.428
  STEP: removing the label kubernetes.io/e2e-74e41817-bc79-4cd0-995f-c353dbd07189 off the node cahyeife7pae-3 @ 12/19/23 10:32:00.447
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-74e41817-bc79-4cd0-995f-c353dbd07189 @ 12/19/23 10:32:00.481
  Dec 19 10:32:00.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-3397" for this suite. @ 12/19/23 10:32:00.511
• [304.477 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 12/19/23 10:32:00.528
  Dec 19 10:32:00.528: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename cronjob @ 12/19/23 10:32:00.535
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:32:00.58
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:32:00.587
  STEP: Creating a cronjob @ 12/19/23 10:32:00.593
  STEP: Ensuring more than one job is running at a time @ 12/19/23 10:32:00.612
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 12/19/23 10:34:00.625
  STEP: Removing cronjob @ 12/19/23 10:34:00.639
  Dec 19 10:34:00.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3214" for this suite. @ 12/19/23 10:34:00.675
• [120.172 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 12/19/23 10:34:00.702
  Dec 19 10:34:00.702: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename replication-controller @ 12/19/23 10:34:00.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:34:00.755
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:34:00.764
  STEP: Creating replication controller my-hostname-basic-69f75448-cdb6-4187-b373-b2bdb33b2efd @ 12/19/23 10:34:00.77
  Dec 19 10:34:00.795: INFO: Pod name my-hostname-basic-69f75448-cdb6-4187-b373-b2bdb33b2efd: Found 0 pods out of 1
  Dec 19 10:34:05.809: INFO: Pod name my-hostname-basic-69f75448-cdb6-4187-b373-b2bdb33b2efd: Found 1 pods out of 1
  Dec 19 10:34:05.809: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-69f75448-cdb6-4187-b373-b2bdb33b2efd" are running
  Dec 19 10:34:05.818: INFO: Pod "my-hostname-basic-69f75448-cdb6-4187-b373-b2bdb33b2efd-9sxsr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-19 10:34:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-19 10:34:01 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-19 10:34:01 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-19 10:34:00 +0000 UTC Reason: Message:}])
  Dec 19 10:34:05.818: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 12/19/23 10:34:05.819
  Dec 19 10:34:05.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9224" for this suite. @ 12/19/23 10:34:05.868
• [5.179 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 12/19/23 10:34:05.891
  Dec 19 10:34:05.892: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename disruption @ 12/19/23 10:34:05.894
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:34:05.927
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:34:05.938
  STEP: Creating a kubernetes client @ 12/19/23 10:34:05.956
  Dec 19 10:34:05.956: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename disruption-2 @ 12/19/23 10:34:05.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:34:05.999
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:34:06.013
  STEP: Waiting for the pdb to be processed @ 12/19/23 10:34:06.034
  STEP: Waiting for the pdb to be processed @ 12/19/23 10:34:08.068
  STEP: Waiting for the pdb to be processed @ 12/19/23 10:34:10.097
  STEP: listing a collection of PDBs across all namespaces @ 12/19/23 10:34:12.114
  STEP: listing a collection of PDBs in namespace disruption-5966 @ 12/19/23 10:34:12.12
  STEP: deleting a collection of PDBs @ 12/19/23 10:34:12.129
  STEP: Waiting for the PDB collection to be deleted @ 12/19/23 10:34:12.18
  Dec 19 10:34:12.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 19 10:34:12.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-9191" for this suite. @ 12/19/23 10:34:12.214
  STEP: Destroying namespace "disruption-5966" for this suite. @ 12/19/23 10:34:12.229
• [6.362 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 12/19/23 10:34:12.254
  Dec 19 10:34:12.254: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename watch @ 12/19/23 10:34:12.257
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:34:12.295
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:34:12.3
  STEP: creating a watch on configmaps with a certain label @ 12/19/23 10:34:12.308
  STEP: creating a new configmap @ 12/19/23 10:34:12.31
  STEP: modifying the configmap once @ 12/19/23 10:34:12.325
  STEP: changing the label value of the configmap @ 12/19/23 10:34:12.347
  STEP: Expecting to observe a delete notification for the watched object @ 12/19/23 10:34:12.366
  Dec 19 10:34:12.366: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5888  a2b4fe99-63db-4b71-befb-9c3b11f7d2e8 11761 0 2023-12-19 10:34:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-12-19 10:34:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 19 10:34:12.367: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5888  a2b4fe99-63db-4b71-befb-9c3b11f7d2e8 11762 0 2023-12-19 10:34:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-12-19 10:34:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 19 10:34:12.367: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5888  a2b4fe99-63db-4b71-befb-9c3b11f7d2e8 11763 0 2023-12-19 10:34:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-12-19 10:34:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 12/19/23 10:34:12.367
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 12/19/23 10:34:12.387
  STEP: changing the label value of the configmap back @ 12/19/23 10:34:22.388
  STEP: modifying the configmap a third time @ 12/19/23 10:34:22.407
  STEP: deleting the configmap @ 12/19/23 10:34:22.421
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 12/19/23 10:34:22.433
  Dec 19 10:34:22.434: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5888  a2b4fe99-63db-4b71-befb-9c3b11f7d2e8 11794 0 2023-12-19 10:34:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-12-19 10:34:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 19 10:34:22.434: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5888  a2b4fe99-63db-4b71-befb-9c3b11f7d2e8 11795 0 2023-12-19 10:34:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-12-19 10:34:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 19 10:34:22.435: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5888  a2b4fe99-63db-4b71-befb-9c3b11f7d2e8 11796 0 2023-12-19 10:34:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-12-19 10:34:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 19 10:34:22.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5888" for this suite. @ 12/19/23 10:34:22.443
• [10.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 12/19/23 10:34:22.459
  Dec 19 10:34:22.460: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename gc @ 12/19/23 10:34:22.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:34:22.495
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:34:22.5
  STEP: create the rc @ 12/19/23 10:34:22.514
  W1219 10:34:22.525563      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 12/19/23 10:34:28.565
  STEP: wait for the rc to be deleted @ 12/19/23 10:34:28.637
  Dec 19 10:34:30.108: INFO: 80 pods remaining
  Dec 19 10:34:30.108: INFO: 80 pods has nil DeletionTimestamp
  Dec 19 10:34:30.108: INFO: 
  Dec 19 10:34:30.946: INFO: 72 pods remaining
  Dec 19 10:34:30.947: INFO: 69 pods has nil DeletionTimestamp
  Dec 19 10:34:30.947: INFO: 
  Dec 19 10:34:31.826: INFO: 58 pods remaining
  Dec 19 10:34:31.826: INFO: 56 pods has nil DeletionTimestamp
  Dec 19 10:34:31.826: INFO: 
  Dec 19 10:34:32.879: INFO: 43 pods remaining
  Dec 19 10:34:32.879: INFO: 42 pods has nil DeletionTimestamp
  Dec 19 10:34:32.879: INFO: 
  Dec 19 10:34:33.844: INFO: 31 pods remaining
  Dec 19 10:34:33.844: INFO: 30 pods has nil DeletionTimestamp
  Dec 19 10:34:33.845: INFO: 
  Dec 19 10:34:34.870: INFO: 18 pods remaining
  Dec 19 10:34:34.870: INFO: 15 pods has nil DeletionTimestamp
  Dec 19 10:34:34.870: INFO: 
  Dec 19 10:34:35.720: INFO: 2 pods remaining
  Dec 19 10:34:35.720: INFO: 0 pods has nil DeletionTimestamp
  Dec 19 10:34:35.720: INFO: 
  STEP: Gathering metrics @ 12/19/23 10:34:36.668
  Dec 19 10:34:37.531: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Dec 19 10:34:37.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7514" for this suite. @ 12/19/23 10:34:37.572
• [15.167 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 12/19/23 10:34:37.629
  Dec 19 10:34:37.629: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename security-context-test @ 12/19/23 10:34:37.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:34:37.775
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:34:37.816
  Dec 19 10:34:44.007: INFO: Got logs for pod "busybox-privileged-false-6842a3be-9de5-4e44-bd63-9a3168e64773": "ip: RTNETLINK answers: Operation not permitted\n"
  Dec 19 10:34:44.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-7401" for this suite. @ 12/19/23 10:34:44.058
• [6.455 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 12/19/23 10:34:44.085
  Dec 19 10:34:44.085: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename containers @ 12/19/23 10:34:44.087
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:34:44.204
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:34:44.209
  STEP: Creating a pod to test override all @ 12/19/23 10:34:44.214
  STEP: Saw pod success @ 12/19/23 10:34:48.276
  Dec 19 10:34:48.286: INFO: Trying to get logs from node cahyeife7pae-3 pod client-containers-4df7da6f-e505-4066-9e4c-af78f2421055 container agnhost-container: <nil>
  STEP: delete the pod @ 12/19/23 10:34:48.318
  Dec 19 10:34:48.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-1012" for this suite. @ 12/19/23 10:34:48.426
• [4.367 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 12/19/23 10:34:48.454
  Dec 19 10:34:48.454: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename podtemplate @ 12/19/23 10:34:48.457
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:34:48.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:34:48.534
  Dec 19 10:34:48.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-1080" for this suite. @ 12/19/23 10:34:48.739
• [0.321 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:474
  STEP: Creating a kubernetes client @ 12/19/23 10:34:48.776
  Dec 19 10:34:48.776: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename field-validation @ 12/19/23 10:34:48.778
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:34:48.856
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:34:48.86
  Dec 19 10:34:48.869: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  W1219 10:34:51.783288      13 warnings.go:70] unknown field "alpha"
  W1219 10:34:51.783774      13 warnings.go:70] unknown field "beta"
  W1219 10:34:51.784085      13 warnings.go:70] unknown field "delta"
  W1219 10:34:51.784398      13 warnings.go:70] unknown field "epsilon"
  W1219 10:34:51.784695      13 warnings.go:70] unknown field "gamma"
  Dec 19 10:34:52.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2348" for this suite. @ 12/19/23 10:34:52.403
• [3.654 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 12/19/23 10:34:52.437
  Dec 19 10:34:52.437: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-probe @ 12/19/23 10:34:52.44
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:34:52.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:34:52.557
  STEP: Creating pod busybox-343e8da1-764a-451f-9c6b-ffc4d8a84fbc in namespace container-probe-5577 @ 12/19/23 10:34:52.564
  Dec 19 10:34:54.643: INFO: Started pod busybox-343e8da1-764a-451f-9c6b-ffc4d8a84fbc in namespace container-probe-5577
  STEP: checking the pod's current state and verifying that restartCount is present @ 12/19/23 10:34:54.644
  Dec 19 10:34:54.650: INFO: Initial restart count of pod busybox-343e8da1-764a-451f-9c6b-ffc4d8a84fbc is 0
  Dec 19 10:38:56.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/19/23 10:38:56.067
  STEP: Destroying namespace "container-probe-5577" for this suite. @ 12/19/23 10:38:56.092
• [243.671 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 12/19/23 10:38:56.11
  Dec 19 10:38:56.111: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename webhook @ 12/19/23 10:38:56.115
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:38:56.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:38:56.166
  STEP: Setting up server cert @ 12/19/23 10:38:56.231
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/19/23 10:38:57.997
  STEP: Deploying the webhook pod @ 12/19/23 10:38:58.01
  STEP: Wait for the deployment to be ready @ 12/19/23 10:38:58.032
  Dec 19 10:38:58.054: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 12/19/23 10:39:00.084
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 10:39:00.117
  Dec 19 10:39:01.118: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 12/19/23 10:39:01.128
  STEP: Creating a custom resource definition that should be denied by the webhook @ 12/19/23 10:39:01.199
  Dec 19 10:39:01.199: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:39:01.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9549" for this suite. @ 12/19/23 10:39:01.411
  STEP: Destroying namespace "webhook-markers-5691" for this suite. @ 12/19/23 10:39:01.428
• [5.335 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 12/19/23 10:39:01.45
  Dec 19 10:39:01.450: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename emptydir @ 12/19/23 10:39:01.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:39:01.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:39:01.505
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 12/19/23 10:39:01.515
  STEP: Saw pod success @ 12/19/23 10:39:05.59
  Dec 19 10:39:05.599: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-8a989353-1905-4112-9039-7958bed0a1e7 container test-container: <nil>
  STEP: delete the pod @ 12/19/23 10:39:05.635
  Dec 19 10:39:05.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-935" for this suite. @ 12/19/23 10:39:05.683
• [4.250 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 12/19/23 10:39:05.702
  Dec 19 10:39:05.703: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename dns @ 12/19/23 10:39:05.706
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:39:05.737
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:39:05.745
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 12/19/23 10:39:05.752
  Dec 19 10:39:05.771: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2644  599b0a22-b433-41c5-a597-4917cc0a102a 13811 0 2023-12-19 10:39:05 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-12-19 10:39:05 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cqt9j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cqt9j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 12/19/23 10:39:07.802
  Dec 19 10:39:07.803: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2644 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 10:39:07.803: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:39:07.807: INFO: ExecWithOptions: Clientset creation
  Dec 19 10:39:07.809: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-2644/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 12/19/23 10:39:07.997
  Dec 19 10:39:07.997: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2644 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 10:39:07.997: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:39:08.001: INFO: ExecWithOptions: Clientset creation
  Dec 19 10:39:08.001: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-2644/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Dec 19 10:39:08.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 19 10:39:08.145: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-2644" for this suite. @ 12/19/23 10:39:08.172
• [2.488 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 12/19/23 10:39:08.195
  Dec 19 10:39:08.195: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename replicaset @ 12/19/23 10:39:08.198
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:39:08.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:39:08.248
  Dec 19 10:39:08.255: INFO: Creating ReplicaSet my-hostname-basic-77b380e2-8b0f-473a-a1fd-bee18a353f38
  Dec 19 10:39:08.274: INFO: Pod name my-hostname-basic-77b380e2-8b0f-473a-a1fd-bee18a353f38: Found 0 pods out of 1
  Dec 19 10:39:13.281: INFO: Pod name my-hostname-basic-77b380e2-8b0f-473a-a1fd-bee18a353f38: Found 1 pods out of 1
  Dec 19 10:39:13.281: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-77b380e2-8b0f-473a-a1fd-bee18a353f38" is running
  Dec 19 10:39:13.292: INFO: Pod "my-hostname-basic-77b380e2-8b0f-473a-a1fd-bee18a353f38-srx84" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-19 10:39:08 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-19 10:39:09 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-19 10:39:09 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-12-19 10:39:08 +0000 UTC Reason: Message:}])
  Dec 19 10:39:13.293: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 12/19/23 10:39:13.293
  Dec 19 10:39:13.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2890" for this suite. @ 12/19/23 10:39:13.347
• [5.183 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 12/19/23 10:39:13.385
  Dec 19 10:39:13.385: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubectl @ 12/19/23 10:39:13.39
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:39:13.439
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:39:13.478
  Dec 19 10:39:13.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-5896 version'
  Dec 19 10:39:13.712: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Dec 19 10:39:13.713: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.8\", GitCommit:\"66fee42707cd7f5a89f1987f7cb81b02dd19161c\", GitTreeState:\"clean\", BuildDate:\"2023-11-15T16:59:43Z\", GoVersion:\"go1.20.11\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.8\", GitCommit:\"66fee42707cd7f5a89f1987f7cb81b02dd19161c\", GitTreeState:\"clean\", BuildDate:\"2023-11-15T16:50:09Z\", GoVersion:\"go1.20.11\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Dec 19 10:39:13.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5896" for this suite. @ 12/19/23 10:39:13.732
• [0.367 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:638
  STEP: Creating a kubernetes client @ 12/19/23 10:39:13.759
  Dec 19 10:39:13.759: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename statefulset @ 12/19/23 10:39:13.761
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:39:13.82
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:39:13.826
  STEP: Creating service test in namespace statefulset-2260 @ 12/19/23 10:39:13.834
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 12/19/23 10:39:13.844
  STEP: Creating stateful set ss in namespace statefulset-2260 @ 12/19/23 10:39:13.857
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2260 @ 12/19/23 10:39:13.876
  Dec 19 10:39:13.881: INFO: Found 0 stateful pods, waiting for 1
  Dec 19 10:39:23.893: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 12/19/23 10:39:23.893
  Dec 19 10:39:23.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-2260 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Dec 19 10:39:24.176: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 19 10:39:24.176: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 19 10:39:24.176: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Dec 19 10:39:24.184: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Dec 19 10:39:34.194: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Dec 19 10:39:34.195: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 19 10:39:34.240: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999076s
  Dec 19 10:39:35.250: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.987226156s
  Dec 19 10:39:36.258: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.97797526s
  Dec 19 10:39:37.269: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.969074823s
  Dec 19 10:39:38.282: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.958385492s
  Dec 19 10:39:39.293: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.94619515s
  Dec 19 10:39:40.305: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.934476503s
  Dec 19 10:39:41.315: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.923075936s
  Dec 19 10:39:42.325: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.912163511s
  Dec 19 10:39:43.336: INFO: Verifying statefulset ss doesn't scale past 1 for another 901.087214ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2260 @ 12/19/23 10:39:44.337
  Dec 19 10:39:44.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-2260 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Dec 19 10:39:44.676: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Dec 19 10:39:44.676: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Dec 19 10:39:44.676: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Dec 19 10:39:44.685: INFO: Found 1 stateful pods, waiting for 3
  Dec 19 10:39:54.699: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Dec 19 10:39:54.699: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Dec 19 10:39:54.699: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 12/19/23 10:39:54.7
  STEP: Scale down will halt with unhealthy stateful pod @ 12/19/23 10:39:54.7
  Dec 19 10:39:54.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-2260 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Dec 19 10:39:55.126: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 19 10:39:55.126: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 19 10:39:55.126: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Dec 19 10:39:55.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-2260 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Dec 19 10:39:55.471: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 19 10:39:55.471: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 19 10:39:55.471: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Dec 19 10:39:55.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-2260 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Dec 19 10:39:55.852: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 19 10:39:55.852: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 19 10:39:55.852: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Dec 19 10:39:55.852: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 19 10:39:55.859: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
  Dec 19 10:40:05.883: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Dec 19 10:40:05.884: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Dec 19 10:40:05.884: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Dec 19 10:40:05.916: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999947s
  Dec 19 10:40:06.929: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991025597s
  Dec 19 10:40:07.963: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.977621408s
  Dec 19 10:40:08.976: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.943818816s
  Dec 19 10:40:09.994: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.931342564s
  Dec 19 10:40:11.008: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.912958267s
  Dec 19 10:40:12.023: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.898507968s
  Dec 19 10:40:13.033: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.884267837s
  Dec 19 10:40:14.051: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.874554244s
  Dec 19 10:40:15.070: INFO: Verifying statefulset ss doesn't scale past 3 for another 855.918004ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2260 @ 12/19/23 10:40:16.071
  Dec 19 10:40:16.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-2260 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Dec 19 10:40:16.493: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Dec 19 10:40:16.493: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Dec 19 10:40:16.493: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Dec 19 10:40:16.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-2260 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Dec 19 10:40:16.820: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Dec 19 10:40:16.820: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Dec 19 10:40:16.820: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Dec 19 10:40:16.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-2260 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Dec 19 10:40:17.141: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Dec 19 10:40:17.141: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Dec 19 10:40:17.141: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Dec 19 10:40:17.141: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 12/19/23 10:40:27.19
  Dec 19 10:40:27.190: INFO: Deleting all statefulset in ns statefulset-2260
  Dec 19 10:40:27.198: INFO: Scaling statefulset ss to 0
  Dec 19 10:40:27.231: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 19 10:40:27.240: INFO: Deleting statefulset ss
  Dec 19 10:40:27.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2260" for this suite. @ 12/19/23 10:40:27.317
• [73.572 seconds]
------------------------------
SS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 12/19/23 10:40:27.335
  Dec 19 10:40:27.335: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename downward-api @ 12/19/23 10:40:27.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:40:27.377
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:40:27.385
  STEP: Creating a pod to test downward api env vars @ 12/19/23 10:40:27.395
  STEP: Saw pod success @ 12/19/23 10:40:31.482
  Dec 19 10:40:31.491: INFO: Trying to get logs from node cahyeife7pae-3 pod downward-api-146774ef-7155-4c9a-a888-e15061eaa8ce container dapi-container: <nil>
  STEP: delete the pod @ 12/19/23 10:40:31.517
  Dec 19 10:40:31.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3152" for this suite. @ 12/19/23 10:40:31.562
• [4.238 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:96
  STEP: Creating a kubernetes client @ 12/19/23 10:40:31.574
  Dec 19 10:40:31.574: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename aggregator @ 12/19/23 10:40:31.578
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:40:31.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:40:31.62
  Dec 19 10:40:31.626: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Registering the sample API server. @ 12/19/23 10:40:31.631
  Dec 19 10:40:32.316: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Dec 19 10:40:32.420: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  Dec 19 10:40:34.598: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 19 10:40:36.608: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 19 10:40:38.608: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 19 10:40:40.615: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 19 10:40:42.612: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 19 10:40:44.609: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 19 10:40:46.612: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 19 10:40:48.611: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 19 10:40:50.610: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 19 10:40:52.610: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 19 10:40:54.608: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 19 10:40:56.608: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-69b6bfc58\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 19 10:40:58.759: INFO: Waited 129.142953ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 12/19/23 10:40:58.897
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 12/19/23 10:40:58.907
  STEP: List APIServices @ 12/19/23 10:40:58.927
  Dec 19 10:40:58.944: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 12/19/23 10:40:58.944
  Dec 19 10:40:58.981: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 12/19/23 10:40:58.981
  Dec 19 10:40:59.023: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.December, 19, 10, 40, 58, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 12/19/23 10:40:59.024
  Dec 19 10:40:59.031: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-12-19 10:40:58 +0000 UTC Passed all checks passed}
  Dec 19 10:40:59.032: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Dec 19 10:40:59.032: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 12/19/23 10:40:59.032
  Dec 19 10:40:59.064: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1485373647" @ 12/19/23 10:40:59.065
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 12/19/23 10:40:59.102
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 12/19/23 10:40:59.124
  STEP: Patch APIService Status @ 12/19/23 10:40:59.134
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 12/19/23 10:40:59.159
  Dec 19 10:40:59.169: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-12-19 10:40:58 +0000 UTC Passed all checks passed}
  Dec 19 10:40:59.169: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Dec 19 10:40:59.169: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Dec 19 10:40:59.169: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 12/19/23 10:40:59.17
  STEP: Confirm that the generated APIService has been deleted @ 12/19/23 10:40:59.183
  Dec 19 10:40:59.183: INFO: Requesting list of APIServices to confirm quantity
  Dec 19 10:40:59.198: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Dec 19 10:40:59.199: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Dec 19 10:40:59.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-9740" for this suite. @ 12/19/23 10:40:59.667
• [28.112 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 12/19/23 10:40:59.687
  Dec 19 10:40:59.687: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename namespaces @ 12/19/23 10:40:59.69
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:40:59.731
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:40:59.739
  STEP: Creating a test namespace @ 12/19/23 10:40:59.747
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:40:59.784
  STEP: Creating a service in the namespace @ 12/19/23 10:40:59.795
  STEP: Deleting the namespace @ 12/19/23 10:40:59.827
  STEP: Waiting for the namespace to be removed. @ 12/19/23 10:40:59.864
  STEP: Recreating the namespace @ 12/19/23 10:41:05.874
  STEP: Verifying there is no service in the namespace @ 12/19/23 10:41:05.909
  Dec 19 10:41:05.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4010" for this suite. @ 12/19/23 10:41:05.926
  STEP: Destroying namespace "nsdeletetest-8078" for this suite. @ 12/19/23 10:41:05.938
  Dec 19 10:41:05.948: INFO: Namespace nsdeletetest-8078 was already deleted
  STEP: Destroying namespace "nsdeletetest-1026" for this suite. @ 12/19/23 10:41:05.948
• [6.280 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 12/19/23 10:41:05.99
  Dec 19 10:41:05.990: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename namespaces @ 12/19/23 10:41:05.996
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:41:06.04
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:41:06.05
  STEP: Creating a test namespace @ 12/19/23 10:41:06.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:41:06.087
  STEP: Creating a pod in the namespace @ 12/19/23 10:41:06.092
  STEP: Waiting for the pod to have running status @ 12/19/23 10:41:06.11
  STEP: Deleting the namespace @ 12/19/23 10:41:08.134
  STEP: Waiting for the namespace to be removed. @ 12/19/23 10:41:08.147
  STEP: Recreating the namespace @ 12/19/23 10:41:20.16
  STEP: Verifying there are no pods in the namespace @ 12/19/23 10:41:20.195
  Dec 19 10:41:20.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3895" for this suite. @ 12/19/23 10:41:20.214
  STEP: Destroying namespace "nsdeletetest-8909" for this suite. @ 12/19/23 10:41:20.234
  Dec 19 10:41:20.240: INFO: Namespace nsdeletetest-8909 was already deleted
  STEP: Destroying namespace "nsdeletetest-6425" for this suite. @ 12/19/23 10:41:20.24
• [14.274 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 12/19/23 10:41:20.265
  Dec 19 10:41:20.265: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename crd-webhook @ 12/19/23 10:41:20.269
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:41:20.302
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:41:20.309
  STEP: Setting up server cert @ 12/19/23 10:41:20.315
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 12/19/23 10:41:21.25
  STEP: Deploying the custom resource conversion webhook pod @ 12/19/23 10:41:21.266
  STEP: Wait for the deployment to be ready @ 12/19/23 10:41:21.289
  Dec 19 10:41:21.307: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 12/19/23 10:41:23.334
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 10:41:23.373
  Dec 19 10:41:24.373: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Dec 19 10:41:24.382: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Creating a v1 custom resource @ 12/19/23 10:41:27.289
  STEP: v2 custom resource should be converted @ 12/19/23 10:41:27.313
  Dec 19 10:41:27.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-9125" for this suite. @ 12/19/23 10:41:28.098
• [7.849 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 12/19/23 10:41:28.129
  Dec 19 10:41:28.129: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-runtime @ 12/19/23 10:41:28.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:41:28.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:41:28.203
  STEP: create the container @ 12/19/23 10:41:28.213
  W1219 10:41:28.245863      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 12/19/23 10:41:28.246
  STEP: get the container status @ 12/19/23 10:41:31.295
  STEP: the container should be terminated @ 12/19/23 10:41:31.309
  STEP: the termination message should be set @ 12/19/23 10:41:31.309
  Dec 19 10:41:31.309: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 12/19/23 10:41:31.31
  Dec 19 10:41:31.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3266" for this suite. @ 12/19/23 10:41:31.404
• [3.291 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 12/19/23 10:41:31.438
  Dec 19 10:41:31.438: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename init-container @ 12/19/23 10:41:31.442
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:41:31.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:41:31.494
  STEP: creating the pod @ 12/19/23 10:41:31.505
  Dec 19 10:41:31.505: INFO: PodSpec: initContainers in spec.initContainers
  Dec 19 10:41:34.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-9638" for this suite. @ 12/19/23 10:41:34.88
• [3.454 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 12/19/23 10:41:34.899
  Dec 19 10:41:34.899: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename pods @ 12/19/23 10:41:34.901
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:41:34.949
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:41:34.964
  STEP: Create a pod @ 12/19/23 10:41:34.98
  STEP: patching /status @ 12/19/23 10:41:37.037
  Dec 19 10:41:37.063: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Dec 19 10:41:37.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7984" for this suite. @ 12/19/23 10:41:37.078
• [2.197 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 12/19/23 10:41:37.103
  Dec 19 10:41:37.103: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-runtime @ 12/19/23 10:41:37.106
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:41:37.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:41:37.157
  STEP: create the container @ 12/19/23 10:41:37.163
  W1219 10:41:37.180847      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 12/19/23 10:41:37.181
  STEP: get the container status @ 12/19/23 10:41:40.235
  STEP: the container should be terminated @ 12/19/23 10:41:40.244
  STEP: the termination message should be set @ 12/19/23 10:41:40.244
  Dec 19 10:41:40.244: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 12/19/23 10:41:40.245
  Dec 19 10:41:40.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-359" for this suite. @ 12/19/23 10:41:40.292
• [3.220 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 12/19/23 10:41:40.327
  Dec 19 10:41:40.327: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename replicaset @ 12/19/23 10:41:40.333
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:41:40.42
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:41:40.434
  STEP: Create a Replicaset @ 12/19/23 10:41:40.464
  STEP: Verify that the required pods have come up. @ 12/19/23 10:41:40.484
  Dec 19 10:41:40.492: INFO: Pod name sample-pod: Found 0 pods out of 1
  Dec 19 10:41:45.543: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 12/19/23 10:41:45.543
  STEP: Getting /status @ 12/19/23 10:41:45.543
  Dec 19 10:41:45.568: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 12/19/23 10:41:45.569
  Dec 19 10:41:45.604: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 12/19/23 10:41:45.604
  Dec 19 10:41:45.613: INFO: Observed &ReplicaSet event: ADDED
  Dec 19 10:41:45.614: INFO: Observed &ReplicaSet event: MODIFIED
  Dec 19 10:41:45.614: INFO: Observed &ReplicaSet event: MODIFIED
  Dec 19 10:41:45.615: INFO: Observed &ReplicaSet event: MODIFIED
  Dec 19 10:41:45.615: INFO: Found replicaset test-rs in namespace replicaset-2450 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Dec 19 10:41:45.616: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 12/19/23 10:41:45.616
  Dec 19 10:41:45.616: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Dec 19 10:41:45.637: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 12/19/23 10:41:45.638
  Dec 19 10:41:45.644: INFO: Observed &ReplicaSet event: ADDED
  Dec 19 10:41:45.646: INFO: Observed &ReplicaSet event: MODIFIED
  Dec 19 10:41:45.647: INFO: Observed &ReplicaSet event: MODIFIED
  Dec 19 10:41:45.648: INFO: Observed &ReplicaSet event: MODIFIED
  Dec 19 10:41:45.648: INFO: Observed replicaset test-rs in namespace replicaset-2450 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Dec 19 10:41:45.648: INFO: Observed &ReplicaSet event: MODIFIED
  Dec 19 10:41:45.649: INFO: Found replicaset test-rs in namespace replicaset-2450 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Dec 19 10:41:45.649: INFO: Replicaset test-rs has a patched status
  Dec 19 10:41:45.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2450" for this suite. @ 12/19/23 10:41:45.666
• [5.365 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 12/19/23 10:41:45.695
  Dec 19 10:41:45.695: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename svcaccounts @ 12/19/23 10:41:45.7
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:41:45.744
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:41:45.75
  STEP: Creating ServiceAccount "e2e-sa-6jz7q"  @ 12/19/23 10:41:45.759
  Dec 19 10:41:45.771: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-6jz7q"  @ 12/19/23 10:41:45.772
  Dec 19 10:41:45.790: INFO: AutomountServiceAccountToken: true
  Dec 19 10:41:45.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8827" for this suite. @ 12/19/23 10:41:45.802
• [0.126 seconds]
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 12/19/23 10:41:45.822
  Dec 19 10:41:45.823: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename dns @ 12/19/23 10:41:45.826
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:41:45.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:41:45.866
  STEP: Creating a test headless service @ 12/19/23 10:41:45.874
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6557.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6557.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6557.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6557.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6557.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6557.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6557.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6557.svc.cluster.local;sleep 1; done
   @ 12/19/23 10:41:45.888
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6557.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6557.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6557.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6557.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6557.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6557.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6557.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6557.svc.cluster.local;sleep 1; done
   @ 12/19/23 10:41:45.889
  STEP: creating a pod to probe DNS @ 12/19/23 10:41:45.889
  STEP: submitting the pod to kubernetes @ 12/19/23 10:41:45.889
  STEP: retrieving the pod @ 12/19/23 10:41:49.975
  STEP: looking for the results for each expected name from probers @ 12/19/23 10:41:49.984
  Dec 19 10:41:50.020: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:41:50.028: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:41:50.035: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:41:50.056: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:41:50.065: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:41:50.065: INFO: Lookups using dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c failed for: [wheezy_udp@dns-test-service-2.dns-6557.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6557.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6557.svc.cluster.local jessie_udp@dns-test-service-2.dns-6557.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6557.svc.cluster.local]

  Dec 19 10:41:55.092: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:41:55.101: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:41:55.132: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:41:55.142: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:41:55.142: INFO: Lookups using dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c failed for: [wheezy_udp@dns-test-service-2.dns-6557.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6557.svc.cluster.local jessie_udp@dns-test-service-2.dns-6557.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6557.svc.cluster.local]

  Dec 19 10:42:00.098: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:42:00.108: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:42:00.138: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:42:00.150: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:42:00.150: INFO: Lookups using dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c failed for: [wheezy_udp@dns-test-service-2.dns-6557.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6557.svc.cluster.local jessie_udp@dns-test-service-2.dns-6557.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6557.svc.cluster.local]

  Dec 19 10:42:05.101: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:42:05.108: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:42:05.139: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:42:05.147: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:42:05.147: INFO: Lookups using dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c failed for: [wheezy_udp@dns-test-service-2.dns-6557.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6557.svc.cluster.local jessie_udp@dns-test-service-2.dns-6557.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6557.svc.cluster.local]

  Dec 19 10:42:10.092: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:42:10.099: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:42:10.123: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:42:10.129: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:42:10.129: INFO: Lookups using dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c failed for: [wheezy_udp@dns-test-service-2.dns-6557.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6557.svc.cluster.local jessie_udp@dns-test-service-2.dns-6557.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6557.svc.cluster.local]

  Dec 19 10:42:15.108: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:42:15.127: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:42:15.159: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:42:15.171: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6557.svc.cluster.local from pod dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c: the server could not find the requested resource (get pods dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c)
  Dec 19 10:42:15.171: INFO: Lookups using dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c failed for: [wheezy_udp@dns-test-service-2.dns-6557.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6557.svc.cluster.local jessie_udp@dns-test-service-2.dns-6557.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6557.svc.cluster.local]

  Dec 19 10:42:20.149: INFO: DNS probes using dns-6557/dns-test-92f6191a-3ce3-41fa-8553-5493dce46b0c succeeded

  Dec 19 10:42:20.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/19/23 10:42:20.167
  STEP: deleting the test headless service @ 12/19/23 10:42:20.217
  STEP: Destroying namespace "dns-6557" for this suite. @ 12/19/23 10:42:20.292
• [34.498 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 12/19/23 10:42:20.323
  Dec 19 10:42:20.324: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename crd-watch @ 12/19/23 10:42:20.33
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:42:20.397
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:42:20.407
  Dec 19 10:42:20.418: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Creating first CR  @ 12/19/23 10:42:23.165
  Dec 19 10:42:23.180: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-12-19T10:42:23Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-12-19T10:42:23Z]] name:name1 resourceVersion:14899 uid:74322dee-8668-4284-8c06-8b7bd9d4696b] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Creating second CR @ 12/19/23 10:42:33.181
  Dec 19 10:42:33.197: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-12-19T10:42:33Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-12-19T10:42:33Z]] name:name2 resourceVersion:14934 uid:d1d5d643-426d-4223-9006-e7b3532a3153] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying first CR @ 12/19/23 10:42:43.204
  Dec 19 10:42:43.226: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-12-19T10:42:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-12-19T10:42:43Z]] name:name1 resourceVersion:14953 uid:74322dee-8668-4284-8c06-8b7bd9d4696b] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying second CR @ 12/19/23 10:42:53.227
  Dec 19 10:42:53.247: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-12-19T10:42:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-12-19T10:42:53Z]] name:name2 resourceVersion:14971 uid:d1d5d643-426d-4223-9006-e7b3532a3153] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting first CR @ 12/19/23 10:43:03.248
  Dec 19 10:43:03.266: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-12-19T10:42:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-12-19T10:42:43Z]] name:name1 resourceVersion:14991 uid:74322dee-8668-4284-8c06-8b7bd9d4696b] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting second CR @ 12/19/23 10:43:13.268
  Dec 19 10:43:13.286: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-12-19T10:42:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-12-19T10:42:53Z]] name:name2 resourceVersion:15009 uid:d1d5d643-426d-4223-9006-e7b3532a3153] num:map[num1:9223372036854775807 num2:1000000]]}
  Dec 19 10:43:23.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-6433" for this suite. @ 12/19/23 10:43:23.841
• [63.536 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 12/19/23 10:43:23.863
  Dec 19 10:43:23.864: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename resourcequota @ 12/19/23 10:43:23.867
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:43:23.906
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:43:23.911
  STEP: Creating a ResourceQuota with best effort scope @ 12/19/23 10:43:23.916
  STEP: Ensuring ResourceQuota status is calculated @ 12/19/23 10:43:23.925
  STEP: Creating a ResourceQuota with not best effort scope @ 12/19/23 10:43:25.935
  STEP: Ensuring ResourceQuota status is calculated @ 12/19/23 10:43:25.949
  STEP: Creating a best-effort pod @ 12/19/23 10:43:27.959
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 12/19/23 10:43:27.994
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 12/19/23 10:43:30.002
  STEP: Deleting the pod @ 12/19/23 10:43:32.01
  STEP: Ensuring resource quota status released the pod usage @ 12/19/23 10:43:32.036
  STEP: Creating a not best-effort pod @ 12/19/23 10:43:34.047
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 12/19/23 10:43:34.074
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 12/19/23 10:43:36.087
  STEP: Deleting the pod @ 12/19/23 10:43:38.093
  STEP: Ensuring resource quota status released the pod usage @ 12/19/23 10:43:38.116
  Dec 19 10:43:40.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9004" for this suite. @ 12/19/23 10:43:40.142
• [16.300 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 12/19/23 10:43:40.164
  Dec 19 10:43:40.164: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename webhook @ 12/19/23 10:43:40.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:43:40.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:43:40.206
  STEP: Setting up server cert @ 12/19/23 10:43:40.27
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/19/23 10:43:41.058
  STEP: Deploying the webhook pod @ 12/19/23 10:43:41.081
  STEP: Wait for the deployment to be ready @ 12/19/23 10:43:41.113
  Dec 19 10:43:41.149: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 12/19/23 10:43:43.172
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 10:43:43.209
  Dec 19 10:43:44.210: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 12/19/23 10:43:44.218
  STEP: create a pod that should be updated by the webhook @ 12/19/23 10:43:44.261
  Dec 19 10:43:44.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9400" for this suite. @ 12/19/23 10:43:44.51
  STEP: Destroying namespace "webhook-markers-8923" for this suite. @ 12/19/23 10:43:44.534
• [4.395 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 12/19/23 10:43:44.56
  Dec 19 10:43:44.560: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename downward-api @ 12/19/23 10:43:44.568
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:43:44.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:43:44.615
  STEP: Creating a pod to test downward api env vars @ 12/19/23 10:43:44.625
  STEP: Saw pod success @ 12/19/23 10:43:46.678
  Dec 19 10:43:46.683: INFO: Trying to get logs from node cahyeife7pae-3 pod downward-api-beaf14c1-10f6-47c8-86c2-37afd2cb4a31 container dapi-container: <nil>
  STEP: delete the pod @ 12/19/23 10:43:46.724
  Dec 19 10:43:46.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5987" for this suite. @ 12/19/23 10:43:46.787
• [2.247 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 12/19/23 10:43:46.811
  Dec 19 10:43:46.811: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename pods @ 12/19/23 10:43:46.812
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:43:46.872
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:43:46.879
  STEP: creating a Pod with a static label @ 12/19/23 10:43:46.913
  STEP: watching for Pod to be ready @ 12/19/23 10:43:46.934
  Dec 19 10:43:46.946: INFO: observed Pod pod-test in namespace pods-1992 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Dec 19 10:43:46.961: INFO: observed Pod pod-test in namespace pods-1992 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 10:43:46 +0000 UTC  }]
  Dec 19 10:43:46.984: INFO: observed Pod pod-test in namespace pods-1992 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 10:43:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-12-19 10:43:46 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-12-19 10:43:46 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 10:43:46 +0000 UTC  }]
  Dec 19 10:43:48.578: INFO: Found Pod pod-test in namespace pods-1992 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 10:43:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 10:43:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 10:43:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 10:43:46 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 12/19/23 10:43:48.585
  STEP: getting the Pod and ensuring that it's patched @ 12/19/23 10:43:48.611
  STEP: replacing the Pod's status Ready condition to False @ 12/19/23 10:43:48.621
  STEP: check the Pod again to ensure its Ready conditions are False @ 12/19/23 10:43:48.643
  STEP: deleting the Pod via a Collection with a LabelSelector @ 12/19/23 10:43:48.643
  STEP: watching for the Pod to be deleted @ 12/19/23 10:43:48.667
  Dec 19 10:43:48.674: INFO: observed event type MODIFIED
  Dec 19 10:43:50.608: INFO: observed event type MODIFIED
  Dec 19 10:43:50.893: INFO: observed event type MODIFIED
  Dec 19 10:43:51.620: INFO: observed event type MODIFIED
  Dec 19 10:43:51.696: INFO: observed event type MODIFIED
  Dec 19 10:43:51.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1992" for this suite. @ 12/19/23 10:43:51.721
• [4.925 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 12/19/23 10:43:51.738
  Dec 19 10:43:51.738: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 10:43:51.744
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:43:51.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:43:51.789
  STEP: Creating a pod to test downward API volume plugin @ 12/19/23 10:43:51.799
  STEP: Saw pod success @ 12/19/23 10:43:55.862
  Dec 19 10:43:55.871: INFO: Trying to get logs from node cahyeife7pae-3 pod downwardapi-volume-c1fa5f68-2d1c-475a-b1b5-88811d95dace container client-container: <nil>
  STEP: delete the pod @ 12/19/23 10:43:55.899
  Dec 19 10:43:55.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1620" for this suite. @ 12/19/23 10:43:55.955
• [4.238 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 12/19/23 10:43:55.984
  Dec 19 10:43:55.985: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename custom-resource-definition @ 12/19/23 10:43:55.988
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:43:56.031
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:43:56.036
  Dec 19 10:43:56.043: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:43:59.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5655" for this suite. @ 12/19/23 10:43:59.731
• [3.778 seconds]
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 12/19/23 10:43:59.762
  Dec 19 10:43:59.763: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 10:43:59.765
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:43:59.821
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:43:59.828
  STEP: Creating projection with secret that has name projected-secret-test-2fe0f7ca-3da9-4b65-9979-b806f785ac65 @ 12/19/23 10:43:59.836
  STEP: Creating a pod to test consume secrets @ 12/19/23 10:43:59.851
  STEP: Saw pod success @ 12/19/23 10:44:03.914
  Dec 19 10:44:03.920: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-projected-secrets-572fd0f1-3d91-4f0d-8c70-2496a7e5ed8c container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 12/19/23 10:44:03.935
  Dec 19 10:44:03.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8346" for this suite. @ 12/19/23 10:44:03.982
• [4.241 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 12/19/23 10:44:04.006
  Dec 19 10:44:04.006: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename emptydir @ 12/19/23 10:44:04.01
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:44:04.043
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:44:04.048
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 12/19/23 10:44:04.053
  STEP: Saw pod success @ 12/19/23 10:44:08.094
  Dec 19 10:44:08.102: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-668d5902-a438-462e-8d65-c206866baa90 container test-container: <nil>
  STEP: delete the pod @ 12/19/23 10:44:08.118
  Dec 19 10:44:08.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9750" for this suite. @ 12/19/23 10:44:08.153
• [4.161 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 12/19/23 10:44:08.176
  Dec 19 10:44:08.176: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename pods @ 12/19/23 10:44:08.179
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:44:08.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:44:08.218
  STEP: creating the pod @ 12/19/23 10:44:08.223
  STEP: setting up watch @ 12/19/23 10:44:08.223
  STEP: submitting the pod to kubernetes @ 12/19/23 10:44:08.333
  STEP: verifying the pod is in kubernetes @ 12/19/23 10:44:08.353
  STEP: verifying pod creation was observed @ 12/19/23 10:44:08.373
  STEP: deleting the pod gracefully @ 12/19/23 10:44:10.41
  STEP: verifying pod deletion was observed @ 12/19/23 10:44:10.432
  Dec 19 10:44:11.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-565" for this suite. @ 12/19/23 10:44:11.822
• [3.667 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 12/19/23 10:44:11.848
  Dec 19 10:44:11.848: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename emptydir @ 12/19/23 10:44:11.85
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:44:11.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:44:11.897
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 12/19/23 10:44:11.902
  STEP: Saw pod success @ 12/19/23 10:44:15.96
  Dec 19 10:44:15.966: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-eb4fa184-63a5-4692-ba36-71906769579c container test-container: <nil>
  STEP: delete the pod @ 12/19/23 10:44:15.98
  Dec 19 10:44:16.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6663" for this suite. @ 12/19/23 10:44:16.036
• [4.201 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 12/19/23 10:44:16.054
  Dec 19 10:44:16.054: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename secrets @ 12/19/23 10:44:16.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:44:16.099
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:44:16.103
  STEP: Creating secret with name secret-test-map-77e88f28-1ec0-4c06-bd31-0ee782d56d63 @ 12/19/23 10:44:16.108
  STEP: Creating a pod to test consume secrets @ 12/19/23 10:44:16.121
  STEP: Saw pod success @ 12/19/23 10:44:20.168
  Dec 19 10:44:20.177: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-secrets-dcc22e55-17fb-406d-96bd-f86fd343a3bd container secret-volume-test: <nil>
  STEP: delete the pod @ 12/19/23 10:44:20.196
  Dec 19 10:44:20.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9348" for this suite. @ 12/19/23 10:44:20.271
• [4.237 seconds]
------------------------------
SSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 12/19/23 10:44:20.291
  Dec 19 10:44:20.291: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename podtemplate @ 12/19/23 10:44:20.295
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:44:20.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:44:20.342
  STEP: Create set of pod templates @ 12/19/23 10:44:20.348
  Dec 19 10:44:20.361: INFO: created test-podtemplate-1
  Dec 19 10:44:20.375: INFO: created test-podtemplate-2
  Dec 19 10:44:20.395: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 12/19/23 10:44:20.395
  STEP: delete collection of pod templates @ 12/19/23 10:44:20.402
  Dec 19 10:44:20.405: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 12/19/23 10:44:20.46
  Dec 19 10:44:20.460: INFO: requesting list of pod templates to confirm quantity
  Dec 19 10:44:20.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-6727" for this suite. @ 12/19/23 10:44:20.487
• [0.211 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 12/19/23 10:44:20.507
  Dec 19 10:44:20.507: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename certificates @ 12/19/23 10:44:20.514
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:44:20.557
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:44:20.568
  STEP: getting /apis @ 12/19/23 10:44:22.486
  STEP: getting /apis/certificates.k8s.io @ 12/19/23 10:44:22.5
  STEP: getting /apis/certificates.k8s.io/v1 @ 12/19/23 10:44:22.504
  STEP: creating @ 12/19/23 10:44:22.507
  STEP: getting @ 12/19/23 10:44:22.586
  STEP: listing @ 12/19/23 10:44:22.595
  STEP: watching @ 12/19/23 10:44:22.606
  Dec 19 10:44:22.607: INFO: starting watch
  STEP: patching @ 12/19/23 10:44:22.616
  STEP: updating @ 12/19/23 10:44:22.707
  Dec 19 10:44:22.735: INFO: waiting for watch events with expected annotations
  Dec 19 10:44:22.735: INFO: saw patched and updated annotations
  STEP: getting /approval @ 12/19/23 10:44:22.735
  STEP: patching /approval @ 12/19/23 10:44:22.747
  STEP: updating /approval @ 12/19/23 10:44:22.788
  STEP: getting /status @ 12/19/23 10:44:22.815
  STEP: patching /status @ 12/19/23 10:44:22.829
  STEP: updating /status @ 12/19/23 10:44:22.862
  STEP: deleting @ 12/19/23 10:44:22.907
  STEP: deleting a collection @ 12/19/23 10:44:22.965
  Dec 19 10:44:23.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-3914" for this suite. @ 12/19/23 10:44:23.052
• [2.566 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 12/19/23 10:44:23.078
  Dec 19 10:44:23.078: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename job @ 12/19/23 10:44:23.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:44:23.117
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:44:23.126
  STEP: Creating Indexed job @ 12/19/23 10:44:23.137
  STEP: Ensuring job reaches completions @ 12/19/23 10:44:23.162
  STEP: Ensuring pods with index for job exist @ 12/19/23 10:44:31.173
  Dec 19 10:44:31.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2309" for this suite. @ 12/19/23 10:44:31.196
• [8.133 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 12/19/23 10:44:31.225
  Dec 19 10:44:31.226: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename configmap @ 12/19/23 10:44:31.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:44:31.264
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:44:31.275
  STEP: Creating configMap with name configmap-test-volume-map-95755813-971a-4c4c-af2a-460e39f2efed @ 12/19/23 10:44:31.282
  STEP: Creating a pod to test consume configMaps @ 12/19/23 10:44:31.302
  STEP: Saw pod success @ 12/19/23 10:44:35.368
  Dec 19 10:44:35.377: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-configmaps-695dbe97-ab3e-4c18-a85a-5e7b49eb72fa container agnhost-container: <nil>
  STEP: delete the pod @ 12/19/23 10:44:35.406
  Dec 19 10:44:35.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1779" for this suite. @ 12/19/23 10:44:35.452
• [4.246 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 12/19/23 10:44:35.474
  Dec 19 10:44:35.475: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename pods @ 12/19/23 10:44:35.479
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:44:35.519
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:44:35.528
  Dec 19 10:44:35.537: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: creating the pod @ 12/19/23 10:44:35.539
  STEP: submitting the pod to kubernetes @ 12/19/23 10:44:35.539
  Dec 19 10:44:37.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3207" for this suite. @ 12/19/23 10:44:37.663
• [2.206 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 12/19/23 10:44:37.686
  Dec 19 10:44:37.687: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 10:44:37.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:44:37.733
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:44:37.744
  STEP: Creating a pod to test downward API volume plugin @ 12/19/23 10:44:37.751
  STEP: Saw pod success @ 12/19/23 10:44:41.806
  Dec 19 10:44:41.815: INFO: Trying to get logs from node cahyeife7pae-3 pod downwardapi-volume-04c16b7a-8a3a-4723-805a-a54d27604b16 container client-container: <nil>
  STEP: delete the pod @ 12/19/23 10:44:41.832
  Dec 19 10:44:41.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6471" for this suite. @ 12/19/23 10:44:41.891
• [4.219 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 12/19/23 10:44:41.916
  Dec 19 10:44:41.916: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename crd-publish-openapi @ 12/19/23 10:44:41.919
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:44:41.961
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:44:41.968
  Dec 19 10:44:41.978: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 12/19/23 10:44:44.654
  Dec 19 10:44:44.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-4708 --namespace=crd-publish-openapi-4708 create -f -'
  Dec 19 10:44:46.511: INFO: stderr: ""
  Dec 19 10:44:46.511: INFO: stdout: "e2e-test-crd-publish-openapi-3347-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Dec 19 10:44:46.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-4708 --namespace=crd-publish-openapi-4708 delete e2e-test-crd-publish-openapi-3347-crds test-foo'
  Dec 19 10:44:46.697: INFO: stderr: ""
  Dec 19 10:44:46.697: INFO: stdout: "e2e-test-crd-publish-openapi-3347-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Dec 19 10:44:46.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-4708 --namespace=crd-publish-openapi-4708 apply -f -'
  Dec 19 10:44:48.256: INFO: stderr: ""
  Dec 19 10:44:48.256: INFO: stdout: "e2e-test-crd-publish-openapi-3347-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Dec 19 10:44:48.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-4708 --namespace=crd-publish-openapi-4708 delete e2e-test-crd-publish-openapi-3347-crds test-foo'
  Dec 19 10:44:48.442: INFO: stderr: ""
  Dec 19 10:44:48.443: INFO: stdout: "e2e-test-crd-publish-openapi-3347-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 12/19/23 10:44:48.443
  Dec 19 10:44:48.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-4708 --namespace=crd-publish-openapi-4708 create -f -'
  Dec 19 10:44:48.921: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 12/19/23 10:44:48.922
  Dec 19 10:44:48.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-4708 --namespace=crd-publish-openapi-4708 create -f -'
  Dec 19 10:44:49.612: INFO: rc: 1
  Dec 19 10:44:49.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-4708 --namespace=crd-publish-openapi-4708 apply -f -'
  Dec 19 10:44:50.209: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 12/19/23 10:44:50.209
  Dec 19 10:44:50.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-4708 --namespace=crd-publish-openapi-4708 create -f -'
  Dec 19 10:44:50.729: INFO: rc: 1
  Dec 19 10:44:50.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-4708 --namespace=crd-publish-openapi-4708 apply -f -'
  Dec 19 10:44:51.257: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 12/19/23 10:44:51.258
  Dec 19 10:44:51.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-4708 explain e2e-test-crd-publish-openapi-3347-crds'
  Dec 19 10:44:51.727: INFO: stderr: ""
  Dec 19 10:44:51.728: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3347-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 12/19/23 10:44:51.73
  Dec 19 10:44:51.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-4708 explain e2e-test-crd-publish-openapi-3347-crds.metadata'
  Dec 19 10:44:52.210: INFO: stderr: ""
  Dec 19 10:44:52.210: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3347-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Dec 19 10:44:52.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-4708 explain e2e-test-crd-publish-openapi-3347-crds.spec'
  Dec 19 10:44:52.688: INFO: stderr: ""
  Dec 19 10:44:52.688: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3347-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Dec 19 10:44:52.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-4708 explain e2e-test-crd-publish-openapi-3347-crds.spec.bars'
  Dec 19 10:44:53.187: INFO: stderr: ""
  Dec 19 10:44:53.187: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3347-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 12/19/23 10:44:53.189
  Dec 19 10:44:53.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-4708 explain e2e-test-crd-publish-openapi-3347-crds.spec.bars2'
  Dec 19 10:44:53.636: INFO: rc: 1
  Dec 19 10:44:55.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4708" for this suite. @ 12/19/23 10:44:55.584
• [13.691 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 12/19/23 10:44:55.61
  Dec 19 10:44:55.610: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename replication-controller @ 12/19/23 10:44:55.614
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:44:55.66
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:44:55.676
  Dec 19 10:44:55.685: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 12/19/23 10:44:55.716
  STEP: Checking rc "condition-test" has the desired failure condition set @ 12/19/23 10:44:55.731
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 12/19/23 10:44:56.751
  Dec 19 10:44:56.780: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 12/19/23 10:44:56.78
  Dec 19 10:44:56.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7874" for this suite. @ 12/19/23 10:44:56.814
• [1.221 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 12/19/23 10:44:56.844
  Dec 19 10:44:56.844: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 10:44:56.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:44:56.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:44:56.896
  STEP: Creating a pod to test downward API volume plugin @ 12/19/23 10:44:56.913
  STEP: Saw pod success @ 12/19/23 10:45:00.975
  Dec 19 10:45:00.986: INFO: Trying to get logs from node cahyeife7pae-3 pod downwardapi-volume-1cead62a-d105-4f7d-bdb9-d457056c0e85 container client-container: <nil>
  STEP: delete the pod @ 12/19/23 10:45:01.01
  Dec 19 10:45:01.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3800" for this suite. @ 12/19/23 10:45:01.058
• [4.229 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 12/19/23 10:45:01.084
  Dec 19 10:45:01.084: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename disruption @ 12/19/23 10:45:01.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:45:01.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:45:01.129
  STEP: creating the pdb @ 12/19/23 10:45:01.136
  STEP: Waiting for the pdb to be processed @ 12/19/23 10:45:01.149
  STEP: updating the pdb @ 12/19/23 10:45:03.179
  STEP: Waiting for the pdb to be processed @ 12/19/23 10:45:03.215
  STEP: patching the pdb @ 12/19/23 10:45:05.237
  STEP: Waiting for the pdb to be processed @ 12/19/23 10:45:05.268
  STEP: Waiting for the pdb to be deleted @ 12/19/23 10:45:07.325
  Dec 19 10:45:07.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-3107" for this suite. @ 12/19/23 10:45:07.385
• [6.320 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 12/19/23 10:45:07.406
  Dec 19 10:45:07.407: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 10:45:07.409
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:45:07.442
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:45:07.449
  STEP: Creating projection with secret that has name projected-secret-test-map-d7374b34-262f-4aa1-8bb3-cc946f6991b5 @ 12/19/23 10:45:07.456
  STEP: Creating a pod to test consume secrets @ 12/19/23 10:45:07.469
  STEP: Saw pod success @ 12/19/23 10:45:11.568
  Dec 19 10:45:11.578: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-projected-secrets-d7098b55-c763-4ba4-abca-20e4e7fee54f container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 12/19/23 10:45:11.604
  Dec 19 10:45:11.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7645" for this suite. @ 12/19/23 10:45:11.668
• [4.286 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 12/19/23 10:45:11.694
  Dec 19 10:45:11.694: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename emptydir @ 12/19/23 10:45:11.697
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:45:11.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:45:11.749
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 12/19/23 10:45:11.758
  STEP: Saw pod success @ 12/19/23 10:45:15.805
  Dec 19 10:45:15.814: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-a7afac65-f26e-4c08-9ce8-fcee59083ca9 container test-container: <nil>
  STEP: delete the pod @ 12/19/23 10:45:15.834
  Dec 19 10:45:15.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6588" for this suite. @ 12/19/23 10:45:15.88
• [4.203 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:1028
  STEP: Creating a kubernetes client @ 12/19/23 10:45:15.898
  Dec 19 10:45:15.899: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename statefulset @ 12/19/23 10:45:15.902
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:45:15.946
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:45:15.956
  STEP: Creating service test in namespace statefulset-3432 @ 12/19/23 10:45:15.972
  STEP: Creating statefulset ss in namespace statefulset-3432 @ 12/19/23 10:45:16.011
  Dec 19 10:45:16.055: INFO: Found 0 stateful pods, waiting for 1
  Dec 19 10:45:26.065: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 12/19/23 10:45:26.083
  STEP: Getting /status @ 12/19/23 10:45:26.104
  Dec 19 10:45:26.114: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 12/19/23 10:45:26.114
  Dec 19 10:45:26.134: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 12/19/23 10:45:26.134
  Dec 19 10:45:26.141: INFO: Observed &StatefulSet event: ADDED
  Dec 19 10:45:26.141: INFO: Found Statefulset ss in namespace statefulset-3432 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Dec 19 10:45:26.141: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 12/19/23 10:45:26.141
  Dec 19 10:45:26.141: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Dec 19 10:45:26.163: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 12/19/23 10:45:26.163
  Dec 19 10:45:26.167: INFO: Observed &StatefulSet event: ADDED
  Dec 19 10:45:26.167: INFO: Deleting all statefulset in ns statefulset-3432
  Dec 19 10:45:26.175: INFO: Scaling statefulset ss to 0
  Dec 19 10:45:36.217: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 19 10:45:36.226: INFO: Deleting statefulset ss
  Dec 19 10:45:36.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3432" for this suite. @ 12/19/23 10:45:36.276
• [20.398 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 12/19/23 10:45:36.299
  Dec 19 10:45:36.299: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename endpointslice @ 12/19/23 10:45:36.318
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:45:36.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:45:36.371
  Dec 19 10:45:36.403: INFO: Endpoints addresses: [192.168.121.223 192.168.121.236] , ports: [6443]
  Dec 19 10:45:36.404: INFO: EndpointSlices addresses: [192.168.121.223 192.168.121.236] , ports: [6443]
  Dec 19 10:45:36.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-909" for this suite. @ 12/19/23 10:45:36.422
• [0.144 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 12/19/23 10:45:36.456
  Dec 19 10:45:36.456: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 10:45:36.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:45:36.5
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:45:36.508
  STEP: Creating a pod to test downward API volume plugin @ 12/19/23 10:45:36.516
  STEP: Saw pod success @ 12/19/23 10:45:40.595
  Dec 19 10:45:40.605: INFO: Trying to get logs from node cahyeife7pae-3 pod downwardapi-volume-e3a80b8f-5472-4219-8287-adeae929badb container client-container: <nil>
  STEP: delete the pod @ 12/19/23 10:45:40.633
  Dec 19 10:45:40.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9198" for this suite. @ 12/19/23 10:45:40.688
• [4.253 seconds]
------------------------------
SS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 12/19/23 10:45:40.712
  Dec 19 10:45:40.712: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename runtimeclass @ 12/19/23 10:45:40.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:45:40.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:45:40.77
  Dec 19 10:45:42.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8926" for this suite. @ 12/19/23 10:45:42.857
• [2.161 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 12/19/23 10:45:42.876
  Dec 19 10:45:42.876: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename containers @ 12/19/23 10:45:42.879
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:45:42.909
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:45:42.915
  STEP: Creating a pod to test override arguments @ 12/19/23 10:45:42.92
  STEP: Saw pod success @ 12/19/23 10:45:46.999
  Dec 19 10:45:47.008: INFO: Trying to get logs from node cahyeife7pae-3 pod client-containers-51a862f0-c55d-4109-a42b-0edd95c8573e container agnhost-container: <nil>
  STEP: delete the pod @ 12/19/23 10:45:47.021
  Dec 19 10:45:47.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5068" for this suite. @ 12/19/23 10:45:47.073
• [4.219 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 12/19/23 10:45:47.103
  Dec 19 10:45:47.103: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename job @ 12/19/23 10:45:47.105
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:45:47.162
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:45:47.17
  STEP: Creating a job @ 12/19/23 10:45:47.18
  STEP: Ensuring active pods == parallelism @ 12/19/23 10:45:47.193
  STEP: Orphaning one of the Job's Pods @ 12/19/23 10:45:49.202
  Dec 19 10:45:49.757: INFO: Successfully updated pod "adopt-release-dtkr8"
  STEP: Checking that the Job readopts the Pod @ 12/19/23 10:45:49.757
  STEP: Removing the labels from the Job's Pod @ 12/19/23 10:45:51.782
  Dec 19 10:45:52.315: INFO: Successfully updated pod "adopt-release-dtkr8"
  STEP: Checking that the Job releases the Pod @ 12/19/23 10:45:52.315
  Dec 19 10:45:54.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-464" for this suite. @ 12/19/23 10:45:54.363
• [7.281 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 12/19/23 10:45:54.386
  Dec 19 10:45:54.386: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename var-expansion @ 12/19/23 10:45:54.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:45:54.432
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:45:54.436
  STEP: creating the pod with failed condition @ 12/19/23 10:45:54.441
  STEP: updating the pod @ 12/19/23 10:47:54.468
  Dec 19 10:47:55.000: INFO: Successfully updated pod "var-expansion-a03c0636-7b6f-4195-8824-d7158023a5df"
  STEP: waiting for pod running @ 12/19/23 10:47:55.001
  STEP: deleting the pod gracefully @ 12/19/23 10:47:57.033
  Dec 19 10:47:57.033: INFO: Deleting pod "var-expansion-a03c0636-7b6f-4195-8824-d7158023a5df" in namespace "var-expansion-9910"
  Dec 19 10:47:57.052: INFO: Wait up to 5m0s for pod "var-expansion-a03c0636-7b6f-4195-8824-d7158023a5df" to be fully deleted
  Dec 19 10:48:29.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9910" for this suite. @ 12/19/23 10:48:29.279
• [154.907 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 12/19/23 10:48:29.299
  Dec 19 10:48:29.299: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename downward-api @ 12/19/23 10:48:29.303
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:48:29.338
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:48:29.344
  STEP: Creating a pod to test downward api env vars @ 12/19/23 10:48:29.35
  STEP: Saw pod success @ 12/19/23 10:48:31.391
  Dec 19 10:48:31.399: INFO: Trying to get logs from node cahyeife7pae-3 pod downward-api-9fb2d95e-0dfd-4666-8203-eb2080b658e3 container dapi-container: <nil>
  STEP: delete the pod @ 12/19/23 10:48:31.435
  Dec 19 10:48:31.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2804" for this suite. @ 12/19/23 10:48:31.471
• [2.189 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 12/19/23 10:48:31.492
  Dec 19 10:48:31.492: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename downward-api @ 12/19/23 10:48:31.494
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:48:31.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:48:31.535
  STEP: Creating a pod to test downward API volume plugin @ 12/19/23 10:48:31.544
  STEP: Saw pod success @ 12/19/23 10:48:35.604
  Dec 19 10:48:35.610: INFO: Trying to get logs from node cahyeife7pae-3 pod downwardapi-volume-4499c369-30cf-4442-bb6f-191425dbcf3e container client-container: <nil>
  STEP: delete the pod @ 12/19/23 10:48:35.625
  Dec 19 10:48:35.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1786" for this suite. @ 12/19/23 10:48:35.666
• [4.192 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 12/19/23 10:48:35.702
  Dec 19 10:48:35.702: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename emptydir @ 12/19/23 10:48:35.704
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:48:35.744
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:48:35.75
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 12/19/23 10:48:35.757
  STEP: Saw pod success @ 12/19/23 10:48:39.806
  Dec 19 10:48:39.816: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-d124362e-1f18-4cf4-804b-f97762651f50 container test-container: <nil>
  STEP: delete the pod @ 12/19/23 10:48:39.836
  Dec 19 10:48:39.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2063" for this suite. @ 12/19/23 10:48:39.892
• [4.213 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 12/19/23 10:48:39.918
  Dec 19 10:48:39.918: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename crd-webhook @ 12/19/23 10:48:39.921
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:48:39.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:48:40.007
  STEP: Setting up server cert @ 12/19/23 10:48:40.017
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 12/19/23 10:48:40.651
  STEP: Deploying the custom resource conversion webhook pod @ 12/19/23 10:48:40.671
  STEP: Wait for the deployment to be ready @ 12/19/23 10:48:40.703
  Dec 19 10:48:40.756: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  Dec 19 10:48:42.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.December, 19, 10, 48, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 48, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 10, 48, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 48, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 12/19/23 10:48:44.798
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 10:48:44.824
  Dec 19 10:48:45.825: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Dec 19 10:48:45.833: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Creating a v1 custom resource @ 12/19/23 10:48:48.725
  STEP: Create a v2 custom resource @ 12/19/23 10:48:48.762
  STEP: List CRs in v1 @ 12/19/23 10:48:49.205
  STEP: List CRs in v2 @ 12/19/23 10:48:49.217
  Dec 19 10:48:49.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-6773" for this suite. @ 12/19/23 10:48:49.918
• [10.025 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 12/19/23 10:48:49.95
  Dec 19 10:48:49.950: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename containers @ 12/19/23 10:48:49.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:48:50.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:48:50.037
  Dec 19 10:48:52.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-4964" for this suite. @ 12/19/23 10:48:52.17
• [2.239 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 12/19/23 10:48:52.194
  Dec 19 10:48:52.194: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 10:48:52.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:48:52.235
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:48:52.243
  STEP: Creating secret with name s-test-opt-del-22952e27-3390-4dff-b5c9-0223c8d19ee7 @ 12/19/23 10:48:52.258
  STEP: Creating secret with name s-test-opt-upd-7bc5d30d-e035-4a0b-a9a1-449be6ed3e06 @ 12/19/23 10:48:52.268
  STEP: Creating the pod @ 12/19/23 10:48:52.279
  STEP: Deleting secret s-test-opt-del-22952e27-3390-4dff-b5c9-0223c8d19ee7 @ 12/19/23 10:48:56.396
  STEP: Updating secret s-test-opt-upd-7bc5d30d-e035-4a0b-a9a1-449be6ed3e06 @ 12/19/23 10:48:56.411
  STEP: Creating secret with name s-test-opt-create-dafb39ff-7817-4466-a746-08d138a838ee @ 12/19/23 10:48:56.423
  STEP: waiting to observe update in volume @ 12/19/23 10:48:56.436
  Dec 19 10:50:21.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6678" for this suite. @ 12/19/23 10:50:21.632
• [89.453 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 12/19/23 10:50:21.649
  Dec 19 10:50:21.649: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename secrets @ 12/19/23 10:50:21.655
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:50:21.709
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:50:21.728
  STEP: Creating secret with name secret-test-cfcd11c3-a4f4-452a-af00-ed1af9935ae3 @ 12/19/23 10:50:21.734
  STEP: Creating a pod to test consume secrets @ 12/19/23 10:50:21.748
  STEP: Saw pod success @ 12/19/23 10:50:25.806
  Dec 19 10:50:25.817: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-secrets-28129549-545b-4391-9853-97f4edc72e41 container secret-volume-test: <nil>
  STEP: delete the pod @ 12/19/23 10:50:25.835
  Dec 19 10:50:25.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1787" for this suite. @ 12/19/23 10:50:25.878
• [4.244 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 12/19/23 10:50:25.899
  Dec 19 10:50:25.899: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename deployment @ 12/19/23 10:50:25.901
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:50:25.942
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:50:25.948
  Dec 19 10:50:25.955: INFO: Creating simple deployment test-new-deployment
  Dec 19 10:50:26.018: INFO: deployment "test-new-deployment" doesn't have the required revision set
  STEP: getting scale subresource @ 12/19/23 10:50:28.06
  STEP: updating a scale subresource @ 12/19/23 10:50:28.07
  STEP: verifying the deployment Spec.Replicas was modified @ 12/19/23 10:50:28.091
  STEP: Patch a scale subresource @ 12/19/23 10:50:28.098
  Dec 19 10:50:28.183: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-3642  09d502af-929f-42d3-8c2a-3781940f21b6 17021 3 2023-12-19 10:50:25 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-12-19 10:50:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 10:50:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0030997a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-12-19 10:50:27 +0000 UTC,LastTransitionTime:2023-12-19 10:50:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-12-19 10:50:27 +0000 UTC,LastTransitionTime:2023-12-19 10:50:26 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Dec 19 10:50:28.195: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-3642  751404bd-b8ca-4c14-99d9-5a8a8e2da777 17026 3 2023-12-19 10:50:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 09d502af-929f-42d3-8c2a-3781940f21b6 0xc004ac4af7 0xc004ac4af8}] [] [{kube-controller-manager Update apps/v1 2023-12-19 10:50:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"09d502af-929f-42d3-8c2a-3781940f21b6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 10:50:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ac4b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Dec 19 10:50:28.222: INFO: Pod "test-new-deployment-67bd4bf6dc-gv2dv" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-gv2dv test-new-deployment-67bd4bf6dc- deployment-3642  77380601-d066-43a4-82e9-640737f8830a 17012 0 2023-12-19 10:50:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 751404bd-b8ca-4c14-99d9-5a8a8e2da777 0xc003099be7 0xc003099be8}] [] [{kube-controller-manager Update v1 2023-12-19 10:50:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"751404bd-b8ca-4c14-99d9-5a8a8e2da777\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 10:50:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j7dcc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j7dcc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 10:50:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 10:50:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 10:50:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 10:50:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.61,PodIP:10.233.66.185,StartTime:2023-12-19 10:50:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-19 10:50:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://c6e773c2c6e489cf0267b82be94d057b94c95b036e0b2f954f4e0bf052b9de1d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.185,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 10:50:28.224: INFO: Pod "test-new-deployment-67bd4bf6dc-qj4nr" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-qj4nr test-new-deployment-67bd4bf6dc- deployment-3642  f7fb3b9f-19ce-4b79-a337-274d881af6e9 17029 0 2023-12-19 10:50:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 751404bd-b8ca-4c14-99d9-5a8a8e2da777 0xc003099de7 0xc003099de8}] [] [{kube-controller-manager Update v1 2023-12-19 10:50:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"751404bd-b8ca-4c14-99d9-5a8a8e2da777\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 10:50:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-62jgh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-62jgh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 10:50:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 10:50:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 10:50:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 10:50:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.223,PodIP:,StartTime:2023-12-19 10:50:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 10:50:28.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3642" for this suite. @ 12/19/23 10:50:28.242
• [2.378 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 12/19/23 10:50:28.281
  Dec 19 10:50:28.281: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename webhook @ 12/19/23 10:50:28.285
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:50:28.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:50:28.38
  STEP: Setting up server cert @ 12/19/23 10:50:28.472
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/19/23 10:50:29.121
  STEP: Deploying the webhook pod @ 12/19/23 10:50:29.141
  STEP: Wait for the deployment to be ready @ 12/19/23 10:50:29.18
  Dec 19 10:50:29.219: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 12/19/23 10:50:31.267
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 10:50:31.33
  Dec 19 10:50:32.331: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Dec 19 10:50:32.340: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6734-crds.webhook.example.com via the AdmissionRegistration API @ 12/19/23 10:50:32.935
  Dec 19 10:50:33.114: INFO: Waiting for webhook configuration to be ready...
  STEP: Creating a custom resource while v1 is storage version @ 12/19/23 10:50:33.241
  STEP: Patching Custom Resource Definition to set v2 as storage @ 12/19/23 10:50:35.28
  STEP: Patching the custom resource while v2 is storage version @ 12/19/23 10:50:35.332
  Dec 19 10:50:35.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1535" for this suite. @ 12/19/23 10:50:36.588
  STEP: Destroying namespace "webhook-markers-3104" for this suite. @ 12/19/23 10:50:36.609
• [8.346 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 12/19/23 10:50:36.628
  Dec 19 10:50:36.628: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubectl @ 12/19/23 10:50:36.631
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:50:36.668
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:50:36.674
  STEP: validating api versions @ 12/19/23 10:50:36.683
  Dec 19 10:50:36.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-5181 api-versions'
  Dec 19 10:50:36.889: INFO: stderr: ""
  Dec 19 10:50:36.889: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Dec 19 10:50:36.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5181" for this suite. @ 12/19/23 10:50:36.9
• [0.287 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 12/19/23 10:50:36.918
  Dec 19 10:50:36.918: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename emptydir @ 12/19/23 10:50:36.92
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:50:36.963
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:50:36.97
  STEP: Creating Pod @ 12/19/23 10:50:36.979
  STEP: Reading file content from the nginx-container @ 12/19/23 10:50:39.026
  Dec 19 10:50:39.026: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5998 PodName:pod-sharedvolume-bc83e0cc-aede-49fb-a6fc-060f8a6bc6cf ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 10:50:39.027: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:50:39.030: INFO: ExecWithOptions: Clientset creation
  Dec 19 10:50:39.031: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-5998/pods/pod-sharedvolume-bc83e0cc-aede-49fb-a6fc-060f8a6bc6cf/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Dec 19 10:50:39.150: INFO: Exec stderr: ""
  Dec 19 10:50:39.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5998" for this suite. @ 12/19/23 10:50:39.165
• [2.260 seconds]
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 12/19/23 10:50:39.189
  Dec 19 10:50:39.189: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 10:50:39.198
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:50:39.228
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:50:39.244
  STEP: Creating secret with name projected-secret-test-50dde5ca-d7d0-4169-94f5-4228691bd763 @ 12/19/23 10:50:39.251
  STEP: Creating a pod to test consume secrets @ 12/19/23 10:50:39.267
  STEP: Saw pod success @ 12/19/23 10:50:43.329
  Dec 19 10:50:43.359: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-projected-secrets-c535f68b-1c2b-4c80-950b-44325c5a4b3c container secret-volume-test: <nil>
  STEP: delete the pod @ 12/19/23 10:50:43.381
  Dec 19 10:50:43.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6030" for this suite. @ 12/19/23 10:50:43.428
• [4.259 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 12/19/23 10:50:43.464
  Dec 19 10:50:43.464: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename events @ 12/19/23 10:50:43.467
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:50:43.505
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:50:43.511
  STEP: creating a test event @ 12/19/23 10:50:43.517
  STEP: listing events in all namespaces @ 12/19/23 10:50:43.534
  STEP: listing events in test namespace @ 12/19/23 10:50:43.552
  STEP: listing events with field selection filtering on source @ 12/19/23 10:50:43.562
  STEP: listing events with field selection filtering on reportingController @ 12/19/23 10:50:43.569
  STEP: getting the test event @ 12/19/23 10:50:43.576
  STEP: patching the test event @ 12/19/23 10:50:43.58
  STEP: getting the test event @ 12/19/23 10:50:43.617
  STEP: updating the test event @ 12/19/23 10:50:43.625
  STEP: getting the test event @ 12/19/23 10:50:43.641
  STEP: deleting the test event @ 12/19/23 10:50:43.647
  STEP: listing events in all namespaces @ 12/19/23 10:50:43.663
  STEP: listing events in test namespace @ 12/19/23 10:50:43.675
  Dec 19 10:50:43.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-1758" for this suite. @ 12/19/23 10:50:43.701
• [0.252 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 12/19/23 10:50:43.72
  Dec 19 10:50:43.720: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename configmap @ 12/19/23 10:50:43.722
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:50:43.755
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:50:43.762
  STEP: Creating configMap with name configmap-test-upd-5788778a-d10f-4b9f-b030-6cf98cb0f6ab @ 12/19/23 10:50:43.777
  STEP: Creating the pod @ 12/19/23 10:50:43.791
  STEP: Waiting for pod with text data @ 12/19/23 10:50:47.856
  STEP: Waiting for pod with binary data @ 12/19/23 10:50:47.873
  Dec 19 10:50:47.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8587" for this suite. @ 12/19/23 10:50:47.914
• [4.217 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 12/19/23 10:50:47.949
  Dec 19 10:50:47.949: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 10:50:47.953
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:50:48.008
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:50:48.014
  STEP: Creating configMap with name projected-configmap-test-volume-f09d3515-667f-4a0c-8d1d-6f01b9d3bd77 @ 12/19/23 10:50:48.02
  STEP: Creating a pod to test consume configMaps @ 12/19/23 10:50:48.031
  STEP: Saw pod success @ 12/19/23 10:50:52.106
  Dec 19 10:50:52.115: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-projected-configmaps-db1ff9c3-50e7-4765-9767-f0be6ce801ad container agnhost-container: <nil>
  STEP: delete the pod @ 12/19/23 10:50:52.131
  Dec 19 10:50:52.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5761" for this suite. @ 12/19/23 10:50:52.204
• [4.275 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 12/19/23 10:50:52.239
  Dec 19 10:50:52.239: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename services @ 12/19/23 10:50:52.246
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:50:52.337
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:50:52.347
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9506 @ 12/19/23 10:50:52.352
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 12/19/23 10:50:52.409
  STEP: creating service externalsvc in namespace services-9506 @ 12/19/23 10:50:52.409
  STEP: creating replication controller externalsvc in namespace services-9506 @ 12/19/23 10:50:52.479
  I1219 10:50:52.504551      13 runners.go:194] Created replication controller with name: externalsvc, namespace: services-9506, replica count: 2
  I1219 10:50:55.559881      13 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 12/19/23 10:50:55.57
  Dec 19 10:50:55.609: INFO: Creating new exec pod
  Dec 19 10:50:57.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-9506 exec execpodbzpbf -- /bin/sh -x -c nslookup clusterip-service.services-9506.svc.cluster.local'
  Dec 19 10:50:58.095: INFO: stderr: "+ nslookup clusterip-service.services-9506.svc.cluster.local\n"
  Dec 19 10:50:58.095: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-9506.svc.cluster.local\tcanonical name = externalsvc.services-9506.svc.cluster.local.\nName:\texternalsvc.services-9506.svc.cluster.local\nAddress: 10.233.25.233\n\n"
  Dec 19 10:50:58.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-9506, will wait for the garbage collector to delete the pods @ 12/19/23 10:50:58.109
  Dec 19 10:50:58.187: INFO: Deleting ReplicationController externalsvc took: 19.665429ms
  Dec 19 10:50:58.289: INFO: Terminating ReplicationController externalsvc pods took: 101.137139ms
  Dec 19 10:51:00.236: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-9506" for this suite. @ 12/19/23 10:51:00.267
• [8.058 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 12/19/23 10:51:00.299
  Dec 19 10:51:00.299: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename replicaset @ 12/19/23 10:51:00.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:51:00.338
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:51:00.344
  Dec 19 10:51:00.384: INFO: Pod name sample-pod: Found 0 pods out of 1
  Dec 19 10:51:05.395: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 12/19/23 10:51:05.395
  STEP: Scaling up "test-rs" replicaset  @ 12/19/23 10:51:05.395
  Dec 19 10:51:05.417: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 12/19/23 10:51:05.417
  W1219 10:51:05.448036      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Dec 19 10:51:05.461: INFO: observed ReplicaSet test-rs in namespace replicaset-7292 with ReadyReplicas 1, AvailableReplicas 1
  Dec 19 10:51:05.529: INFO: observed ReplicaSet test-rs in namespace replicaset-7292 with ReadyReplicas 1, AvailableReplicas 1
  Dec 19 10:51:05.617: INFO: observed ReplicaSet test-rs in namespace replicaset-7292 with ReadyReplicas 1, AvailableReplicas 1
  Dec 19 10:51:05.661: INFO: observed ReplicaSet test-rs in namespace replicaset-7292 with ReadyReplicas 1, AvailableReplicas 1
  Dec 19 10:51:07.031: INFO: observed ReplicaSet test-rs in namespace replicaset-7292 with ReadyReplicas 2, AvailableReplicas 2
  Dec 19 10:51:07.850: INFO: observed Replicaset test-rs in namespace replicaset-7292 with ReadyReplicas 3 found true
  Dec 19 10:51:07.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7292" for this suite. @ 12/19/23 10:51:07.864
• [7.579 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 12/19/23 10:51:07.882
  Dec 19 10:51:07.883: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename prestop @ 12/19/23 10:51:07.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:51:07.909
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:51:07.915
  STEP: Creating server pod server in namespace prestop-7001 @ 12/19/23 10:51:07.921
  STEP: Waiting for pods to come up. @ 12/19/23 10:51:07.936
  STEP: Creating tester pod tester in namespace prestop-7001 @ 12/19/23 10:51:09.969
  STEP: Deleting pre-stop pod @ 12/19/23 10:51:12.022
  Dec 19 10:51:17.056: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Dec 19 10:51:17.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 12/19/23 10:51:17.074
  STEP: Destroying namespace "prestop-7001" for this suite. @ 12/19/23 10:51:17.148
• [9.293 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 12/19/23 10:51:17.182
  Dec 19 10:51:17.182: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename controllerrevisions @ 12/19/23 10:51:17.185
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:51:17.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:51:17.25
  STEP: Creating DaemonSet "e2e-cp65m-daemon-set" @ 12/19/23 10:51:17.322
  STEP: Check that daemon pods launch on every node of the cluster. @ 12/19/23 10:51:17.338
  Dec 19 10:51:17.361: INFO: Number of nodes with available pods controlled by daemonset e2e-cp65m-daemon-set: 0
  Dec 19 10:51:17.361: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:51:18.441: INFO: Number of nodes with available pods controlled by daemonset e2e-cp65m-daemon-set: 0
  Dec 19 10:51:18.442: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:51:19.392: INFO: Number of nodes with available pods controlled by daemonset e2e-cp65m-daemon-set: 3
  Dec 19 10:51:19.393: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-cp65m-daemon-set
  STEP: Confirm DaemonSet "e2e-cp65m-daemon-set" successfully created with "daemonset-name=e2e-cp65m-daemon-set" label @ 12/19/23 10:51:19.405
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-cp65m-daemon-set" @ 12/19/23 10:51:19.424
  Dec 19 10:51:19.433: INFO: Located ControllerRevision: "e2e-cp65m-daemon-set-7cfb8c4d76"
  STEP: Patching ControllerRevision "e2e-cp65m-daemon-set-7cfb8c4d76" @ 12/19/23 10:51:19.441
  Dec 19 10:51:19.455: INFO: e2e-cp65m-daemon-set-7cfb8c4d76 has been patched
  STEP: Create a new ControllerRevision @ 12/19/23 10:51:19.455
  Dec 19 10:51:19.472: INFO: Created ControllerRevision: e2e-cp65m-daemon-set-69fd594b8b
  STEP: Confirm that there are two ControllerRevisions @ 12/19/23 10:51:19.472
  Dec 19 10:51:19.472: INFO: Requesting list of ControllerRevisions to confirm quantity
  Dec 19 10:51:19.480: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-cp65m-daemon-set-7cfb8c4d76" @ 12/19/23 10:51:19.48
  STEP: Confirm that there is only one ControllerRevision @ 12/19/23 10:51:19.495
  Dec 19 10:51:19.495: INFO: Requesting list of ControllerRevisions to confirm quantity
  Dec 19 10:51:19.504: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-cp65m-daemon-set-69fd594b8b" @ 12/19/23 10:51:19.511
  Dec 19 10:51:19.534: INFO: e2e-cp65m-daemon-set-69fd594b8b has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 12/19/23 10:51:19.534
  W1219 10:51:19.564428      13 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 12/19/23 10:51:19.565
  Dec 19 10:51:19.565: INFO: Requesting list of ControllerRevisions to confirm quantity
  Dec 19 10:51:20.579: INFO: Requesting list of ControllerRevisions to confirm quantity
  Dec 19 10:51:20.588: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-cp65m-daemon-set-69fd594b8b=updated" @ 12/19/23 10:51:20.589
  STEP: Confirm that there is only one ControllerRevision @ 12/19/23 10:51:20.608
  Dec 19 10:51:20.608: INFO: Requesting list of ControllerRevisions to confirm quantity
  Dec 19 10:51:20.617: INFO: Found 1 ControllerRevisions
  Dec 19 10:51:20.627: INFO: ControllerRevision "e2e-cp65m-daemon-set-846dcf9795" has revision 3
  STEP: Deleting DaemonSet "e2e-cp65m-daemon-set" @ 12/19/23 10:51:20.639
  STEP: deleting DaemonSet.extensions e2e-cp65m-daemon-set in namespace controllerrevisions-1243, will wait for the garbage collector to delete the pods @ 12/19/23 10:51:20.64
  Dec 19 10:51:20.721: INFO: Deleting DaemonSet.extensions e2e-cp65m-daemon-set took: 14.616264ms
  Dec 19 10:51:20.822: INFO: Terminating DaemonSet.extensions e2e-cp65m-daemon-set pods took: 101.386876ms
  Dec 19 10:51:23.330: INFO: Number of nodes with available pods controlled by daemonset e2e-cp65m-daemon-set: 0
  Dec 19 10:51:23.330: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-cp65m-daemon-set
  Dec 19 10:51:23.338: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17673"},"items":null}

  Dec 19 10:51:23.373: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17673"},"items":null}

  Dec 19 10:51:23.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-1243" for this suite. @ 12/19/23 10:51:23.43
• [6.267 seconds]
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 12/19/23 10:51:23.454
  Dec 19 10:51:23.455: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename security-context-test @ 12/19/23 10:51:23.459
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:51:23.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:51:23.505
  Dec 19 10:51:27.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6983" for this suite. @ 12/19/23 10:51:27.601
• [4.163 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 12/19/23 10:51:27.618
  Dec 19 10:51:27.618: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename sched-preemption @ 12/19/23 10:51:27.621
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:51:27.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:51:27.674
  Dec 19 10:51:27.718: INFO: Waiting up to 1m0s for all nodes to be ready
  Dec 19 10:52:27.797: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 12/19/23 10:52:27.81
  Dec 19 10:52:27.881: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Dec 19 10:52:27.895: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Dec 19 10:52:28.006: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Dec 19 10:52:28.028: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Dec 19 10:52:28.123: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Dec 19 10:52:28.165: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 12/19/23 10:52:28.165
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 12/19/23 10:52:32.264
  Dec 19 10:52:36.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-6806" for this suite. @ 12/19/23 10:52:36.494
• [68.892 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 12/19/23 10:52:36.519
  Dec 19 10:52:36.519: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename secrets @ 12/19/23 10:52:36.522
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:52:36.561
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:52:36.57
  STEP: Creating secret with name secret-test-75e2cf41-57ea-4c43-ab78-34b5e008acc5 @ 12/19/23 10:52:36.616
  STEP: Creating a pod to test consume secrets @ 12/19/23 10:52:36.631
  STEP: Saw pod success @ 12/19/23 10:52:40.686
  Dec 19 10:52:40.694: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-secrets-a71bf27d-0ff1-4df9-bcef-8f3856d781c0 container secret-volume-test: <nil>
  STEP: delete the pod @ 12/19/23 10:52:40.738
  Dec 19 10:52:40.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9094" for this suite. @ 12/19/23 10:52:40.781
  STEP: Destroying namespace "secret-namespace-8336" for this suite. @ 12/19/23 10:52:40.796
• [4.292 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 12/19/23 10:52:40.817
  Dec 19 10:52:40.818: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename init-container @ 12/19/23 10:52:40.82
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:52:40.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:52:40.86
  STEP: creating the pod @ 12/19/23 10:52:40.866
  Dec 19 10:52:40.866: INFO: PodSpec: initContainers in spec.initContainers
  Dec 19 10:53:20.761: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b6089ff3-db9a-4692-9e52-ff1b9ebe0078", GenerateName:"", Namespace:"init-container-7317", SelfLink:"", UID:"b8d6cd66-1df2-4693-896e-8bdc97f34d47", ResourceVersion:"18143", Generation:0, CreationTimestamp:time.Date(2023, time.December, 19, 10, 52, 40, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"866827531"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.December, 19, 10, 52, 40, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0013b0d50), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.December, 19, 10, 53, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0013b0d98), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-8skhz", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc007614860), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8skhz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8skhz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-8skhz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004d095f0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cahyeife7pae-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004318ee0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004d09680)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004d096a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004d096a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004d096ac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001074ab0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.December, 19, 10, 52, 40, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.December, 19, 10, 52, 40, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.December, 19, 10, 52, 40, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.December, 19, 10, 52, 40, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.61", PodIP:"10.233.66.201", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.66.201"}}, StartTime:time.Date(2023, time.December, 19, 10, 52, 40, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004318fc0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004319030)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://da896c305025d08cc4e31b913f8b92b12e513dd4ed9296cb0b84bb20023d9b5b", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc007614900), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0076148e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004d09724), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Dec 19 10:53:20.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-7317" for this suite. @ 12/19/23 10:53:20.785
• [39.983 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 12/19/23 10:53:20.844
  Dec 19 10:53:20.845: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename pods @ 12/19/23 10:53:20.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:53:20.886
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:53:20.891
  STEP: creating the pod @ 12/19/23 10:53:20.896
  STEP: submitting the pod to kubernetes @ 12/19/23 10:53:20.896
  STEP: verifying QOS class is set on the pod @ 12/19/23 10:53:20.915
  Dec 19 10:53:20.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8580" for this suite. @ 12/19/23 10:53:20.952
• [0.137 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 12/19/23 10:53:20.986
  Dec 19 10:53:20.986: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename var-expansion @ 12/19/23 10:53:20.989
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:53:21.028
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:53:21.053
  Dec 19 10:53:23.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 19 10:53:23.137: INFO: Deleting pod "var-expansion-7b0a5233-279c-45a9-9042-fa0f34f861dc" in namespace "var-expansion-9893"
  Dec 19 10:53:23.154: INFO: Wait up to 5m0s for pod "var-expansion-7b0a5233-279c-45a9-9042-fa0f34f861dc" to be fully deleted
  STEP: Destroying namespace "var-expansion-9893" for this suite. @ 12/19/23 10:53:25.173
• [4.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 12/19/23 10:53:25.195
  Dec 19 10:53:25.195: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename namespaces @ 12/19/23 10:53:25.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:53:25.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:53:25.249
  STEP: creating a Namespace @ 12/19/23 10:53:25.255
  STEP: patching the Namespace @ 12/19/23 10:53:25.3
  STEP: get the Namespace and ensuring it has the label @ 12/19/23 10:53:25.312
  Dec 19 10:53:25.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5362" for this suite. @ 12/19/23 10:53:25.333
  STEP: Destroying namespace "nspatchtest-bdb51652-a762-452a-b059-134f51bb2c69-8703" for this suite. @ 12/19/23 10:53:25.352
• [0.172 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 12/19/23 10:53:25.375
  Dec 19 10:53:25.375: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename pod-network-test @ 12/19/23 10:53:25.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:53:25.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:53:25.425
  STEP: Performing setup for networking test in namespace pod-network-test-6499 @ 12/19/23 10:53:25.431
  STEP: creating a selector @ 12/19/23 10:53:25.431
  STEP: Creating the service pods in kubernetes @ 12/19/23 10:53:25.432
  Dec 19 10:53:25.432: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 12/19/23 10:53:47.843
  Dec 19 10:53:49.905: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Dec 19 10:53:49.907: INFO: Breadth first check of 10.233.64.84 on host 192.168.121.236...
  Dec 19 10:53:49.914: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.204:9080/dial?request=hostname&protocol=udp&host=10.233.64.84&port=8081&tries=1'] Namespace:pod-network-test-6499 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 10:53:49.914: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:53:49.916: INFO: ExecWithOptions: Clientset creation
  Dec 19 10:53:49.916: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6499/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.204%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.84%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Dec 19 10:53:50.157: INFO: Waiting for responses: map[]
  Dec 19 10:53:50.157: INFO: reached 10.233.64.84 after 0/1 tries
  Dec 19 10:53:50.158: INFO: Breadth first check of 10.233.65.88 on host 192.168.121.223...
  Dec 19 10:53:50.167: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.204:9080/dial?request=hostname&protocol=udp&host=10.233.65.88&port=8081&tries=1'] Namespace:pod-network-test-6499 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 10:53:50.167: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:53:50.169: INFO: ExecWithOptions: Clientset creation
  Dec 19 10:53:50.169: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6499/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.204%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.65.88%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Dec 19 10:53:50.336: INFO: Waiting for responses: map[]
  Dec 19 10:53:50.337: INFO: reached 10.233.65.88 after 0/1 tries
  Dec 19 10:53:50.337: INFO: Breadth first check of 10.233.66.203 on host 192.168.121.61...
  Dec 19 10:53:50.357: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.204:9080/dial?request=hostname&protocol=udp&host=10.233.66.203&port=8081&tries=1'] Namespace:pod-network-test-6499 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 10:53:50.357: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:53:50.360: INFO: ExecWithOptions: Clientset creation
  Dec 19 10:53:50.361: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6499/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.204%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.203%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Dec 19 10:53:50.516: INFO: Waiting for responses: map[]
  Dec 19 10:53:50.517: INFO: reached 10.233.66.203 after 0/1 tries
  Dec 19 10:53:50.517: INFO: Going to retry 0 out of 3 pods....
  Dec 19 10:53:50.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-6499" for this suite. @ 12/19/23 10:53:50.534
• [25.187 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 12/19/23 10:53:50.573
  Dec 19 10:53:50.573: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 12/19/23 10:53:50.578
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:53:50.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:53:50.652
  STEP: create the container to handle the HTTPGet hook request. @ 12/19/23 10:53:50.675
  STEP: create the pod with lifecycle hook @ 12/19/23 10:53:52.754
  STEP: delete the pod with lifecycle hook @ 12/19/23 10:53:54.809
  STEP: check prestop hook @ 12/19/23 10:53:56.899
  Dec 19 10:53:56.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-45" for this suite. @ 12/19/23 10:53:56.987
• [6.448 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 12/19/23 10:53:57.021
  Dec 19 10:53:57.021: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename subjectreview @ 12/19/23 10:53:57.025
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:53:57.094
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:53:57.119
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-2014" @ 12/19/23 10:53:57.128
  Dec 19 10:53:57.174: INFO: saUsername: "system:serviceaccount:subjectreview-2014:e2e"
  Dec 19 10:53:57.174: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-2014"}
  Dec 19 10:53:57.175: INFO: saUID: "b6c0bb3c-9d28-45c9-8912-aad0644934aa"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-2014:e2e" @ 12/19/23 10:53:57.175
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-2014:e2e" @ 12/19/23 10:53:57.176
  Dec 19 10:53:57.186: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-2014:e2e" api 'list' configmaps in "subjectreview-2014" namespace @ 12/19/23 10:53:57.186
  Dec 19 10:53:57.192: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-2014:e2e" @ 12/19/23 10:53:57.193
  Dec 19 10:53:57.205: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Dec 19 10:53:57.205: INFO: LocalSubjectAccessReview has been verified
  Dec 19 10:53:57.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-2014" for this suite. @ 12/19/23 10:53:57.22
• [0.246 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 12/19/23 10:53:57.276
  Dec 19 10:53:57.276: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename endpointslice @ 12/19/23 10:53:57.283
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:53:57.313
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:53:57.322
  STEP: referencing a single matching pod @ 12/19/23 10:54:02.549
  STEP: referencing matching pods with named port @ 12/19/23 10:54:07.581
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 12/19/23 10:54:12.602
  STEP: recreating EndpointSlices after they've been deleted @ 12/19/23 10:54:17.626
  Dec 19 10:54:17.680: INFO: EndpointSlice for Service endpointslice-5330/example-named-port not found
  Dec 19 10:54:27.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5330" for this suite. @ 12/19/23 10:54:27.735
• [30.475 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 12/19/23 10:54:27.753
  Dec 19 10:54:27.753: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename secrets @ 12/19/23 10:54:27.76
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:54:27.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:54:27.816
  STEP: Creating secret with name secret-test-96d89466-3775-4d7b-bc1d-f1555c4a7803 @ 12/19/23 10:54:27.825
  STEP: Creating a pod to test consume secrets @ 12/19/23 10:54:27.841
  STEP: Saw pod success @ 12/19/23 10:54:31.909
  Dec 19 10:54:31.916: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-secrets-63a08560-2630-4787-b586-ea800a364fc4 container secret-volume-test: <nil>
  STEP: delete the pod @ 12/19/23 10:54:31.958
  Dec 19 10:54:31.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1581" for this suite. @ 12/19/23 10:54:32.002
• [4.264 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 12/19/23 10:54:32.018
  Dec 19 10:54:32.019: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 12/19/23 10:54:32.021
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:54:32.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:54:32.058
  STEP: creating a target pod @ 12/19/23 10:54:32.065
  STEP: adding an ephemeral container @ 12/19/23 10:54:34.12
  STEP: checking pod container endpoints @ 12/19/23 10:54:36.202
  Dec 19 10:54:36.203: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-8195 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 10:54:36.203: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:54:36.207: INFO: ExecWithOptions: Clientset creation
  Dec 19 10:54:36.208: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-8195/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Dec 19 10:54:36.384: INFO: Exec stderr: ""
  Dec 19 10:54:36.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-8195" for this suite. @ 12/19/23 10:54:36.409
• [4.405 seconds]
------------------------------
S
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 12/19/23 10:54:36.425
  Dec 19 10:54:36.425: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename endpointslice @ 12/19/23 10:54:36.428
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:54:36.476
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:54:36.483
  STEP: getting /apis @ 12/19/23 10:54:36.491
  STEP: getting /apis/discovery.k8s.io @ 12/19/23 10:54:36.503
  STEP: getting /apis/discovery.k8s.iov1 @ 12/19/23 10:54:36.507
  STEP: creating @ 12/19/23 10:54:36.51
  STEP: getting @ 12/19/23 10:54:36.544
  STEP: listing @ 12/19/23 10:54:36.55
  STEP: watching @ 12/19/23 10:54:36.561
  Dec 19 10:54:36.561: INFO: starting watch
  STEP: cluster-wide listing @ 12/19/23 10:54:36.566
  STEP: cluster-wide watching @ 12/19/23 10:54:36.575
  Dec 19 10:54:36.575: INFO: starting watch
  STEP: patching @ 12/19/23 10:54:36.578
  STEP: updating @ 12/19/23 10:54:36.591
  Dec 19 10:54:36.614: INFO: waiting for watch events with expected annotations
  Dec 19 10:54:36.614: INFO: saw patched and updated annotations
  STEP: deleting @ 12/19/23 10:54:36.615
  STEP: deleting a collection @ 12/19/23 10:54:36.651
  Dec 19 10:54:36.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6152" for this suite. @ 12/19/23 10:54:36.711
• [0.302 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 12/19/23 10:54:36.729
  Dec 19 10:54:36.730: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename gc @ 12/19/23 10:54:36.731
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:54:36.77
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:54:36.775
  STEP: create the deployment @ 12/19/23 10:54:36.79
  W1219 10:54:36.809426      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 12/19/23 10:54:36.809
  STEP: delete the deployment @ 12/19/23 10:54:37.367
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 12/19/23 10:54:37.393
  STEP: Gathering metrics @ 12/19/23 10:54:37.973
  Dec 19 10:54:38.284: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Dec 19 10:54:38.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1309" for this suite. @ 12/19/23 10:54:38.299
• [1.582 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 12/19/23 10:54:38.317
  Dec 19 10:54:38.317: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename replicaset @ 12/19/23 10:54:38.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:54:38.382
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:54:38.39
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 12/19/23 10:54:38.396
  STEP: When a replicaset with a matching selector is created @ 12/19/23 10:54:40.447
  STEP: Then the orphan pod is adopted @ 12/19/23 10:54:40.463
  STEP: When the matched label of one of its pods change @ 12/19/23 10:54:41.492
  Dec 19 10:54:41.510: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 12/19/23 10:54:41.582
  Dec 19 10:54:42.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2223" for this suite. @ 12/19/23 10:54:42.631
• [4.362 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 12/19/23 10:54:42.685
  Dec 19 10:54:42.686: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename downward-api @ 12/19/23 10:54:42.69
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:54:42.747
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:54:42.766
  STEP: Creating a pod to test downward API volume plugin @ 12/19/23 10:54:42.775
  STEP: Saw pod success @ 12/19/23 10:54:46.85
  Dec 19 10:54:46.858: INFO: Trying to get logs from node cahyeife7pae-3 pod downwardapi-volume-875a937a-c315-48ae-94c3-c85fc7a48884 container client-container: <nil>
  STEP: delete the pod @ 12/19/23 10:54:46.872
  Dec 19 10:54:46.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8513" for this suite. @ 12/19/23 10:54:46.915
• [4.246 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 12/19/23 10:54:46.937
  Dec 19 10:54:46.937: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename crd-publish-openapi @ 12/19/23 10:54:46.94
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:54:46.98
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:54:46.99
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 12/19/23 10:54:46.996
  Dec 19 10:54:46.997: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 12/19/23 10:54:55.52
  Dec 19 10:54:55.522: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:54:57.339: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 10:55:05.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8621" for this suite. @ 12/19/23 10:55:05.405
• [18.485 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 12/19/23 10:55:05.425
  Dec 19 10:55:05.426: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename pods @ 12/19/23 10:55:05.429
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:55:05.47
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:55:05.479
  STEP: creating the pod @ 12/19/23 10:55:05.49
  STEP: submitting the pod to kubernetes @ 12/19/23 10:55:05.491
  STEP: verifying the pod is in kubernetes @ 12/19/23 10:55:07.631
  STEP: updating the pod @ 12/19/23 10:55:07.641
  Dec 19 10:55:08.170: INFO: Successfully updated pod "pod-update-d868bca0-57c2-48f5-8435-c9c5abcfda5f"
  STEP: verifying the updated pod is in kubernetes @ 12/19/23 10:55:08.179
  Dec 19 10:55:08.189: INFO: Pod update OK
  Dec 19 10:55:08.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9450" for this suite. @ 12/19/23 10:55:08.199
• [2.797 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 12/19/23 10:55:08.237
  Dec 19 10:55:08.238: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename conformance-tests @ 12/19/23 10:55:08.242
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:55:08.277
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:55:08.284
  STEP: Getting node addresses @ 12/19/23 10:55:08.297
  Dec 19 10:55:08.297: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Dec 19 10:55:08.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-378" for this suite. @ 12/19/23 10:55:08.343
• [0.121 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 12/19/23 10:55:08.36
  Dec 19 10:55:08.360: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename server-version @ 12/19/23 10:55:08.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:55:08.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:55:08.423
  STEP: Request ServerVersion @ 12/19/23 10:55:08.43
  STEP: Confirm major version @ 12/19/23 10:55:08.433
  Dec 19 10:55:08.433: INFO: Major version: 1
  STEP: Confirm minor version @ 12/19/23 10:55:08.433
  Dec 19 10:55:08.433: INFO: cleanMinorVersion: 27
  Dec 19 10:55:08.433: INFO: Minor version: 27
  Dec 19 10:55:08.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-4068" for this suite. @ 12/19/23 10:55:08.443
• [0.101 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 12/19/23 10:55:08.47
  Dec 19 10:55:08.470: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename runtimeclass @ 12/19/23 10:55:08.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:55:08.514
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:55:08.521
  STEP: getting /apis @ 12/19/23 10:55:08.528
  STEP: getting /apis/node.k8s.io @ 12/19/23 10:55:08.545
  STEP: getting /apis/node.k8s.io/v1 @ 12/19/23 10:55:08.549
  STEP: creating @ 12/19/23 10:55:08.558
  STEP: watching @ 12/19/23 10:55:08.614
  Dec 19 10:55:08.615: INFO: starting watch
  STEP: getting @ 12/19/23 10:55:08.634
  STEP: listing @ 12/19/23 10:55:08.643
  STEP: patching @ 12/19/23 10:55:08.652
  STEP: updating @ 12/19/23 10:55:08.671
  Dec 19 10:55:08.687: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 12/19/23 10:55:08.688
  STEP: deleting a collection @ 12/19/23 10:55:08.729
  Dec 19 10:55:08.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7216" for this suite. @ 12/19/23 10:55:08.786
• [0.345 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 12/19/23 10:55:08.834
  Dec 19 10:55:08.834: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 10:55:08.837
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:55:08.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:55:08.897
  STEP: Creating configMap with name cm-test-opt-del-94c6c565-b225-406d-97fd-b395eef7d6a6 @ 12/19/23 10:55:08.912
  STEP: Creating configMap with name cm-test-opt-upd-a773d7a9-1d41-45e0-9c61-ff848c353383 @ 12/19/23 10:55:08.924
  STEP: Creating the pod @ 12/19/23 10:55:08.942
  STEP: Deleting configmap cm-test-opt-del-94c6c565-b225-406d-97fd-b395eef7d6a6 @ 12/19/23 10:55:11.087
  STEP: Updating configmap cm-test-opt-upd-a773d7a9-1d41-45e0-9c61-ff848c353383 @ 12/19/23 10:55:11.106
  STEP: Creating configMap with name cm-test-opt-create-2c28e289-d651-4c7b-bb0c-e88a3647004f @ 12/19/23 10:55:11.119
  STEP: waiting to observe update in volume @ 12/19/23 10:55:11.132
  Dec 19 10:56:22.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7012" for this suite. @ 12/19/23 10:56:22.029
• [73.213 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 12/19/23 10:56:22.061
  Dec 19 10:56:22.061: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename downward-api @ 12/19/23 10:56:22.067
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:56:22.117
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:56:22.126
  STEP: Creating the pod @ 12/19/23 10:56:22.138
  Dec 19 10:56:24.749: INFO: Successfully updated pod "labelsupdate670141c8-827e-4c3c-8b00-ab092d30c94e"
  Dec 19 10:56:26.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4889" for this suite. @ 12/19/23 10:56:26.8
• [4.754 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 12/19/23 10:56:26.818
  Dec 19 10:56:26.818: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename webhook @ 12/19/23 10:56:26.821
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:56:26.864
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:56:26.873
  STEP: Setting up server cert @ 12/19/23 10:56:26.945
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/19/23 10:56:27.743
  STEP: Deploying the webhook pod @ 12/19/23 10:56:27.768
  STEP: Wait for the deployment to be ready @ 12/19/23 10:56:27.806
  Dec 19 10:56:27.835: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 12/19/23 10:56:29.869
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 10:56:29.903
  Dec 19 10:56:30.904: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 12/19/23 10:56:30.913
  STEP: create a pod that should be denied by the webhook @ 12/19/23 10:56:30.964
  STEP: create a pod that causes the webhook to hang @ 12/19/23 10:56:31.007
  STEP: create a configmap that should be denied by the webhook @ 12/19/23 10:56:41.028
  STEP: create a configmap that should be admitted by the webhook @ 12/19/23 10:56:41.094
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 12/19/23 10:56:41.127
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 12/19/23 10:56:41.158
  STEP: create a namespace that bypass the webhook @ 12/19/23 10:56:41.172
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 12/19/23 10:56:41.207
  Dec 19 10:56:41.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3454" for this suite. @ 12/19/23 10:56:41.397
  STEP: Destroying namespace "webhook-markers-1122" for this suite. @ 12/19/23 10:56:41.417
  STEP: Destroying namespace "exempted-namespace-1895" for this suite. @ 12/19/23 10:56:41.447
• [14.702 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 12/19/23 10:56:41.522
  Dec 19 10:56:41.522: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename gc @ 12/19/23 10:56:41.525
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:56:41.667
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:56:41.673
  STEP: create the rc @ 12/19/23 10:56:41.68
  W1219 10:56:41.691974      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 12/19/23 10:56:46.703
  STEP: wait for all pods to be garbage collected @ 12/19/23 10:56:46.745
  STEP: Gathering metrics @ 12/19/23 10:56:51.764
  Dec 19 10:56:52.016: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Dec 19 10:56:52.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1868" for this suite. @ 12/19/23 10:56:52.04
• [10.538 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 12/19/23 10:56:52.061
  Dec 19 10:56:52.061: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubectl @ 12/19/23 10:56:52.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:56:52.115
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:56:52.126
  STEP: validating cluster-info @ 12/19/23 10:56:52.137
  Dec 19 10:56:52.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-7871 cluster-info'
  Dec 19 10:56:52.429: INFO: stderr: ""
  Dec 19 10:56:52.429: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Dec 19 10:56:52.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7871" for this suite. @ 12/19/23 10:56:52.448
• [0.405 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 12/19/23 10:56:52.466
  Dec 19 10:56:52.466: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename configmap @ 12/19/23 10:56:52.47
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:56:52.503
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:56:52.512
  STEP: Creating configMap with name configmap-test-volume-9f5543ea-affc-4047-ae30-858aa1fc4db2 @ 12/19/23 10:56:52.519
  STEP: Creating a pod to test consume configMaps @ 12/19/23 10:56:52.53
  STEP: Saw pod success @ 12/19/23 10:56:56.587
  Dec 19 10:56:56.594: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-configmaps-6cbdaa72-089d-4e81-bc8c-a61f4c2c7cd0 container agnhost-container: <nil>
  STEP: delete the pod @ 12/19/23 10:56:56.615
  Dec 19 10:56:56.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8013" for this suite. @ 12/19/23 10:56:56.687
• [4.239 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 12/19/23 10:56:56.726
  Dec 19 10:56:56.727: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename resourcequota @ 12/19/23 10:56:56.729
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:56:56.77
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:56:56.778
  STEP: Discovering how many secrets are in namespace by default @ 12/19/23 10:56:56.79
  STEP: Counting existing ResourceQuota @ 12/19/23 10:57:01.806
  STEP: Creating a ResourceQuota @ 12/19/23 10:57:06.814
  STEP: Ensuring resource quota status is calculated @ 12/19/23 10:57:06.826
  STEP: Creating a Secret @ 12/19/23 10:57:08.839
  STEP: Ensuring resource quota status captures secret creation @ 12/19/23 10:57:08.87
  STEP: Deleting a secret @ 12/19/23 10:57:10.881
  STEP: Ensuring resource quota status released usage @ 12/19/23 10:57:10.9
  Dec 19 10:57:12.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4098" for this suite. @ 12/19/23 10:57:12.95
• [16.244 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 12/19/23 10:57:12.974
  Dec 19 10:57:12.974: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename field-validation @ 12/19/23 10:57:12.979
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:57:13.068
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:57:13.083
  STEP: apply creating a deployment @ 12/19/23 10:57:13.09
  Dec 19 10:57:13.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9949" for this suite. @ 12/19/23 10:57:13.171
• [0.213 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 12/19/23 10:57:13.187
  Dec 19 10:57:13.187: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename security-context-test @ 12/19/23 10:57:13.192
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:57:13.253
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:57:13.26
  Dec 19 10:57:19.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9544" for this suite. @ 12/19/23 10:57:19.396
• [6.222 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 12/19/23 10:57:19.414
  Dec 19 10:57:19.414: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename configmap @ 12/19/23 10:57:19.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:57:19.45
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:57:19.457
  STEP: Creating configMap with name configmap-test-upd-99bf3507-2ee4-4335-896b-e6bbf04146e6 @ 12/19/23 10:57:19.476
  STEP: Creating the pod @ 12/19/23 10:57:19.491
  STEP: Updating configmap configmap-test-upd-99bf3507-2ee4-4335-896b-e6bbf04146e6 @ 12/19/23 10:57:21.558
  STEP: waiting to observe update in volume @ 12/19/23 10:57:21.571
  Dec 19 10:57:23.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5002" for this suite. @ 12/19/23 10:57:23.613
• [4.216 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 12/19/23 10:57:23.633
  Dec 19 10:57:23.633: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename webhook @ 12/19/23 10:57:23.637
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:57:23.67
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:57:23.677
  STEP: Setting up server cert @ 12/19/23 10:57:23.738
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/19/23 10:57:25.025
  STEP: Deploying the webhook pod @ 12/19/23 10:57:25.05
  STEP: Wait for the deployment to be ready @ 12/19/23 10:57:25.072
  Dec 19 10:57:25.102: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 12/19/23 10:57:27.129
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 10:57:27.155
  Dec 19 10:57:28.156: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 12/19/23 10:57:28.297
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 12/19/23 10:57:28.359
  STEP: Deleting the collection of validation webhooks @ 12/19/23 10:57:28.4
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 12/19/23 10:57:28.506
  Dec 19 10:57:28.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-821" for this suite. @ 12/19/23 10:57:28.643
  STEP: Destroying namespace "webhook-markers-6633" for this suite. @ 12/19/23 10:57:28.661
• [5.045 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 12/19/23 10:57:28.687
  Dec 19 10:57:28.687: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 10:57:28.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:57:28.722
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:57:28.738
  STEP: Creating configMap with name projected-configmap-test-volume-map-8afa0c83-f9b7-4cc9-bb3f-03f43cba0728 @ 12/19/23 10:57:28.749
  STEP: Creating a pod to test consume configMaps @ 12/19/23 10:57:28.767
  STEP: Saw pod success @ 12/19/23 10:57:32.825
  Dec 19 10:57:32.833: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-projected-configmaps-5332c789-ccf1-4a9a-8c75-b3eb14fb56a9 container agnhost-container: <nil>
  STEP: delete the pod @ 12/19/23 10:57:32.848
  Dec 19 10:57:32.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4972" for this suite. @ 12/19/23 10:57:32.892
• [4.217 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 12/19/23 10:57:32.912
  Dec 19 10:57:32.912: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 10:57:32.914
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:57:32.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:57:32.949
  STEP: Creating the pod @ 12/19/23 10:57:32.957
  Dec 19 10:57:35.556: INFO: Successfully updated pod "labelsupdatebb9f3c75-c4f5-4605-94f5-fb020f57e99b"
  Dec 19 10:57:39.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8026" for this suite. @ 12/19/23 10:57:39.615
• [6.719 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 12/19/23 10:57:39.634
  Dec 19 10:57:39.634: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename configmap @ 12/19/23 10:57:39.636
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:57:39.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:57:39.684
  STEP: Creating configMap with name configmap-test-volume-4522c1e4-8f70-484b-a1d2-193e586c0973 @ 12/19/23 10:57:39.691
  STEP: Creating a pod to test consume configMaps @ 12/19/23 10:57:39.703
  STEP: Saw pod success @ 12/19/23 10:57:43.758
  Dec 19 10:57:43.767: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-configmaps-9183125a-58bf-49de-8695-8b34f61a233d container configmap-volume-test: <nil>
  STEP: delete the pod @ 12/19/23 10:57:43.783
  Dec 19 10:57:43.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6139" for this suite. @ 12/19/23 10:57:43.837
• [4.218 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 12/19/23 10:57:43.859
  Dec 19 10:57:43.860: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename sched-preemption @ 12/19/23 10:57:43.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:57:43.907
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:57:43.913
  Dec 19 10:57:43.953: INFO: Waiting up to 1m0s for all nodes to be ready
  Dec 19 10:58:44.018: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 12/19/23 10:58:44.026
  Dec 19 10:58:44.027: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename sched-preemption-path @ 12/19/23 10:58:44.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:58:44.067
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:58:44.073
  STEP: Finding an available node @ 12/19/23 10:58:44.079
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 12/19/23 10:58:44.08
  STEP: Explicitly delete pod here to free the resource it takes. @ 12/19/23 10:58:46.121
  Dec 19 10:58:46.144: INFO: found a healthy node: cahyeife7pae-3
  Dec 19 10:58:52.369: INFO: pods created so far: [1 1 1]
  Dec 19 10:58:52.369: INFO: length of pods created so far: 3
  Dec 19 10:58:54.406: INFO: pods created so far: [2 2 1]
  Dec 19 10:59:01.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 19 10:59:01.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-6286" for this suite. @ 12/19/23 10:59:01.647
  STEP: Destroying namespace "sched-preemption-4162" for this suite. @ 12/19/23 10:59:01.662
• [77.815 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:177
  STEP: Creating a kubernetes client @ 12/19/23 10:59:01.682
  Dec 19 10:59:01.682: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename daemonsets @ 12/19/23 10:59:01.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:59:01.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:59:01.72
  STEP: Creating simple DaemonSet "daemon-set" @ 12/19/23 10:59:01.768
  STEP: Check that daemon pods launch on every node of the cluster. @ 12/19/23 10:59:01.785
  Dec 19 10:59:01.806: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:59:01.806: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:59:02.824: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:59:02.825: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:59:03.822: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Dec 19 10:59:03.822: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 12/19/23 10:59:03.829
  Dec 19 10:59:03.877: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 19 10:59:03.877: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:59:04.902: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 19 10:59:04.902: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:59:05.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 19 10:59:05.897: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 10:59:06.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Dec 19 10:59:06.895: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 12/19/23 10:59:06.905
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5071, will wait for the garbage collector to delete the pods @ 12/19/23 10:59:06.905
  Dec 19 10:59:06.984: INFO: Deleting DaemonSet.extensions daemon-set took: 16.104754ms
  Dec 19 10:59:07.085: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.974641ms
  Dec 19 10:59:09.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 10:59:09.292: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Dec 19 10:59:09.298: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20055"},"items":null}

  Dec 19 10:59:09.304: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20055"},"items":null}

  Dec 19 10:59:09.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5071" for this suite. @ 12/19/23 10:59:09.345
• [7.677 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:790
  STEP: Creating a kubernetes client @ 12/19/23 10:59:09.359
  Dec 19 10:59:09.359: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename statefulset @ 12/19/23 10:59:09.361
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:59:09.399
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:59:09.403
  STEP: Creating service test in namespace statefulset-267 @ 12/19/23 10:59:09.409
  STEP: Looking for a node to schedule stateful set and pod @ 12/19/23 10:59:09.418
  STEP: Creating pod with conflicting port in namespace statefulset-267 @ 12/19/23 10:59:09.433
  STEP: Waiting until pod test-pod will start running in namespace statefulset-267 @ 12/19/23 10:59:09.452
  STEP: Creating statefulset with conflicting port in namespace statefulset-267 @ 12/19/23 10:59:11.471
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-267 @ 12/19/23 10:59:11.484
  Dec 19 10:59:11.524: INFO: Observed stateful pod in namespace: statefulset-267, name: ss-0, uid: b229c7be-2065-4a64-b033-7d66cec1bfc6, status phase: Pending. Waiting for statefulset controller to delete.
  Dec 19 10:59:11.549: INFO: Observed stateful pod in namespace: statefulset-267, name: ss-0, uid: b229c7be-2065-4a64-b033-7d66cec1bfc6, status phase: Failed. Waiting for statefulset controller to delete.
  Dec 19 10:59:11.609: INFO: Observed stateful pod in namespace: statefulset-267, name: ss-0, uid: b229c7be-2065-4a64-b033-7d66cec1bfc6, status phase: Failed. Waiting for statefulset controller to delete.
  Dec 19 10:59:11.621: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-267
  STEP: Removing pod with conflicting port in namespace statefulset-267 @ 12/19/23 10:59:11.622
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-267 and will be in running state @ 12/19/23 10:59:11.664
  Dec 19 10:59:13.685: INFO: Deleting all statefulset in ns statefulset-267
  Dec 19 10:59:13.692: INFO: Scaling statefulset ss to 0
  Dec 19 10:59:23.731: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 19 10:59:23.738: INFO: Deleting statefulset ss
  Dec 19 10:59:23.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-267" for this suite. @ 12/19/23 10:59:23.787
• [14.445 seconds]
------------------------------
S
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 12/19/23 10:59:23.805
  Dec 19 10:59:23.805: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename svc-latency @ 12/19/23 10:59:23.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:59:23.833
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:59:23.838
  Dec 19 10:59:23.845: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-1628 @ 12/19/23 10:59:23.848
  I1219 10:59:23.862428      13 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1628, replica count: 1
  I1219 10:59:24.913675      13 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Dec 19 10:59:25.043: INFO: Created: latency-svc-mvmcw
  Dec 19 10:59:25.057: INFO: Got endpoints: latency-svc-mvmcw [43.506076ms]
  Dec 19 10:59:25.092: INFO: Created: latency-svc-9r5xw
  Dec 19 10:59:25.102: INFO: Created: latency-svc-9g2sn
  Dec 19 10:59:25.107: INFO: Got endpoints: latency-svc-9r5xw [47.946182ms]
  Dec 19 10:59:25.121: INFO: Got endpoints: latency-svc-9g2sn [62.261789ms]
  Dec 19 10:59:25.126: INFO: Created: latency-svc-d2qll
  Dec 19 10:59:25.136: INFO: Got endpoints: latency-svc-d2qll [75.987036ms]
  Dec 19 10:59:25.147: INFO: Created: latency-svc-8z5dg
  Dec 19 10:59:25.161: INFO: Got endpoints: latency-svc-8z5dg [102.843585ms]
  Dec 19 10:59:25.168: INFO: Created: latency-svc-q98k5
  Dec 19 10:59:25.192: INFO: Created: latency-svc-5fsvr
  Dec 19 10:59:25.201: INFO: Got endpoints: latency-svc-q98k5 [140.719961ms]
  Dec 19 10:59:25.210: INFO: Got endpoints: latency-svc-5fsvr [150.068591ms]
  Dec 19 10:59:25.216: INFO: Created: latency-svc-m82bw
  Dec 19 10:59:25.226: INFO: Got endpoints: latency-svc-m82bw [165.118398ms]
  Dec 19 10:59:25.239: INFO: Created: latency-svc-6nprh
  Dec 19 10:59:25.247: INFO: Created: latency-svc-b4mwx
  Dec 19 10:59:25.255: INFO: Got endpoints: latency-svc-6nprh [194.700249ms]
  Dec 19 10:59:25.272: INFO: Got endpoints: latency-svc-b4mwx [210.194454ms]
  Dec 19 10:59:25.289: INFO: Created: latency-svc-2bfzn
  Dec 19 10:59:25.305: INFO: Created: latency-svc-6f6fh
  Dec 19 10:59:25.334: INFO: Got endpoints: latency-svc-2bfzn [272.757356ms]
  Dec 19 10:59:25.337: INFO: Got endpoints: latency-svc-6f6fh [276.292541ms]
  Dec 19 10:59:25.631: INFO: Created: latency-svc-rzxbh
  Dec 19 10:59:25.631: INFO: Created: latency-svc-hdncx
  Dec 19 10:59:25.636: INFO: Created: latency-svc-zcxf7
  Dec 19 10:59:25.675: INFO: Created: latency-svc-zt6pv
  Dec 19 10:59:25.691: INFO: Got endpoints: latency-svc-rzxbh [554.774429ms]
  Dec 19 10:59:25.691: INFO: Got endpoints: latency-svc-hdncx [490.04407ms]
  Dec 19 10:59:25.699: INFO: Created: latency-svc-2ckdm
  Dec 19 10:59:25.700: INFO: Created: latency-svc-zzgqf
  Dec 19 10:59:25.700: INFO: Created: latency-svc-gwlg4
  Dec 19 10:59:25.700: INFO: Created: latency-svc-wmqc8
  Dec 19 10:59:25.702: INFO: Created: latency-svc-xssxv
  Dec 19 10:59:25.703: INFO: Created: latency-svc-f44s5
  Dec 19 10:59:25.703: INFO: Created: latency-svc-ln5rz
  Dec 19 10:59:25.704: INFO: Created: latency-svc-s45hz
  Dec 19 10:59:25.704: INFO: Created: latency-svc-6td6b
  Dec 19 10:59:25.705: INFO: Created: latency-svc-sccms
  Dec 19 10:59:25.715: INFO: Got endpoints: latency-svc-wmqc8 [653.968071ms]
  Dec 19 10:59:25.716: INFO: Got endpoints: latency-svc-zcxf7 [653.487028ms]
  Dec 19 10:59:25.716: INFO: Got endpoints: latency-svc-zt6pv [460.668221ms]
  Dec 19 10:59:25.737: INFO: Created: latency-svc-tqsqh
  Dec 19 10:59:25.747: INFO: Got endpoints: latency-svc-xssxv [408.626798ms]
  Dec 19 10:59:25.761: INFO: Got endpoints: latency-svc-zzgqf [488.802616ms]
  Dec 19 10:59:25.761: INFO: Got endpoints: latency-svc-tqsqh [701.12267ms]
  Dec 19 10:59:25.761: INFO: Got endpoints: latency-svc-2ckdm [600.526743ms]
  Dec 19 10:59:25.773: INFO: Got endpoints: latency-svc-gwlg4 [562.437624ms]
  Dec 19 10:59:25.807: INFO: Got endpoints: latency-svc-s45hz [685.1308ms]
  Dec 19 10:59:25.824: INFO: Created: latency-svc-2vjlv
  Dec 19 10:59:25.834: INFO: Created: latency-svc-h8g9f
  Dec 19 10:59:25.836: INFO: Got endpoints: latency-svc-6td6b [773.356132ms]
  Dec 19 10:59:25.836: INFO: Got endpoints: latency-svc-ln5rz [610.71629ms]
  Dec 19 10:59:25.837: INFO: Got endpoints: latency-svc-f44s5 [502.37743ms]
  Dec 19 10:59:25.838: INFO: Got endpoints: latency-svc-2vjlv [146.396593ms]
  Dec 19 10:59:25.838: INFO: Got endpoints: latency-svc-sccms [730.774881ms]
  Dec 19 10:59:25.863: INFO: Created: latency-svc-t25dq
  Dec 19 10:59:25.866: INFO: Got endpoints: latency-svc-h8g9f [175.713597ms]
  Dec 19 10:59:25.875: INFO: Got endpoints: latency-svc-t25dq [159.30623ms]
  Dec 19 10:59:25.878: INFO: Created: latency-svc-mdtkc
  Dec 19 10:59:25.901: INFO: Got endpoints: latency-svc-mdtkc [184.659733ms]
  Dec 19 10:59:25.906: INFO: Created: latency-svc-8kk2z
  Dec 19 10:59:25.920: INFO: Created: latency-svc-lffct
  Dec 19 10:59:25.925: INFO: Got endpoints: latency-svc-8kk2z [208.634651ms]
  Dec 19 10:59:25.938: INFO: Created: latency-svc-c7wrd
  Dec 19 10:59:25.957: INFO: Got endpoints: latency-svc-lffct [209.816164ms]
  Dec 19 10:59:25.958: INFO: Got endpoints: latency-svc-c7wrd [196.343591ms]
  Dec 19 10:59:25.983: INFO: Created: latency-svc-mqtpb
  Dec 19 10:59:25.988: INFO: Got endpoints: latency-svc-mqtpb [225.90281ms]
  Dec 19 10:59:26.003: INFO: Created: latency-svc-5wr2s
  Dec 19 10:59:26.022: INFO: Created: latency-svc-jtz2q
  Dec 19 10:59:26.030: INFO: Got endpoints: latency-svc-5wr2s [268.56644ms]
  Dec 19 10:59:26.041: INFO: Created: latency-svc-qxnw7
  Dec 19 10:59:26.059: INFO: Got endpoints: latency-svc-qxnw7 [286.143132ms]
  Dec 19 10:59:26.068: INFO: Created: latency-svc-2w8c5
  Dec 19 10:59:26.074: INFO: Got endpoints: latency-svc-jtz2q [266.502716ms]
  Dec 19 10:59:26.083: INFO: Got endpoints: latency-svc-2w8c5 [247.332537ms]
  Dec 19 10:59:26.083: INFO: Created: latency-svc-x4cvm
  Dec 19 10:59:26.096: INFO: Got endpoints: latency-svc-x4cvm [259.920521ms]
  Dec 19 10:59:26.105: INFO: Created: latency-svc-sg5hc
  Dec 19 10:59:26.129: INFO: Created: latency-svc-6nsdz
  Dec 19 10:59:26.133: INFO: Got endpoints: latency-svc-sg5hc [295.40624ms]
  Dec 19 10:59:26.145: INFO: Created: latency-svc-w4nwb
  Dec 19 10:59:26.156: INFO: Got endpoints: latency-svc-6nsdz [317.71829ms]
  Dec 19 10:59:26.171: INFO: Got endpoints: latency-svc-w4nwb [333.087316ms]
  Dec 19 10:59:26.172: INFO: Created: latency-svc-gdgqr
  Dec 19 10:59:26.181: INFO: Got endpoints: latency-svc-gdgqr [314.294422ms]
  Dec 19 10:59:26.193: INFO: Created: latency-svc-2b7dc
  Dec 19 10:59:26.199: INFO: Got endpoints: latency-svc-2b7dc [324.2789ms]
  Dec 19 10:59:26.213: INFO: Created: latency-svc-2kwbn
  Dec 19 10:59:26.231: INFO: Got endpoints: latency-svc-2kwbn [330.180967ms]
  Dec 19 10:59:26.237: INFO: Created: latency-svc-pxptm
  Dec 19 10:59:26.247: INFO: Created: latency-svc-wg597
  Dec 19 10:59:26.258: INFO: Got endpoints: latency-svc-pxptm [332.669725ms]
  Dec 19 10:59:26.259: INFO: Got endpoints: latency-svc-wg597 [302.17734ms]
  Dec 19 10:59:26.287: INFO: Created: latency-svc-xvzts
  Dec 19 10:59:26.286: INFO: Created: latency-svc-cb44m
  Dec 19 10:59:26.287: INFO: Got endpoints: latency-svc-cb44m [329.373483ms]
  Dec 19 10:59:26.305: INFO: Created: latency-svc-jtzwf
  Dec 19 10:59:26.307: INFO: Got endpoints: latency-svc-xvzts [319.198648ms]
  Dec 19 10:59:26.318: INFO: Got endpoints: latency-svc-jtzwf [287.907242ms]
  Dec 19 10:59:26.467: INFO: Created: latency-svc-bbww6
  Dec 19 10:59:26.467: INFO: Created: latency-svc-tv9t2
  Dec 19 10:59:26.473: INFO: Created: latency-svc-mf8gx
  Dec 19 10:59:26.488: INFO: Created: latency-svc-jqwsl
  Dec 19 10:59:26.490: INFO: Created: latency-svc-7fbt9
  Dec 19 10:59:26.490: INFO: Created: latency-svc-jx2c2
  Dec 19 10:59:26.503: INFO: Created: latency-svc-m64jx
  Dec 19 10:59:26.503: INFO: Created: latency-svc-thbmw
  Dec 19 10:59:26.504: INFO: Created: latency-svc-qqf4f
  Dec 19 10:59:26.505: INFO: Created: latency-svc-7nh6n
  Dec 19 10:59:26.505: INFO: Created: latency-svc-6968z
  Dec 19 10:59:26.505: INFO: Created: latency-svc-tbplp
  Dec 19 10:59:26.506: INFO: Created: latency-svc-4rphp
  Dec 19 10:59:26.506: INFO: Created: latency-svc-dlq87
  Dec 19 10:59:26.507: INFO: Created: latency-svc-6hbxc
  Dec 19 10:59:26.509: INFO: Got endpoints: latency-svc-tv9t2 [425.255763ms]
  Dec 19 10:59:26.509: INFO: Got endpoints: latency-svc-bbww6 [352.740838ms]
  Dec 19 10:59:26.541: INFO: Got endpoints: latency-svc-mf8gx [253.337981ms]
  Dec 19 10:59:26.541: INFO: Got endpoints: latency-svc-7fbt9 [466.624972ms]
  Dec 19 10:59:26.541: INFO: Got endpoints: latency-svc-4rphp [282.381389ms]
  Dec 19 10:59:26.550: INFO: Got endpoints: latency-svc-m64jx [291.154814ms]
  Dec 19 10:59:26.551: INFO: Got endpoints: latency-svc-6968z [417.753506ms]
  Dec 19 10:59:26.571: INFO: Got endpoints: latency-svc-jqwsl [252.172468ms]
  Dec 19 10:59:26.571: INFO: Created: latency-svc-sc7t2
  Dec 19 10:59:26.573: INFO: Got endpoints: latency-svc-jx2c2 [373.085306ms]
  Dec 19 10:59:26.605: INFO: Got endpoints: latency-svc-dlq87 [507.504327ms]
  Dec 19 10:59:26.662: INFO: Created: latency-svc-czwjn
  Dec 19 10:59:26.662: INFO: Created: latency-svc-rxfsw
  Dec 19 10:59:26.673: INFO: Got endpoints: latency-svc-tbplp [365.585368ms]
  Dec 19 10:59:26.676: INFO: Created: latency-svc-cln8f
  Dec 19 10:59:26.676: INFO: Created: latency-svc-b57gs
  Dec 19 10:59:26.677: INFO: Created: latency-svc-lwwg6
  Dec 19 10:59:26.679: INFO: Created: latency-svc-8qtjq
  Dec 19 10:59:26.683: INFO: Created: latency-svc-b8bxl
  Dec 19 10:59:26.683: INFO: Created: latency-svc-nwtvb
  Dec 19 10:59:26.683: INFO: Created: latency-svc-9rs44
  Dec 19 10:59:26.714: INFO: Created: latency-svc-q2w9c
  Dec 19 10:59:26.715: INFO: Got endpoints: latency-svc-qqf4f [655.509235ms]
  Dec 19 10:59:26.733: INFO: Created: latency-svc-rgjvw
  Dec 19 10:59:26.756: INFO: Got endpoints: latency-svc-6hbxc [524.487427ms]
  Dec 19 10:59:26.776: INFO: Created: latency-svc-4cqhz
  Dec 19 10:59:26.804: INFO: Got endpoints: latency-svc-thbmw [622.739714ms]
  Dec 19 10:59:26.821: INFO: Created: latency-svc-t8xqn
  Dec 19 10:59:26.852: INFO: Got endpoints: latency-svc-7nh6n [680.77776ms]
  Dec 19 10:59:26.872: INFO: Created: latency-svc-vp2df
  Dec 19 10:59:26.903: INFO: Got endpoints: latency-svc-sc7t2 [393.642042ms]
  Dec 19 10:59:26.923: INFO: Created: latency-svc-6twcf
  Dec 19 10:59:26.959: INFO: Got endpoints: latency-svc-rxfsw [416.564419ms]
  Dec 19 10:59:26.978: INFO: Created: latency-svc-jmx6k
  Dec 19 10:59:27.006: INFO: Got endpoints: latency-svc-cln8f [432.805819ms]
  Dec 19 10:59:27.023: INFO: Created: latency-svc-zlqb4
  Dec 19 10:59:27.051: INFO: Got endpoints: latency-svc-b57gs [509.759674ms]
  Dec 19 10:59:27.077: INFO: Created: latency-svc-5t6wb
  Dec 19 10:59:27.105: INFO: Got endpoints: latency-svc-czwjn [553.961572ms]
  Dec 19 10:59:27.129: INFO: Created: latency-svc-fc892
  Dec 19 10:59:27.160: INFO: Got endpoints: latency-svc-lwwg6 [619.019102ms]
  Dec 19 10:59:27.188: INFO: Created: latency-svc-spjfx
  Dec 19 10:59:27.207: INFO: Got endpoints: latency-svc-8qtjq [698.307309ms]
  Dec 19 10:59:27.234: INFO: Created: latency-svc-qkmfl
  Dec 19 10:59:27.266: INFO: Got endpoints: latency-svc-nwtvb [661.327293ms]
  Dec 19 10:59:27.372: INFO: Got endpoints: latency-svc-9rs44 [821.505833ms]
  Dec 19 10:59:27.379: INFO: Got endpoints: latency-svc-b8bxl [808.05074ms]
  Dec 19 10:59:27.411: INFO: Got endpoints: latency-svc-q2w9c [736.983095ms]
  Dec 19 10:59:27.421: INFO: Created: latency-svc-v5tp5
  Dec 19 10:59:27.472: INFO: Got endpoints: latency-svc-rgjvw [757.01927ms]
  Dec 19 10:59:27.516: INFO: Got endpoints: latency-svc-4cqhz [759.882146ms]
  Dec 19 10:59:27.520: INFO: Created: latency-svc-thjcx
  Dec 19 10:59:27.528: INFO: Created: latency-svc-2llw4
  Dec 19 10:59:27.528: INFO: Created: latency-svc-hkw2q
  Dec 19 10:59:27.528: INFO: Created: latency-svc-24r5j
  Dec 19 10:59:27.544: INFO: Created: latency-svc-29zlj
  Dec 19 10:59:27.559: INFO: Got endpoints: latency-svc-t8xqn [755.157207ms]
  Dec 19 10:59:27.593: INFO: Created: latency-svc-5d6xg
  Dec 19 10:59:27.617: INFO: Got endpoints: latency-svc-vp2df [764.951657ms]
  Dec 19 10:59:27.650: INFO: Created: latency-svc-7jk7s
  Dec 19 10:59:27.658: INFO: Got endpoints: latency-svc-6twcf [754.800663ms]
  Dec 19 10:59:27.677: INFO: Created: latency-svc-25vsd
  Dec 19 10:59:27.699: INFO: Got endpoints: latency-svc-jmx6k [739.006558ms]
  Dec 19 10:59:27.725: INFO: Created: latency-svc-pdc5n
  Dec 19 10:59:27.751: INFO: Got endpoints: latency-svc-zlqb4 [745.111783ms]
  Dec 19 10:59:27.771: INFO: Created: latency-svc-7vcvr
  Dec 19 10:59:27.801: INFO: Got endpoints: latency-svc-5t6wb [749.9331ms]
  Dec 19 10:59:27.820: INFO: Created: latency-svc-l7fhp
  Dec 19 10:59:27.857: INFO: Got endpoints: latency-svc-fc892 [750.692541ms]
  Dec 19 10:59:27.875: INFO: Created: latency-svc-nlxm4
  Dec 19 10:59:27.909: INFO: Got endpoints: latency-svc-spjfx [748.002316ms]
  Dec 19 10:59:27.928: INFO: Created: latency-svc-jlj2d
  Dec 19 10:59:27.952: INFO: Got endpoints: latency-svc-qkmfl [743.834206ms]
  Dec 19 10:59:27.973: INFO: Created: latency-svc-tgp88
  Dec 19 10:59:28.013: INFO: Got endpoints: latency-svc-v5tp5 [746.338361ms]
  Dec 19 10:59:28.035: INFO: Created: latency-svc-dfj6w
  Dec 19 10:59:28.052: INFO: Got endpoints: latency-svc-thjcx [680.138603ms]
  Dec 19 10:59:28.084: INFO: Created: latency-svc-tnrtv
  Dec 19 10:59:28.108: INFO: Got endpoints: latency-svc-24r5j [635.77863ms]
  Dec 19 10:59:28.128: INFO: Created: latency-svc-22s4m
  Dec 19 10:59:28.156: INFO: Got endpoints: latency-svc-2llw4 [776.837374ms]
  Dec 19 10:59:28.185: INFO: Created: latency-svc-7js8l
  Dec 19 10:59:28.203: INFO: Got endpoints: latency-svc-hkw2q [790.927303ms]
  Dec 19 10:59:28.248: INFO: Created: latency-svc-xnzh6
  Dec 19 10:59:28.258: INFO: Got endpoints: latency-svc-29zlj [741.816353ms]
  Dec 19 10:59:28.275: INFO: Created: latency-svc-nt48d
  Dec 19 10:59:28.306: INFO: Got endpoints: latency-svc-5d6xg [746.660611ms]
  Dec 19 10:59:28.329: INFO: Created: latency-svc-7bc84
  Dec 19 10:59:28.351: INFO: Got endpoints: latency-svc-7jk7s [732.650883ms]
  Dec 19 10:59:28.373: INFO: Created: latency-svc-ngngl
  Dec 19 10:59:28.404: INFO: Got endpoints: latency-svc-25vsd [746.128065ms]
  Dec 19 10:59:28.422: INFO: Created: latency-svc-mt6r6
  Dec 19 10:59:28.453: INFO: Got endpoints: latency-svc-pdc5n [754.122658ms]
  Dec 19 10:59:28.474: INFO: Created: latency-svc-rpwfz
  Dec 19 10:59:28.504: INFO: Got endpoints: latency-svc-7vcvr [752.698511ms]
  Dec 19 10:59:28.521: INFO: Created: latency-svc-rp7kb
  Dec 19 10:59:28.556: INFO: Got endpoints: latency-svc-l7fhp [754.637943ms]
  Dec 19 10:59:28.573: INFO: Created: latency-svc-zw89q
  Dec 19 10:59:28.604: INFO: Got endpoints: latency-svc-nlxm4 [746.747379ms]
  Dec 19 10:59:28.624: INFO: Created: latency-svc-9txpf
  Dec 19 10:59:28.653: INFO: Got endpoints: latency-svc-jlj2d [743.08033ms]
  Dec 19 10:59:28.672: INFO: Created: latency-svc-4xknp
  Dec 19 10:59:28.701: INFO: Got endpoints: latency-svc-tgp88 [748.476765ms]
  Dec 19 10:59:28.731: INFO: Created: latency-svc-whbkl
  Dec 19 10:59:28.750: INFO: Got endpoints: latency-svc-dfj6w [737.488996ms]
  Dec 19 10:59:28.786: INFO: Created: latency-svc-mk5xr
  Dec 19 10:59:28.803: INFO: Got endpoints: latency-svc-tnrtv [750.745363ms]
  Dec 19 10:59:28.832: INFO: Created: latency-svc-7jhsn
  Dec 19 10:59:28.856: INFO: Got endpoints: latency-svc-22s4m [747.541222ms]
  Dec 19 10:59:28.897: INFO: Created: latency-svc-hf2zr
  Dec 19 10:59:28.915: INFO: Got endpoints: latency-svc-7js8l [759.515526ms]
  Dec 19 10:59:28.947: INFO: Created: latency-svc-75hc6
  Dec 19 10:59:28.964: INFO: Got endpoints: latency-svc-xnzh6 [760.988421ms]
  Dec 19 10:59:29.007: INFO: Created: latency-svc-hc6d6
  Dec 19 10:59:29.018: INFO: Got endpoints: latency-svc-nt48d [759.912436ms]
  Dec 19 10:59:29.042: INFO: Created: latency-svc-t7l56
  Dec 19 10:59:29.056: INFO: Got endpoints: latency-svc-7bc84 [749.515852ms]
  Dec 19 10:59:29.091: INFO: Created: latency-svc-z6x58
  Dec 19 10:59:29.110: INFO: Got endpoints: latency-svc-ngngl [758.907723ms]
  Dec 19 10:59:29.144: INFO: Created: latency-svc-b4v8s
  Dec 19 10:59:29.167: INFO: Got endpoints: latency-svc-mt6r6 [762.452412ms]
  Dec 19 10:59:29.195: INFO: Created: latency-svc-w6rcq
  Dec 19 10:59:29.221: INFO: Got endpoints: latency-svc-rpwfz [767.54052ms]
  Dec 19 10:59:29.267: INFO: Got endpoints: latency-svc-rp7kb [763.142619ms]
  Dec 19 10:59:29.270: INFO: Created: latency-svc-jsq7r
  Dec 19 10:59:29.317: INFO: Got endpoints: latency-svc-zw89q [760.287208ms]
  Dec 19 10:59:29.319: INFO: Created: latency-svc-55k6n
  Dec 19 10:59:29.352: INFO: Created: latency-svc-5xkgr
  Dec 19 10:59:29.359: INFO: Got endpoints: latency-svc-9txpf [755.281301ms]
  Dec 19 10:59:29.386: INFO: Created: latency-svc-486lj
  Dec 19 10:59:29.407: INFO: Got endpoints: latency-svc-4xknp [753.944719ms]
  Dec 19 10:59:29.436: INFO: Created: latency-svc-bw5t2
  Dec 19 10:59:29.451: INFO: Got endpoints: latency-svc-whbkl [749.965697ms]
  Dec 19 10:59:29.480: INFO: Created: latency-svc-ztrsd
  Dec 19 10:59:29.505: INFO: Got endpoints: latency-svc-mk5xr [754.568798ms]
  Dec 19 10:59:29.555: INFO: Got endpoints: latency-svc-7jhsn [751.301212ms]
  Dec 19 10:59:29.576: INFO: Created: latency-svc-2hk98
  Dec 19 10:59:29.592: INFO: Created: latency-svc-kfbq2
  Dec 19 10:59:29.656: INFO: Got endpoints: latency-svc-hf2zr [799.863365ms]
  Dec 19 10:59:29.678: INFO: Created: latency-svc-t5j69
  Dec 19 10:59:29.707: INFO: Got endpoints: latency-svc-75hc6 [791.111837ms]
  Dec 19 10:59:29.734: INFO: Created: latency-svc-xspxz
  Dec 19 10:59:29.753: INFO: Got endpoints: latency-svc-hc6d6 [787.68796ms]
  Dec 19 10:59:29.778: INFO: Created: latency-svc-xnqrt
  Dec 19 10:59:29.800: INFO: Got endpoints: latency-svc-t7l56 [781.599822ms]
  Dec 19 10:59:29.823: INFO: Created: latency-svc-v8vj2
  Dec 19 10:59:29.856: INFO: Got endpoints: latency-svc-z6x58 [799.201824ms]
  Dec 19 10:59:29.880: INFO: Created: latency-svc-vzb6v
  Dec 19 10:59:29.905: INFO: Got endpoints: latency-svc-b4v8s [794.718015ms]
  Dec 19 10:59:29.928: INFO: Created: latency-svc-9g4gg
  Dec 19 10:59:29.957: INFO: Got endpoints: latency-svc-w6rcq [789.465076ms]
  Dec 19 10:59:29.980: INFO: Created: latency-svc-wwzvn
  Dec 19 10:59:30.004: INFO: Got endpoints: latency-svc-jsq7r [782.531982ms]
  Dec 19 10:59:30.025: INFO: Created: latency-svc-hncp2
  Dec 19 10:59:30.054: INFO: Got endpoints: latency-svc-55k6n [786.92358ms]
  Dec 19 10:59:30.079: INFO: Created: latency-svc-7kb4k
  Dec 19 10:59:30.107: INFO: Got endpoints: latency-svc-5xkgr [789.339596ms]
  Dec 19 10:59:30.129: INFO: Created: latency-svc-w5m65
  Dec 19 10:59:30.164: INFO: Got endpoints: latency-svc-486lj [804.25747ms]
  Dec 19 10:59:30.193: INFO: Created: latency-svc-zvb6l
  Dec 19 10:59:30.209: INFO: Got endpoints: latency-svc-bw5t2 [801.683106ms]
  Dec 19 10:59:30.243: INFO: Created: latency-svc-jlwnb
  Dec 19 10:59:30.255: INFO: Got endpoints: latency-svc-ztrsd [803.688432ms]
  Dec 19 10:59:30.288: INFO: Created: latency-svc-bctw5
  Dec 19 10:59:30.304: INFO: Got endpoints: latency-svc-2hk98 [798.14512ms]
  Dec 19 10:59:30.327: INFO: Created: latency-svc-7jdrt
  Dec 19 10:59:30.355: INFO: Got endpoints: latency-svc-kfbq2 [799.662888ms]
  Dec 19 10:59:30.391: INFO: Created: latency-svc-bx459
  Dec 19 10:59:30.403: INFO: Got endpoints: latency-svc-t5j69 [746.523327ms]
  Dec 19 10:59:30.427: INFO: Created: latency-svc-pkpj5
  Dec 19 10:59:30.471: INFO: Got endpoints: latency-svc-xspxz [763.932483ms]
  Dec 19 10:59:30.500: INFO: Created: latency-svc-gf4pr
  Dec 19 10:59:30.512: INFO: Got endpoints: latency-svc-xnqrt [759.568441ms]
  Dec 19 10:59:30.570: INFO: Created: latency-svc-wcl4x
  Dec 19 10:59:30.606: INFO: Got endpoints: latency-svc-v8vj2 [806.307803ms]
  Dec 19 10:59:30.633: INFO: Created: latency-svc-plxcj
  Dec 19 10:59:30.655: INFO: Got endpoints: latency-svc-vzb6v [798.151303ms]
  Dec 19 10:59:30.680: INFO: Created: latency-svc-52xgs
  Dec 19 10:59:30.710: INFO: Got endpoints: latency-svc-9g4gg [804.56382ms]
  Dec 19 10:59:30.738: INFO: Created: latency-svc-xndhs
  Dec 19 10:59:30.775: INFO: Got endpoints: latency-svc-wwzvn [817.615301ms]
  Dec 19 10:59:30.812: INFO: Created: latency-svc-pdjqc
  Dec 19 10:59:30.814: INFO: Got endpoints: latency-svc-hncp2 [809.022377ms]
  Dec 19 10:59:30.840: INFO: Created: latency-svc-md2pr
  Dec 19 10:59:30.856: INFO: Got endpoints: latency-svc-7kb4k [800.986403ms]
  Dec 19 10:59:30.881: INFO: Created: latency-svc-2skfq
  Dec 19 10:59:30.902: INFO: Got endpoints: latency-svc-w5m65 [795.416216ms]
  Dec 19 10:59:30.922: INFO: Created: latency-svc-dtgnb
  Dec 19 10:59:30.958: INFO: Got endpoints: latency-svc-zvb6l [793.511043ms]
  Dec 19 10:59:30.981: INFO: Created: latency-svc-fv5vc
  Dec 19 10:59:31.008: INFO: Got endpoints: latency-svc-jlwnb [799.239525ms]
  Dec 19 10:59:31.033: INFO: Created: latency-svc-5n4nb
  Dec 19 10:59:31.051: INFO: Got endpoints: latency-svc-bctw5 [794.78085ms]
  Dec 19 10:59:31.078: INFO: Created: latency-svc-hbx8w
  Dec 19 10:59:31.104: INFO: Got endpoints: latency-svc-7jdrt [799.481897ms]
  Dec 19 10:59:31.127: INFO: Created: latency-svc-db99l
  Dec 19 10:59:31.155: INFO: Got endpoints: latency-svc-bx459 [799.297405ms]
  Dec 19 10:59:31.179: INFO: Created: latency-svc-rtblw
  Dec 19 10:59:31.200: INFO: Got endpoints: latency-svc-pkpj5 [796.410325ms]
  Dec 19 10:59:31.227: INFO: Created: latency-svc-mg6nv
  Dec 19 10:59:31.263: INFO: Got endpoints: latency-svc-gf4pr [792.342204ms]
  Dec 19 10:59:31.295: INFO: Created: latency-svc-wbbt8
  Dec 19 10:59:31.308: INFO: Got endpoints: latency-svc-wcl4x [795.334046ms]
  Dec 19 10:59:31.331: INFO: Created: latency-svc-72cvr
  Dec 19 10:59:31.360: INFO: Got endpoints: latency-svc-plxcj [753.285127ms]
  Dec 19 10:59:31.399: INFO: Created: latency-svc-2bmzk
  Dec 19 10:59:31.412: INFO: Got endpoints: latency-svc-52xgs [756.344106ms]
  Dec 19 10:59:31.450: INFO: Created: latency-svc-hdw8g
  Dec 19 10:59:31.460: INFO: Got endpoints: latency-svc-xndhs [749.457141ms]
  Dec 19 10:59:31.486: INFO: Created: latency-svc-2mskh
  Dec 19 10:59:31.503: INFO: Got endpoints: latency-svc-pdjqc [727.410867ms]
  Dec 19 10:59:31.522: INFO: Created: latency-svc-jgn6x
  Dec 19 10:59:31.552: INFO: Got endpoints: latency-svc-md2pr [737.247039ms]
  Dec 19 10:59:31.574: INFO: Created: latency-svc-d8k7d
  Dec 19 10:59:31.605: INFO: Got endpoints: latency-svc-2skfq [748.475913ms]
  Dec 19 10:59:31.621: INFO: Created: latency-svc-sdjcq
  Dec 19 10:59:31.655: INFO: Got endpoints: latency-svc-dtgnb [752.429618ms]
  Dec 19 10:59:31.678: INFO: Created: latency-svc-tzqd4
  Dec 19 10:59:31.704: INFO: Got endpoints: latency-svc-fv5vc [745.337089ms]
  Dec 19 10:59:31.723: INFO: Created: latency-svc-c9ww9
  Dec 19 10:59:31.751: INFO: Got endpoints: latency-svc-5n4nb [742.916494ms]
  Dec 19 10:59:31.773: INFO: Created: latency-svc-5f9gj
  Dec 19 10:59:31.806: INFO: Got endpoints: latency-svc-hbx8w [755.202668ms]
  Dec 19 10:59:31.826: INFO: Created: latency-svc-nj9jc
  Dec 19 10:59:31.851: INFO: Got endpoints: latency-svc-db99l [745.794467ms]
  Dec 19 10:59:31.876: INFO: Created: latency-svc-lkmtr
  Dec 19 10:59:31.904: INFO: Got endpoints: latency-svc-rtblw [747.921847ms]
  Dec 19 10:59:31.947: INFO: Created: latency-svc-snz4m
  Dec 19 10:59:31.965: INFO: Got endpoints: latency-svc-mg6nv [764.576956ms]
  Dec 19 10:59:31.985: INFO: Created: latency-svc-4sbdl
  Dec 19 10:59:32.019: INFO: Got endpoints: latency-svc-wbbt8 [755.40492ms]
  Dec 19 10:59:32.049: INFO: Created: latency-svc-hz9fh
  Dec 19 10:59:32.062: INFO: Got endpoints: latency-svc-72cvr [753.551008ms]
  Dec 19 10:59:32.097: INFO: Created: latency-svc-c84zp
  Dec 19 10:59:32.116: INFO: Got endpoints: latency-svc-2bmzk [755.907031ms]
  Dec 19 10:59:32.140: INFO: Created: latency-svc-58lk9
  Dec 19 10:59:32.171: INFO: Got endpoints: latency-svc-hdw8g [759.154429ms]
  Dec 19 10:59:32.218: INFO: Created: latency-svc-kpmpm
  Dec 19 10:59:32.221: INFO: Got endpoints: latency-svc-2mskh [760.979578ms]
  Dec 19 10:59:32.260: INFO: Got endpoints: latency-svc-jgn6x [756.666053ms]
  Dec 19 10:59:32.264: INFO: Created: latency-svc-c5tgj
  Dec 19 10:59:32.287: INFO: Created: latency-svc-q7rr4
  Dec 19 10:59:32.303: INFO: Got endpoints: latency-svc-d8k7d [751.140874ms]
  Dec 19 10:59:32.329: INFO: Created: latency-svc-kjphn
  Dec 19 10:59:32.354: INFO: Got endpoints: latency-svc-sdjcq [748.342739ms]
  Dec 19 10:59:32.384: INFO: Created: latency-svc-kmtzn
  Dec 19 10:59:32.409: INFO: Got endpoints: latency-svc-tzqd4 [753.879229ms]
  Dec 19 10:59:32.437: INFO: Created: latency-svc-v5mdc
  Dec 19 10:59:32.458: INFO: Got endpoints: latency-svc-c9ww9 [753.611351ms]
  Dec 19 10:59:32.492: INFO: Created: latency-svc-x995q
  Dec 19 10:59:32.504: INFO: Got endpoints: latency-svc-5f9gj [752.640713ms]
  Dec 19 10:59:32.529: INFO: Created: latency-svc-p5b5j
  Dec 19 10:59:32.560: INFO: Got endpoints: latency-svc-nj9jc [753.974648ms]
  Dec 19 10:59:32.581: INFO: Created: latency-svc-tbl7p
  Dec 19 10:59:32.609: INFO: Got endpoints: latency-svc-lkmtr [758.258113ms]
  Dec 19 10:59:32.635: INFO: Created: latency-svc-l2796
  Dec 19 10:59:32.657: INFO: Got endpoints: latency-svc-snz4m [752.075012ms]
  Dec 19 10:59:32.684: INFO: Created: latency-svc-zztt8
  Dec 19 10:59:32.709: INFO: Got endpoints: latency-svc-4sbdl [744.249529ms]
  Dec 19 10:59:32.730: INFO: Created: latency-svc-mfzhd
  Dec 19 10:59:32.754: INFO: Got endpoints: latency-svc-hz9fh [734.297371ms]
  Dec 19 10:59:32.777: INFO: Created: latency-svc-7qlrf
  Dec 19 10:59:32.804: INFO: Got endpoints: latency-svc-c84zp [741.946079ms]
  Dec 19 10:59:32.832: INFO: Created: latency-svc-tsdtm
  Dec 19 10:59:32.859: INFO: Got endpoints: latency-svc-58lk9 [741.340572ms]
  Dec 19 10:59:32.886: INFO: Created: latency-svc-btztq
  Dec 19 10:59:32.907: INFO: Got endpoints: latency-svc-kpmpm [735.239833ms]
  Dec 19 10:59:32.934: INFO: Created: latency-svc-tkpdp
  Dec 19 10:59:32.951: INFO: Got endpoints: latency-svc-c5tgj [729.891957ms]
  Dec 19 10:59:32.982: INFO: Created: latency-svc-xrxn4
  Dec 19 10:59:33.004: INFO: Got endpoints: latency-svc-q7rr4 [743.488088ms]
  Dec 19 10:59:33.051: INFO: Got endpoints: latency-svc-kjphn [746.812708ms]
  Dec 19 10:59:33.103: INFO: Got endpoints: latency-svc-kmtzn [749.062358ms]
  Dec 19 10:59:33.156: INFO: Got endpoints: latency-svc-v5mdc [746.664334ms]
  Dec 19 10:59:33.214: INFO: Got endpoints: latency-svc-x995q [756.100451ms]
  Dec 19 10:59:33.252: INFO: Got endpoints: latency-svc-p5b5j [748.168784ms]
  Dec 19 10:59:33.305: INFO: Got endpoints: latency-svc-tbl7p [744.083787ms]
  Dec 19 10:59:33.354: INFO: Got endpoints: latency-svc-l2796 [744.53127ms]
  Dec 19 10:59:33.402: INFO: Got endpoints: latency-svc-zztt8 [745.369151ms]
  Dec 19 10:59:33.454: INFO: Got endpoints: latency-svc-mfzhd [743.936404ms]
  Dec 19 10:59:33.541: INFO: Got endpoints: latency-svc-7qlrf [786.764188ms]
  Dec 19 10:59:33.558: INFO: Got endpoints: latency-svc-tsdtm [753.903448ms]
  Dec 19 10:59:33.605: INFO: Got endpoints: latency-svc-btztq [746.254018ms]
  Dec 19 10:59:33.651: INFO: Got endpoints: latency-svc-tkpdp [743.596499ms]
  Dec 19 10:59:33.702: INFO: Got endpoints: latency-svc-xrxn4 [749.43085ms]
  Dec 19 10:59:33.702: INFO: Latencies: [47.946182ms 62.261789ms 75.987036ms 102.843585ms 140.719961ms 146.396593ms 150.068591ms 159.30623ms 165.118398ms 175.713597ms 184.659733ms 194.700249ms 196.343591ms 208.634651ms 209.816164ms 210.194454ms 225.90281ms 247.332537ms 252.172468ms 253.337981ms 259.920521ms 266.502716ms 268.56644ms 272.757356ms 276.292541ms 282.381389ms 286.143132ms 287.907242ms 291.154814ms 295.40624ms 302.17734ms 314.294422ms 317.71829ms 319.198648ms 324.2789ms 329.373483ms 330.180967ms 332.669725ms 333.087316ms 352.740838ms 365.585368ms 373.085306ms 393.642042ms 408.626798ms 416.564419ms 417.753506ms 425.255763ms 432.805819ms 460.668221ms 466.624972ms 488.802616ms 490.04407ms 502.37743ms 507.504327ms 509.759674ms 524.487427ms 553.961572ms 554.774429ms 562.437624ms 600.526743ms 610.71629ms 619.019102ms 622.739714ms 635.77863ms 653.487028ms 653.968071ms 655.509235ms 661.327293ms 680.138603ms 680.77776ms 685.1308ms 698.307309ms 701.12267ms 727.410867ms 729.891957ms 730.774881ms 732.650883ms 734.297371ms 735.239833ms 736.983095ms 737.247039ms 737.488996ms 739.006558ms 741.340572ms 741.816353ms 741.946079ms 742.916494ms 743.08033ms 743.488088ms 743.596499ms 743.834206ms 743.936404ms 744.083787ms 744.249529ms 744.53127ms 745.111783ms 745.337089ms 745.369151ms 745.794467ms 746.128065ms 746.254018ms 746.338361ms 746.523327ms 746.660611ms 746.664334ms 746.747379ms 746.812708ms 747.541222ms 747.921847ms 748.002316ms 748.168784ms 748.342739ms 748.475913ms 748.476765ms 749.062358ms 749.43085ms 749.457141ms 749.515852ms 749.9331ms 749.965697ms 750.692541ms 750.745363ms 751.140874ms 751.301212ms 752.075012ms 752.429618ms 752.640713ms 752.698511ms 753.285127ms 753.551008ms 753.611351ms 753.879229ms 753.903448ms 753.944719ms 753.974648ms 754.122658ms 754.568798ms 754.637943ms 754.800663ms 755.157207ms 755.202668ms 755.281301ms 755.40492ms 755.907031ms 756.100451ms 756.344106ms 756.666053ms 757.01927ms 758.258113ms 758.907723ms 759.154429ms 759.515526ms 759.568441ms 759.882146ms 759.912436ms 760.287208ms 760.979578ms 760.988421ms 762.452412ms 763.142619ms 763.932483ms 764.576956ms 764.951657ms 767.54052ms 773.356132ms 776.837374ms 781.599822ms 782.531982ms 786.764188ms 786.92358ms 787.68796ms 789.339596ms 789.465076ms 790.927303ms 791.111837ms 792.342204ms 793.511043ms 794.718015ms 794.78085ms 795.334046ms 795.416216ms 796.410325ms 798.14512ms 798.151303ms 799.201824ms 799.239525ms 799.297405ms 799.481897ms 799.662888ms 799.863365ms 800.986403ms 801.683106ms 803.688432ms 804.25747ms 804.56382ms 806.307803ms 808.05074ms 809.022377ms 817.615301ms 821.505833ms]
  Dec 19 10:59:33.703: INFO: 50 %ile: 746.254018ms
  Dec 19 10:59:33.703: INFO: 90 %ile: 795.416216ms
  Dec 19 10:59:33.703: INFO: 99 %ile: 817.615301ms
  Dec 19 10:59:33.703: INFO: Total sample count: 200
  Dec 19 10:59:33.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-1628" for this suite. @ 12/19/23 10:59:33.718
• [9.927 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 12/19/23 10:59:33.734
  Dec 19 10:59:33.735: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename sysctl @ 12/19/23 10:59:33.737
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:59:33.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:59:33.793
  STEP: Creating a pod with one valid and two invalid sysctls @ 12/19/23 10:59:33.804
  Dec 19 10:59:33.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-4315" for this suite. @ 12/19/23 10:59:33.828
• [0.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 12/19/23 10:59:33.846
  Dec 19 10:59:33.846: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubelet-test @ 12/19/23 10:59:33.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:59:33.878
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:59:33.883
  Dec 19 10:59:35.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4575" for this suite. @ 12/19/23 10:59:35.975
• [2.142 seconds]
------------------------------
S
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 12/19/23 10:59:35.989
  Dec 19 10:59:35.989: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename dns @ 12/19/23 10:59:35.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:59:36.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:59:36.023
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 12/19/23 10:59:36.031
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 12/19/23 10:59:36.032
  STEP: creating a pod to probe DNS @ 12/19/23 10:59:36.032
  STEP: submitting the pod to kubernetes @ 12/19/23 10:59:36.032
  STEP: retrieving the pod @ 12/19/23 10:59:38.074
  STEP: looking for the results for each expected name from probers @ 12/19/23 10:59:38.083
  Dec 19 10:59:38.116: INFO: DNS probes using dns-5774/dns-test-ab9ff283-cdc4-4bc4-8d83-5c8dd0e2f126 succeeded

  Dec 19 10:59:38.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/19/23 10:59:38.126
  STEP: Destroying namespace "dns-5774" for this suite. @ 12/19/23 10:59:38.157
• [2.186 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 12/19/23 10:59:38.197
  Dec 19 10:59:38.197: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename podtemplate @ 12/19/23 10:59:38.2
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:59:38.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:59:38.242
  STEP: Create a pod template @ 12/19/23 10:59:38.247
  STEP: Replace a pod template @ 12/19/23 10:59:38.255
  Dec 19 10:59:38.273: INFO: Found updated podtemplate annotation: "true"

  Dec 19 10:59:38.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-8682" for this suite. @ 12/19/23 10:59:38.284
• [0.102 seconds]
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 12/19/23 10:59:38.299
  Dec 19 10:59:38.299: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 12/19/23 10:59:38.301
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:59:38.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:59:38.348
  STEP: create the container to handle the HTTPGet hook request. @ 12/19/23 10:59:38.367
  STEP: create the pod with lifecycle hook @ 12/19/23 10:59:40.41
  STEP: delete the pod with lifecycle hook @ 12/19/23 10:59:42.482
  STEP: check prestop hook @ 12/19/23 10:59:44.565
  Dec 19 10:59:44.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-4939" for this suite. @ 12/19/23 10:59:44.664
• [6.396 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 12/19/23 10:59:44.701
  Dec 19 10:59:44.701: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename deployment @ 12/19/23 10:59:44.703
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:59:44.753
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:59:44.761
  STEP: creating a Deployment @ 12/19/23 10:59:44.796
  Dec 19 10:59:44.796: INFO: Creating simple deployment test-deployment-crl4w
  Dec 19 10:59:44.847: INFO: deployment "test-deployment-crl4w" doesn't have the required revision set
  STEP: Getting /status @ 12/19/23 10:59:46.899
  Dec 19 10:59:46.918: INFO: Deployment test-deployment-crl4w has Conditions: [{Available True 2023-12-19 10:59:45 +0000 UTC 2023-12-19 10:59:45 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-12-19 10:59:45 +0000 UTC 2023-12-19 10:59:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-crl4w-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 12/19/23 10:59:46.918
  Dec 19 10:59:46.981: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 10, 59, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 59, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 10, 59, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 10, 59, 44, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-crl4w-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 12/19/23 10:59:46.982
  Dec 19 10:59:46.991: INFO: Observed &Deployment event: ADDED
  Dec 19 10:59:46.991: INFO: Observed Deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-19 10:59:44 +0000 UTC 2023-12-19 10:59:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-crl4w-5994cf9475"}
  Dec 19 10:59:46.991: INFO: Observed &Deployment event: MODIFIED
  Dec 19 10:59:46.992: INFO: Observed Deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-19 10:59:44 +0000 UTC 2023-12-19 10:59:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-crl4w-5994cf9475"}
  Dec 19 10:59:46.992: INFO: Observed Deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-12-19 10:59:44 +0000 UTC 2023-12-19 10:59:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Dec 19 10:59:46.993: INFO: Observed &Deployment event: MODIFIED
  Dec 19 10:59:46.993: INFO: Observed Deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-12-19 10:59:44 +0000 UTC 2023-12-19 10:59:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Dec 19 10:59:46.993: INFO: Observed Deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-19 10:59:45 +0000 UTC 2023-12-19 10:59:44 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-crl4w-5994cf9475" is progressing.}
  Dec 19 10:59:46.994: INFO: Observed &Deployment event: MODIFIED
  Dec 19 10:59:46.994: INFO: Observed Deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-12-19 10:59:45 +0000 UTC 2023-12-19 10:59:45 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Dec 19 10:59:46.995: INFO: Observed Deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-19 10:59:45 +0000 UTC 2023-12-19 10:59:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-crl4w-5994cf9475" has successfully progressed.}
  Dec 19 10:59:46.995: INFO: Observed &Deployment event: MODIFIED
  Dec 19 10:59:46.996: INFO: Observed Deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-12-19 10:59:45 +0000 UTC 2023-12-19 10:59:45 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Dec 19 10:59:46.996: INFO: Observed Deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-19 10:59:45 +0000 UTC 2023-12-19 10:59:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-crl4w-5994cf9475" has successfully progressed.}
  Dec 19 10:59:46.996: INFO: Found Deployment test-deployment-crl4w in namespace deployment-9375 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Dec 19 10:59:46.997: INFO: Deployment test-deployment-crl4w has an updated status
  STEP: patching the Statefulset Status @ 12/19/23 10:59:46.998
  Dec 19 10:59:46.998: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Dec 19 10:59:47.014: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 12/19/23 10:59:47.015
  Dec 19 10:59:47.021: INFO: Observed &Deployment event: ADDED
  Dec 19 10:59:47.021: INFO: Observed deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-19 10:59:44 +0000 UTC 2023-12-19 10:59:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-crl4w-5994cf9475"}
  Dec 19 10:59:47.022: INFO: Observed &Deployment event: MODIFIED
  Dec 19 10:59:47.023: INFO: Observed deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-19 10:59:44 +0000 UTC 2023-12-19 10:59:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-crl4w-5994cf9475"}
  Dec 19 10:59:47.023: INFO: Observed deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-12-19 10:59:44 +0000 UTC 2023-12-19 10:59:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Dec 19 10:59:47.024: INFO: Observed &Deployment event: MODIFIED
  Dec 19 10:59:47.024: INFO: Observed deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-12-19 10:59:44 +0000 UTC 2023-12-19 10:59:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Dec 19 10:59:47.025: INFO: Observed deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-19 10:59:45 +0000 UTC 2023-12-19 10:59:44 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-crl4w-5994cf9475" is progressing.}
  Dec 19 10:59:47.026: INFO: Observed &Deployment event: MODIFIED
  Dec 19 10:59:47.026: INFO: Observed deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-12-19 10:59:45 +0000 UTC 2023-12-19 10:59:45 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Dec 19 10:59:47.027: INFO: Observed deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-19 10:59:45 +0000 UTC 2023-12-19 10:59:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-crl4w-5994cf9475" has successfully progressed.}
  Dec 19 10:59:47.027: INFO: Observed &Deployment event: MODIFIED
  Dec 19 10:59:47.028: INFO: Observed deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-12-19 10:59:45 +0000 UTC 2023-12-19 10:59:45 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Dec 19 10:59:47.028: INFO: Observed deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-12-19 10:59:45 +0000 UTC 2023-12-19 10:59:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-crl4w-5994cf9475" has successfully progressed.}
  Dec 19 10:59:47.029: INFO: Observed deployment test-deployment-crl4w in namespace deployment-9375 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Dec 19 10:59:47.030: INFO: Observed &Deployment event: MODIFIED
  Dec 19 10:59:47.030: INFO: Found deployment test-deployment-crl4w in namespace deployment-9375 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Dec 19 10:59:47.030: INFO: Deployment test-deployment-crl4w has a patched status
  Dec 19 10:59:47.058: INFO: Deployment "test-deployment-crl4w":
  &Deployment{ObjectMeta:{test-deployment-crl4w  deployment-9375  aad92584-9a92-4a96-98ed-0a3a226f4e1f 21808 1 2023-12-19 10:59:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-12-19 10:59:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-12-19 10:59:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-12-19 10:59:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00578f408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-crl4w-5994cf9475",LastUpdateTime:2023-12-19 10:59:47 +0000 UTC,LastTransitionTime:2023-12-19 10:59:47 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Dec 19 10:59:47.087: INFO: New ReplicaSet "test-deployment-crl4w-5994cf9475" of Deployment "test-deployment-crl4w":
  &ReplicaSet{ObjectMeta:{test-deployment-crl4w-5994cf9475  deployment-9375  1b06ac49-b4fc-41a3-98cb-e4d598653700 21690 1 2023-12-19 10:59:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-crl4w aad92584-9a92-4a96-98ed-0a3a226f4e1f 0xc00578f7f0 0xc00578f7f1}] [] [{kube-controller-manager Update apps/v1 2023-12-19 10:59:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aad92584-9a92-4a96-98ed-0a3a226f4e1f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 10:59:45 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00578f898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Dec 19 10:59:47.120: INFO: Pod "test-deployment-crl4w-5994cf9475-25jxf" is available:
  &Pod{ObjectMeta:{test-deployment-crl4w-5994cf9475-25jxf test-deployment-crl4w-5994cf9475- deployment-9375  5a6bbf8b-e473-425e-8f88-9dccc7594d88 21689 0 2023-12-19 10:59:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-crl4w-5994cf9475 1b06ac49-b4fc-41a3-98cb-e4d598653700 0xc00578fc50 0xc00578fc51}] [] [{kube-controller-manager Update v1 2023-12-19 10:59:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b06ac49-b4fc-41a3-98cb-e4d598653700\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 10:59:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.234\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zcvvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zcvvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 10:59:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 10:59:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 10:59:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 10:59:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.61,PodIP:10.233.66.234,StartTime:2023-12-19 10:59:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-19 10:59:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://93075951865fd88518aab878bd74ea0fe04e836389e25ab7270e8568721e5533,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.234,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 10:59:47.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9375" for this suite. @ 12/19/23 10:59:47.153
• [2.477 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 12/19/23 10:59:47.181
  Dec 19 10:59:47.182: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-probe @ 12/19/23 10:59:47.183
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 10:59:47.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 10:59:47.263
  STEP: Creating pod busybox-7e69a511-5b0a-4351-aabd-812a6e570437 in namespace container-probe-4924 @ 12/19/23 10:59:47.273
  Dec 19 10:59:49.350: INFO: Started pod busybox-7e69a511-5b0a-4351-aabd-812a6e570437 in namespace container-probe-4924
  STEP: checking the pod's current state and verifying that restartCount is present @ 12/19/23 10:59:49.35
  Dec 19 10:59:49.361: INFO: Initial restart count of pod busybox-7e69a511-5b0a-4351-aabd-812a6e570437 is 0
  Dec 19 11:00:39.633: INFO: Restart count of pod container-probe-4924/busybox-7e69a511-5b0a-4351-aabd-812a6e570437 is now 1 (50.271441614s elapsed)
  Dec 19 11:00:39.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/19/23 11:00:39.648
  STEP: Destroying namespace "container-probe-4924" for this suite. @ 12/19/23 11:00:39.673
• [52.504 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 12/19/23 11:00:39.691
  Dec 19 11:00:39.691: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename resourcequota @ 12/19/23 11:00:39.695
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:00:39.723
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:00:39.733
  STEP: Counting existing ResourceQuota @ 12/19/23 11:00:39.739
  STEP: Creating a ResourceQuota @ 12/19/23 11:00:44.749
  STEP: Ensuring resource quota status is calculated @ 12/19/23 11:00:44.765
  STEP: Creating a ReplicaSet @ 12/19/23 11:00:46.778
  STEP: Ensuring resource quota status captures replicaset creation @ 12/19/23 11:00:46.815
  STEP: Deleting a ReplicaSet @ 12/19/23 11:00:48.826
  STEP: Ensuring resource quota status released usage @ 12/19/23 11:00:48.844
  Dec 19 11:00:50.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6532" for this suite. @ 12/19/23 11:00:50.863
• [11.186 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 12/19/23 11:00:50.886
  Dec 19 11:00:50.886: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubectl @ 12/19/23 11:00:50.889
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:00:50.947
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:00:50.958
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 12/19/23 11:00:50.964
  Dec 19 11:00:50.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-8719 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Dec 19 11:00:51.205: INFO: stderr: ""
  Dec 19 11:00:51.205: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 12/19/23 11:00:51.205
  STEP: verifying the pod e2e-test-httpd-pod was created @ 12/19/23 11:00:56.258
  Dec 19 11:00:56.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-8719 get pod e2e-test-httpd-pod -o json'
  Dec 19 11:00:56.447: INFO: stderr: ""
  Dec 19 11:00:56.448: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-12-19T11:00:51Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8719\",\n        \"resourceVersion\": \"22368\",\n        \"uid\": \"63e72df2-e2a4-4610-ae4d-4e4d1ea9d0f5\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-m8wbm\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cahyeife7pae-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-m8wbm\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-12-19T11:00:51Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-12-19T11:00:51Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-12-19T11:00:51Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-12-19T11:00:51Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://f544810cba2c0bd5ef03aecf492e7dc4de6b1bc4477d921795175135d10161f4\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-12-19T11:00:51Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.61\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.66.236\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.66.236\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-12-19T11:00:51Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 12/19/23 11:00:56.448
  Dec 19 11:00:56.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-8719 replace -f -'
  Dec 19 11:00:57.376: INFO: stderr: ""
  Dec 19 11:00:57.376: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 12/19/23 11:00:57.376
  Dec 19 11:00:57.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-8719 delete pods e2e-test-httpd-pod'
  Dec 19 11:00:59.092: INFO: stderr: ""
  Dec 19 11:00:59.092: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Dec 19 11:00:59.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8719" for this suite. @ 12/19/23 11:00:59.105
• [8.238 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 12/19/23 11:00:59.138
  Dec 19 11:00:59.139: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename gc @ 12/19/23 11:00:59.141
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:00:59.179
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:00:59.184
  STEP: create the rc1 @ 12/19/23 11:00:59.204
  STEP: create the rc2 @ 12/19/23 11:00:59.216
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 12/19/23 11:01:05.268
  STEP: delete the rc simpletest-rc-to-be-deleted @ 12/19/23 11:01:09.483
  STEP: wait for the rc to be deleted @ 12/19/23 11:01:09.676
  Dec 19 11:01:14.750: INFO: 73 pods remaining
  Dec 19 11:01:14.751: INFO: 73 pods has nil DeletionTimestamp
  Dec 19 11:01:14.752: INFO: 
  STEP: Gathering metrics @ 12/19/23 11:01:19.714
  Dec 19 11:01:19.949: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Dec 19 11:01:19.949: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zxdq" in namespace "gc-4481"
  Dec 19 11:01:20.041: INFO: Deleting pod "simpletest-rc-to-be-deleted-45522" in namespace "gc-4481"
  Dec 19 11:01:20.095: INFO: Deleting pod "simpletest-rc-to-be-deleted-4b4k9" in namespace "gc-4481"
  Dec 19 11:01:20.217: INFO: Deleting pod "simpletest-rc-to-be-deleted-4bqx2" in namespace "gc-4481"
  Dec 19 11:01:20.282: INFO: Deleting pod "simpletest-rc-to-be-deleted-4gccr" in namespace "gc-4481"
  Dec 19 11:01:20.309: INFO: Deleting pod "simpletest-rc-to-be-deleted-4nm8x" in namespace "gc-4481"
  Dec 19 11:01:20.366: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dx2q" in namespace "gc-4481"
  Dec 19 11:01:20.479: INFO: Deleting pod "simpletest-rc-to-be-deleted-5gjfh" in namespace "gc-4481"
  Dec 19 11:01:20.532: INFO: Deleting pod "simpletest-rc-to-be-deleted-5sclj" in namespace "gc-4481"
  Dec 19 11:01:20.597: INFO: Deleting pod "simpletest-rc-to-be-deleted-666rz" in namespace "gc-4481"
  Dec 19 11:01:20.744: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fmgd" in namespace "gc-4481"
  Dec 19 11:01:20.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-6xggf" in namespace "gc-4481"
  Dec 19 11:01:20.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-6z7tz" in namespace "gc-4481"
  Dec 19 11:01:20.923: INFO: Deleting pod "simpletest-rc-to-be-deleted-75z46" in namespace "gc-4481"
  Dec 19 11:01:20.972: INFO: Deleting pod "simpletest-rc-to-be-deleted-78m2h" in namespace "gc-4481"
  Dec 19 11:01:21.059: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bzqn" in namespace "gc-4481"
  Dec 19 11:01:21.169: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pgn6" in namespace "gc-4481"
  Dec 19 11:01:21.295: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rfrc" in namespace "gc-4481"
  Dec 19 11:01:21.336: INFO: Deleting pod "simpletest-rc-to-be-deleted-7skhl" in namespace "gc-4481"
  Dec 19 11:01:21.399: INFO: Deleting pod "simpletest-rc-to-be-deleted-8r5rq" in namespace "gc-4481"
  Dec 19 11:01:21.461: INFO: Deleting pod "simpletest-rc-to-be-deleted-8x79g" in namespace "gc-4481"
  Dec 19 11:01:21.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-94xth" in namespace "gc-4481"
  Dec 19 11:01:21.828: INFO: Deleting pod "simpletest-rc-to-be-deleted-9hvrd" in namespace "gc-4481"
  Dec 19 11:01:21.873: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jk8c" in namespace "gc-4481"
  Dec 19 11:01:21.950: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jkhl" in namespace "gc-4481"
  Dec 19 11:01:22.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-9k642" in namespace "gc-4481"
  Dec 19 11:01:22.077: INFO: Deleting pod "simpletest-rc-to-be-deleted-9x8rj" in namespace "gc-4481"
  Dec 19 11:01:22.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4mx7" in namespace "gc-4481"
  Dec 19 11:01:22.155: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5gmw" in namespace "gc-4481"
  Dec 19 11:01:22.201: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjsws" in namespace "gc-4481"
  Dec 19 11:01:22.269: INFO: Deleting pod "simpletest-rc-to-be-deleted-csgss" in namespace "gc-4481"
  Dec 19 11:01:22.399: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6qns" in namespace "gc-4481"
  Dec 19 11:01:22.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmbzh" in namespace "gc-4481"
  Dec 19 11:01:22.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-f89k7" in namespace "gc-4481"
  Dec 19 11:01:22.734: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkln6" in namespace "gc-4481"
  Dec 19 11:01:22.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmlkn" in namespace "gc-4481"
  Dec 19 11:01:23.001: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmpgs" in namespace "gc-4481"
  Dec 19 11:01:23.047: INFO: Deleting pod "simpletest-rc-to-be-deleted-g4k8l" in namespace "gc-4481"
  Dec 19 11:01:23.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfcmd" in namespace "gc-4481"
  Dec 19 11:01:23.219: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsnzb" in namespace "gc-4481"
  Dec 19 11:01:23.258: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtpw4" in namespace "gc-4481"
  Dec 19 11:01:23.369: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2df4" in namespace "gc-4481"
  Dec 19 11:01:23.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-h44lq" in namespace "gc-4481"
  Dec 19 11:01:23.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-htvjn" in namespace "gc-4481"
  Dec 19 11:01:23.637: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6qh8" in namespace "gc-4481"
  Dec 19 11:01:23.687: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7znn" in namespace "gc-4481"
  Dec 19 11:01:23.756: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9h5p" in namespace "gc-4481"
  Dec 19 11:01:23.800: INFO: Deleting pod "simpletest-rc-to-be-deleted-jz2rt" in namespace "gc-4481"
  Dec 19 11:01:23.913: INFO: Deleting pod "simpletest-rc-to-be-deleted-jzj68" in namespace "gc-4481"
  Dec 19 11:01:24.046: INFO: Deleting pod "simpletest-rc-to-be-deleted-k4kth" in namespace "gc-4481"
  Dec 19 11:01:24.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4481" for this suite. @ 12/19/23 11:01:24.197
• [25.132 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 12/19/23 11:01:24.271
  Dec 19 11:01:24.272: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename resourcequota @ 12/19/23 11:01:24.276
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:01:24.365
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:01:24.374
  STEP: Creating a ResourceQuota @ 12/19/23 11:01:24.396
  STEP: Getting a ResourceQuota @ 12/19/23 11:01:24.419
  STEP: Listing all ResourceQuotas with LabelSelector @ 12/19/23 11:01:24.441
  STEP: Patching the ResourceQuota @ 12/19/23 11:01:24.483
  STEP: Deleting a Collection of ResourceQuotas @ 12/19/23 11:01:24.515
  STEP: Verifying the deleted ResourceQuota @ 12/19/23 11:01:24.571
  Dec 19 11:01:24.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9351" for this suite. @ 12/19/23 11:01:24.588
• [0.343 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 12/19/23 11:01:24.617
  Dec 19 11:01:24.617: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename svcaccounts @ 12/19/23 11:01:24.619
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:01:24.696
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:01:24.702
  STEP: Creating a pod to test service account token:  @ 12/19/23 11:01:24.709
  STEP: Saw pod success @ 12/19/23 11:01:28.835
  Dec 19 11:01:28.845: INFO: Trying to get logs from node cahyeife7pae-3 pod test-pod-02246afd-aa24-442a-90b1-75fbf1c63d3c container agnhost-container: <nil>
  STEP: delete the pod @ 12/19/23 11:01:28.901
  Dec 19 11:01:28.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-708" for this suite. @ 12/19/23 11:01:29.005
• [4.415 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 12/19/23 11:01:29.04
  Dec 19 11:01:29.040: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename csistoragecapacity @ 12/19/23 11:01:29.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:01:29.086
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:01:29.095
  STEP: getting /apis @ 12/19/23 11:01:29.105
  STEP: getting /apis/storage.k8s.io @ 12/19/23 11:01:29.117
  STEP: getting /apis/storage.k8s.io/v1 @ 12/19/23 11:01:29.122
  STEP: creating @ 12/19/23 11:01:29.127
  STEP: watching @ 12/19/23 11:01:29.213
  Dec 19 11:01:29.213: INFO: starting watch
  STEP: getting @ 12/19/23 11:01:29.244
  STEP: listing in namespace @ 12/19/23 11:01:29.255
  STEP: listing across namespaces @ 12/19/23 11:01:29.278
  STEP: patching @ 12/19/23 11:01:29.304
  STEP: updating @ 12/19/23 11:01:29.335
  Dec 19 11:01:29.370: INFO: waiting for watch events with expected annotations in namespace
  Dec 19 11:01:29.371: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 12/19/23 11:01:29.371
  STEP: deleting a collection @ 12/19/23 11:01:29.601
  Dec 19 11:01:29.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-131" for this suite. @ 12/19/23 11:01:29.917
• [0.911 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 12/19/23 11:01:29.969
  Dec 19 11:01:29.969: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 11:01:29.971
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:01:30.049
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:01:30.057
  STEP: Creating a pod to test downward API volume plugin @ 12/19/23 11:01:30.068
  STEP: Saw pod success @ 12/19/23 11:01:34.17
  Dec 19 11:01:34.189: INFO: Trying to get logs from node cahyeife7pae-3 pod downwardapi-volume-461ba8bc-d3b3-4e88-9650-8f3858147ea7 container client-container: <nil>
  STEP: delete the pod @ 12/19/23 11:01:34.223
  Dec 19 11:01:34.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5923" for this suite. @ 12/19/23 11:01:34.299
• [4.346 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 12/19/23 11:01:34.326
  Dec 19 11:01:34.326: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename pod-network-test @ 12/19/23 11:01:34.332
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:01:34.393
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:01:34.399
  STEP: Performing setup for networking test in namespace pod-network-test-6585 @ 12/19/23 11:01:34.405
  STEP: creating a selector @ 12/19/23 11:01:34.405
  STEP: Creating the service pods in kubernetes @ 12/19/23 11:01:34.405
  Dec 19 11:01:34.405: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 12/19/23 11:01:56.705
  Dec 19 11:01:58.800: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Dec 19 11:01:58.800: INFO: Going to poll 10.233.64.120 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Dec 19 11:01:58.807: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.120:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6585 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:01:58.807: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:01:58.808: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:01:58.808: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6585/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.120%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Dec 19 11:01:58.991: INFO: Found all 1 expected endpoints: [netserver-0]
  Dec 19 11:01:58.991: INFO: Going to poll 10.233.65.129 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Dec 19 11:01:59.000: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.129:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6585 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:01:59.000: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:01:59.003: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:01:59.003: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6585/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.65.129%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Dec 19 11:01:59.136: INFO: Found all 1 expected endpoints: [netserver-1]
  Dec 19 11:01:59.136: INFO: Going to poll 10.233.66.24 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Dec 19 11:01:59.144: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.24:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6585 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:01:59.145: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:01:59.146: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:01:59.147: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6585/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.24%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Dec 19 11:01:59.259: INFO: Found all 1 expected endpoints: [netserver-2]
  Dec 19 11:01:59.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-6585" for this suite. @ 12/19/23 11:01:59.269
• [24.957 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 12/19/23 11:01:59.287
  Dec 19 11:01:59.287: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename dns @ 12/19/23 11:01:59.29
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:01:59.311
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:01:59.32
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2739.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2739.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 12/19/23 11:01:59.326
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2739.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2739.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 12/19/23 11:01:59.326
  STEP: creating a pod to probe /etc/hosts @ 12/19/23 11:01:59.326
  STEP: submitting the pod to kubernetes @ 12/19/23 11:01:59.327
  STEP: retrieving the pod @ 12/19/23 11:02:01.395
  STEP: looking for the results for each expected name from probers @ 12/19/23 11:02:01.403
  Dec 19 11:02:01.446: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-2739/dns-test-81b7f4e0-972d-4ca2-8731-93e5ef74eb36: the server could not find the requested resource (get pods dns-test-81b7f4e0-972d-4ca2-8731-93e5ef74eb36)
  Dec 19 11:02:01.446: INFO: Lookups using dns-2739/dns-test-81b7f4e0-972d-4ca2-8731-93e5ef74eb36 failed for: [jessie_hosts@dns-querier-1]

  Dec 19 11:02:06.477: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-2739/dns-test-81b7f4e0-972d-4ca2-8731-93e5ef74eb36: the server could not find the requested resource (get pods dns-test-81b7f4e0-972d-4ca2-8731-93e5ef74eb36)
  Dec 19 11:02:06.477: INFO: Lookups using dns-2739/dns-test-81b7f4e0-972d-4ca2-8731-93e5ef74eb36 failed for: [jessie_hosts@dns-querier-1]

  Dec 19 11:02:11.498: INFO: DNS probes using dns-2739/dns-test-81b7f4e0-972d-4ca2-8731-93e5ef74eb36 succeeded

  Dec 19 11:02:11.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/19/23 11:02:11.517
  STEP: Destroying namespace "dns-2739" for this suite. @ 12/19/23 11:02:11.565
• [12.308 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 12/19/23 11:02:11.596
  Dec 19 11:02:11.596: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename var-expansion @ 12/19/23 11:02:11.597
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:02:11.643
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:02:11.654
  STEP: Creating a pod to test substitution in container's command @ 12/19/23 11:02:11.661
  STEP: Saw pod success @ 12/19/23 11:02:15.71
  Dec 19 11:02:15.719: INFO: Trying to get logs from node cahyeife7pae-3 pod var-expansion-5e85239c-2b35-4165-ade4-a2fc5a467b4c container dapi-container: <nil>
  STEP: delete the pod @ 12/19/23 11:02:15.735
  Dec 19 11:02:15.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5517" for this suite. @ 12/19/23 11:02:15.778
• [4.198 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 12/19/23 11:02:15.808
  Dec 19 11:02:15.808: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename disruption @ 12/19/23 11:02:15.81
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:02:15.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:02:15.85
  STEP: Waiting for the pdb to be processed @ 12/19/23 11:02:15.869
  STEP: Waiting for all pods to be running @ 12/19/23 11:02:17.95
  Dec 19 11:02:17.963: INFO: running pods: 0 < 3
  Dec 19 11:02:19.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2443" for this suite. @ 12/19/23 11:02:20.003
• [4.214 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 12/19/23 11:02:20.028
  Dec 19 11:02:20.028: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename secrets @ 12/19/23 11:02:20.03
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:02:20.063
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:02:20.069
  STEP: Creating secret with name secret-test-c9255585-61f7-4908-ae01-0dfd8e83fabc @ 12/19/23 11:02:20.074
  STEP: Creating a pod to test consume secrets @ 12/19/23 11:02:20.085
  STEP: Saw pod success @ 12/19/23 11:02:24.145
  Dec 19 11:02:24.158: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-secrets-78c2ad68-87be-4314-82ed-044cfd3255d6 container secret-volume-test: <nil>
  STEP: delete the pod @ 12/19/23 11:02:24.184
  Dec 19 11:02:24.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-231" for this suite. @ 12/19/23 11:02:24.23
• [4.219 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 12/19/23 11:02:24.25
  Dec 19 11:02:24.250: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename secrets @ 12/19/23 11:02:24.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:02:24.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:02:24.299
  Dec 19 11:02:24.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7961" for this suite. @ 12/19/23 11:02:24.429
• [0.198 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 12/19/23 11:02:24.464
  Dec 19 11:02:24.465: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename sched-pred @ 12/19/23 11:02:24.467
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:02:24.512
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:02:24.518
  Dec 19 11:02:24.524: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Dec 19 11:02:24.545: INFO: Waiting for terminating namespaces to be deleted...
  Dec 19 11:02:24.557: INFO: 
  Logging pods the apiserver thinks is on node cahyeife7pae-1 before test
  Dec 19 11:02:24.579: INFO: coredns-5d78c9869d-b55f2 from kube-system started at 2023-12-19 09:34:24 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.579: INFO: 	Container coredns ready: true, restart count 1
  Dec 19 11:02:24.579: INFO: coredns-5d78c9869d-pvg7c from kube-system started at 2023-12-19 09:34:24 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.579: INFO: 	Container coredns ready: true, restart count 1
  Dec 19 11:02:24.579: INFO: kube-addon-manager-cahyeife7pae-1 from kube-system started at 2023-12-19 09:46:25 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.579: INFO: 	Container kube-addon-manager ready: true, restart count 1
  Dec 19 11:02:24.579: INFO: kube-apiserver-cahyeife7pae-1 from kube-system started at 2023-12-19 09:46:25 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.579: INFO: 	Container kube-apiserver ready: true, restart count 1
  Dec 19 11:02:24.579: INFO: kube-controller-manager-cahyeife7pae-1 from kube-system started at 2023-12-19 09:46:25 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.579: INFO: 	Container kube-controller-manager ready: true, restart count 1
  Dec 19 11:02:24.579: INFO: kube-flannel-ds-84xmx from kube-system started at 2023-12-19 09:35:44 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.579: INFO: 	Container kube-flannel ready: true, restart count 1
  Dec 19 11:02:24.579: INFO: kube-proxy-xmh99 from kube-system started at 2023-12-19 09:34:23 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.579: INFO: 	Container kube-proxy ready: true, restart count 1
  Dec 19 11:02:24.579: INFO: kube-scheduler-cahyeife7pae-1 from kube-system started at 2023-12-19 09:46:25 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.579: INFO: 	Container kube-scheduler ready: true, restart count 1
  Dec 19 11:02:24.579: INFO: sonobuoy-systemd-logs-daemon-set-a1a4b21fb49145dd-557wq from sonobuoy started at 2023-12-19 10:01:11 +0000 UTC (2 container statuses recorded)
  Dec 19 11:02:24.579: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 19 11:02:24.579: INFO: 	Container systemd-logs ready: true, restart count 0
  Dec 19 11:02:24.580: INFO: 
  Logging pods the apiserver thinks is on node cahyeife7pae-2 before test
  Dec 19 11:02:24.605: INFO: kube-addon-manager-cahyeife7pae-2 from kube-system started at 2023-12-19 09:46:48 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.605: INFO: 	Container kube-addon-manager ready: true, restart count 1
  Dec 19 11:02:24.606: INFO: kube-apiserver-cahyeife7pae-2 from kube-system started at 2023-12-19 09:46:48 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.606: INFO: 	Container kube-apiserver ready: true, restart count 1
  Dec 19 11:02:24.607: INFO: kube-controller-manager-cahyeife7pae-2 from kube-system started at 2023-12-19 09:46:48 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.608: INFO: 	Container kube-controller-manager ready: true, restart count 1
  Dec 19 11:02:24.608: INFO: kube-flannel-ds-zfl5g from kube-system started at 2023-12-19 09:35:44 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.609: INFO: 	Container kube-flannel ready: true, restart count 1
  Dec 19 11:02:24.609: INFO: kube-proxy-qhj8b from kube-system started at 2023-12-19 09:34:50 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.610: INFO: 	Container kube-proxy ready: true, restart count 1
  Dec 19 11:02:24.611: INFO: kube-scheduler-cahyeife7pae-2 from kube-system started at 2023-12-19 09:46:48 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.611: INFO: 	Container kube-scheduler ready: true, restart count 1
  Dec 19 11:02:24.611: INFO: sonobuoy-systemd-logs-daemon-set-a1a4b21fb49145dd-rfjdj from sonobuoy started at 2023-12-19 10:01:11 +0000 UTC (2 container statuses recorded)
  Dec 19 11:02:24.611: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 19 11:02:24.611: INFO: 	Container systemd-logs ready: true, restart count 0
  Dec 19 11:02:24.611: INFO: 
  Logging pods the apiserver thinks is on node cahyeife7pae-3 before test
  Dec 19 11:02:24.640: INFO: pod-0 from disruption-2443 started at 2023-12-19 11:02:17 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.641: INFO: 	Container donothing ready: true, restart count 0
  Dec 19 11:02:24.642: INFO: pod-1 from disruption-2443 started at 2023-12-19 11:02:17 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.643: INFO: 	Container donothing ready: true, restart count 0
  Dec 19 11:02:24.644: INFO: pod-2 from disruption-2443 started at 2023-12-19 11:02:18 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.644: INFO: 	Container donothing ready: true, restart count 0
  Dec 19 11:02:24.645: INFO: kube-flannel-ds-knpnq from kube-system started at 2023-12-19 10:14:07 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.646: INFO: 	Container kube-flannel ready: true, restart count 0
  Dec 19 11:02:24.646: INFO: kube-proxy-v2qp9 from kube-system started at 2023-12-19 09:35:14 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.647: INFO: 	Container kube-proxy ready: true, restart count 1
  Dec 19 11:02:24.647: INFO: sonobuoy from sonobuoy started at 2023-12-19 10:01:00 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:24.648: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Dec 19 11:02:24.648: INFO: sonobuoy-e2e-job-b6558dadff6840cc from sonobuoy started at 2023-12-19 10:01:11 +0000 UTC (2 container statuses recorded)
  Dec 19 11:02:24.648: INFO: 	Container e2e ready: true, restart count 0
  Dec 19 11:02:24.649: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 19 11:02:24.649: INFO: sonobuoy-systemd-logs-daemon-set-a1a4b21fb49145dd-xsfr4 from sonobuoy started at 2023-12-19 10:01:11 +0000 UTC (2 container statuses recorded)
  Dec 19 11:02:24.650: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 19 11:02:24.650: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node cahyeife7pae-1 @ 12/19/23 11:02:24.714
  STEP: verifying the node has the label node cahyeife7pae-2 @ 12/19/23 11:02:24.753
  STEP: verifying the node has the label node cahyeife7pae-3 @ 12/19/23 11:02:24.794
  Dec 19 11:02:24.840: INFO: Pod pod-0 requesting resource cpu=0m on Node cahyeife7pae-3
  Dec 19 11:02:24.841: INFO: Pod pod-1 requesting resource cpu=0m on Node cahyeife7pae-3
  Dec 19 11:02:24.843: INFO: Pod pod-2 requesting resource cpu=0m on Node cahyeife7pae-3
  Dec 19 11:02:24.843: INFO: Pod coredns-5d78c9869d-b55f2 requesting resource cpu=100m on Node cahyeife7pae-1
  Dec 19 11:02:24.845: INFO: Pod coredns-5d78c9869d-pvg7c requesting resource cpu=100m on Node cahyeife7pae-1
  Dec 19 11:02:24.846: INFO: Pod kube-addon-manager-cahyeife7pae-1 requesting resource cpu=5m on Node cahyeife7pae-1
  Dec 19 11:02:24.847: INFO: Pod kube-addon-manager-cahyeife7pae-2 requesting resource cpu=5m on Node cahyeife7pae-2
  Dec 19 11:02:24.849: INFO: Pod kube-apiserver-cahyeife7pae-1 requesting resource cpu=250m on Node cahyeife7pae-1
  Dec 19 11:02:24.850: INFO: Pod kube-apiserver-cahyeife7pae-2 requesting resource cpu=250m on Node cahyeife7pae-2
  Dec 19 11:02:24.851: INFO: Pod kube-controller-manager-cahyeife7pae-1 requesting resource cpu=200m on Node cahyeife7pae-1
  Dec 19 11:02:24.851: INFO: Pod kube-controller-manager-cahyeife7pae-2 requesting resource cpu=200m on Node cahyeife7pae-2
  Dec 19 11:02:24.851: INFO: Pod kube-flannel-ds-84xmx requesting resource cpu=100m on Node cahyeife7pae-1
  Dec 19 11:02:24.851: INFO: Pod kube-flannel-ds-knpnq requesting resource cpu=100m on Node cahyeife7pae-3
  Dec 19 11:02:24.851: INFO: Pod kube-flannel-ds-zfl5g requesting resource cpu=100m on Node cahyeife7pae-2
  Dec 19 11:02:24.851: INFO: Pod kube-proxy-qhj8b requesting resource cpu=0m on Node cahyeife7pae-2
  Dec 19 11:02:24.851: INFO: Pod kube-proxy-v2qp9 requesting resource cpu=0m on Node cahyeife7pae-3
  Dec 19 11:02:24.851: INFO: Pod kube-proxy-xmh99 requesting resource cpu=0m on Node cahyeife7pae-1
  Dec 19 11:02:24.851: INFO: Pod kube-scheduler-cahyeife7pae-1 requesting resource cpu=100m on Node cahyeife7pae-1
  Dec 19 11:02:24.851: INFO: Pod kube-scheduler-cahyeife7pae-2 requesting resource cpu=100m on Node cahyeife7pae-2
  Dec 19 11:02:24.851: INFO: Pod sonobuoy requesting resource cpu=0m on Node cahyeife7pae-3
  Dec 19 11:02:24.851: INFO: Pod sonobuoy-e2e-job-b6558dadff6840cc requesting resource cpu=0m on Node cahyeife7pae-3
  Dec 19 11:02:24.851: INFO: Pod sonobuoy-systemd-logs-daemon-set-a1a4b21fb49145dd-557wq requesting resource cpu=0m on Node cahyeife7pae-1
  Dec 19 11:02:24.851: INFO: Pod sonobuoy-systemd-logs-daemon-set-a1a4b21fb49145dd-rfjdj requesting resource cpu=0m on Node cahyeife7pae-2
  Dec 19 11:02:24.851: INFO: Pod sonobuoy-systemd-logs-daemon-set-a1a4b21fb49145dd-xsfr4 requesting resource cpu=0m on Node cahyeife7pae-3
  STEP: Starting Pods to consume most of the cluster CPU. @ 12/19/23 11:02:24.851
  Dec 19 11:02:24.851: INFO: Creating a pod which consumes cpu=521m on Node cahyeife7pae-1
  Dec 19 11:02:24.886: INFO: Creating a pod which consumes cpu=661m on Node cahyeife7pae-2
  Dec 19 11:02:24.917: INFO: Creating a pod which consumes cpu=1050m on Node cahyeife7pae-3
  STEP: Creating another pod that requires unavailable amount of CPU. @ 12/19/23 11:02:27.024
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-5e3de14f-90ad-4752-874a-e9bf8d69bd24.17a236b17d7923e9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4923/filler-pod-5e3de14f-90ad-4752-874a-e9bf8d69bd24 to cahyeife7pae-3] @ 12/19/23 11:02:27.038
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-5e3de14f-90ad-4752-874a-e9bf8d69bd24.17a236b199d0e812], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 12/19/23 11:02:27.038
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-5e3de14f-90ad-4752-874a-e9bf8d69bd24.17a236b1a2468eec], Reason = [Created], Message = [Created container filler-pod-5e3de14f-90ad-4752-874a-e9bf8d69bd24] @ 12/19/23 11:02:27.038
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-5e3de14f-90ad-4752-874a-e9bf8d69bd24.17a236b1a3d10dd7], Reason = [Started], Message = [Started container filler-pod-5e3de14f-90ad-4752-874a-e9bf8d69bd24] @ 12/19/23 11:02:27.039
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-78184db5-9f55-4340-8c2c-631bdd4c62ea.17a236b179487038], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4923/filler-pod-78184db5-9f55-4340-8c2c-631bdd4c62ea to cahyeife7pae-2] @ 12/19/23 11:02:27.039
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-78184db5-9f55-4340-8c2c-631bdd4c62ea.17a236b1bccc9f4e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 12/19/23 11:02:27.039
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-78184db5-9f55-4340-8c2c-631bdd4c62ea.17a236b1c9ea545f], Reason = [Created], Message = [Created container filler-pod-78184db5-9f55-4340-8c2c-631bdd4c62ea] @ 12/19/23 11:02:27.039
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-78184db5-9f55-4340-8c2c-631bdd4c62ea.17a236b1ce8d147d], Reason = [Started], Message = [Started container filler-pod-78184db5-9f55-4340-8c2c-631bdd4c62ea] @ 12/19/23 11:02:27.04
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c4a2e47e-7a1b-49b9-984e-822c0f520176.17a236b1781e0a06], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4923/filler-pod-c4a2e47e-7a1b-49b9-984e-822c0f520176 to cahyeife7pae-1] @ 12/19/23 11:02:27.04
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c4a2e47e-7a1b-49b9-984e-822c0f520176.17a236b1a7bfa5dc], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 12/19/23 11:02:27.04
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c4a2e47e-7a1b-49b9-984e-822c0f520176.17a236b1c13e94e1], Reason = [Created], Message = [Created container filler-pod-c4a2e47e-7a1b-49b9-984e-822c0f520176] @ 12/19/23 11:02:27.04
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c4a2e47e-7a1b-49b9-984e-822c0f520176.17a236b1ca7207d2], Reason = [Started], Message = [Started container filler-pod-c4a2e47e-7a1b-49b9-984e-822c0f520176] @ 12/19/23 11:02:27.041
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.17a236b1f63709ee], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] @ 12/19/23 11:02:27.076
  STEP: removing the label node off the node cahyeife7pae-1 @ 12/19/23 11:02:28.073
  STEP: verifying the node doesn't have the label node @ 12/19/23 11:02:28.1
  STEP: removing the label node off the node cahyeife7pae-2 @ 12/19/23 11:02:28.111
  STEP: verifying the node doesn't have the label node @ 12/19/23 11:02:28.146
  STEP: removing the label node off the node cahyeife7pae-3 @ 12/19/23 11:02:28.154
  STEP: verifying the node doesn't have the label node @ 12/19/23 11:02:28.197
  Dec 19 11:02:28.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-4923" for this suite. @ 12/19/23 11:02:28.232
• [3.799 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 12/19/23 11:02:28.274
  Dec 19 11:02:28.275: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 11:02:28.28
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:02:28.309
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:02:28.318
  STEP: Creating configMap with name projected-configmap-test-volume-map-3dc9a037-4082-40c1-8a92-17dfc6183b4b @ 12/19/23 11:02:28.328
  STEP: Creating a pod to test consume configMaps @ 12/19/23 11:02:28.342
  STEP: Saw pod success @ 12/19/23 11:02:32.428
  Dec 19 11:02:32.436: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-projected-configmaps-705df236-af57-473e-9c6e-dddd5ee5d543 container agnhost-container: <nil>
  STEP: delete the pod @ 12/19/23 11:02:32.452
  Dec 19 11:02:32.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6742" for this suite. @ 12/19/23 11:02:32.494
• [4.237 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 12/19/23 11:02:32.512
  Dec 19 11:02:32.513: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename custom-resource-definition @ 12/19/23 11:02:32.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:02:32.55
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:02:32.557
  Dec 19 11:02:32.564: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:02:33.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-424" for this suite. @ 12/19/23 11:02:33.179
• [0.684 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 12/19/23 11:02:33.198
  Dec 19 11:02:33.198: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename sched-pred @ 12/19/23 11:02:33.2
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:02:33.233
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:02:33.238
  Dec 19 11:02:33.243: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Dec 19 11:02:33.265: INFO: Waiting for terminating namespaces to be deleted...
  Dec 19 11:02:33.272: INFO: 
  Logging pods the apiserver thinks is on node cahyeife7pae-1 before test
  Dec 19 11:02:33.305: INFO: coredns-5d78c9869d-b55f2 from kube-system started at 2023-12-19 09:34:24 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.305: INFO: 	Container coredns ready: true, restart count 1
  Dec 19 11:02:33.306: INFO: coredns-5d78c9869d-pvg7c from kube-system started at 2023-12-19 09:34:24 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.306: INFO: 	Container coredns ready: true, restart count 1
  Dec 19 11:02:33.306: INFO: kube-addon-manager-cahyeife7pae-1 from kube-system started at 2023-12-19 09:46:25 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.306: INFO: 	Container kube-addon-manager ready: true, restart count 1
  Dec 19 11:02:33.306: INFO: kube-apiserver-cahyeife7pae-1 from kube-system started at 2023-12-19 09:46:25 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.307: INFO: 	Container kube-apiserver ready: true, restart count 1
  Dec 19 11:02:33.307: INFO: kube-controller-manager-cahyeife7pae-1 from kube-system started at 2023-12-19 09:46:25 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.307: INFO: 	Container kube-controller-manager ready: true, restart count 1
  Dec 19 11:02:33.307: INFO: kube-flannel-ds-84xmx from kube-system started at 2023-12-19 09:35:44 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.307: INFO: 	Container kube-flannel ready: true, restart count 1
  Dec 19 11:02:33.308: INFO: kube-proxy-xmh99 from kube-system started at 2023-12-19 09:34:23 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.308: INFO: 	Container kube-proxy ready: true, restart count 1
  Dec 19 11:02:33.308: INFO: kube-scheduler-cahyeife7pae-1 from kube-system started at 2023-12-19 09:46:25 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.308: INFO: 	Container kube-scheduler ready: true, restart count 1
  Dec 19 11:02:33.309: INFO: filler-pod-c4a2e47e-7a1b-49b9-984e-822c0f520176 from sched-pred-4923 started at 2023-12-19 11:02:24 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.309: INFO: 	Container filler-pod-c4a2e47e-7a1b-49b9-984e-822c0f520176 ready: true, restart count 0
  Dec 19 11:02:33.309: INFO: sonobuoy-systemd-logs-daemon-set-a1a4b21fb49145dd-557wq from sonobuoy started at 2023-12-19 10:01:11 +0000 UTC (2 container statuses recorded)
  Dec 19 11:02:33.309: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 19 11:02:33.310: INFO: 	Container systemd-logs ready: true, restart count 0
  Dec 19 11:02:33.310: INFO: 
  Logging pods the apiserver thinks is on node cahyeife7pae-2 before test
  Dec 19 11:02:33.341: INFO: kube-addon-manager-cahyeife7pae-2 from kube-system started at 2023-12-19 09:46:48 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.341: INFO: 	Container kube-addon-manager ready: true, restart count 1
  Dec 19 11:02:33.341: INFO: kube-apiserver-cahyeife7pae-2 from kube-system started at 2023-12-19 09:46:48 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.341: INFO: 	Container kube-apiserver ready: true, restart count 1
  Dec 19 11:02:33.341: INFO: kube-controller-manager-cahyeife7pae-2 from kube-system started at 2023-12-19 09:46:48 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.342: INFO: 	Container kube-controller-manager ready: true, restart count 1
  Dec 19 11:02:33.342: INFO: kube-flannel-ds-zfl5g from kube-system started at 2023-12-19 09:35:44 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.342: INFO: 	Container kube-flannel ready: true, restart count 1
  Dec 19 11:02:33.342: INFO: kube-proxy-qhj8b from kube-system started at 2023-12-19 09:34:50 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.343: INFO: 	Container kube-proxy ready: true, restart count 1
  Dec 19 11:02:33.343: INFO: kube-scheduler-cahyeife7pae-2 from kube-system started at 2023-12-19 09:46:48 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.343: INFO: 	Container kube-scheduler ready: true, restart count 1
  Dec 19 11:02:33.343: INFO: filler-pod-78184db5-9f55-4340-8c2c-631bdd4c62ea from sched-pred-4923 started at 2023-12-19 11:02:24 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.343: INFO: 	Container filler-pod-78184db5-9f55-4340-8c2c-631bdd4c62ea ready: true, restart count 0
  Dec 19 11:02:33.343: INFO: sonobuoy-systemd-logs-daemon-set-a1a4b21fb49145dd-rfjdj from sonobuoy started at 2023-12-19 10:01:11 +0000 UTC (2 container statuses recorded)
  Dec 19 11:02:33.343: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 19 11:02:33.343: INFO: 	Container systemd-logs ready: true, restart count 0
  Dec 19 11:02:33.343: INFO: 
  Logging pods the apiserver thinks is on node cahyeife7pae-3 before test
  Dec 19 11:02:33.371: INFO: kube-flannel-ds-knpnq from kube-system started at 2023-12-19 10:14:07 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.371: INFO: 	Container kube-flannel ready: true, restart count 0
  Dec 19 11:02:33.371: INFO: kube-proxy-v2qp9 from kube-system started at 2023-12-19 09:35:14 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.371: INFO: 	Container kube-proxy ready: true, restart count 1
  Dec 19 11:02:33.371: INFO: filler-pod-5e3de14f-90ad-4752-874a-e9bf8d69bd24 from sched-pred-4923 started at 2023-12-19 11:02:25 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.371: INFO: 	Container filler-pod-5e3de14f-90ad-4752-874a-e9bf8d69bd24 ready: true, restart count 0
  Dec 19 11:02:33.372: INFO: sonobuoy from sonobuoy started at 2023-12-19 10:01:00 +0000 UTC (1 container statuses recorded)
  Dec 19 11:02:33.372: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Dec 19 11:02:33.372: INFO: sonobuoy-e2e-job-b6558dadff6840cc from sonobuoy started at 2023-12-19 10:01:11 +0000 UTC (2 container statuses recorded)
  Dec 19 11:02:33.372: INFO: 	Container e2e ready: true, restart count 0
  Dec 19 11:02:33.372: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 19 11:02:33.372: INFO: sonobuoy-systemd-logs-daemon-set-a1a4b21fb49145dd-xsfr4 from sonobuoy started at 2023-12-19 10:01:11 +0000 UTC (2 container statuses recorded)
  Dec 19 11:02:33.372: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 19 11:02:33.372: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 12/19/23 11:02:33.372
  STEP: Explicitly delete pod here to free the resource it takes. @ 12/19/23 11:02:35.425
  STEP: Trying to apply a random label on the found node. @ 12/19/23 11:02:35.453
  STEP: verifying the node has the label kubernetes.io/e2e-623432f5-d4c1-4f73-848e-7cec1ccd849e 42 @ 12/19/23 11:02:35.494
  STEP: Trying to relaunch the pod, now with labels. @ 12/19/23 11:02:35.505
  STEP: removing the label kubernetes.io/e2e-623432f5-d4c1-4f73-848e-7cec1ccd849e off the node cahyeife7pae-3 @ 12/19/23 11:02:37.573
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-623432f5-d4c1-4f73-848e-7cec1ccd849e @ 12/19/23 11:02:37.62
  Dec 19 11:02:37.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-3792" for this suite. @ 12/19/23 11:02:37.643
• [4.465 seconds]
------------------------------
SSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 12/19/23 11:02:37.669
  Dec 19 11:02:37.669: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename csiinlinevolumes @ 12/19/23 11:02:37.681
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:02:37.729
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:02:37.737
  STEP: creating @ 12/19/23 11:02:37.748
  STEP: getting @ 12/19/23 11:02:37.793
  STEP: listing @ 12/19/23 11:02:37.815
  STEP: deleting @ 12/19/23 11:02:37.823
  Dec 19 11:02:37.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-2169" for this suite. @ 12/19/23 11:02:37.892
• [0.239 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 12/19/23 11:02:37.913
  Dec 19 11:02:37.914: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename subpath @ 12/19/23 11:02:37.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:02:37.965
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:02:37.972
  STEP: Setting up data @ 12/19/23 11:02:38.009
  STEP: Creating pod pod-subpath-test-downwardapi-nj42 @ 12/19/23 11:02:38.046
  STEP: Creating a pod to test atomic-volume-subpath @ 12/19/23 11:02:38.047
  STEP: Saw pod success @ 12/19/23 11:03:02.23
  Dec 19 11:03:02.241: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-subpath-test-downwardapi-nj42 container test-container-subpath-downwardapi-nj42: <nil>
  STEP: delete the pod @ 12/19/23 11:03:02.265
  STEP: Deleting pod pod-subpath-test-downwardapi-nj42 @ 12/19/23 11:03:02.31
  Dec 19 11:03:02.310: INFO: Deleting pod "pod-subpath-test-downwardapi-nj42" in namespace "subpath-5437"
  Dec 19 11:03:02.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5437" for this suite. @ 12/19/23 11:03:02.34
• [24.444 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 12/19/23 11:03:02.361
  Dec 19 11:03:02.361: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename runtimeclass @ 12/19/23 11:03:02.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:03:02.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:03:02.423
  Dec 19 11:03:04.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8892" for this suite. @ 12/19/23 11:03:04.539
• [2.192 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 12/19/23 11:03:04.556
  Dec 19 11:03:04.556: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename svcaccounts @ 12/19/23 11:03:04.558
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:03:04.588
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:03:04.593
  STEP: reading a file in the container @ 12/19/23 11:03:06.659
  Dec 19 11:03:06.660: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7122 pod-service-account-bc7e4a66-a99a-4ec6-9fdc-61a584811a49 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 12/19/23 11:03:07.04
  Dec 19 11:03:07.040: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7122 pod-service-account-bc7e4a66-a99a-4ec6-9fdc-61a584811a49 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 12/19/23 11:03:07.349
  Dec 19 11:03:07.350: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7122 pod-service-account-bc7e4a66-a99a-4ec6-9fdc-61a584811a49 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Dec 19 11:03:07.727: INFO: Got root ca configmap in namespace "svcaccounts-7122"
  Dec 19 11:03:07.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7122" for this suite. @ 12/19/23 11:03:07.756
• [3.216 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 12/19/23 11:03:07.776
  Dec 19 11:03:07.776: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename crd-publish-openapi @ 12/19/23 11:03:07.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:03:07.825
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:03:07.832
  STEP: set up a multi version CRD @ 12/19/23 11:03:07.842
  Dec 19 11:03:07.843: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: rename a version @ 12/19/23 11:03:14.127
  STEP: check the new version name is served @ 12/19/23 11:03:14.161
  STEP: check the old version name is removed @ 12/19/23 11:03:16.068
  STEP: check the other version is not changed @ 12/19/23 11:03:17.072
  Dec 19 11:03:21.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9617" for this suite. @ 12/19/23 11:03:21.071
• [13.311 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 12/19/23 11:03:21.095
  Dec 19 11:03:21.095: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename dns @ 12/19/23 11:03:21.1
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:03:21.15
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:03:21.159
  STEP: Creating a test headless service @ 12/19/23 11:03:21.167
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3814 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3814;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3814 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3814;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3814.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3814.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3814.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3814.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3814.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3814.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3814.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3814.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3814.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3814.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3814.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3814.svc;check="$$(dig +notcp +noall +answer +search 215.44.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.44.215_udp@PTR;check="$$(dig +tcp +noall +answer +search 215.44.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.44.215_tcp@PTR;sleep 1; done
   @ 12/19/23 11:03:21.213
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3814 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3814;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3814 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3814;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3814.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3814.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3814.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3814.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3814.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3814.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3814.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3814.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3814.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3814.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3814.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3814.svc;check="$$(dig +notcp +noall +answer +search 215.44.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.44.215_udp@PTR;check="$$(dig +tcp +noall +answer +search 215.44.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.44.215_tcp@PTR;sleep 1; done
   @ 12/19/23 11:03:21.213
  STEP: creating a pod to probe DNS @ 12/19/23 11:03:21.214
  STEP: submitting the pod to kubernetes @ 12/19/23 11:03:21.214
  STEP: retrieving the pod @ 12/19/23 11:03:23.269
  STEP: looking for the results for each expected name from probers @ 12/19/23 11:03:23.29
  Dec 19 11:03:23.316: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:23.325: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:23.334: INFO: Unable to read wheezy_udp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:23.342: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:23.370: INFO: Unable to read wheezy_udp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:23.378: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:23.393: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:23.402: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:23.447: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:23.455: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:23.463: INFO: Unable to read jessie_udp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:23.473: INFO: Unable to read jessie_tcp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:23.482: INFO: Unable to read jessie_udp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:23.491: INFO: Unable to read jessie_tcp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:23.499: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:23.508: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:23.516: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:23.531: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:23.549: INFO: Lookups using dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3814 wheezy_tcp@dns-test-service.dns-3814 wheezy_udp@dns-test-service.dns-3814.svc wheezy_tcp@dns-test-service.dns-3814.svc wheezy_udp@_http._tcp.dns-test-service.dns-3814.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3814.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3814 jessie_tcp@dns-test-service.dns-3814 jessie_udp@dns-test-service.dns-3814.svc jessie_tcp@dns-test-service.dns-3814.svc jessie_udp@_http._tcp.dns-test-service.dns-3814.svc jessie_tcp@_http._tcp.dns-test-service.dns-3814.svc jessie_udp@_http._tcp.test-service-2.dns-3814.svc jessie_tcp@_http._tcp.test-service-2.dns-3814.svc]

  Dec 19 11:03:28.568: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:28.579: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:28.590: INFO: Unable to read wheezy_udp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:28.600: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:28.613: INFO: Unable to read wheezy_udp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:28.631: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:28.645: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:28.657: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:28.714: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:28.727: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:28.747: INFO: Unable to read jessie_udp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:28.756: INFO: Unable to read jessie_tcp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:28.766: INFO: Unable to read jessie_udp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:28.778: INFO: Unable to read jessie_tcp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:28.788: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:28.800: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:28.838: INFO: Lookups using dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3814 wheezy_tcp@dns-test-service.dns-3814 wheezy_udp@dns-test-service.dns-3814.svc wheezy_tcp@dns-test-service.dns-3814.svc wheezy_udp@_http._tcp.dns-test-service.dns-3814.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3814.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3814 jessie_tcp@dns-test-service.dns-3814 jessie_udp@dns-test-service.dns-3814.svc jessie_tcp@dns-test-service.dns-3814.svc jessie_udp@_http._tcp.dns-test-service.dns-3814.svc jessie_tcp@_http._tcp.dns-test-service.dns-3814.svc]

  Dec 19 11:03:33.557: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:33.568: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:33.577: INFO: Unable to read wheezy_udp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:33.584: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:33.592: INFO: Unable to read wheezy_udp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:33.601: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:33.610: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:33.619: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:33.660: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:33.669: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:33.677: INFO: Unable to read jessie_udp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:33.686: INFO: Unable to read jessie_tcp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:33.697: INFO: Unable to read jessie_udp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:33.708: INFO: Unable to read jessie_tcp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:33.719: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:33.734: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:33.780: INFO: Lookups using dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3814 wheezy_tcp@dns-test-service.dns-3814 wheezy_udp@dns-test-service.dns-3814.svc wheezy_tcp@dns-test-service.dns-3814.svc wheezy_udp@_http._tcp.dns-test-service.dns-3814.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3814.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3814 jessie_tcp@dns-test-service.dns-3814 jessie_udp@dns-test-service.dns-3814.svc jessie_tcp@dns-test-service.dns-3814.svc jessie_udp@_http._tcp.dns-test-service.dns-3814.svc jessie_tcp@_http._tcp.dns-test-service.dns-3814.svc]

  Dec 19 11:03:38.565: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:38.573: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:38.581: INFO: Unable to read wheezy_udp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:38.590: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:38.598: INFO: Unable to read wheezy_udp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:38.606: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:38.614: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:38.623: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:38.671: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:38.680: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:38.689: INFO: Unable to read jessie_udp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:38.700: INFO: Unable to read jessie_tcp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:38.712: INFO: Unable to read jessie_udp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:38.721: INFO: Unable to read jessie_tcp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:38.731: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:38.751: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:38.788: INFO: Lookups using dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3814 wheezy_tcp@dns-test-service.dns-3814 wheezy_udp@dns-test-service.dns-3814.svc wheezy_tcp@dns-test-service.dns-3814.svc wheezy_udp@_http._tcp.dns-test-service.dns-3814.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3814.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3814 jessie_tcp@dns-test-service.dns-3814 jessie_udp@dns-test-service.dns-3814.svc jessie_tcp@dns-test-service.dns-3814.svc jessie_udp@_http._tcp.dns-test-service.dns-3814.svc jessie_tcp@_http._tcp.dns-test-service.dns-3814.svc]

  Dec 19 11:03:43.565: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:43.575: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:43.583: INFO: Unable to read wheezy_udp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:43.593: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:43.603: INFO: Unable to read wheezy_udp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:43.611: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:43.621: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:43.631: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:43.685: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:43.695: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:43.704: INFO: Unable to read jessie_udp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:43.714: INFO: Unable to read jessie_tcp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:43.726: INFO: Unable to read jessie_udp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:43.738: INFO: Unable to read jessie_tcp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:43.751: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:43.761: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:43.806: INFO: Lookups using dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3814 wheezy_tcp@dns-test-service.dns-3814 wheezy_udp@dns-test-service.dns-3814.svc wheezy_tcp@dns-test-service.dns-3814.svc wheezy_udp@_http._tcp.dns-test-service.dns-3814.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3814.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3814 jessie_tcp@dns-test-service.dns-3814 jessie_udp@dns-test-service.dns-3814.svc jessie_tcp@dns-test-service.dns-3814.svc jessie_udp@_http._tcp.dns-test-service.dns-3814.svc jessie_tcp@_http._tcp.dns-test-service.dns-3814.svc]

  Dec 19 11:03:48.569: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:48.583: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:48.593: INFO: Unable to read wheezy_udp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:48.604: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:48.616: INFO: Unable to read wheezy_udp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:48.630: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:48.642: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:48.656: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:48.708: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:48.717: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:48.730: INFO: Unable to read jessie_udp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:48.740: INFO: Unable to read jessie_tcp@dns-test-service.dns-3814 from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:48.751: INFO: Unable to read jessie_udp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:48.765: INFO: Unable to read jessie_tcp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:48.776: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:48.792: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:48.835: INFO: Lookups using dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3814 wheezy_tcp@dns-test-service.dns-3814 wheezy_udp@dns-test-service.dns-3814.svc wheezy_tcp@dns-test-service.dns-3814.svc wheezy_udp@_http._tcp.dns-test-service.dns-3814.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3814.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3814 jessie_tcp@dns-test-service.dns-3814 jessie_udp@dns-test-service.dns-3814.svc jessie_tcp@dns-test-service.dns-3814.svc jessie_udp@_http._tcp.dns-test-service.dns-3814.svc jessie_tcp@_http._tcp.dns-test-service.dns-3814.svc]

  Dec 19 11:03:53.587: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:53.763: INFO: Unable to read jessie_udp@dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:53.793: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:53.807: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3814.svc from pod dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a: the server could not find the requested resource (get pods dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a)
  Dec 19 11:03:53.865: INFO: Lookups using dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a failed for: [wheezy_tcp@dns-test-service jessie_udp@dns-test-service.dns-3814.svc jessie_udp@_http._tcp.dns-test-service.dns-3814.svc jessie_tcp@_http._tcp.dns-test-service.dns-3814.svc]

  Dec 19 11:03:58.825: INFO: DNS probes using dns-3814/dns-test-e7a8171b-7e2d-4cfd-9132-38797b68a31a succeeded

  Dec 19 11:03:58.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/19/23 11:03:58.846
  STEP: deleting the test service @ 12/19/23 11:03:58.979
  STEP: deleting the test headless service @ 12/19/23 11:03:59.066
  STEP: Destroying namespace "dns-3814" for this suite. @ 12/19/23 11:03:59.097
• [38.018 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 12/19/23 11:03:59.115
  Dec 19 11:03:59.115: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename deployment @ 12/19/23 11:03:59.125
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:03:59.166
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:03:59.17
  Dec 19 11:03:59.176: INFO: Creating deployment "test-recreate-deployment"
  Dec 19 11:03:59.189: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Dec 19 11:03:59.207: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  Dec 19 11:04:01.228: INFO: Waiting deployment "test-recreate-deployment" to complete
  Dec 19 11:04:01.235: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Dec 19 11:04:01.257: INFO: Updating deployment test-recreate-deployment
  Dec 19 11:04:01.258: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Dec 19 11:04:01.600: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-8910  1b27a625-cb3c-4586-b726-7b3499cc52c3 25141 2 2023-12-19 11:03:59 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-12-19 11:04:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 11:04:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049d9238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-12-19 11:04:01 +0000 UTC,LastTransitionTime:2023-12-19 11:04:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-12-19 11:04:01 +0000 UTC,LastTransitionTime:2023-12-19 11:03:59 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Dec 19 11:04:01.613: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-8910  565ebeed-4b52-4df0-805e-563b5e89904a 25140 1 2023-12-19 11:04:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 1b27a625-cb3c-4586-b726-7b3499cc52c3 0xc0073d5dd7 0xc0073d5dd8}] [] [{kube-controller-manager Update apps/v1 2023-12-19 11:04:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b27a625-cb3c-4586-b726-7b3499cc52c3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 11:04:01 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0073d5e88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Dec 19 11:04:01.616: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Dec 19 11:04:01.617: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-8910  3c3c3152-1a2a-4cc5-9421-fbe6fc456335 25130 2 2023-12-19 11:03:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 1b27a625-cb3c-4586-b726-7b3499cc52c3 0xc0073d5ef7 0xc0073d5ef8}] [] [{kube-controller-manager Update apps/v1 2023-12-19 11:04:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b27a625-cb3c-4586-b726-7b3499cc52c3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 11:04:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0073d5fa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Dec 19 11:04:01.631: INFO: Pod "test-recreate-deployment-54757ffd6c-lg4bm" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-lg4bm test-recreate-deployment-54757ffd6c- deployment-8910  03b7d7b4-6fcb-4f30-9aec-5c516f6c5403 25139 0 2023-12-19 11:04:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 565ebeed-4b52-4df0-805e-563b5e89904a 0xc0049d4417 0xc0049d4418}] [] [{kube-controller-manager Update v1 2023-12-19 11:04:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"565ebeed-4b52-4df0-805e-563b5e89904a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:04:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tzhcf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tzhcf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:04:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:04:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:04:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:04:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.61,PodIP:,StartTime:2023-12-19 11:04:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:04:01.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8910" for this suite. @ 12/19/23 11:04:01.653
• [2.560 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 12/19/23 11:04:01.713
  Dec 19 11:04:01.713: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename downward-api @ 12/19/23 11:04:01.717
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:04:01.768
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:04:01.776
  STEP: Creating a pod to test downward api env vars @ 12/19/23 11:04:01.786
  STEP: Saw pod success @ 12/19/23 11:04:05.844
  Dec 19 11:04:05.855: INFO: Trying to get logs from node cahyeife7pae-3 pod downward-api-324af046-8bb2-42b5-9d00-e86b5dc809d9 container dapi-container: <nil>
  STEP: delete the pod @ 12/19/23 11:04:05.904
  Dec 19 11:04:05.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9892" for this suite. @ 12/19/23 11:04:05.956
• [4.266 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 12/19/23 11:04:05.981
  Dec 19 11:04:05.981: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename replicaset @ 12/19/23 11:04:05.984
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:04:06.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:04:06.039
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 12/19/23 11:04:06.046
  Dec 19 11:04:06.073: INFO: Pod name sample-pod: Found 0 pods out of 1
  Dec 19 11:04:11.090: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 12/19/23 11:04:11.091
  STEP: getting scale subresource @ 12/19/23 11:04:11.092
  STEP: updating a scale subresource @ 12/19/23 11:04:11.105
  STEP: verifying the replicaset Spec.Replicas was modified @ 12/19/23 11:04:11.118
  STEP: Patch a scale subresource @ 12/19/23 11:04:11.13
  Dec 19 11:04:11.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1887" for this suite. @ 12/19/23 11:04:11.181
• [5.246 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 12/19/23 11:04:11.228
  Dec 19 11:04:11.228: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename services @ 12/19/23 11:04:11.231
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:04:11.334
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:04:11.341
  STEP: creating service in namespace services-8353 @ 12/19/23 11:04:11.35
  STEP: creating service affinity-nodeport-transition in namespace services-8353 @ 12/19/23 11:04:11.351
  STEP: creating replication controller affinity-nodeport-transition in namespace services-8353 @ 12/19/23 11:04:11.41
  I1219 11:04:11.449958      13 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-8353, replica count: 3
  I1219 11:04:14.503377      13 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Dec 19 11:04:14.527: INFO: Creating new exec pod
  Dec 19 11:04:17.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-8353 exec execpod-affinityzrzm2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Dec 19 11:04:18.007: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Dec 19 11:04:18.007: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 19 11:04:18.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-8353 exec execpod-affinityzrzm2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.56.107 80'
  Dec 19 11:04:18.252: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.56.107 80\nConnection to 10.233.56.107 80 port [tcp/http] succeeded!\n"
  Dec 19 11:04:18.252: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 19 11:04:18.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-8353 exec execpod-affinityzrzm2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.223 30003'
  Dec 19 11:04:18.543: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.223 30003\nConnection to 192.168.121.223 30003 port [tcp/*] succeeded!\n"
  Dec 19 11:04:18.543: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 19 11:04:18.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-8353 exec execpod-affinityzrzm2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.236 30003'
  Dec 19 11:04:18.820: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.236 30003\nConnection to 192.168.121.236 30003 port [tcp/*] succeeded!\n"
  Dec 19 11:04:18.820: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Dec 19 11:04:18.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-8353 exec execpod-affinityzrzm2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.236:30003/ ; done'
  Dec 19 11:04:19.416: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n"
  Dec 19 11:04:19.416: INFO: stdout: "\naffinity-nodeport-transition-cm6nt\naffinity-nodeport-transition-4q2q6\naffinity-nodeport-transition-cm6nt\naffinity-nodeport-transition-4q2q6\naffinity-nodeport-transition-4q2q6\naffinity-nodeport-transition-cm6nt\naffinity-nodeport-transition-4m2x2\naffinity-nodeport-transition-4q2q6\naffinity-nodeport-transition-cm6nt\naffinity-nodeport-transition-4q2q6\naffinity-nodeport-transition-4q2q6\naffinity-nodeport-transition-cm6nt\naffinity-nodeport-transition-4m2x2\naffinity-nodeport-transition-cm6nt\naffinity-nodeport-transition-4m2x2\naffinity-nodeport-transition-4m2x2"
  Dec 19 11:04:19.416: INFO: Received response from host: affinity-nodeport-transition-cm6nt
  Dec 19 11:04:19.416: INFO: Received response from host: affinity-nodeport-transition-4q2q6
  Dec 19 11:04:19.416: INFO: Received response from host: affinity-nodeport-transition-cm6nt
  Dec 19 11:04:19.416: INFO: Received response from host: affinity-nodeport-transition-4q2q6
  Dec 19 11:04:19.416: INFO: Received response from host: affinity-nodeport-transition-4q2q6
  Dec 19 11:04:19.416: INFO: Received response from host: affinity-nodeport-transition-cm6nt
  Dec 19 11:04:19.416: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.416: INFO: Received response from host: affinity-nodeport-transition-4q2q6
  Dec 19 11:04:19.416: INFO: Received response from host: affinity-nodeport-transition-cm6nt
  Dec 19 11:04:19.416: INFO: Received response from host: affinity-nodeport-transition-4q2q6
  Dec 19 11:04:19.416: INFO: Received response from host: affinity-nodeport-transition-4q2q6
  Dec 19 11:04:19.416: INFO: Received response from host: affinity-nodeport-transition-cm6nt
  Dec 19 11:04:19.416: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.416: INFO: Received response from host: affinity-nodeport-transition-cm6nt
  Dec 19 11:04:19.416: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.416: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-8353 exec execpod-affinityzrzm2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.236:30003/ ; done'
  Dec 19 11:04:19.880: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.236:30003/\n"
  Dec 19 11:04:19.880: INFO: stdout: "\naffinity-nodeport-transition-4m2x2\naffinity-nodeport-transition-4m2x2\naffinity-nodeport-transition-4m2x2\naffinity-nodeport-transition-4m2x2\naffinity-nodeport-transition-4m2x2\naffinity-nodeport-transition-4m2x2\naffinity-nodeport-transition-4m2x2\naffinity-nodeport-transition-4m2x2\naffinity-nodeport-transition-4m2x2\naffinity-nodeport-transition-4m2x2\naffinity-nodeport-transition-4m2x2\naffinity-nodeport-transition-4m2x2\naffinity-nodeport-transition-4m2x2\naffinity-nodeport-transition-4m2x2\naffinity-nodeport-transition-4m2x2\naffinity-nodeport-transition-4m2x2"
  Dec 19 11:04:19.880: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.880: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.880: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.880: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.880: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.880: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.880: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.880: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.880: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.880: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.880: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.880: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.880: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.880: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.881: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.881: INFO: Received response from host: affinity-nodeport-transition-4m2x2
  Dec 19 11:04:19.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 19 11:04:19.889: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8353, will wait for the garbage collector to delete the pods @ 12/19/23 11:04:19.923
  Dec 19 11:04:19.996: INFO: Deleting ReplicationController affinity-nodeport-transition took: 15.013119ms
  Dec 19 11:04:20.097: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.522558ms
  STEP: Destroying namespace "services-8353" for this suite. @ 12/19/23 11:04:22.351
• [11.148 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 12/19/23 11:04:22.377
  Dec 19 11:04:22.377: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename job @ 12/19/23 11:04:22.379
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:04:22.429
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:04:22.433
  STEP: Creating a job @ 12/19/23 11:04:22.437
  STEP: Ensuring job reaches completions @ 12/19/23 11:04:22.448
  Dec 19 11:04:32.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2970" for this suite. @ 12/19/23 11:04:32.473
• [10.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 12/19/23 11:04:32.494
  Dec 19 11:04:32.494: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubectl @ 12/19/23 11:04:32.496
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:04:32.536
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:04:32.544
  STEP: creating all guestbook components @ 12/19/23 11:04:32.55
  Dec 19 11:04:32.551: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Dec 19 11:04:32.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9888 create -f -'
  Dec 19 11:04:33.873: INFO: stderr: ""
  Dec 19 11:04:33.874: INFO: stdout: "service/agnhost-replica created\n"
  Dec 19 11:04:33.874: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Dec 19 11:04:33.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9888 create -f -'
  Dec 19 11:04:34.482: INFO: stderr: ""
  Dec 19 11:04:34.482: INFO: stdout: "service/agnhost-primary created\n"
  Dec 19 11:04:34.482: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Dec 19 11:04:34.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9888 create -f -'
  Dec 19 11:04:35.023: INFO: stderr: ""
  Dec 19 11:04:35.023: INFO: stdout: "service/frontend created\n"
  Dec 19 11:04:35.024: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Dec 19 11:04:35.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9888 create -f -'
  Dec 19 11:04:35.575: INFO: stderr: ""
  Dec 19 11:04:35.575: INFO: stdout: "deployment.apps/frontend created\n"
  Dec 19 11:04:35.575: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Dec 19 11:04:35.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9888 create -f -'
  Dec 19 11:04:36.202: INFO: stderr: ""
  Dec 19 11:04:36.202: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Dec 19 11:04:36.203: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Dec 19 11:04:36.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9888 create -f -'
  Dec 19 11:04:36.986: INFO: stderr: ""
  Dec 19 11:04:36.986: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 12/19/23 11:04:36.988
  Dec 19 11:04:36.989: INFO: Waiting for all frontend pods to be Running.
  Dec 19 11:04:42.042: INFO: Waiting for frontend to serve content.
  Dec 19 11:04:42.069: INFO: Trying to add a new entry to the guestbook.
  Dec 19 11:04:42.094: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 12/19/23 11:04:42.119
  Dec 19 11:04:42.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9888 delete --grace-period=0 --force -f -'
  Dec 19 11:04:42.338: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Dec 19 11:04:42.338: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 12/19/23 11:04:42.339
  Dec 19 11:04:42.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9888 delete --grace-period=0 --force -f -'
  Dec 19 11:04:42.544: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Dec 19 11:04:42.544: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 12/19/23 11:04:42.545
  Dec 19 11:04:42.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9888 delete --grace-period=0 --force -f -'
  Dec 19 11:04:42.733: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Dec 19 11:04:42.733: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 12/19/23 11:04:42.734
  Dec 19 11:04:42.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9888 delete --grace-period=0 --force -f -'
  Dec 19 11:04:42.884: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Dec 19 11:04:42.884: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 12/19/23 11:04:42.884
  Dec 19 11:04:42.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9888 delete --grace-period=0 --force -f -'
  Dec 19 11:04:43.102: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Dec 19 11:04:43.102: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 12/19/23 11:04:43.103
  Dec 19 11:04:43.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9888 delete --grace-period=0 --force -f -'
  Dec 19 11:04:43.454: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Dec 19 11:04:43.454: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Dec 19 11:04:43.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9888" for this suite. @ 12/19/23 11:04:43.509
• [11.076 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 12/19/23 11:04:43.572
  Dec 19 11:04:43.572: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename custom-resource-definition @ 12/19/23 11:04:43.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:04:43.667
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:04:43.67
  Dec 19 11:04:43.676: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:04:50.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9034" for this suite. @ 12/19/23 11:04:50.323
• [6.768 seconds]
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 12/19/23 11:04:50.342
  Dec 19 11:04:50.342: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubelet-test @ 12/19/23 11:04:50.345
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:04:50.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:04:50.392
  Dec 19 11:04:52.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-7498" for this suite. @ 12/19/23 11:04:52.457
• [2.127 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 12/19/23 11:04:52.469
  Dec 19 11:04:52.469: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename resourcequota @ 12/19/23 11:04:52.471
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:04:52.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:04:52.502
  STEP: Counting existing ResourceQuota @ 12/19/23 11:04:52.506
  STEP: Creating a ResourceQuota @ 12/19/23 11:04:57.527
  STEP: Ensuring resource quota status is calculated @ 12/19/23 11:04:57.559
  STEP: Creating a Pod that fits quota @ 12/19/23 11:04:59.566
  STEP: Ensuring ResourceQuota status captures the pod usage @ 12/19/23 11:04:59.596
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 12/19/23 11:05:01.605
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 12/19/23 11:05:01.61
  STEP: Ensuring a pod cannot update its resource requirements @ 12/19/23 11:05:01.618
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 12/19/23 11:05:01.627
  STEP: Deleting the pod @ 12/19/23 11:05:03.638
  STEP: Ensuring resource quota status released the pod usage @ 12/19/23 11:05:03.678
  Dec 19 11:05:05.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5913" for this suite. @ 12/19/23 11:05:05.702
• [13.247 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 12/19/23 11:05:05.723
  Dec 19 11:05:05.723: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubectl @ 12/19/23 11:05:05.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:05:05.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:05:05.77
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 12/19/23 11:05:05.778
  Dec 19 11:05:05.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9011 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Dec 19 11:05:05.973: INFO: stderr: ""
  Dec 19 11:05:05.973: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 12/19/23 11:05:05.973
  Dec 19 11:05:05.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9011 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Dec 19 11:05:06.139: INFO: stderr: ""
  Dec 19 11:05:06.139: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 12/19/23 11:05:06.139
  Dec 19 11:05:06.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9011 delete pods e2e-test-httpd-pod'
  Dec 19 11:05:08.618: INFO: stderr: ""
  Dec 19 11:05:08.618: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Dec 19 11:05:08.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9011" for this suite. @ 12/19/23 11:05:08.628
• [2.920 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 12/19/23 11:05:08.643
  Dec 19 11:05:08.643: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename gc @ 12/19/23 11:05:08.645
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:05:08.682
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:05:08.69
  STEP: create the deployment @ 12/19/23 11:05:08.697
  W1219 11:05:08.707876      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 12/19/23 11:05:08.708
  STEP: delete the deployment @ 12/19/23 11:05:09.231
  STEP: wait for all rs to be garbage collected @ 12/19/23 11:05:09.266
  STEP: expected 0 pods, got 2 pods @ 12/19/23 11:05:09.315
  STEP: Gathering metrics @ 12/19/23 11:05:09.834
  Dec 19 11:05:10.023: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Dec 19 11:05:10.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2018" for this suite. @ 12/19/23 11:05:10.034
• [1.410 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 12/19/23 11:05:10.055
  Dec 19 11:05:10.055: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename custom-resource-definition @ 12/19/23 11:05:10.057
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:05:10.101
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:05:10.106
  STEP: fetching the /apis discovery document @ 12/19/23 11:05:10.113
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 12/19/23 11:05:10.116
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 12/19/23 11:05:10.117
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 12/19/23 11:05:10.117
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 12/19/23 11:05:10.119
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 12/19/23 11:05:10.12
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 12/19/23 11:05:10.122
  Dec 19 11:05:10.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8098" for this suite. @ 12/19/23 11:05:10.132
• [0.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 12/19/23 11:05:10.175
  Dec 19 11:05:10.175: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename configmap @ 12/19/23 11:05:10.179
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:05:10.222
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:05:10.233
  Dec 19 11:05:10.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8470" for this suite. @ 12/19/23 11:05:10.343
• [0.180 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 12/19/23 11:05:10.363
  Dec 19 11:05:10.363: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename resourcequota @ 12/19/23 11:05:10.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:05:10.463
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:05:10.472
  STEP: Counting existing ResourceQuota @ 12/19/23 11:05:10.479
  STEP: Creating a ResourceQuota @ 12/19/23 11:05:15.486
  STEP: Ensuring resource quota status is calculated @ 12/19/23 11:05:15.5
  Dec 19 11:05:17.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9613" for this suite. @ 12/19/23 11:05:17.571
• [7.227 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 12/19/23 11:05:17.6
  Dec 19 11:05:17.600: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename job @ 12/19/23 11:05:17.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:05:17.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:05:17.643
  STEP: Creating a job @ 12/19/23 11:05:17.648
  STEP: Ensure pods equal to parallelism count is attached to the job @ 12/19/23 11:05:17.661
  STEP: patching /status @ 12/19/23 11:05:19.674
  STEP: updating /status @ 12/19/23 11:05:19.69
  STEP: get /status @ 12/19/23 11:05:19.708
  Dec 19 11:05:19.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9696" for this suite. @ 12/19/23 11:05:19.725
• [2.140 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 12/19/23 11:05:19.745
  Dec 19 11:05:19.745: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename field-validation @ 12/19/23 11:05:19.748
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:05:19.787
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:05:19.795
  STEP: apply creating a deployment @ 12/19/23 11:05:19.801
  Dec 19 11:05:19.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5142" for this suite. @ 12/19/23 11:05:19.837
• [0.106 seconds]
------------------------------
SSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 12/19/23 11:05:19.854
  Dec 19 11:05:19.854: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename hostport @ 12/19/23 11:05:19.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:05:19.895
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:05:19.9
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 12/19/23 11:05:19.912
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.236 on the node which pod1 resides and expect scheduled @ 12/19/23 11:05:21.953
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.236 but use UDP protocol on the node which pod2 resides @ 12/19/23 11:05:23.983
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 12/19/23 11:05:28.062
  Dec 19 11:05:28.063: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.236 http://127.0.0.1:54323/hostname] Namespace:hostport-5243 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:05:28.063: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:05:28.067: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:05:28.067: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-5243/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.236+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.236, port: 54323 @ 12/19/23 11:05:28.299
  Dec 19 11:05:28.299: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.236:54323/hostname] Namespace:hostport-5243 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:05:28.299: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:05:28.300: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:05:28.301: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-5243/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.236%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.236, port: 54323 UDP @ 12/19/23 11:05:28.519
  Dec 19 11:05:28.519: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.121.236 54323] Namespace:hostport-5243 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:05:28.519: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:05:28.520: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:05:28.520: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-5243/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.121.236+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  Dec 19 11:05:33.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-5243" for this suite. @ 12/19/23 11:05:33.677
• [13.843 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 12/19/23 11:05:33.705
  Dec 19 11:05:33.705: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-probe @ 12/19/23 11:05:33.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:05:33.74
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:05:33.746
  STEP: Creating pod test-grpc-3562fb04-d5e8-45fb-9cc6-b2eb4f6bef5e in namespace container-probe-5587 @ 12/19/23 11:05:33.752
  Dec 19 11:05:35.788: INFO: Started pod test-grpc-3562fb04-d5e8-45fb-9cc6-b2eb4f6bef5e in namespace container-probe-5587
  STEP: checking the pod's current state and verifying that restartCount is present @ 12/19/23 11:05:35.788
  Dec 19 11:05:35.796: INFO: Initial restart count of pod test-grpc-3562fb04-d5e8-45fb-9cc6-b2eb4f6bef5e is 0
  Dec 19 11:06:40.118: INFO: Restart count of pod container-probe-5587/test-grpc-3562fb04-d5e8-45fb-9cc6-b2eb4f6bef5e is now 1 (1m4.322096971s elapsed)
  Dec 19 11:06:40.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/19/23 11:06:40.129
  STEP: Destroying namespace "container-probe-5587" for this suite. @ 12/19/23 11:06:40.159
• [66.466 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 12/19/23 11:06:40.175
  Dec 19 11:06:40.175: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubectl-logs @ 12/19/23 11:06:40.178
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:06:40.225
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:06:40.23
  STEP: creating an pod @ 12/19/23 11:06:40.235
  Dec 19 11:06:40.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-logs-5299 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Dec 19 11:06:40.427: INFO: stderr: ""
  Dec 19 11:06:40.427: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 12/19/23 11:06:40.427
  Dec 19 11:06:40.428: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  Dec 19 11:06:42.450: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 12/19/23 11:06:42.45
  Dec 19 11:06:42.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-logs-5299 logs logs-generator logs-generator'
  Dec 19 11:06:42.681: INFO: stderr: ""
  Dec 19 11:06:42.681: INFO: stdout: "I1219 11:06:41.341449       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/h8m 577\nI1219 11:06:41.541666       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/sx9m 556\nI1219 11:06:41.742889       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/hmqp 391\nI1219 11:06:41.941518       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/tcqt 448\nI1219 11:06:42.141832       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/hls5 583\nI1219 11:06:42.341895       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/lvb 304\nI1219 11:06:42.542691       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/85p 318\n"
  STEP: limiting log lines @ 12/19/23 11:06:42.682
  Dec 19 11:06:42.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-logs-5299 logs logs-generator logs-generator --tail=1'
  Dec 19 11:06:42.862: INFO: stderr: ""
  Dec 19 11:06:42.862: INFO: stdout: "I1219 11:06:42.741824       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/zw7 548\n"
  Dec 19 11:06:42.862: INFO: got output "I1219 11:06:42.741824       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/zw7 548\n"
  STEP: limiting log bytes @ 12/19/23 11:06:42.862
  Dec 19 11:06:42.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-logs-5299 logs logs-generator logs-generator --limit-bytes=1'
  Dec 19 11:06:43.074: INFO: stderr: ""
  Dec 19 11:06:43.074: INFO: stdout: "I"
  Dec 19 11:06:43.074: INFO: got output "I"
  STEP: exposing timestamps @ 12/19/23 11:06:43.074
  Dec 19 11:06:43.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-logs-5299 logs logs-generator logs-generator --tail=1 --timestamps'
  Dec 19 11:06:43.310: INFO: stderr: ""
  Dec 19 11:06:43.310: INFO: stdout: "2023-12-19T11:06:43.142000584Z I1219 11:06:43.141881       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/452b 205\n"
  Dec 19 11:06:43.310: INFO: got output "2023-12-19T11:06:43.142000584Z I1219 11:06:43.141881       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/452b 205\n"
  STEP: restricting to a time range @ 12/19/23 11:06:43.31
  Dec 19 11:06:45.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-logs-5299 logs logs-generator logs-generator --since=1s'
  Dec 19 11:06:46.003: INFO: stderr: ""
  Dec 19 11:06:46.003: INFO: stdout: "I1219 11:06:45.142144       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/x85h 207\nI1219 11:06:45.341874       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/v4dn 563\nI1219 11:06:45.541271       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/t4gl 380\nI1219 11:06:45.741691       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/249 446\nI1219 11:06:45.941348       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/dhxm 544\n"
  Dec 19 11:06:46.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-logs-5299 logs logs-generator logs-generator --since=24h'
  Dec 19 11:06:46.177: INFO: stderr: ""
  Dec 19 11:06:46.177: INFO: stdout: "I1219 11:06:41.341449       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/h8m 577\nI1219 11:06:41.541666       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/sx9m 556\nI1219 11:06:41.742889       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/hmqp 391\nI1219 11:06:41.941518       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/tcqt 448\nI1219 11:06:42.141832       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/hls5 583\nI1219 11:06:42.341895       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/lvb 304\nI1219 11:06:42.542691       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/85p 318\nI1219 11:06:42.741824       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/zw7 548\nI1219 11:06:42.941528       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/bjz 442\nI1219 11:06:43.141881       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/452b 205\nI1219 11:06:43.341289       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/jfh 561\nI1219 11:06:43.541889       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/9nc4 421\nI1219 11:06:43.742153       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/gq7 410\nI1219 11:06:43.941659       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/pgmw 289\nI1219 11:06:44.142216       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/l92 501\nI1219 11:06:44.341314       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/mcn 415\nI1219 11:06:44.541756       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/9w6x 441\nI1219 11:06:44.742466       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/jgw 578\nI1219 11:06:44.941639       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/g8zc 416\nI1219 11:06:45.142144       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/x85h 207\nI1219 11:06:45.341874       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/v4dn 563\nI1219 11:06:45.541271       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/t4gl 380\nI1219 11:06:45.741691       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/249 446\nI1219 11:06:45.941348       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/ns/pods/dhxm 544\nI1219 11:06:46.141786       1 logs_generator.go:76] 24 POST /api/v1/namespaces/default/pods/7j9 346\n"
  Dec 19 11:06:46.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-logs-5299 delete pod logs-generator'
  Dec 19 11:06:47.087: INFO: stderr: ""
  Dec 19 11:06:47.087: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Dec 19 11:06:47.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-5299" for this suite. @ 12/19/23 11:06:47.105
• [6.950 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 12/19/23 11:06:47.131
  Dec 19 11:06:47.131: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename downward-api @ 12/19/23 11:06:47.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:06:47.173
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:06:47.185
  STEP: Creating a pod to test downward API volume plugin @ 12/19/23 11:06:47.192
  STEP: Saw pod success @ 12/19/23 11:06:51.259
  Dec 19 11:06:51.269: INFO: Trying to get logs from node cahyeife7pae-3 pod downwardapi-volume-399e7190-cd84-4258-a7b2-7d7376010539 container client-container: <nil>
  STEP: delete the pod @ 12/19/23 11:06:51.304
  Dec 19 11:06:51.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5394" for this suite. @ 12/19/23 11:06:51.362
• [4.250 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 12/19/23 11:06:51.382
  Dec 19 11:06:51.382: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-runtime @ 12/19/23 11:06:51.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:06:51.439
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:06:51.448
  STEP: create the container @ 12/19/23 11:06:51.458
  W1219 11:06:51.483282      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 12/19/23 11:06:51.484
  STEP: get the container status @ 12/19/23 11:06:54.542
  STEP: the container should be terminated @ 12/19/23 11:06:54.551
  STEP: the termination message should be set @ 12/19/23 11:06:54.551
  Dec 19 11:06:54.551: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 12/19/23 11:06:54.551
  Dec 19 11:06:54.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-4974" for this suite. @ 12/19/23 11:06:54.605
• [3.243 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:329
  STEP: Creating a kubernetes client @ 12/19/23 11:06:54.634
  Dec 19 11:06:54.634: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename statefulset @ 12/19/23 11:06:54.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:06:54.686
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:06:54.691
  STEP: Creating service test in namespace statefulset-7141 @ 12/19/23 11:06:54.701
  STEP: Creating a new StatefulSet @ 12/19/23 11:06:54.734
  Dec 19 11:06:54.782: INFO: Found 0 stateful pods, waiting for 3
  Dec 19 11:07:04.798: INFO: Found 1 stateful pods, waiting for 3
  Dec 19 11:07:14.794: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Dec 19 11:07:14.795: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Dec 19 11:07:14.795: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 12/19/23 11:07:14.827
  Dec 19 11:07:14.875: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 12/19/23 11:07:14.875
  STEP: Not applying an update when the partition is greater than the number of replicas @ 12/19/23 11:07:24.923
  STEP: Performing a canary update @ 12/19/23 11:07:24.924
  Dec 19 11:07:24.968: INFO: Updating stateful set ss2
  Dec 19 11:07:24.993: INFO: Waiting for Pod statefulset-7141/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 12/19/23 11:07:35.015
  Dec 19 11:07:35.220: INFO: Found 2 stateful pods, waiting for 3
  Dec 19 11:07:45.242: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Dec 19 11:07:45.242: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Dec 19 11:07:45.242: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
  Dec 19 11:07:55.242: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Dec 19 11:07:55.242: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Dec 19 11:07:55.242: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 12/19/23 11:07:55.276
  Dec 19 11:07:55.332: INFO: Updating stateful set ss2
  Dec 19 11:07:55.364: INFO: Waiting for Pod statefulset-7141/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Dec 19 11:08:05.415: INFO: Updating stateful set ss2
  Dec 19 11:08:05.445: INFO: Waiting for StatefulSet statefulset-7141/ss2 to complete update
  Dec 19 11:08:05.445: INFO: Waiting for Pod statefulset-7141/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Dec 19 11:08:15.475: INFO: Waiting for StatefulSet statefulset-7141/ss2 to complete update
  Dec 19 11:08:25.466: INFO: Deleting all statefulset in ns statefulset-7141
  Dec 19 11:08:25.479: INFO: Scaling statefulset ss2 to 0
  Dec 19 11:08:35.561: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 19 11:08:35.568: INFO: Deleting statefulset ss2
  Dec 19 11:08:35.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7141" for this suite. @ 12/19/23 11:08:35.619
• [101.003 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 12/19/23 11:08:35.64
  Dec 19 11:08:35.640: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename downward-api @ 12/19/23 11:08:35.645
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:08:35.692
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:08:35.7
  STEP: Creating a pod to test downward API volume plugin @ 12/19/23 11:08:35.708
  STEP: Saw pod success @ 12/19/23 11:08:39.75
  Dec 19 11:08:39.758: INFO: Trying to get logs from node cahyeife7pae-3 pod downwardapi-volume-ef22040d-666f-4d3c-90b2-1275774a16a4 container client-container: <nil>
  STEP: delete the pod @ 12/19/23 11:08:39.803
  Dec 19 11:08:39.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2296" for this suite. @ 12/19/23 11:08:39.859
• [4.248 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 12/19/23 11:08:39.892
  Dec 19 11:08:39.892: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename configmap @ 12/19/23 11:08:39.895
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:08:39.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:08:39.94
  STEP: creating a ConfigMap @ 12/19/23 11:08:39.947
  STEP: fetching the ConfigMap @ 12/19/23 11:08:39.953
  STEP: patching the ConfigMap @ 12/19/23 11:08:39.961
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 12/19/23 11:08:39.977
  STEP: deleting the ConfigMap by collection with a label selector @ 12/19/23 11:08:39.984
  STEP: listing all ConfigMaps in test namespace @ 12/19/23 11:08:40.002
  Dec 19 11:08:40.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3548" for this suite. @ 12/19/23 11:08:40.017
• [0.136 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 12/19/23 11:08:40.029
  Dec 19 11:08:40.029: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename emptydir @ 12/19/23 11:08:40.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:08:40.068
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:08:40.074
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 12/19/23 11:08:40.08
  STEP: Saw pod success @ 12/19/23 11:08:44.133
  Dec 19 11:08:44.139: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-545a84fe-c567-4ae5-807a-df4859563617 container test-container: <nil>
  STEP: delete the pod @ 12/19/23 11:08:44.155
  Dec 19 11:08:44.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9221" for this suite. @ 12/19/23 11:08:44.199
• [4.187 seconds]
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 12/19/23 11:08:44.217
  Dec 19 11:08:44.217: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename services @ 12/19/23 11:08:44.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:08:44.259
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:08:44.267
  STEP: fetching services @ 12/19/23 11:08:44.273
  Dec 19 11:08:44.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4221" for this suite. @ 12/19/23 11:08:44.294
• [0.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 12/19/23 11:08:44.324
  Dec 19 11:08:44.324: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename services @ 12/19/23 11:08:44.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:08:44.359
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:08:44.367
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-3422 @ 12/19/23 11:08:44.374
  STEP: changing the ExternalName service to type=ClusterIP @ 12/19/23 11:08:44.387
  STEP: creating replication controller externalname-service in namespace services-3422 @ 12/19/23 11:08:44.411
  I1219 11:08:44.425314      13 runners.go:194] Created replication controller with name: externalname-service, namespace: services-3422, replica count: 2
  I1219 11:08:47.476892      13 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Dec 19 11:08:47.477: INFO: Creating new exec pod
  Dec 19 11:08:50.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-3422 exec execpodnvfph -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Dec 19 11:08:50.882: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Dec 19 11:08:50.882: INFO: stdout: ""
  Dec 19 11:08:51.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-3422 exec execpodnvfph -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Dec 19 11:08:52.155: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Dec 19 11:08:52.155: INFO: stdout: "externalname-service-r2pq5"
  Dec 19 11:08:52.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-3422 exec execpodnvfph -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.19.22 80'
  Dec 19 11:08:52.433: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.19.22 80\nConnection to 10.233.19.22 80 port [tcp/http] succeeded!\n"
  Dec 19 11:08:52.433: INFO: stdout: "externalname-service-bhkmj"
  Dec 19 11:08:52.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 19 11:08:52.444: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-3422" for this suite. @ 12/19/23 11:08:52.488
• [8.177 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 12/19/23 11:08:52.507
  Dec 19 11:08:52.507: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename services @ 12/19/23 11:08:52.508
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:08:52.54
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:08:52.548
  STEP: creating an Endpoint @ 12/19/23 11:08:52.558
  STEP: waiting for available Endpoint @ 12/19/23 11:08:52.567
  STEP: listing all Endpoints @ 12/19/23 11:08:52.57
  STEP: updating the Endpoint @ 12/19/23 11:08:52.575
  STEP: fetching the Endpoint @ 12/19/23 11:08:52.588
  STEP: patching the Endpoint @ 12/19/23 11:08:52.595
  STEP: fetching the Endpoint @ 12/19/23 11:08:52.611
  STEP: deleting the Endpoint by Collection @ 12/19/23 11:08:52.62
  STEP: waiting for Endpoint deletion @ 12/19/23 11:08:52.64
  STEP: fetching the Endpoint @ 12/19/23 11:08:52.644
  Dec 19 11:08:52.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4275" for this suite. @ 12/19/23 11:08:52.661
• [0.167 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 12/19/23 11:08:52.679
  Dec 19 11:08:52.679: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename watch @ 12/19/23 11:08:52.68
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:08:52.712
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:08:52.717
  STEP: creating a watch on configmaps @ 12/19/23 11:08:52.722
  STEP: creating a new configmap @ 12/19/23 11:08:52.725
  STEP: modifying the configmap once @ 12/19/23 11:08:52.733
  STEP: closing the watch once it receives two notifications @ 12/19/23 11:08:52.748
  Dec 19 11:08:52.748: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4670  17a9b39e-4973-4064-84aa-984e09ec0ad1 27082 0 2023-12-19 11:08:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-12-19 11:08:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 19 11:08:52.748: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4670  17a9b39e-4973-4064-84aa-984e09ec0ad1 27083 0 2023-12-19 11:08:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-12-19 11:08:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 12/19/23 11:08:52.748
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 12/19/23 11:08:52.76
  STEP: deleting the configmap @ 12/19/23 11:08:52.762
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 12/19/23 11:08:52.777
  Dec 19 11:08:52.777: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4670  17a9b39e-4973-4064-84aa-984e09ec0ad1 27084 0 2023-12-19 11:08:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-12-19 11:08:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 19 11:08:52.778: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4670  17a9b39e-4973-4064-84aa-984e09ec0ad1 27085 0 2023-12-19 11:08:52 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-12-19 11:08:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 19 11:08:52.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-4670" for this suite. @ 12/19/23 11:08:52.788
• [0.127 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:443
  STEP: Creating a kubernetes client @ 12/19/23 11:08:52.813
  Dec 19 11:08:52.813: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename daemonsets @ 12/19/23 11:08:52.815
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:08:52.849
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:08:52.855
  Dec 19 11:08:52.914: INFO: Create a RollingUpdate DaemonSet
  Dec 19 11:08:52.925: INFO: Check that daemon pods launch on every node of the cluster
  Dec 19 11:08:52.940: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 11:08:52.941: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 11:08:53.959: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 11:08:53.959: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 11:08:54.965: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 19 11:08:54.966: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 11:08:55.959: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Dec 19 11:08:55.959: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Dec 19 11:08:55.959: INFO: Update the DaemonSet to trigger a rollout
  Dec 19 11:08:55.977: INFO: Updating DaemonSet daemon-set
  Dec 19 11:08:57.012: INFO: Roll back the DaemonSet before rollout is complete
  Dec 19 11:08:57.031: INFO: Updating DaemonSet daemon-set
  Dec 19 11:08:57.031: INFO: Make sure DaemonSet rollback is complete
  Dec 19 11:08:57.055: INFO: Wrong image for pod: daemon-set-8l228. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Dec 19 11:08:57.055: INFO: Pod daemon-set-8l228 is not available
  Dec 19 11:09:05.090: INFO: Pod daemon-set-wlppf is not available
  STEP: Deleting DaemonSet "daemon-set" @ 12/19/23 11:09:05.112
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-243, will wait for the garbage collector to delete the pods @ 12/19/23 11:09:05.112
  Dec 19 11:09:05.186: INFO: Deleting DaemonSet.extensions daemon-set took: 15.370576ms
  Dec 19 11:09:05.287: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.387379ms
  Dec 19 11:09:07.595: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 11:09:07.595: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Dec 19 11:09:07.601: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27250"},"items":null}

  Dec 19 11:09:07.606: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27250"},"items":null}

  Dec 19 11:09:07.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-243" for this suite. @ 12/19/23 11:09:07.645
• [14.850 seconds]
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 12/19/23 11:09:07.663
  Dec 19 11:09:07.663: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename pods @ 12/19/23 11:09:07.665
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:09:07.699
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:09:07.703
  STEP: creating pod @ 12/19/23 11:09:07.707
  Dec 19 11:09:09.760: INFO: Pod pod-hostip-481ba8a7-2b98-4af0-af09-b833cfddaf7f has hostIP: 192.168.121.61
  Dec 19 11:09:09.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7588" for this suite. @ 12/19/23 11:09:09.774
• [2.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 12/19/23 11:09:09.793
  Dec 19 11:09:09.793: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename pods @ 12/19/23 11:09:09.796
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:09:09.835
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:09:09.843
  STEP: Create set of pods @ 12/19/23 11:09:09.854
  Dec 19 11:09:09.874: INFO: created test-pod-1
  Dec 19 11:09:09.886: INFO: created test-pod-2
  Dec 19 11:09:09.908: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 12/19/23 11:09:09.908
  STEP: waiting for all pods to be deleted @ 12/19/23 11:09:12.031
  Dec 19 11:09:12.043: INFO: Pod quantity 3 is different from expected quantity 0
  Dec 19 11:09:13.057: INFO: Pod quantity 3 is different from expected quantity 0
  Dec 19 11:09:14.057: INFO: Pod quantity 3 is different from expected quantity 0
  Dec 19 11:09:15.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4828" for this suite. @ 12/19/23 11:09:15.061
• [5.281 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 12/19/23 11:09:15.079
  Dec 19 11:09:15.079: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename deployment @ 12/19/23 11:09:15.082
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:09:15.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:09:15.144
  Dec 19 11:09:15.178: INFO: Pod name rollover-pod: Found 0 pods out of 1
  Dec 19 11:09:20.191: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 12/19/23 11:09:20.192
  Dec 19 11:09:20.192: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  Dec 19 11:09:22.206: INFO: Creating deployment "test-rollover-deployment"
  Dec 19 11:09:22.234: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  Dec 19 11:09:24.262: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Dec 19 11:09:24.289: INFO: Ensure that both replica sets have 1 created replica
  Dec 19 11:09:24.315: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Dec 19 11:09:24.340: INFO: Updating deployment test-rollover-deployment
  Dec 19 11:09:24.341: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  Dec 19 11:09:26.374: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Dec 19 11:09:26.396: INFO: Make sure deployment "test-rollover-deployment" is complete
  Dec 19 11:09:26.418: INFO: all replica sets need to contain the pod-template-hash label
  Dec 19 11:09:26.419: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 11, 9, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 11, 9, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 11, 9, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 19 11:09:28.437: INFO: all replica sets need to contain the pod-template-hash label
  Dec 19 11:09:28.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 11, 9, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 11, 9, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 11, 9, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 19 11:09:30.445: INFO: all replica sets need to contain the pod-template-hash label
  Dec 19 11:09:30.445: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 11, 9, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 11, 9, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 11, 9, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 19 11:09:32.437: INFO: all replica sets need to contain the pod-template-hash label
  Dec 19 11:09:32.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 11, 9, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 11, 9, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 11, 9, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 19 11:09:34.461: INFO: all replica sets need to contain the pod-template-hash label
  Dec 19 11:09:34.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 11, 9, 22, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.December, 19, 11, 9, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.December, 19, 11, 9, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Dec 19 11:09:36.438: INFO: 
  Dec 19 11:09:36.439: INFO: Ensure that both old replica sets have no replicas
  Dec 19 11:09:36.465: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-5370  b3fc1312-fdd8-4ab3-8745-c67fd1421da0 27503 2 2023-12-19 11:09:22 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-12-19 11:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 11:09:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d4acb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-12-19 11:09:22 +0000 UTC,LastTransitionTime:2023-12-19 11:09:22 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-12-19 11:09:36 +0000 UTC,LastTransitionTime:2023-12-19 11:09:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Dec 19 11:09:36.473: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-5370  3477575c-d9f3-42c5-966e-6b40b0160144 27493 2 2023-12-19 11:09:24 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b3fc1312-fdd8-4ab3-8745-c67fd1421da0 0xc0049d8e67 0xc0049d8e68}] [] [{kube-controller-manager Update apps/v1 2023-12-19 11:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fc1312-fdd8-4ab3-8745-c67fd1421da0\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 11:09:35 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049d8f18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Dec 19 11:09:36.473: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Dec 19 11:09:36.474: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5370  12254b24-280a-44ad-bb48-a6cc6ac44b50 27502 2 2023-12-19 11:09:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b3fc1312-fdd8-4ab3-8745-c67fd1421da0 0xc0049d8d37 0xc0049d8d38}] [] [{e2e.test Update apps/v1 2023-12-19 11:09:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 11:09:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fc1312-fdd8-4ab3-8745-c67fd1421da0\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-12-19 11:09:36 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0049d8df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Dec 19 11:09:36.475: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-5370  d415a349-bf06-49d7-a44d-bcc593b10f1f 27464 2 2023-12-19 11:09:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b3fc1312-fdd8-4ab3-8745-c67fd1421da0 0xc0049d8f87 0xc0049d8f88}] [] [{kube-controller-manager Update apps/v1 2023-12-19 11:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fc1312-fdd8-4ab3-8745-c67fd1421da0\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 11:09:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049d9038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Dec 19 11:09:36.485: INFO: Pod "test-rollover-deployment-57777854c9-lnnw6" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-lnnw6 test-rollover-deployment-57777854c9- deployment-5370  50963ba3-8daf-4fca-90d9-9ce2a53640f5 27473 0 2023-12-19 11:09:24 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 3477575c-d9f3-42c5-966e-6b40b0160144 0xc0049d9577 0xc0049d9578}] [] [{kube-controller-manager Update v1 2023-12-19 11:09:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3477575c-d9f3-42c5-966e-6b40b0160144\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:09:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gz5w2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gz5w2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:09:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:09:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:09:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:09:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.61,PodIP:10.233.66.74,StartTime:2023-12-19 11:09:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-19 11:09:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://e3bd18bb42845e4aa9c15bd2423243c9cd44fb4ebb9904f868f282baa8bae0c7,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.74,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:09:36.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5370" for this suite. @ 12/19/23 11:09:36.499
• [21.440 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 12/19/23 11:09:36.526
  Dec 19 11:09:36.526: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename services @ 12/19/23 11:09:36.531
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:09:36.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:09:36.571
  STEP: creating a collection of services @ 12/19/23 11:09:36.578
  Dec 19 11:09:36.578: INFO: Creating e2e-svc-a-drzpz
  Dec 19 11:09:36.604: INFO: Creating e2e-svc-b-btgd4
  Dec 19 11:09:36.639: INFO: Creating e2e-svc-c-2c4kj
  STEP: deleting service collection @ 12/19/23 11:09:36.683
  Dec 19 11:09:36.767: INFO: Collection of services has been deleted
  Dec 19 11:09:36.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6521" for this suite. @ 12/19/23 11:09:36.78
• [0.269 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 12/19/23 11:09:36.805
  Dec 19 11:09:36.805: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubectl @ 12/19/23 11:09:36.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:09:36.843
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:09:36.852
  Dec 19 11:09:36.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-3758 create -f -'
  Dec 19 11:09:37.641: INFO: stderr: ""
  Dec 19 11:09:37.641: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Dec 19 11:09:37.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-3758 create -f -'
  Dec 19 11:09:38.487: INFO: stderr: ""
  Dec 19 11:09:38.487: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 12/19/23 11:09:38.487
  Dec 19 11:09:39.498: INFO: Selector matched 1 pods for map[app:agnhost]
  Dec 19 11:09:39.498: INFO: Found 1 / 1
  Dec 19 11:09:39.498: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Dec 19 11:09:39.506: INFO: Selector matched 1 pods for map[app:agnhost]
  Dec 19 11:09:39.506: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Dec 19 11:09:39.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-3758 describe pod agnhost-primary-sbtgw'
  Dec 19 11:09:39.689: INFO: stderr: ""
  Dec 19 11:09:39.689: INFO: stdout: "Name:             agnhost-primary-sbtgw\nNamespace:        kubectl-3758\nPriority:         0\nService Account:  default\nNode:             cahyeife7pae-3/192.168.121.61\nStart Time:       Tue, 19 Dec 2023 11:09:37 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.66.75\nIPs:\n  IP:           10.233.66.75\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://187bfb662c29c8b744818783cfb3d336bc7adaa6049b5db934daa66c55dd2f50\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 19 Dec 2023 11:09:38 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qqpz5 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-qqpz5:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-3758/agnhost-primary-sbtgw to cahyeife7pae-3\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  Dec 19 11:09:39.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-3758 describe rc agnhost-primary'
  Dec 19 11:09:39.956: INFO: stderr: ""
  Dec 19 11:09:39.956: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3758\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-sbtgw\n"
  Dec 19 11:09:39.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-3758 describe service agnhost-primary'
  Dec 19 11:09:40.141: INFO: stderr: ""
  Dec 19 11:09:40.141: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3758\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.34.31\nIPs:               10.233.34.31\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.66.75:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Dec 19 11:09:40.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-3758 describe node cahyeife7pae-1'
  Dec 19 11:09:40.404: INFO: stderr: ""
  Dec 19 11:09:40.404: INFO: stdout: "Name:               cahyeife7pae-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=cahyeife7pae-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"16:25:00:90:98:f5\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.121.236\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 19 Dec 2023 09:34:07 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  cahyeife7pae-1\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 19 Dec 2023 11:09:31 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 19 Dec 2023 09:38:48 +0000   Tue, 19 Dec 2023 09:38:48 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Tue, 19 Dec 2023 11:08:03 +0000   Tue, 19 Dec 2023 09:34:00 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 19 Dec 2023 11:08:03 +0000   Tue, 19 Dec 2023 09:34:00 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 19 Dec 2023 11:08:03 +0000   Tue, 19 Dec 2023 09:34:00 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 19 Dec 2023 11:08:03 +0000   Tue, 19 Dec 2023 09:46:25 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.236\n  Hostname:    cahyeife7pae-1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  115008636Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8123984Ki\n  pods:               110\nAllocatable:\n  cpu:                1600m\n  ephemeral-storage:  111880401014\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3274320Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 3da074635114458980dbe793a71daf0b\n  System UUID:                3da07463-5114-4589-80db-e793a71daf0b\n  Boot ID:                    e06f45a1-b21c-4f80-83af-617fd9f7f812\n  Kernel Version:             6.2.0-39-generic\n  OS Image:                   Ubuntu 22.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.27.1\n  Kubelet Version:            v1.27.8\n  Kube-Proxy Version:         v1.27.8\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-5d78c9869d-b55f2                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (5%)     95m\n  kube-system                 coredns-5d78c9869d-pvg7c                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (5%)     95m\n  kube-system                 kube-addon-manager-cahyeife7pae-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         94m\n  kube-system                 kube-apiserver-cahyeife7pae-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         95m\n  kube-system                 kube-controller-manager-cahyeife7pae-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         95m\n  kube-system                 kube-flannel-ds-84xmx                                      100m (6%)     0 (0%)      50Mi (1%)        0 (0%)         93m\n  kube-system                 kube-proxy-xmh99                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         95m\n  kube-system                 kube-scheduler-cahyeife7pae-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         95m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-a1a4b21fb49145dd-557wq    0 (0%)        0 (0%)      0 (0%)           0 (0%)         68m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                855m (53%)  0 (0%)\n  memory             240Mi (7%)  340Mi (10%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
  Dec 19 11:09:40.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-3758 describe namespace kubectl-3758'
  Dec 19 11:09:40.632: INFO: stderr: ""
  Dec 19 11:09:40.632: INFO: stdout: "Name:         kubectl-3758\nLabels:       e2e-framework=kubectl\n              e2e-run=7c3c8437-b26f-4a1e-8f35-74d4cc7f162a\n              kubernetes.io/metadata.name=kubectl-3758\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Dec 19 11:09:40.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3758" for this suite. @ 12/19/23 11:09:40.646
• [3.855 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 12/19/23 11:09:40.663
  Dec 19 11:09:40.663: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename pod-network-test @ 12/19/23 11:09:40.665
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:09:40.701
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:09:40.709
  STEP: Performing setup for networking test in namespace pod-network-test-4198 @ 12/19/23 11:09:40.716
  STEP: creating a selector @ 12/19/23 11:09:40.717
  STEP: Creating the service pods in kubernetes @ 12/19/23 11:09:40.717
  Dec 19 11:09:40.717: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 12/19/23 11:10:06.984
  Dec 19 11:10:09.063: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Dec 19 11:10:09.063: INFO: Going to poll 10.233.64.133 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Dec 19 11:10:09.071: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.133 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4198 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:10:09.072: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:10:09.075: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:10:09.076: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4198/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.133+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Dec 19 11:10:10.231: INFO: Found all 1 expected endpoints: [netserver-0]
  Dec 19 11:10:10.232: INFO: Going to poll 10.233.65.139 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Dec 19 11:10:10.240: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.139 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4198 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:10:10.240: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:10:10.242: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:10:10.242: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4198/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.65.139+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Dec 19 11:10:11.465: INFO: Found all 1 expected endpoints: [netserver-1]
  Dec 19 11:10:11.465: INFO: Going to poll 10.233.66.76 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Dec 19 11:10:11.477: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.76 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4198 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:10:11.478: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:10:11.480: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:10:11.481: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4198/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.76+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Dec 19 11:10:12.602: INFO: Found all 1 expected endpoints: [netserver-2]
  Dec 19 11:10:12.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4198" for this suite. @ 12/19/23 11:10:12.62
• [31.973 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 12/19/23 11:10:12.64
  Dec 19 11:10:12.640: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubectl @ 12/19/23 11:10:12.642
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:10:12.696
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:10:12.702
  STEP: create deployment with httpd image @ 12/19/23 11:10:12.708
  Dec 19 11:10:12.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-3940 create -f -'
  Dec 19 11:10:13.571: INFO: stderr: ""
  Dec 19 11:10:13.571: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 12/19/23 11:10:13.571
  Dec 19 11:10:13.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-3940 diff -f -'
  Dec 19 11:10:14.228: INFO: rc: 1
  Dec 19 11:10:14.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-3940 delete -f -'
  Dec 19 11:10:14.490: INFO: stderr: ""
  Dec 19 11:10:14.490: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Dec 19 11:10:14.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3940" for this suite. @ 12/19/23 11:10:14.513
• [1.896 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 12/19/23 11:10:14.541
  Dec 19 11:10:14.541: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename lease-test @ 12/19/23 11:10:14.545
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:10:14.608
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:10:14.613
  Dec 19 11:10:14.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-3380" for this suite. @ 12/19/23 11:10:14.799
• [0.273 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 12/19/23 11:10:14.815
  Dec 19 11:10:14.815: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename downward-api @ 12/19/23 11:10:14.817
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:10:14.872
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:10:14.878
  STEP: Creating a pod to test downward API volume plugin @ 12/19/23 11:10:14.886
  STEP: Saw pod success @ 12/19/23 11:10:18.948
  Dec 19 11:10:18.977: INFO: Trying to get logs from node cahyeife7pae-3 pod downwardapi-volume-dd270383-24c8-49b8-9c4e-a10680805f15 container client-container: <nil>
  STEP: delete the pod @ 12/19/23 11:10:19.059
  Dec 19 11:10:19.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-99" for this suite. @ 12/19/23 11:10:19.224
• [4.438 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 12/19/23 11:10:19.254
  Dec 19 11:10:19.254: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 11:10:19.271
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:10:19.337
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:10:19.346
  STEP: Creating a pod to test downward API volume plugin @ 12/19/23 11:10:19.355
  STEP: Saw pod success @ 12/19/23 11:10:23.487
  Dec 19 11:10:23.496: INFO: Trying to get logs from node cahyeife7pae-3 pod downwardapi-volume-0d635f97-2431-4472-8f14-294ead01e3fc container client-container: <nil>
  STEP: delete the pod @ 12/19/23 11:10:23.515
  Dec 19 11:10:23.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6052" for this suite. @ 12/19/23 11:10:23.564
• [4.329 seconds]
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 12/19/23 11:10:23.585
  Dec 19 11:10:23.586: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename watch @ 12/19/23 11:10:23.59
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:10:23.646
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:10:23.656
  STEP: creating a watch on configmaps with label A @ 12/19/23 11:10:23.661
  STEP: creating a watch on configmaps with label B @ 12/19/23 11:10:23.664
  STEP: creating a watch on configmaps with label A or B @ 12/19/23 11:10:23.671
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 12/19/23 11:10:23.675
  Dec 19 11:10:23.691: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1874  59b4d00c-da8c-46fa-b35a-23f19dc42d49 27864 0 2023-12-19 11:10:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-12-19 11:10:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 19 11:10:23.691: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1874  59b4d00c-da8c-46fa-b35a-23f19dc42d49 27864 0 2023-12-19 11:10:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-12-19 11:10:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 12/19/23 11:10:23.692
  Dec 19 11:10:23.710: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1874  59b4d00c-da8c-46fa-b35a-23f19dc42d49 27865 0 2023-12-19 11:10:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-12-19 11:10:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 19 11:10:23.711: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1874  59b4d00c-da8c-46fa-b35a-23f19dc42d49 27865 0 2023-12-19 11:10:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-12-19 11:10:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 12/19/23 11:10:23.711
  Dec 19 11:10:23.739: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1874  59b4d00c-da8c-46fa-b35a-23f19dc42d49 27866 0 2023-12-19 11:10:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-12-19 11:10:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 19 11:10:23.740: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1874  59b4d00c-da8c-46fa-b35a-23f19dc42d49 27866 0 2023-12-19 11:10:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-12-19 11:10:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 12/19/23 11:10:23.74
  Dec 19 11:10:23.756: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1874  59b4d00c-da8c-46fa-b35a-23f19dc42d49 27867 0 2023-12-19 11:10:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-12-19 11:10:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 19 11:10:23.757: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1874  59b4d00c-da8c-46fa-b35a-23f19dc42d49 27867 0 2023-12-19 11:10:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-12-19 11:10:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 12/19/23 11:10:23.758
  Dec 19 11:10:23.771: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1874  f136ad46-823e-4bf6-948a-9fc047f96f90 27868 0 2023-12-19 11:10:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-12-19 11:10:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 19 11:10:23.771: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1874  f136ad46-823e-4bf6-948a-9fc047f96f90 27868 0 2023-12-19 11:10:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-12-19 11:10:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 12/19/23 11:10:33.773
  Dec 19 11:10:33.787: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1874  f136ad46-823e-4bf6-948a-9fc047f96f90 27904 0 2023-12-19 11:10:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-12-19 11:10:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 19 11:10:33.788: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1874  f136ad46-823e-4bf6-948a-9fc047f96f90 27904 0 2023-12-19 11:10:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-12-19 11:10:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Dec 19 11:10:43.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1874" for this suite. @ 12/19/23 11:10:43.802
• [20.232 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 12/19/23 11:10:43.831
  Dec 19 11:10:43.831: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 11:10:43.837
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:10:43.879
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:10:43.886
  STEP: Creating the pod @ 12/19/23 11:10:43.895
  Dec 19 11:10:46.489: INFO: Successfully updated pod "annotationupdateb3479f9e-b2e5-4d61-bb21-4a1834f4cfb8"
  Dec 19 11:10:48.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1559" for this suite. @ 12/19/23 11:10:48.537
• [4.722 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 12/19/23 11:10:48.563
  Dec 19 11:10:48.564: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename crd-publish-openapi @ 12/19/23 11:10:48.567
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:10:48.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:10:48.622
  Dec 19 11:10:48.630: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 12/19/23 11:10:50.705
  Dec 19 11:10:50.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-2870 --namespace=crd-publish-openapi-2870 create -f -'
  Dec 19 11:10:52.262: INFO: stderr: ""
  Dec 19 11:10:52.262: INFO: stdout: "e2e-test-crd-publish-openapi-4051-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Dec 19 11:10:52.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-2870 --namespace=crd-publish-openapi-2870 delete e2e-test-crd-publish-openapi-4051-crds test-cr'
  Dec 19 11:10:52.441: INFO: stderr: ""
  Dec 19 11:10:52.441: INFO: stdout: "e2e-test-crd-publish-openapi-4051-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Dec 19 11:10:52.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-2870 --namespace=crd-publish-openapi-2870 apply -f -'
  Dec 19 11:10:53.735: INFO: stderr: ""
  Dec 19 11:10:53.735: INFO: stdout: "e2e-test-crd-publish-openapi-4051-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Dec 19 11:10:53.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-2870 --namespace=crd-publish-openapi-2870 delete e2e-test-crd-publish-openapi-4051-crds test-cr'
  Dec 19 11:10:53.919: INFO: stderr: ""
  Dec 19 11:10:53.919: INFO: stdout: "e2e-test-crd-publish-openapi-4051-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 12/19/23 11:10:53.919
  Dec 19 11:10:53.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-2870 explain e2e-test-crd-publish-openapi-4051-crds'
  Dec 19 11:10:54.519: INFO: stderr: ""
  Dec 19 11:10:54.519: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-4051-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  Dec 19 11:10:56.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2870" for this suite. @ 12/19/23 11:10:56.37
• [7.821 seconds]
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:305
  STEP: Creating a kubernetes client @ 12/19/23 11:10:56.385
  Dec 19 11:10:56.385: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename daemonsets @ 12/19/23 11:10:56.388
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:10:56.428
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:10:56.435
  STEP: Creating a simple DaemonSet "daemon-set" @ 12/19/23 11:10:56.49
  STEP: Check that daemon pods launch on every node of the cluster. @ 12/19/23 11:10:56.51
  Dec 19 11:10:56.539: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 11:10:56.539: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 11:10:57.600: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Dec 19 11:10:57.600: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 11:10:58.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Dec 19 11:10:58.560: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 12/19/23 11:10:58.568
  Dec 19 11:10:58.625: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 19 11:10:58.625: INFO: Node cahyeife7pae-2 is running 0 daemon pod, expected 1
  Dec 19 11:10:59.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 19 11:10:59.655: INFO: Node cahyeife7pae-2 is running 0 daemon pod, expected 1
  Dec 19 11:11:00.645: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Dec 19 11:11:00.645: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 12/19/23 11:11:00.645
  STEP: Deleting DaemonSet "daemon-set" @ 12/19/23 11:11:00.665
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7407, will wait for the garbage collector to delete the pods @ 12/19/23 11:11:00.665
  Dec 19 11:11:00.741: INFO: Deleting DaemonSet.extensions daemon-set took: 16.636303ms
  Dec 19 11:11:00.943: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.804623ms
  Dec 19 11:11:02.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 11:11:02.252: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Dec 19 11:11:02.259: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28076"},"items":null}

  Dec 19 11:11:02.266: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28076"},"items":null}

  Dec 19 11:11:02.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7407" for this suite. @ 12/19/23 11:11:02.309
• [5.940 seconds]
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 12/19/23 11:11:02.331
  Dec 19 11:11:02.332: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename gc @ 12/19/23 11:11:02.338
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:11:02.396
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:11:02.403
  Dec 19 11:11:02.492: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"3ed21708-cb78-4b30-a3dd-115fa0702c8a", Controller:(*bool)(0xc003d4a7e6), BlockOwnerDeletion:(*bool)(0xc003d4a7e7)}}
  Dec 19 11:11:02.520: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"aec8a120-963b-4159-acf0-4d890be42bf5", Controller:(*bool)(0xc003d4aa26), BlockOwnerDeletion:(*bool)(0xc003d4aa27)}}
  Dec 19 11:11:02.541: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"1304dc5c-a2ac-4cee-bcb2-5635a0a797f4", Controller:(*bool)(0xc003d4acb6), BlockOwnerDeletion:(*bool)(0xc003d4acb7)}}
  Dec 19 11:11:07.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1964" for this suite. @ 12/19/23 11:11:07.6
• [5.287 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 12/19/23 11:11:07.62
  Dec 19 11:11:07.620: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename crd-publish-openapi @ 12/19/23 11:11:07.622
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:11:07.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:11:07.68
  Dec 19 11:11:07.694: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 12/19/23 11:11:09.729
  Dec 19 11:11:09.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-3471 --namespace=crd-publish-openapi-3471 create -f -'
  Dec 19 11:11:11.463: INFO: stderr: ""
  Dec 19 11:11:11.463: INFO: stdout: "e2e-test-crd-publish-openapi-1318-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Dec 19 11:11:11.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-3471 --namespace=crd-publish-openapi-3471 delete e2e-test-crd-publish-openapi-1318-crds test-cr'
  Dec 19 11:11:11.676: INFO: stderr: ""
  Dec 19 11:11:11.676: INFO: stdout: "e2e-test-crd-publish-openapi-1318-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Dec 19 11:11:11.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-3471 --namespace=crd-publish-openapi-3471 apply -f -'
  Dec 19 11:11:12.177: INFO: stderr: ""
  Dec 19 11:11:12.177: INFO: stdout: "e2e-test-crd-publish-openapi-1318-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Dec 19 11:11:12.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-3471 --namespace=crd-publish-openapi-3471 delete e2e-test-crd-publish-openapi-1318-crds test-cr'
  Dec 19 11:11:12.361: INFO: stderr: ""
  Dec 19 11:11:12.362: INFO: stdout: "e2e-test-crd-publish-openapi-1318-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 12/19/23 11:11:12.362
  Dec 19 11:11:12.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-3471 explain e2e-test-crd-publish-openapi-1318-crds'
  Dec 19 11:11:12.893: INFO: stderr: ""
  Dec 19 11:11:12.893: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-1318-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Dec 19 11:11:15.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3471" for this suite. @ 12/19/23 11:11:15.483
• [7.879 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 12/19/23 11:11:15.504
  Dec 19 11:11:15.504: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename emptydir @ 12/19/23 11:11:15.508
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:11:15.556
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:11:15.568
  STEP: Creating a pod to test emptydir volume type on node default medium @ 12/19/23 11:11:15.576
  STEP: Saw pod success @ 12/19/23 11:11:19.623
  Dec 19 11:11:19.632: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-ecc62f40-71e9-4d07-acff-20422d02ace7 container test-container: <nil>
  STEP: delete the pod @ 12/19/23 11:11:19.651
  Dec 19 11:11:19.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6003" for this suite. @ 12/19/23 11:11:19.703
• [4.216 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 12/19/23 11:11:19.721
  Dec 19 11:11:19.721: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename cronjob @ 12/19/23 11:11:19.723
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:11:19.771
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:11:19.781
  STEP: Creating a ForbidConcurrent cronjob @ 12/19/23 11:11:19.787
  STEP: Ensuring a job is scheduled @ 12/19/23 11:11:19.798
  STEP: Ensuring exactly one is scheduled @ 12/19/23 11:12:01.806
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 12/19/23 11:12:01.815
  STEP: Ensuring no more jobs are scheduled @ 12/19/23 11:12:01.823
  STEP: Removing cronjob @ 12/19/23 11:17:01.86
  Dec 19 11:17:01.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9736" for this suite. @ 12/19/23 11:17:01.903
• [342.209 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 12/19/23 11:17:01.949
  Dec 19 11:17:01.949: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 12/19/23 11:17:01.962
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:17:02.051
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:17:02.057
  STEP: create the container to handle the HTTPGet hook request. @ 12/19/23 11:17:02.076
  STEP: create the pod with lifecycle hook @ 12/19/23 11:17:04.139
  STEP: check poststart hook @ 12/19/23 11:17:06.18
  STEP: delete the pod with lifecycle hook @ 12/19/23 11:17:06.222
  Dec 19 11:17:08.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-684" for this suite. @ 12/19/23 11:17:08.268
• [6.336 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:622
  STEP: Creating a kubernetes client @ 12/19/23 11:17:08.291
  Dec 19 11:17:08.291: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename field-validation @ 12/19/23 11:17:08.295
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:17:08.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:17:08.392
  Dec 19 11:17:08.403: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  W1219 11:17:11.249602      13 warnings.go:70] unknown field "alpha"
  W1219 11:17:11.250403      13 warnings.go:70] unknown field "beta"
  W1219 11:17:11.251023      13 warnings.go:70] unknown field "delta"
  W1219 11:17:11.251541      13 warnings.go:70] unknown field "epsilon"
  W1219 11:17:11.252091      13 warnings.go:70] unknown field "gamma"
  Dec 19 11:17:11.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2411" for this suite. @ 12/19/23 11:17:11.886
• [3.627 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 12/19/23 11:17:11.92
  Dec 19 11:17:11.920: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubectl @ 12/19/23 11:17:11.924
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:17:11.973
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:17:11.982
  STEP: creating Agnhost RC @ 12/19/23 11:17:11.989
  Dec 19 11:17:11.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-8466 create -f -'
  Dec 19 11:17:13.817: INFO: stderr: ""
  Dec 19 11:17:13.817: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 12/19/23 11:17:13.817
  Dec 19 11:17:14.834: INFO: Selector matched 1 pods for map[app:agnhost]
  Dec 19 11:17:14.834: INFO: Found 0 / 1
  Dec 19 11:17:15.827: INFO: Selector matched 1 pods for map[app:agnhost]
  Dec 19 11:17:15.827: INFO: Found 1 / 1
  Dec 19 11:17:15.827: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 12/19/23 11:17:15.827
  Dec 19 11:17:15.835: INFO: Selector matched 1 pods for map[app:agnhost]
  Dec 19 11:17:15.835: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Dec 19 11:17:15.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-8466 patch pod agnhost-primary-fk5wz -p {"metadata":{"annotations":{"x":"y"}}}'
  Dec 19 11:17:16.042: INFO: stderr: ""
  Dec 19 11:17:16.042: INFO: stdout: "pod/agnhost-primary-fk5wz patched\n"
  STEP: checking annotations @ 12/19/23 11:17:16.042
  Dec 19 11:17:16.052: INFO: Selector matched 1 pods for map[app:agnhost]
  Dec 19 11:17:16.052: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Dec 19 11:17:16.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8466" for this suite. @ 12/19/23 11:17:16.067
• [4.164 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 12/19/23 11:17:16.086
  Dec 19 11:17:16.086: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubectl @ 12/19/23 11:17:16.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:17:16.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:17:16.137
  STEP: creating Agnhost RC @ 12/19/23 11:17:16.152
  Dec 19 11:17:16.152: INFO: namespace kubectl-9288
  Dec 19 11:17:16.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9288 create -f -'
  Dec 19 11:17:16.815: INFO: stderr: ""
  Dec 19 11:17:16.815: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 12/19/23 11:17:16.815
  Dec 19 11:17:17.829: INFO: Selector matched 1 pods for map[app:agnhost]
  Dec 19 11:17:17.830: INFO: Found 0 / 1
  Dec 19 11:17:18.825: INFO: Selector matched 1 pods for map[app:agnhost]
  Dec 19 11:17:18.826: INFO: Found 1 / 1
  Dec 19 11:17:18.826: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Dec 19 11:17:18.838: INFO: Selector matched 1 pods for map[app:agnhost]
  Dec 19 11:17:18.838: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Dec 19 11:17:18.838: INFO: wait on agnhost-primary startup in kubectl-9288 
  Dec 19 11:17:18.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9288 logs agnhost-primary-k9fgz agnhost-primary'
  Dec 19 11:17:19.070: INFO: stderr: ""
  Dec 19 11:17:19.070: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 12/19/23 11:17:19.07
  Dec 19 11:17:19.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9288 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Dec 19 11:17:19.395: INFO: stderr: ""
  Dec 19 11:17:19.395: INFO: stdout: "service/rm2 exposed\n"
  Dec 19 11:17:19.405: INFO: Service rm2 in namespace kubectl-9288 found.
  STEP: exposing service @ 12/19/23 11:17:21.425
  Dec 19 11:17:21.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9288 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Dec 19 11:17:21.732: INFO: stderr: ""
  Dec 19 11:17:21.732: INFO: stdout: "service/rm3 exposed\n"
  Dec 19 11:17:21.745: INFO: Service rm3 in namespace kubectl-9288 found.
  Dec 19 11:17:23.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9288" for this suite. @ 12/19/23 11:17:23.781
• [7.715 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 12/19/23 11:17:23.804
  Dec 19 11:17:23.804: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename emptydir-wrapper @ 12/19/23 11:17:23.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:17:23.855
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:17:23.862
  STEP: Creating 50 configmaps @ 12/19/23 11:17:23.869
  STEP: Creating RC which spawns configmap-volume pods @ 12/19/23 11:17:24.505
  Dec 19 11:17:24.543: INFO: Pod name wrapped-volume-race-199cd7d7-04df-4578-8209-9e0a2ee7db15: Found 0 pods out of 5
  Dec 19 11:17:29.611: INFO: Pod name wrapped-volume-race-199cd7d7-04df-4578-8209-9e0a2ee7db15: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 12/19/23 11:17:29.611
  STEP: Creating RC which spawns configmap-volume pods @ 12/19/23 11:17:29.685
  Dec 19 11:17:29.740: INFO: Pod name wrapped-volume-race-900c2b3d-3907-4f04-b377-3acbd894d017: Found 0 pods out of 5
  Dec 19 11:17:34.777: INFO: Pod name wrapped-volume-race-900c2b3d-3907-4f04-b377-3acbd894d017: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 12/19/23 11:17:34.777
  STEP: Creating RC which spawns configmap-volume pods @ 12/19/23 11:17:34.871
  Dec 19 11:17:34.922: INFO: Pod name wrapped-volume-race-2873f222-abde-464e-9483-332f4fb1270a: Found 0 pods out of 5
  Dec 19 11:17:39.967: INFO: Pod name wrapped-volume-race-2873f222-abde-464e-9483-332f4fb1270a: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 12/19/23 11:17:39.967
  Dec 19 11:17:40.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-2873f222-abde-464e-9483-332f4fb1270a in namespace emptydir-wrapper-8014, will wait for the garbage collector to delete the pods @ 12/19/23 11:17:40.044
  Dec 19 11:17:40.131: INFO: Deleting ReplicationController wrapped-volume-race-2873f222-abde-464e-9483-332f4fb1270a took: 20.382124ms
  Dec 19 11:17:40.233: INFO: Terminating ReplicationController wrapped-volume-race-2873f222-abde-464e-9483-332f4fb1270a pods took: 102.086094ms
  STEP: deleting ReplicationController wrapped-volume-race-900c2b3d-3907-4f04-b377-3acbd894d017 in namespace emptydir-wrapper-8014, will wait for the garbage collector to delete the pods @ 12/19/23 11:17:42.534
  Dec 19 11:17:42.643: INFO: Deleting ReplicationController wrapped-volume-race-900c2b3d-3907-4f04-b377-3acbd894d017 took: 26.503315ms
  Dec 19 11:17:42.945: INFO: Terminating ReplicationController wrapped-volume-race-900c2b3d-3907-4f04-b377-3acbd894d017 pods took: 302.079242ms
  STEP: deleting ReplicationController wrapped-volume-race-199cd7d7-04df-4578-8209-9e0a2ee7db15 in namespace emptydir-wrapper-8014, will wait for the garbage collector to delete the pods @ 12/19/23 11:17:45.347
  Dec 19 11:17:45.424: INFO: Deleting ReplicationController wrapped-volume-race-199cd7d7-04df-4578-8209-9e0a2ee7db15 took: 15.090209ms
  Dec 19 11:17:45.725: INFO: Terminating ReplicationController wrapped-volume-race-199cd7d7-04df-4578-8209-9e0a2ee7db15 pods took: 300.764153ms
  STEP: Cleaning up the configMaps @ 12/19/23 11:17:48.427
  STEP: Destroying namespace "emptydir-wrapper-8014" for this suite. @ 12/19/23 11:17:49.19
• [25.404 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 12/19/23 11:17:49.214
  Dec 19 11:17:49.214: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename endpointslicemirroring @ 12/19/23 11:17:49.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:17:49.267
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:17:49.274
  STEP: mirroring a new custom Endpoint @ 12/19/23 11:17:49.306
  Dec 19 11:17:49.332: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 12/19/23 11:17:51.352
  Dec 19 11:17:51.376: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  STEP: mirroring deletion of a custom Endpoint @ 12/19/23 11:17:53.386
  Dec 19 11:17:53.411: INFO: Waiting for 0 EndpointSlices to exist, got 1
  Dec 19 11:17:55.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-9634" for this suite. @ 12/19/23 11:17:55.443
• [6.246 seconds]
------------------------------
S
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 12/19/23 11:17:55.46
  Dec 19 11:17:55.461: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename events @ 12/19/23 11:17:55.464
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:17:55.516
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:17:55.522
  STEP: Create set of events @ 12/19/23 11:17:55.531
  STEP: get a list of Events with a label in the current namespace @ 12/19/23 11:17:55.584
  STEP: delete a list of events @ 12/19/23 11:17:55.596
  Dec 19 11:17:55.596: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 12/19/23 11:17:55.668
  Dec 19 11:17:55.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-7081" for this suite. @ 12/19/23 11:17:55.69
• [0.245 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 12/19/23 11:17:55.71
  Dec 19 11:17:55.710: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename configmap @ 12/19/23 11:17:55.714
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:17:55.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:17:55.774
  STEP: Creating configMap with name configmap-test-volume-eab337b4-0659-44e3-8259-46095d23ccbe @ 12/19/23 11:17:55.783
  STEP: Creating a pod to test consume configMaps @ 12/19/23 11:17:55.798
  STEP: Saw pod success @ 12/19/23 11:17:59.858
  Dec 19 11:17:59.868: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-configmaps-2dae8ad1-b0ec-4b0d-8ed3-c8ee3689addb container agnhost-container: <nil>
  STEP: delete the pod @ 12/19/23 11:17:59.909
  Dec 19 11:17:59.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2769" for this suite. @ 12/19/23 11:17:59.962
• [4.270 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 12/19/23 11:17:59.983
  Dec 19 11:17:59.983: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename sched-preemption @ 12/19/23 11:17:59.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:18:00.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:18:00.056
  Dec 19 11:18:00.097: INFO: Waiting up to 1m0s for all nodes to be ready
  Dec 19 11:19:00.165: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 12/19/23 11:19:00.174
  Dec 19 11:19:00.248: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Dec 19 11:19:00.267: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Dec 19 11:19:00.342: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Dec 19 11:19:00.356: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Dec 19 11:19:00.478: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Dec 19 11:19:00.523: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 12/19/23 11:19:00.523
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 12/19/23 11:19:02.579
  Dec 19 11:19:08.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-6948" for this suite. @ 12/19/23 11:19:08.927
• [68.961 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 12/19/23 11:19:08.949
  Dec 19 11:19:08.949: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename pods @ 12/19/23 11:19:08.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:19:09.004
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:19:09.011
  STEP: Saw pod success @ 12/19/23 11:19:15.144
  Dec 19 11:19:15.160: INFO: Trying to get logs from node cahyeife7pae-3 pod client-envvars-92c08469-c54d-4a95-a72e-7bd4ade6f388 container env3cont: <nil>
  STEP: delete the pod @ 12/19/23 11:19:15.18
  Dec 19 11:19:15.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4758" for this suite. @ 12/19/23 11:19:15.23
• [6.309 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 12/19/23 11:19:15.262
  Dec 19 11:19:15.262: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename configmap @ 12/19/23 11:19:15.265
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:19:15.326
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:19:15.333
  STEP: Creating configMap configmap-5668/configmap-test-161c809e-2085-424e-b14e-12919e9126b9 @ 12/19/23 11:19:15.34
  STEP: Creating a pod to test consume configMaps @ 12/19/23 11:19:15.354
  STEP: Saw pod success @ 12/19/23 11:19:19.504
  Dec 19 11:19:19.511: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-configmaps-a429cb47-b349-4a33-a585-1f13ad6edbb0 container env-test: <nil>
  STEP: delete the pod @ 12/19/23 11:19:19.531
  Dec 19 11:19:19.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5668" for this suite. @ 12/19/23 11:19:19.586
• [4.345 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 12/19/23 11:19:19.616
  Dec 19 11:19:19.616: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename configmap @ 12/19/23 11:19:19.619
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:19:19.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:19:19.664
  STEP: Creating configMap with name configmap-test-volume-5cfa3654-960f-4c30-8186-5931bd72da2c @ 12/19/23 11:19:19.67
  STEP: Creating a pod to test consume configMaps @ 12/19/23 11:19:19.683
  STEP: Saw pod success @ 12/19/23 11:19:23.739
  Dec 19 11:19:23.748: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-configmaps-3af74aa3-f304-4d3a-bd71-bb8c80d4a88a container agnhost-container: <nil>
  STEP: delete the pod @ 12/19/23 11:19:23.762
  Dec 19 11:19:23.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9246" for this suite. @ 12/19/23 11:19:23.801
• [4.196 seconds]
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 12/19/23 11:19:23.812
  Dec 19 11:19:23.812: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 11:19:23.816
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:19:23.846
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:19:23.851
  STEP: Creating a pod to test downward API volume plugin @ 12/19/23 11:19:23.857
  STEP: Saw pod success @ 12/19/23 11:19:27.909
  Dec 19 11:19:27.921: INFO: Trying to get logs from node cahyeife7pae-3 pod downwardapi-volume-d5ebdca1-8c73-44df-b0b0-4ca4e70288b8 container client-container: <nil>
  STEP: delete the pod @ 12/19/23 11:19:27.937
  Dec 19 11:19:27.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4169" for this suite. @ 12/19/23 11:19:27.999
• [4.204 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:385
  STEP: Creating a kubernetes client @ 12/19/23 11:19:28.017
  Dec 19 11:19:28.017: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename daemonsets @ 12/19/23 11:19:28.02
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:19:28.054
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:19:28.069
  Dec 19 11:19:28.149: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 12/19/23 11:19:28.173
  Dec 19 11:19:28.195: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 11:19:28.196: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 11:19:29.224: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 11:19:29.224: INFO: Node cahyeife7pae-1 is running 0 daemon pod, expected 1
  Dec 19 11:19:30.227: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Dec 19 11:19:30.227: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 12/19/23 11:19:30.264
  STEP: Check that daemon pods images are updated. @ 12/19/23 11:19:30.299
  Dec 19 11:19:30.309: INFO: Wrong image for pod: daemon-set-296c2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Dec 19 11:19:30.310: INFO: Wrong image for pod: daemon-set-r2x74. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Dec 19 11:19:30.310: INFO: Wrong image for pod: daemon-set-rj2lf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Dec 19 11:19:31.335: INFO: Wrong image for pod: daemon-set-296c2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Dec 19 11:19:31.336: INFO: Wrong image for pod: daemon-set-rj2lf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Dec 19 11:19:32.339: INFO: Wrong image for pod: daemon-set-296c2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Dec 19 11:19:32.339: INFO: Pod daemon-set-qsp87 is not available
  Dec 19 11:19:32.340: INFO: Wrong image for pod: daemon-set-rj2lf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Dec 19 11:19:33.343: INFO: Wrong image for pod: daemon-set-rj2lf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Dec 19 11:19:34.339: INFO: Wrong image for pod: daemon-set-rj2lf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Dec 19 11:19:34.340: INFO: Pod daemon-set-t9jq7 is not available
  Dec 19 11:19:35.339: INFO: Pod daemon-set-777m4 is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 12/19/23 11:19:35.351
  Dec 19 11:19:35.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Dec 19 11:19:35.372: INFO: Node cahyeife7pae-2 is running 0 daemon pod, expected 1
  Dec 19 11:19:36.391: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Dec 19 11:19:36.391: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 12/19/23 11:19:36.429
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9190, will wait for the garbage collector to delete the pods @ 12/19/23 11:19:36.43
  Dec 19 11:19:36.502: INFO: Deleting DaemonSet.extensions daemon-set took: 12.022448ms
  Dec 19 11:19:36.702: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.742893ms
  Dec 19 11:19:38.313: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Dec 19 11:19:38.314: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Dec 19 11:19:38.322: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30174"},"items":null}

  Dec 19 11:19:38.330: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30174"},"items":null}

  Dec 19 11:19:38.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9190" for this suite. @ 12/19/23 11:19:38.384
• [10.386 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 12/19/23 11:19:38.421
  Dec 19 11:19:38.421: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename resourcequota @ 12/19/23 11:19:38.425
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:19:38.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:19:38.495
  STEP: Counting existing ResourceQuota @ 12/19/23 11:19:55.515
  STEP: Creating a ResourceQuota @ 12/19/23 11:20:00.523
  STEP: Ensuring resource quota status is calculated @ 12/19/23 11:20:00.541
  STEP: Creating a ConfigMap @ 12/19/23 11:20:02.552
  STEP: Ensuring resource quota status captures configMap creation @ 12/19/23 11:20:02.583
  STEP: Deleting a ConfigMap @ 12/19/23 11:20:04.592
  STEP: Ensuring resource quota status released usage @ 12/19/23 11:20:04.604
  Dec 19 11:20:06.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9106" for this suite. @ 12/19/23 11:20:06.625
• [28.221 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 12/19/23 11:20:06.645
  Dec 19 11:20:06.645: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubectl @ 12/19/23 11:20:06.649
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:20:06.683
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:20:06.691
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 12/19/23 11:20:06.7
  Dec 19 11:20:06.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-6226 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Dec 19 11:20:06.892: INFO: stderr: ""
  Dec 19 11:20:06.892: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 12/19/23 11:20:06.892
  Dec 19 11:20:06.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-6226 delete pods e2e-test-httpd-pod'
  Dec 19 11:20:09.147: INFO: stderr: ""
  Dec 19 11:20:09.147: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Dec 19 11:20:09.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6226" for this suite. @ 12/19/23 11:20:09.161
• [2.532 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 12/19/23 11:20:09.178
  Dec 19 11:20:09.178: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename replication-controller @ 12/19/23 11:20:09.18
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:20:09.242
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:20:09.249
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 12/19/23 11:20:09.255
  STEP: When a replication controller with a matching selector is created @ 12/19/23 11:20:11.305
  STEP: Then the orphan pod is adopted @ 12/19/23 11:20:11.319
  Dec 19 11:20:12.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8215" for this suite. @ 12/19/23 11:20:12.352
• [3.192 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 12/19/23 11:20:12.38
  Dec 19 11:20:12.380: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename var-expansion @ 12/19/23 11:20:12.382
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:20:12.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:20:12.424
  STEP: creating the pod @ 12/19/23 11:20:12.43
  STEP: waiting for pod running @ 12/19/23 11:20:12.453
  STEP: creating a file in subpath @ 12/19/23 11:20:14.475
  Dec 19 11:20:14.485: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8633 PodName:var-expansion-ae91a61c-273b-4b7c-8416-a6463687d33c ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:20:14.485: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:20:14.487: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:20:14.487: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-8633/pods/var-expansion-ae91a61c-273b-4b7c-8416-a6463687d33c/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 12/19/23 11:20:14.666
  Dec 19 11:20:14.676: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8633 PodName:var-expansion-ae91a61c-273b-4b7c-8416-a6463687d33c ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:20:14.676: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:20:14.681: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:20:14.682: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-8633/pods/var-expansion-ae91a61c-273b-4b7c-8416-a6463687d33c/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 12/19/23 11:20:14.872
  Dec 19 11:20:15.416: INFO: Successfully updated pod "var-expansion-ae91a61c-273b-4b7c-8416-a6463687d33c"
  STEP: waiting for annotated pod running @ 12/19/23 11:20:15.417
  STEP: deleting the pod gracefully @ 12/19/23 11:20:15.428
  Dec 19 11:20:15.428: INFO: Deleting pod "var-expansion-ae91a61c-273b-4b7c-8416-a6463687d33c" in namespace "var-expansion-8633"
  Dec 19 11:20:15.453: INFO: Wait up to 5m0s for pod "var-expansion-ae91a61c-273b-4b7c-8416-a6463687d33c" to be fully deleted
  Dec 19 11:20:49.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8633" for this suite. @ 12/19/23 11:20:49.696
• [37.333 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 12/19/23 11:20:49.724
  Dec 19 11:20:49.725: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename custom-resource-definition @ 12/19/23 11:20:49.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:20:49.771
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:20:49.779
  Dec 19 11:20:49.786: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:20:50.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9343" for this suite. @ 12/19/23 11:20:50.843
• [1.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 12/19/23 11:20:50.885
  Dec 19 11:20:50.885: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename cronjob @ 12/19/23 11:20:50.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:20:50.923
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:20:50.931
  STEP: Creating a cronjob @ 12/19/23 11:20:50.938
  STEP: creating @ 12/19/23 11:20:50.938
  STEP: getting @ 12/19/23 11:20:50.953
  STEP: listing @ 12/19/23 11:20:50.964
  STEP: watching @ 12/19/23 11:20:50.971
  Dec 19 11:20:50.972: INFO: starting watch
  STEP: cluster-wide listing @ 12/19/23 11:20:50.974
  STEP: cluster-wide watching @ 12/19/23 11:20:50.982
  Dec 19 11:20:50.983: INFO: starting watch
  STEP: patching @ 12/19/23 11:20:50.986
  STEP: updating @ 12/19/23 11:20:51.001
  Dec 19 11:20:51.024: INFO: waiting for watch events with expected annotations
  Dec 19 11:20:51.026: INFO: saw patched and updated annotations
  STEP: patching /status @ 12/19/23 11:20:51.027
  STEP: updating /status @ 12/19/23 11:20:51.043
  STEP: get /status @ 12/19/23 11:20:51.064
  STEP: deleting @ 12/19/23 11:20:51.073
  STEP: deleting a collection @ 12/19/23 11:20:51.111
  Dec 19 11:20:51.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7792" for this suite. @ 12/19/23 11:20:51.15
• [0.280 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 12/19/23 11:20:51.167
  Dec 19 11:20:51.167: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename watch @ 12/19/23 11:20:51.169
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:20:51.209
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:20:51.216
  STEP: getting a starting resourceVersion @ 12/19/23 11:20:51.222
  STEP: starting a background goroutine to produce watch events @ 12/19/23 11:20:51.23
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 12/19/23 11:20:51.23
  Dec 19 11:20:53.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-6546" for this suite. @ 12/19/23 11:20:54.032
• [2.924 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 12/19/23 11:20:54.097
  Dec 19 11:20:54.097: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-probe @ 12/19/23 11:20:54.1
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:20:54.136
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:20:54.143
  Dec 19 11:21:16.294: INFO: Container started at 2023-12-19 11:20:54 +0000 UTC, pod became ready at 2023-12-19 11:21:14 +0000 UTC
  Dec 19 11:21:16.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-2837" for this suite. @ 12/19/23 11:21:16.311
• [22.229 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 12/19/23 11:21:16.329
  Dec 19 11:21:16.329: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename limitrange @ 12/19/23 11:21:16.332
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:21:16.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:21:16.377
  STEP: Creating LimitRange "e2e-limitrange-4ww2r" in namespace "limitrange-5109" @ 12/19/23 11:21:16.382
  STEP: Creating another limitRange in another namespace @ 12/19/23 11:21:16.397
  Dec 19 11:21:16.433: INFO: Namespace "e2e-limitrange-4ww2r-8762" created
  Dec 19 11:21:16.433: INFO: Creating LimitRange "e2e-limitrange-4ww2r" in namespace "e2e-limitrange-4ww2r-8762"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-4ww2r" @ 12/19/23 11:21:16.443
  Dec 19 11:21:16.452: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-4ww2r" in "limitrange-5109" namespace @ 12/19/23 11:21:16.452
  Dec 19 11:21:16.476: INFO: LimitRange "e2e-limitrange-4ww2r" has been patched
  STEP: Delete LimitRange "e2e-limitrange-4ww2r" by Collection with labelSelector: "e2e-limitrange-4ww2r=patched" @ 12/19/23 11:21:16.476
  STEP: Confirm that the limitRange "e2e-limitrange-4ww2r" has been deleted @ 12/19/23 11:21:16.492
  Dec 19 11:21:16.492: INFO: Requesting list of LimitRange to confirm quantity
  Dec 19 11:21:16.499: INFO: Found 0 LimitRange with label "e2e-limitrange-4ww2r=patched"
  Dec 19 11:21:16.499: INFO: LimitRange "e2e-limitrange-4ww2r" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-4ww2r" @ 12/19/23 11:21:16.5
  Dec 19 11:21:16.507: INFO: Found 1 limitRange
  Dec 19 11:21:16.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-5109" for this suite. @ 12/19/23 11:21:16.518
  STEP: Destroying namespace "e2e-limitrange-4ww2r-8762" for this suite. @ 12/19/23 11:21:16.531
• [0.219 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 12/19/23 11:21:16.553
  Dec 19 11:21:16.553: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename webhook @ 12/19/23 11:21:16.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:21:16.604
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:21:16.611
  STEP: Setting up server cert @ 12/19/23 11:21:16.664
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/19/23 11:21:17.856
  STEP: Deploying the webhook pod @ 12/19/23 11:21:17.872
  STEP: Wait for the deployment to be ready @ 12/19/23 11:21:17.895
  Dec 19 11:21:17.920: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 12/19/23 11:21:19.942
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 11:21:19.964
  Dec 19 11:21:20.964: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 12/19/23 11:21:20.978
  STEP: create a pod @ 12/19/23 11:21:21.018
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 12/19/23 11:21:23.062
  Dec 19 11:21:23.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=webhook-3579 attach --namespace=webhook-3579 to-be-attached-pod -i -c=container1'
  Dec 19 11:21:23.283: INFO: rc: 1
  Dec 19 11:21:23.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3579" for this suite. @ 12/19/23 11:21:23.48
  STEP: Destroying namespace "webhook-markers-872" for this suite. @ 12/19/23 11:21:23.52
• [6.984 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 12/19/23 11:21:23.542
  Dec 19 11:21:23.542: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename limitrange @ 12/19/23 11:21:23.545
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:21:23.589
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:21:23.595
  STEP: Creating a LimitRange @ 12/19/23 11:21:23.602
  STEP: Setting up watch @ 12/19/23 11:21:23.603
  STEP: Submitting a LimitRange @ 12/19/23 11:21:23.714
  STEP: Verifying LimitRange creation was observed @ 12/19/23 11:21:23.729
  STEP: Fetching the LimitRange to ensure it has proper values @ 12/19/23 11:21:23.729
  Dec 19 11:21:23.738: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Dec 19 11:21:23.738: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 12/19/23 11:21:23.739
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 12/19/23 11:21:23.757
  Dec 19 11:21:23.768: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Dec 19 11:21:23.768: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 12/19/23 11:21:23.769
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 12/19/23 11:21:23.782
  Dec 19 11:21:23.790: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Dec 19 11:21:23.790: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 12/19/23 11:21:23.791
  STEP: Failing to create a Pod with more than max resources @ 12/19/23 11:21:23.796
  STEP: Updating a LimitRange @ 12/19/23 11:21:23.803
  STEP: Verifying LimitRange updating is effective @ 12/19/23 11:21:23.816
  STEP: Creating a Pod with less than former min resources @ 12/19/23 11:21:25.826
  STEP: Failing to create a Pod with more than max resources @ 12/19/23 11:21:25.839
  STEP: Deleting a LimitRange @ 12/19/23 11:21:25.85
  STEP: Verifying the LimitRange was deleted @ 12/19/23 11:21:25.874
  Dec 19 11:21:30.887: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 12/19/23 11:21:30.888
  Dec 19 11:21:30.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-6664" for this suite. @ 12/19/23 11:21:30.925
• [7.404 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 12/19/23 11:21:30.948
  Dec 19 11:21:30.948: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-runtime @ 12/19/23 11:21:30.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:21:31.008
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:21:31.015
  STEP: create the container @ 12/19/23 11:21:31.024
  W1219 11:21:31.049017      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 12/19/23 11:21:31.05
  STEP: get the container status @ 12/19/23 11:21:34.09
  STEP: the container should be terminated @ 12/19/23 11:21:34.097
  STEP: the termination message should be set @ 12/19/23 11:21:34.097
  Dec 19 11:21:34.097: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 12/19/23 11:21:34.097
  Dec 19 11:21:34.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7845" for this suite. @ 12/19/23 11:21:34.149
• [3.219 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 12/19/23 11:21:34.172
  Dec 19 11:21:34.172: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename deployment @ 12/19/23 11:21:34.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:21:34.218
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:21:34.228
  STEP: creating a Deployment @ 12/19/23 11:21:34.245
  STEP: waiting for Deployment to be created @ 12/19/23 11:21:34.261
  STEP: waiting for all Replicas to be Ready @ 12/19/23 11:21:34.266
  Dec 19 11:21:34.270: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Dec 19 11:21:34.270: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Dec 19 11:21:34.305: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Dec 19 11:21:34.305: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Dec 19 11:21:34.360: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Dec 19 11:21:34.360: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Dec 19 11:21:34.493: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Dec 19 11:21:34.493: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Dec 19 11:21:35.605: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Dec 19 11:21:35.605: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Dec 19 11:21:35.632: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 12/19/23 11:21:35.633
  W1219 11:21:35.665185      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Dec 19 11:21:35.669: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 12/19/23 11:21:35.67
  Dec 19 11:21:35.675: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 0
  Dec 19 11:21:35.675: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 0
  Dec 19 11:21:35.675: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 0
  Dec 19 11:21:35.675: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 0
  Dec 19 11:21:35.675: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 0
  Dec 19 11:21:35.675: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 0
  Dec 19 11:21:35.676: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 0
  Dec 19 11:21:35.676: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 0
  Dec 19 11:21:35.676: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1
  Dec 19 11:21:35.676: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1
  Dec 19 11:21:35.676: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 2
  Dec 19 11:21:35.676: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 2
  Dec 19 11:21:35.676: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 2
  Dec 19 11:21:35.676: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 2
  Dec 19 11:21:35.702: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 2
  Dec 19 11:21:35.703: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 2
  Dec 19 11:21:35.738: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 2
  Dec 19 11:21:35.738: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 2
  Dec 19 11:21:35.788: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1
  Dec 19 11:21:35.788: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1
  Dec 19 11:21:35.826: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1
  Dec 19 11:21:35.826: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1
  Dec 19 11:21:36.738: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 2
  Dec 19 11:21:36.738: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 2
  Dec 19 11:21:36.807: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1
  STEP: listing Deployments @ 12/19/23 11:21:36.808
  Dec 19 11:21:36.822: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 12/19/23 11:21:36.822
  Dec 19 11:21:36.852: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 12/19/23 11:21:36.853
  Dec 19 11:21:36.873: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Dec 19 11:21:36.899: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Dec 19 11:21:37.065: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Dec 19 11:21:37.156: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Dec 19 11:21:37.187: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Dec 19 11:21:38.673: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Dec 19 11:21:38.701: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  Dec 19 11:21:38.834: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Dec 19 11:21:38.848: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Dec 19 11:21:39.710: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 12/19/23 11:21:39.805
  STEP: fetching the DeploymentStatus @ 12/19/23 11:21:39.829
  Dec 19 11:21:39.845: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1
  Dec 19 11:21:39.846: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1
  Dec 19 11:21:39.847: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1
  Dec 19 11:21:39.847: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1
  Dec 19 11:21:39.848: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 1
  Dec 19 11:21:39.849: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 2
  Dec 19 11:21:39.849: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 3
  Dec 19 11:21:39.850: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 2
  Dec 19 11:21:39.851: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 2
  Dec 19 11:21:39.852: INFO: observed Deployment test-deployment in namespace deployment-5253 with ReadyReplicas 3
  STEP: deleting the Deployment @ 12/19/23 11:21:39.854
  Dec 19 11:21:39.876: INFO: observed event type MODIFIED
  Dec 19 11:21:39.877: INFO: observed event type MODIFIED
  Dec 19 11:21:39.878: INFO: observed event type MODIFIED
  Dec 19 11:21:39.879: INFO: observed event type MODIFIED
  Dec 19 11:21:39.880: INFO: observed event type MODIFIED
  Dec 19 11:21:39.882: INFO: observed event type MODIFIED
  Dec 19 11:21:39.882: INFO: observed event type MODIFIED
  Dec 19 11:21:39.882: INFO: observed event type MODIFIED
  Dec 19 11:21:39.882: INFO: observed event type MODIFIED
  Dec 19 11:21:39.883: INFO: observed event type MODIFIED
  Dec 19 11:21:39.883: INFO: observed event type MODIFIED
  Dec 19 11:21:39.888: INFO: Log out all the ReplicaSets if there is no deployment created
  Dec 19 11:21:39.899: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-5253  b11dab37-b84b-423c-bf70-05680ab54d8c 30919 3 2023-12-19 11:21:34 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 4f28b258-8957-4fbd-bd84-4a1b17bd611d 0xc004e0a5f7 0xc004e0a5f8}] [] [{kube-controller-manager Update apps/v1 2023-12-19 11:21:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f28b258-8957-4fbd-bd84-4a1b17bd611d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 11:21:36 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e0a680 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Dec 19 11:21:39.908: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-5253  db2d6549-8a4c-4d21-b644-2f7fdb3e8769 31011 4 2023-12-19 11:21:35 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 4f28b258-8957-4fbd-bd84-4a1b17bd611d 0xc004e0a6e7 0xc004e0a6e8}] [] [{kube-controller-manager Update apps/v1 2023-12-19 11:21:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f28b258-8957-4fbd-bd84-4a1b17bd611d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 11:21:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e0a770 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Dec 19 11:21:39.921: INFO: pod: "test-deployment-5b5dcbcd95-pjjfn":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-pjjfn test-deployment-5b5dcbcd95- deployment-5253  4ea919ee-5969-4edd-bbaa-ebbd13947a2d 31008 0 2023-12-19 11:21:36 +0000 UTC 2023-12-19 11:21:40 +0000 UTC 0xc004e0ad78 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 db2d6549-8a4c-4d21-b644-2f7fdb3e8769 0xc004e0ada7 0xc004e0ada8}] [] [{kube-controller-manager Update v1 2023-12-19 11:21:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"db2d6549-8a4c-4d21-b644-2f7fdb3e8769\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:21:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4sk89,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4sk89,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:21:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:21:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:21:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:21:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.223,PodIP:10.233.65.147,StartTime:2023-12-19 11:21:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-19 11:21:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://4d86a89fa56731a79d2db8c6a52132a956b54ce8160ed0007b8f8eff1d42deba,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.147,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Dec 19 11:21:39.922: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-5253  7b8778fb-700f-4d32-91e9-2e2345e939dd 31002 2 2023-12-19 11:21:36 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 4f28b258-8957-4fbd-bd84-4a1b17bd611d 0xc004e0a7d7 0xc004e0a7d8}] [] [{kube-controller-manager Update apps/v1 2023-12-19 11:21:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f28b258-8957-4fbd-bd84-4a1b17bd611d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 11:21:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e0a860 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Dec 19 11:21:39.932: INFO: pod: "test-deployment-6fc78d85c6-7nhnn":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-7nhnn test-deployment-6fc78d85c6- deployment-5253  6649a952-a19a-4b64-b9f4-75e30ada3d3b 30967 0 2023-12-19 11:21:36 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 7b8778fb-700f-4d32-91e9-2e2345e939dd 0xc004e0b9d7 0xc004e0b9d8}] [] [{kube-controller-manager Update v1 2023-12-19 11:21:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b8778fb-700f-4d32-91e9-2e2345e939dd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:21:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.114\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pvvxh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pvvxh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:21:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:21:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:21:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:21:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.61,PodIP:10.233.66.114,StartTime:2023-12-19 11:21:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-19 11:21:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://b7d9afd57c02f7170eb3114f44c6857bf04bde3bcb7c66915c4007621885f77e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.114,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Dec 19 11:21:39.933: INFO: pod: "test-deployment-6fc78d85c6-np8b4":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-np8b4 test-deployment-6fc78d85c6- deployment-5253  bad73b3e-9de2-48e8-b011-e6a43d7414fc 31000 0 2023-12-19 11:21:38 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 7b8778fb-700f-4d32-91e9-2e2345e939dd 0xc004e0bbc7 0xc004e0bbc8}] [] [{kube-controller-manager Update v1 2023-12-19 11:21:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b8778fb-700f-4d32-91e9-2e2345e939dd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:21:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wpzjn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wpzjn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:21:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:21:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:21:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:21:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.223,PodIP:10.233.65.148,StartTime:2023-12-19 11:21:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-19 11:21:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://da703890ab63c81f7ea00bddb8294d648d6f6b428e20e9d5693c9db39c9ad703,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.148,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Dec 19 11:21:39.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5253" for this suite. @ 12/19/23 11:21:39.947
• [5.795 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 12/19/23 11:21:39.969
  Dec 19 11:21:39.969: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename webhook @ 12/19/23 11:21:39.974
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:21:40.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:21:40.038
  STEP: Setting up server cert @ 12/19/23 11:21:40.092
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/19/23 11:21:41.252
  STEP: Deploying the webhook pod @ 12/19/23 11:21:41.268
  STEP: Wait for the deployment to be ready @ 12/19/23 11:21:41.292
  Dec 19 11:21:41.312: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 12/19/23 11:21:43.336
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 11:21:43.383
  Dec 19 11:21:44.385: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Dec 19 11:21:44.393: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3010-crds.webhook.example.com via the AdmissionRegistration API @ 12/19/23 11:21:44.921
  STEP: Creating a custom resource that should be mutated by the webhook @ 12/19/23 11:21:44.959
  Dec 19 11:21:47.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2525" for this suite. @ 12/19/23 11:21:47.764
  STEP: Destroying namespace "webhook-markers-8988" for this suite. @ 12/19/23 11:21:47.779
• [7.823 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 12/19/23 11:21:47.795
  Dec 19 11:21:47.795: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename services @ 12/19/23 11:21:47.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:21:47.831
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:21:47.837
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-2932 @ 12/19/23 11:21:47.842
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 12/19/23 11:21:47.875
  STEP: creating service externalsvc in namespace services-2932 @ 12/19/23 11:21:47.876
  STEP: creating replication controller externalsvc in namespace services-2932 @ 12/19/23 11:21:47.898
  I1219 11:21:47.907669      13 runners.go:194] Created replication controller with name: externalsvc, namespace: services-2932, replica count: 2
  I1219 11:21:50.958716      13 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 12/19/23 11:21:50.968
  Dec 19 11:21:51.011: INFO: Creating new exec pod
  Dec 19 11:21:53.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-2932 exec execpodb576v -- /bin/sh -x -c nslookup nodeport-service.services-2932.svc.cluster.local'
  Dec 19 11:21:53.592: INFO: stderr: "+ nslookup nodeport-service.services-2932.svc.cluster.local\n"
  Dec 19 11:21:53.592: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-2932.svc.cluster.local\tcanonical name = externalsvc.services-2932.svc.cluster.local.\nName:\texternalsvc.services-2932.svc.cluster.local\nAddress: 10.233.2.245\n\n"
  Dec 19 11:21:53.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-2932, will wait for the garbage collector to delete the pods @ 12/19/23 11:21:53.627
  Dec 19 11:21:53.720: INFO: Deleting ReplicationController externalsvc took: 18.707718ms
  Dec 19 11:21:53.822: INFO: Terminating ReplicationController externalsvc pods took: 101.566208ms
  Dec 19 11:21:55.973: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-2932" for this suite. @ 12/19/23 11:21:56.014
• [8.242 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 12/19/23 11:21:56.039
  Dec 19 11:21:56.040: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 11:21:56.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:21:56.077
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:21:56.081
  STEP: Creating a pod to test downward API volume plugin @ 12/19/23 11:21:56.086
  STEP: Saw pod success @ 12/19/23 11:22:00.129
  Dec 19 11:22:00.135: INFO: Trying to get logs from node cahyeife7pae-3 pod downwardapi-volume-4555875e-2125-49eb-8021-3fa108a7e377 container client-container: <nil>
  STEP: delete the pod @ 12/19/23 11:22:00.168
  Dec 19 11:22:00.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9683" for this suite. @ 12/19/23 11:22:00.214
• [4.189 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 12/19/23 11:22:00.232
  Dec 19 11:22:00.232: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename crd-publish-openapi @ 12/19/23 11:22:00.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:22:00.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:22:00.276
  STEP: set up a multi version CRD @ 12/19/23 11:22:00.283
  Dec 19 11:22:00.285: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: mark a version not serverd @ 12/19/23 11:22:05.691
  STEP: check the unserved version gets removed @ 12/19/23 11:22:05.738
  STEP: check the other version is not changed @ 12/19/23 11:22:07.475
  Dec 19 11:22:11.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9174" for this suite. @ 12/19/23 11:22:11.261
• [11.061 seconds]
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 12/19/23 11:22:11.293
  Dec 19 11:22:11.293: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename deployment @ 12/19/23 11:22:11.295
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:22:11.329
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:22:11.335
  Dec 19 11:22:11.342: INFO: Creating deployment "webserver-deployment"
  Dec 19 11:22:11.356: INFO: Waiting for observed generation 1
  Dec 19 11:22:13.379: INFO: Waiting for all required pods to come up
  Dec 19 11:22:13.393: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 12/19/23 11:22:13.394
  Dec 19 11:22:15.478: INFO: Waiting for deployment "webserver-deployment" to complete
  Dec 19 11:22:15.508: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Dec 19 11:22:15.581: INFO: Updating deployment webserver-deployment
  Dec 19 11:22:15.581: INFO: Waiting for observed generation 2
  Dec 19 11:22:17.624: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Dec 19 11:22:17.632: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Dec 19 11:22:17.638: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Dec 19 11:22:17.655: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Dec 19 11:22:17.656: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Dec 19 11:22:17.662: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Dec 19 11:22:17.672: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Dec 19 11:22:17.672: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Dec 19 11:22:17.691: INFO: Updating deployment webserver-deployment
  Dec 19 11:22:17.691: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Dec 19 11:22:17.709: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Dec 19 11:22:17.724: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Dec 19 11:22:17.792: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-7051  fbb9155c-fc51-41ec-89cf-a3cc070d13c7 31553 3 2023-12-19 11:22:11 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00760fed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-12-19 11:22:16 +0000 UTC,LastTransitionTime:2023-12-19 11:22:11 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-12-19 11:22:17 +0000 UTC,LastTransitionTime:2023-12-19 11:22:17 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Dec 19 11:22:17.840: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-7051  0785a7ef-614d-4abe-8566-9386c6ca8926 31549 3 2023-12-19 11:22:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment fbb9155c-fc51-41ec-89cf-a3cc070d13c7 0xc0040ddf77 0xc0040ddf78}] [] [{kube-controller-manager Update apps/v1 2023-12-19 11:22:16 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fbb9155c-fc51-41ec-89cf-a3cc070d13c7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c38018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Dec 19 11:22:17.841: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Dec 19 11:22:17.841: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-7051  104eb883-0ca2-4e69-b611-2255762bb888 31546 3 2023-12-19 11:22:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment fbb9155c-fc51-41ec-89cf-a3cc070d13c7 0xc0040dde77 0xc0040dde78}] [] [{kube-controller-manager Update apps/v1 2023-12-19 11:22:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fbb9155c-fc51-41ec-89cf-a3cc070d13c7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040ddf08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Dec 19 11:22:17.936: INFO: Pod "webserver-deployment-67bd4bf6dc-2cgkz" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2cgkz webserver-deployment-67bd4bf6dc- deployment-7051  3639ad4f-f9fe-48f3-8fa6-e71e6f033a66 31566 0 2023-12-19 11:22:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 104eb883-0ca2-4e69-b611-2255762bb888 0xc004c384c7 0xc004c384c8}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"104eb883-0ca2-4e69-b611-2255762bb888\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tr9fj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tr9fj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.952: INFO: Pod "webserver-deployment-67bd4bf6dc-428kl" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-428kl webserver-deployment-67bd4bf6dc- deployment-7051  cdc64895-b8a3-4566-8e16-0dee209a727f 31455 0 2023-12-19 11:22:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 104eb883-0ca2-4e69-b611-2255762bb888 0xc004c38607 0xc004c38608}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"104eb883-0ca2-4e69-b611-2255762bb888\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:22:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xkqsg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xkqsg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.223,PodIP:10.233.65.150,StartTime:2023-12-19 11:22:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-19 11:22:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://310b6e560ff5dd658d79635ba4036615a40dfe2e6e8e2b1244f3736380e147e5,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.150,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.953: INFO: Pod "webserver-deployment-67bd4bf6dc-cbsxr" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-cbsxr webserver-deployment-67bd4bf6dc- deployment-7051  607da277-c2a3-4856-9683-3fa2aa222661 31435 0 2023-12-19 11:22:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 104eb883-0ca2-4e69-b611-2255762bb888 0xc004c387f7 0xc004c387f8}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"104eb883-0ca2-4e69-b611-2255762bb888\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:22:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9mpv5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9mpv5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.223,PodIP:10.233.65.151,StartTime:2023-12-19 11:22:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-19 11:22:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://d888294bdbf867d354971e2c5b87662e30f47e79c9c50931352c39dbab070237,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.151,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.955: INFO: Pod "webserver-deployment-67bd4bf6dc-dsfq9" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dsfq9 webserver-deployment-67bd4bf6dc- deployment-7051  ad7c1533-52e4-4b34-904c-de030e6bd225 31475 0 2023-12-19 11:22:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 104eb883-0ca2-4e69-b611-2255762bb888 0xc004c389e7 0xc004c389e8}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"104eb883-0ca2-4e69-b611-2255762bb888\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:22:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wlrmz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wlrmz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.236,PodIP:10.233.64.154,StartTime:2023-12-19 11:22:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-19 11:22:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://79f7a25dfb7ef69e3cb3430e5b18bece1ab79deabeb413d1d344bdda19dff6d9,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.154,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.955: INFO: Pod "webserver-deployment-67bd4bf6dc-fx4wg" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-fx4wg webserver-deployment-67bd4bf6dc- deployment-7051  c4184588-8eb6-4773-bd54-99e2f10700e0 31571 0 2023-12-19 11:22:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 104eb883-0ca2-4e69-b611-2255762bb888 0xc004c38bd7 0xc004c38bd8}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"104eb883-0ca2-4e69-b611-2255762bb888\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f4t79,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f4t79,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.956: INFO: Pod "webserver-deployment-67bd4bf6dc-gnntw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gnntw webserver-deployment-67bd4bf6dc- deployment-7051  eef15277-fcf7-4635-9688-48945bb24183 31567 0 2023-12-19 11:22:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 104eb883-0ca2-4e69-b611-2255762bb888 0xc004c38d37 0xc004c38d38}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"104eb883-0ca2-4e69-b611-2255762bb888\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vq82p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vq82p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.956: INFO: Pod "webserver-deployment-67bd4bf6dc-h6xmq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-h6xmq webserver-deployment-67bd4bf6dc- deployment-7051  4927f09b-c91e-4501-b76f-ddf23613efae 31564 0 2023-12-19 11:22:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 104eb883-0ca2-4e69-b611-2255762bb888 0xc004c38e77 0xc004c38e78}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"104eb883-0ca2-4e69-b611-2255762bb888\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d2bnw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d2bnw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.957: INFO: Pod "webserver-deployment-67bd4bf6dc-hdjvj" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hdjvj webserver-deployment-67bd4bf6dc- deployment-7051  38e5f187-dc7e-421f-8de4-667a15360279 31436 0 2023-12-19 11:22:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 104eb883-0ca2-4e69-b611-2255762bb888 0xc004c38fc7 0xc004c38fc8}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"104eb883-0ca2-4e69-b611-2255762bb888\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:22:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rsvww,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rsvww,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.61,PodIP:10.233.66.119,StartTime:2023-12-19 11:22:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-19 11:22:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://cf16f608ce06aa41183b2e3853a4aff691007cace1a2d98a34f7a06a3d1d4ee1,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.119,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.958: INFO: Pod "webserver-deployment-67bd4bf6dc-n74d7" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-n74d7 webserver-deployment-67bd4bf6dc- deployment-7051  a61b656f-4956-404a-aa90-0c0dbb41c5cd 31473 0 2023-12-19 11:22:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 104eb883-0ca2-4e69-b611-2255762bb888 0xc004c391b7 0xc004c391b8}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"104eb883-0ca2-4e69-b611-2255762bb888\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:22:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dl4vm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dl4vm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.236,PodIP:10.233.64.153,StartTime:2023-12-19 11:22:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-19 11:22:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://7c19ecdf27afa7986e4236b08138799807859b402ec1c574168565695b0a6952,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.153,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.959: INFO: Pod "webserver-deployment-67bd4bf6dc-qnnzq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-qnnzq webserver-deployment-67bd4bf6dc- deployment-7051  d71f9ecd-9c01-4596-a84d-c87c204c417a 31570 0 2023-12-19 11:22:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 104eb883-0ca2-4e69-b611-2255762bb888 0xc004c393b7 0xc004c393b8}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"104eb883-0ca2-4e69-b611-2255762bb888\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lxkrr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lxkrr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.959: INFO: Pod "webserver-deployment-67bd4bf6dc-rhsc8" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rhsc8 webserver-deployment-67bd4bf6dc- deployment-7051  8f9190d0-891e-449a-a45d-7965f3bc7e05 31452 0 2023-12-19 11:22:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 104eb883-0ca2-4e69-b611-2255762bb888 0xc004c39520 0xc004c39521}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"104eb883-0ca2-4e69-b611-2255762bb888\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7pzrv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7pzrv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.61,PodIP:10.233.66.120,StartTime:2023-12-19 11:22:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-19 11:22:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://fd84efb0a642ab4abd7903af89af92c372346e8ce330322979ed1e043138e663,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.120,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.960: INFO: Pod "webserver-deployment-67bd4bf6dc-sdk47" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-sdk47 webserver-deployment-67bd4bf6dc- deployment-7051  bb64b3f4-57d2-4c68-9334-4de5236d4628 31569 0 2023-12-19 11:22:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 104eb883-0ca2-4e69-b611-2255762bb888 0xc004c39707 0xc004c39708}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"104eb883-0ca2-4e69-b611-2255762bb888\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pzpd7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pzpd7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.223,PodIP:,StartTime:2023-12-19 11:22:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.963: INFO: Pod "webserver-deployment-67bd4bf6dc-vxdjd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vxdjd webserver-deployment-67bd4bf6dc- deployment-7051  e14136f8-2336-4d6d-a84d-ea3f42281971 31560 0 2023-12-19 11:22:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 104eb883-0ca2-4e69-b611-2255762bb888 0xc004c398e7 0xc004c398e8}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"104eb883-0ca2-4e69-b611-2255762bb888\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wfzmc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wfzmc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.61,PodIP:,StartTime:2023-12-19 11:22:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.963: INFO: Pod "webserver-deployment-67bd4bf6dc-w2fr6" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-w2fr6 webserver-deployment-67bd4bf6dc- deployment-7051  1a953698-4b52-4455-93e2-e813e399eded 31471 0 2023-12-19 11:22:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 104eb883-0ca2-4e69-b611-2255762bb888 0xc004c39ab7 0xc004c39ab8}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"104eb883-0ca2-4e69-b611-2255762bb888\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:22:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nhft8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nhft8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.236,PodIP:10.233.64.152,StartTime:2023-12-19 11:22:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-19 11:22:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://7c63db8d0fb8383d1038930682a58b3bce0c163e86465dab48a2c374c0a0a6f9,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.152,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.964: INFO: Pod "webserver-deployment-67bd4bf6dc-wqmjb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-wqmjb webserver-deployment-67bd4bf6dc- deployment-7051  10b1ed6d-9a81-4840-80b7-3a9594c36d3e 31577 0 2023-12-19 11:22:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 104eb883-0ca2-4e69-b611-2255762bb888 0xc004c39ca7 0xc004c39ca8}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"104eb883-0ca2-4e69-b611-2255762bb888\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jmc7r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jmc7r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.61,PodIP:,StartTime:2023-12-19 11:22:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.965: INFO: Pod "webserver-deployment-67bd4bf6dc-zv7sp" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-zv7sp webserver-deployment-67bd4bf6dc- deployment-7051  84268d6f-2b67-4d87-89dd-e252f59f60f5 31449 0 2023-12-19 11:22:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 104eb883-0ca2-4e69-b611-2255762bb888 0xc004c39e77 0xc004c39e78}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"104eb883-0ca2-4e69-b611-2255762bb888\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:22:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kp2g4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kp2g4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.223,PodIP:10.233.65.152,StartTime:2023-12-19 11:22:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-19 11:22:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://72a036bb3454b4199abd1d28da17f9a37bdcdb61c2bc0f81d583cdb6259552fa,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.152,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.966: INFO: Pod "webserver-deployment-7b75d79cf5-8mxf4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-8mxf4 webserver-deployment-7b75d79cf5- deployment-7051  00dbd22d-72a5-434f-90b1-38590d2163a0 31572 0 2023-12-19 11:22:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0785a7ef-614d-4abe-8566-9386c6ca8926 0xc0049d8067 0xc0049d8068}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0785a7ef-614d-4abe-8566-9386c6ca8926\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fkx2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fkx2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.236,PodIP:,StartTime:2023-12-19 11:22:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.966: INFO: Pod "webserver-deployment-7b75d79cf5-dc5gd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-dc5gd webserver-deployment-7b75d79cf5- deployment-7051  99565280-9b69-4b17-ae9d-8b6279f056f5 31563 0 2023-12-19 11:22:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0785a7ef-614d-4abe-8566-9386c6ca8926 0xc0049d8257 0xc0049d8258}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0785a7ef-614d-4abe-8566-9386c6ca8926\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cmdhq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cmdhq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.966: INFO: Pod "webserver-deployment-7b75d79cf5-djhjc" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-djhjc webserver-deployment-7b75d79cf5- deployment-7051  e0ec96cf-259a-48ff-beea-cc976ecaa0b5 31527 0 2023-12-19 11:22:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0785a7ef-614d-4abe-8566-9386c6ca8926 0xc0049d83a7 0xc0049d83a8}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0785a7ef-614d-4abe-8566-9386c6ca8926\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:22:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kwqhj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kwqhj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.61,PodIP:,StartTime:2023-12-19 11:22:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.967: INFO: Pod "webserver-deployment-7b75d79cf5-g8v6m" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-g8v6m webserver-deployment-7b75d79cf5- deployment-7051  7a91acc3-800f-465e-bdb7-ac01b807116c 31575 0 2023-12-19 11:22:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0785a7ef-614d-4abe-8566-9386c6ca8926 0xc0049d8597 0xc0049d8598}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0785a7ef-614d-4abe-8566-9386c6ca8926\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fp5qq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fp5qq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.967: INFO: Pod "webserver-deployment-7b75d79cf5-hb75t" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-hb75t webserver-deployment-7b75d79cf5- deployment-7051  8663caee-6cf0-4bfd-b891-3c4088d1ce8c 31576 0 2023-12-19 11:22:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0785a7ef-614d-4abe-8566-9386c6ca8926 0xc0049d86e7 0xc0049d86e8}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0785a7ef-614d-4abe-8566-9386c6ca8926\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gfwr9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gfwr9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.967: INFO: Pod "webserver-deployment-7b75d79cf5-kzvwg" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-kzvwg webserver-deployment-7b75d79cf5- deployment-7051  e00a9e9b-2243-4660-a9bc-ca554c5fc9df 31530 0 2023-12-19 11:22:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0785a7ef-614d-4abe-8566-9386c6ca8926 0xc0049d8837 0xc0049d8838}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0785a7ef-614d-4abe-8566-9386c6ca8926\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:22:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r6kph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r6kph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.223,PodIP:,StartTime:2023-12-19 11:22:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.967: INFO: Pod "webserver-deployment-7b75d79cf5-sgs9w" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-sgs9w webserver-deployment-7b75d79cf5- deployment-7051  fb0e87c3-cae9-4572-9e86-27820d7b40a2 31500 0 2023-12-19 11:22:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0785a7ef-614d-4abe-8566-9386c6ca8926 0xc0049d8a27 0xc0049d8a28}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0785a7ef-614d-4abe-8566-9386c6ca8926\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:22:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vdfft,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vdfft,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.223,PodIP:,StartTime:2023-12-19 11:22:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.968: INFO: Pod "webserver-deployment-7b75d79cf5-vdbhb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-vdbhb webserver-deployment-7b75d79cf5- deployment-7051  1b4cb6da-9567-48d0-9b3e-0594ed4a73ab 31578 0 2023-12-19 11:22:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0785a7ef-614d-4abe-8566-9386c6ca8926 0xc0049d8c17 0xc0049d8c18}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0785a7ef-614d-4abe-8566-9386c6ca8926\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-thcmj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-thcmj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.968: INFO: Pod "webserver-deployment-7b75d79cf5-wwpzl" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-wwpzl webserver-deployment-7b75d79cf5- deployment-7051  b865fe38-eb76-40e7-9c32-e1e41a0795ff 31507 0 2023-12-19 11:22:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0785a7ef-614d-4abe-8566-9386c6ca8926 0xc0049d8d67 0xc0049d8d68}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0785a7ef-614d-4abe-8566-9386c6ca8926\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:22:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x595m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x595m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.236,PodIP:,StartTime:2023-12-19 11:22:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.968: INFO: Pod "webserver-deployment-7b75d79cf5-x8h8p" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-x8h8p webserver-deployment-7b75d79cf5- deployment-7051  dcf7bea2-1c1a-46f8-8733-f44b8d24148e 31562 0 2023-12-19 11:22:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0785a7ef-614d-4abe-8566-9386c6ca8926 0xc0049d8f57 0xc0049d8f58}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0785a7ef-614d-4abe-8566-9386c6ca8926\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8q99p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8q99p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.969: INFO: Pod "webserver-deployment-7b75d79cf5-xtff8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-xtff8 webserver-deployment-7b75d79cf5- deployment-7051  3806ed60-6581-4329-b4a0-7c1a2bb8fa4b 31497 0 2023-12-19 11:22:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0785a7ef-614d-4abe-8566-9386c6ca8926 0xc0049d90a7 0xc0049d90a8}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0785a7ef-614d-4abe-8566-9386c6ca8926\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:22:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l8d54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l8d54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:22:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.61,PodIP:,StartTime:2023-12-19 11:22:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.969: INFO: Pod "webserver-deployment-7b75d79cf5-zsqj4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-zsqj4 webserver-deployment-7b75d79cf5- deployment-7051  15eacc4a-4129-4d7d-a8fb-a2c87290defc 31573 0 2023-12-19 11:22:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 0785a7ef-614d-4abe-8566-9386c6ca8926 0xc0049d9297 0xc0049d9298}] [] [{kube-controller-manager Update v1 2023-12-19 11:22:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0785a7ef-614d-4abe-8566-9386c6ca8926\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9l26h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9l26h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:22:17.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7051" for this suite. @ 12/19/23 11:22:18.071
• [6.917 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 12/19/23 11:22:18.213
  Dec 19 11:22:18.213: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename svcaccounts @ 12/19/23 11:22:18.215
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:22:18.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:22:18.409
  Dec 19 11:22:18.463: INFO: Got root ca configmap in namespace "svcaccounts-7461"
  Dec 19 11:22:18.596: INFO: Deleted root ca configmap in namespace "svcaccounts-7461"
  STEP: waiting for a new root ca configmap created @ 12/19/23 11:22:19.099
  Dec 19 11:22:19.108: INFO: Recreated root ca configmap in namespace "svcaccounts-7461"
  Dec 19 11:22:19.138: INFO: Updated root ca configmap in namespace "svcaccounts-7461"
  STEP: waiting for the root ca configmap reconciled @ 12/19/23 11:22:19.639
  Dec 19 11:22:19.649: INFO: Reconciled root ca configmap in namespace "svcaccounts-7461"
  Dec 19 11:22:19.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7461" for this suite. @ 12/19/23 11:22:19.66
• [1.464 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 12/19/23 11:22:19.681
  Dec 19 11:22:19.681: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename var-expansion @ 12/19/23 11:22:19.683
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:22:19.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:22:19.722
  STEP: Creating a pod to test substitution in volume subpath @ 12/19/23 11:22:19.729
  STEP: Saw pod success @ 12/19/23 11:22:23.812
  Dec 19 11:22:23.830: INFO: Trying to get logs from node cahyeife7pae-3 pod var-expansion-861882ac-2d79-4b55-ae5d-9b0fe085e8ea container dapi-container: <nil>
  STEP: delete the pod @ 12/19/23 11:22:23.894
  Dec 19 11:22:24.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2785" for this suite. @ 12/19/23 11:22:24.065
• [4.509 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 12/19/23 11:22:24.204
  Dec 19 11:22:24.204: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename resourcequota @ 12/19/23 11:22:24.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:22:24.45
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:22:24.463
  STEP: Creating a ResourceQuota @ 12/19/23 11:22:24.471
  STEP: Getting a ResourceQuota @ 12/19/23 11:22:24.508
  STEP: Updating a ResourceQuota @ 12/19/23 11:22:24.572
  STEP: Verifying a ResourceQuota was modified @ 12/19/23 11:22:24.584
  STEP: Deleting a ResourceQuota @ 12/19/23 11:22:24.604
  STEP: Verifying the deleted ResourceQuota @ 12/19/23 11:22:24.626
  Dec 19 11:22:24.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4900" for this suite. @ 12/19/23 11:22:24.647
• [0.466 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 12/19/23 11:22:24.672
  Dec 19 11:22:24.672: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename downward-api @ 12/19/23 11:22:24.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:22:24.893
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:22:24.905
  STEP: Creating the pod @ 12/19/23 11:22:24.919
  Dec 19 11:22:27.546: INFO: Successfully updated pod "annotationupdate77e9e3f3-5a83-43af-ac91-5f019f92e314"
  Dec 19 11:22:29.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8298" for this suite. @ 12/19/23 11:22:29.595
• [4.936 seconds]
------------------------------
SS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 12/19/23 11:22:29.611
  Dec 19 11:22:29.611: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename job @ 12/19/23 11:22:29.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:22:29.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:22:29.667
  STEP: Creating a suspended job @ 12/19/23 11:22:29.682
  STEP: Patching the Job @ 12/19/23 11:22:29.695
  STEP: Watching for Job to be patched @ 12/19/23 11:22:29.737
  Dec 19 11:22:29.744: INFO: Event ADDED observed for Job e2e-45dm2 in namespace job-8536 with labels: map[e2e-job-label:e2e-45dm2] and annotations: map[batch.kubernetes.io/job-tracking:]
  Dec 19 11:22:29.744: INFO: Event MODIFIED observed for Job e2e-45dm2 in namespace job-8536 with labels: map[e2e-job-label:e2e-45dm2] and annotations: map[batch.kubernetes.io/job-tracking:]
  Dec 19 11:22:29.745: INFO: Event MODIFIED found for Job e2e-45dm2 in namespace job-8536 with labels: map[e2e-45dm2:patched e2e-job-label:e2e-45dm2] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 12/19/23 11:22:29.745
  STEP: Watching for Job to be updated @ 12/19/23 11:22:29.768
  Dec 19 11:22:29.772: INFO: Event MODIFIED found for Job e2e-45dm2 in namespace job-8536 with labels: map[e2e-45dm2:patched e2e-job-label:e2e-45dm2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Dec 19 11:22:29.772: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 12/19/23 11:22:29.772
  Dec 19 11:22:29.788: INFO: Job: e2e-45dm2 as labels: map[e2e-45dm2:patched e2e-job-label:e2e-45dm2]
  STEP: Waiting for job to complete @ 12/19/23 11:22:29.788
  STEP: Delete a job collection with a labelselector @ 12/19/23 11:22:37.799
  STEP: Watching for Job to be deleted @ 12/19/23 11:22:37.818
  Dec 19 11:22:37.824: INFO: Event MODIFIED observed for Job e2e-45dm2 in namespace job-8536 with labels: map[e2e-45dm2:patched e2e-job-label:e2e-45dm2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Dec 19 11:22:37.826: INFO: Event MODIFIED observed for Job e2e-45dm2 in namespace job-8536 with labels: map[e2e-45dm2:patched e2e-job-label:e2e-45dm2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Dec 19 11:22:37.826: INFO: Event MODIFIED observed for Job e2e-45dm2 in namespace job-8536 with labels: map[e2e-45dm2:patched e2e-job-label:e2e-45dm2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Dec 19 11:22:37.828: INFO: Event MODIFIED observed for Job e2e-45dm2 in namespace job-8536 with labels: map[e2e-45dm2:patched e2e-job-label:e2e-45dm2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Dec 19 11:22:37.828: INFO: Event MODIFIED observed for Job e2e-45dm2 in namespace job-8536 with labels: map[e2e-45dm2:patched e2e-job-label:e2e-45dm2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Dec 19 11:22:37.829: INFO: Event MODIFIED observed for Job e2e-45dm2 in namespace job-8536 with labels: map[e2e-45dm2:patched e2e-job-label:e2e-45dm2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Dec 19 11:22:37.830: INFO: Event MODIFIED observed for Job e2e-45dm2 in namespace job-8536 with labels: map[e2e-45dm2:patched e2e-job-label:e2e-45dm2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Dec 19 11:22:37.831: INFO: Event DELETED found for Job e2e-45dm2 in namespace job-8536 with labels: map[e2e-45dm2:patched e2e-job-label:e2e-45dm2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 12/19/23 11:22:37.831
  Dec 19 11:22:37.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8536" for this suite. @ 12/19/23 11:22:37.893
• [8.306 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 12/19/23 11:22:37.922
  Dec 19 11:22:37.923: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename sched-preemption @ 12/19/23 11:22:37.926
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:22:37.989
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:22:38.001
  Dec 19 11:22:38.036: INFO: Waiting up to 1m0s for all nodes to be ready
  Dec 19 11:23:38.093: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 12/19/23 11:23:38.103
  Dec 19 11:23:38.103: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename sched-preemption-path @ 12/19/23 11:23:38.106
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:23:38.15
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:23:38.155
  Dec 19 11:23:38.195: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Dec 19 11:23:38.203: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Dec 19 11:23:38.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 19 11:23:38.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-6636" for this suite. @ 12/19/23 11:23:38.421
  STEP: Destroying namespace "sched-preemption-9752" for this suite. @ 12/19/23 11:23:38.436
• [60.536 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 12/19/23 11:23:38.478
  Dec 19 11:23:38.478: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 11:23:38.481
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:23:38.515
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:23:38.52
  STEP: Creating configMap with name projected-configmap-test-volume-c1ba4453-a9ae-497e-aed9-ac33ba43b939 @ 12/19/23 11:23:38.526
  STEP: Creating a pod to test consume configMaps @ 12/19/23 11:23:38.538
  STEP: Saw pod success @ 12/19/23 11:23:42.61
  Dec 19 11:23:42.616: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-projected-configmaps-99559b6e-a597-49c5-b859-883e76cf107e container agnhost-container: <nil>
  STEP: delete the pod @ 12/19/23 11:23:42.633
  Dec 19 11:23:42.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6002" for this suite. @ 12/19/23 11:23:42.687
• [4.228 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 12/19/23 11:23:42.707
  Dec 19 11:23:42.707: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-probe @ 12/19/23 11:23:42.71
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:23:42.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:23:42.769
  Dec 19 11:24:42.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-3010" for this suite. @ 12/19/23 11:24:42.831
• [60.144 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 12/19/23 11:24:42.854
  Dec 19 11:24:42.854: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename services @ 12/19/23 11:24:42.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:24:42.928
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:24:42.936
  STEP: creating service nodeport-test with type=NodePort in namespace services-3713 @ 12/19/23 11:24:42.941
  STEP: creating replication controller nodeport-test in namespace services-3713 @ 12/19/23 11:24:42.984
  I1219 11:24:43.028340      13 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-3713, replica count: 2
  I1219 11:24:46.081117      13 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Dec 19 11:24:46.081: INFO: Creating new exec pod
  Dec 19 11:24:49.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-3713 exec execpod69kkm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Dec 19 11:24:49.598: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Dec 19 11:24:49.598: INFO: stdout: "nodeport-test-zblr7"
  Dec 19 11:24:49.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-3713 exec execpod69kkm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.58.94 80'
  Dec 19 11:24:49.891: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.58.94 80\nConnection to 10.233.58.94 80 port [tcp/http] succeeded!\n"
  Dec 19 11:24:49.891: INFO: stdout: "nodeport-test-zblr7"
  Dec 19 11:24:49.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-3713 exec execpod69kkm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.236 32594'
  Dec 19 11:24:50.195: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.236 32594\nConnection to 192.168.121.236 32594 port [tcp/*] succeeded!\n"
  Dec 19 11:24:50.195: INFO: stdout: ""
  Dec 19 11:24:51.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-3713 exec execpod69kkm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.236 32594'
  Dec 19 11:24:51.516: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.236 32594\nConnection to 192.168.121.236 32594 port [tcp/*] succeeded!\n"
  Dec 19 11:24:51.516: INFO: stdout: "nodeport-test-zblr7"
  Dec 19 11:24:51.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-3713 exec execpod69kkm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.223 32594'
  Dec 19 11:24:51.822: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.223 32594\nConnection to 192.168.121.223 32594 port [tcp/*] succeeded!\n"
  Dec 19 11:24:51.822: INFO: stdout: "nodeport-test-zblr7"
  Dec 19 11:24:51.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3713" for this suite. @ 12/19/23 11:24:51.836
• [8.999 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 12/19/23 11:24:51.861
  Dec 19 11:24:51.861: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename security-context-test @ 12/19/23 11:24:51.865
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:24:51.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:24:51.912
  Dec 19 11:24:55.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8857" for this suite. @ 12/19/23 11:24:56.001
• [4.158 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 12/19/23 11:24:56.031
  Dec 19 11:24:56.031: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-runtime @ 12/19/23 11:24:56.033
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:24:56.058
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:24:56.064
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 12/19/23 11:24:56.088
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 12/19/23 11:25:13.301
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 12/19/23 11:25:13.31
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 12/19/23 11:25:13.323
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 12/19/23 11:25:13.323
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 12/19/23 11:25:13.37
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 12/19/23 11:25:16.402
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 12/19/23 11:25:17.417
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 12/19/23 11:25:17.432
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 12/19/23 11:25:17.433
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 12/19/23 11:25:17.49
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 12/19/23 11:25:18.505
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 12/19/23 11:25:20.535
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 12/19/23 11:25:20.554
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 12/19/23 11:25:20.554
  Dec 19 11:25:20.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1269" for this suite. @ 12/19/23 11:25:20.628
• [24.612 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 12/19/23 11:25:20.646
  Dec 19 11:25:20.646: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename csiinlinevolumes @ 12/19/23 11:25:20.649
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:25:20.678
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:25:20.683
  STEP: creating @ 12/19/23 11:25:20.687
  STEP: getting @ 12/19/23 11:25:20.733
  STEP: listing in namespace @ 12/19/23 11:25:20.742
  STEP: patching @ 12/19/23 11:25:20.779
  STEP: deleting @ 12/19/23 11:25:20.802
  Dec 19 11:25:20.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-1316" for this suite. @ 12/19/23 11:25:20.839
• [0.210 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 12/19/23 11:25:20.862
  Dec 19 11:25:20.862: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename namespaces @ 12/19/23 11:25:20.864
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:25:20.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:25:20.903
  STEP: Creating namespace "e2e-ns-dvjn6" @ 12/19/23 11:25:20.916
  Dec 19 11:25:20.958: INFO: Namespace "e2e-ns-dvjn6-1466" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-dvjn6-1466" @ 12/19/23 11:25:20.959
  Dec 19 11:25:20.987: INFO: Namespace "e2e-ns-dvjn6-1466" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-dvjn6-1466" @ 12/19/23 11:25:20.988
  Dec 19 11:25:21.004: INFO: Namespace "e2e-ns-dvjn6-1466" has []v1.FinalizerName{"kubernetes"}
  Dec 19 11:25:21.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7197" for this suite. @ 12/19/23 11:25:21.017
  STEP: Destroying namespace "e2e-ns-dvjn6-1466" for this suite. @ 12/19/23 11:25:21.036
• [0.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 12/19/23 11:25:21.084
  Dec 19 11:25:21.085: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubectl @ 12/19/23 11:25:21.089
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:25:21.122
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:25:21.13
  STEP: creating a replication controller @ 12/19/23 11:25:21.137
  Dec 19 11:25:21.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-1082 create -f -'
  Dec 19 11:25:21.888: INFO: stderr: ""
  Dec 19 11:25:21.888: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 12/19/23 11:25:21.889
  Dec 19 11:25:21.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-1082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Dec 19 11:25:22.112: INFO: stderr: ""
  Dec 19 11:25:22.112: INFO: stdout: "update-demo-nautilus-6qxcj update-demo-nautilus-nkzlp "
  Dec 19 11:25:22.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-1082 get pods update-demo-nautilus-6qxcj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Dec 19 11:25:22.402: INFO: stderr: ""
  Dec 19 11:25:22.402: INFO: stdout: ""
  Dec 19 11:25:22.402: INFO: update-demo-nautilus-6qxcj is created but not running
  Dec 19 11:25:27.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-1082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Dec 19 11:25:27.645: INFO: stderr: ""
  Dec 19 11:25:27.645: INFO: stdout: "update-demo-nautilus-6qxcj update-demo-nautilus-nkzlp "
  Dec 19 11:25:27.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-1082 get pods update-demo-nautilus-6qxcj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Dec 19 11:25:27.853: INFO: stderr: ""
  Dec 19 11:25:27.853: INFO: stdout: ""
  Dec 19 11:25:27.853: INFO: update-demo-nautilus-6qxcj is created but not running
  Dec 19 11:25:32.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-1082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Dec 19 11:25:33.022: INFO: stderr: ""
  Dec 19 11:25:33.023: INFO: stdout: "update-demo-nautilus-6qxcj update-demo-nautilus-nkzlp "
  Dec 19 11:25:33.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-1082 get pods update-demo-nautilus-6qxcj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Dec 19 11:25:33.198: INFO: stderr: ""
  Dec 19 11:25:33.198: INFO: stdout: ""
  Dec 19 11:25:33.198: INFO: update-demo-nautilus-6qxcj is created but not running
  Dec 19 11:25:38.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-1082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Dec 19 11:25:38.419: INFO: stderr: ""
  Dec 19 11:25:38.419: INFO: stdout: "update-demo-nautilus-6qxcj update-demo-nautilus-nkzlp "
  Dec 19 11:25:38.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-1082 get pods update-demo-nautilus-6qxcj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Dec 19 11:25:38.572: INFO: stderr: ""
  Dec 19 11:25:38.572: INFO: stdout: ""
  Dec 19 11:25:38.572: INFO: update-demo-nautilus-6qxcj is created but not running
  Dec 19 11:25:43.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-1082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Dec 19 11:25:43.752: INFO: stderr: ""
  Dec 19 11:25:43.752: INFO: stdout: "update-demo-nautilus-6qxcj update-demo-nautilus-nkzlp "
  Dec 19 11:25:43.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-1082 get pods update-demo-nautilus-6qxcj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Dec 19 11:25:43.909: INFO: stderr: ""
  Dec 19 11:25:43.909: INFO: stdout: "true"
  Dec 19 11:25:43.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-1082 get pods update-demo-nautilus-6qxcj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Dec 19 11:25:44.066: INFO: stderr: ""
  Dec 19 11:25:44.066: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Dec 19 11:25:44.066: INFO: validating pod update-demo-nautilus-6qxcj
  Dec 19 11:25:44.087: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Dec 19 11:25:44.087: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Dec 19 11:25:44.087: INFO: update-demo-nautilus-6qxcj is verified up and running
  Dec 19 11:25:44.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-1082 get pods update-demo-nautilus-nkzlp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Dec 19 11:25:44.296: INFO: stderr: ""
  Dec 19 11:25:44.296: INFO: stdout: "true"
  Dec 19 11:25:44.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-1082 get pods update-demo-nautilus-nkzlp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Dec 19 11:25:44.474: INFO: stderr: ""
  Dec 19 11:25:44.474: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Dec 19 11:25:44.474: INFO: validating pod update-demo-nautilus-nkzlp
  Dec 19 11:25:59.796: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Dec 19 11:25:59.796: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Dec 19 11:25:59.796: INFO: update-demo-nautilus-nkzlp is verified up and running
  STEP: using delete to clean up resources @ 12/19/23 11:25:59.796
  Dec 19 11:25:59.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-1082 delete --grace-period=0 --force -f -'
  Dec 19 11:25:59.989: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Dec 19 11:25:59.989: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Dec 19 11:25:59.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-1082 get rc,svc -l name=update-demo --no-headers'
  Dec 19 11:26:00.283: INFO: stderr: "No resources found in kubectl-1082 namespace.\n"
  Dec 19 11:26:00.283: INFO: stdout: ""
  Dec 19 11:26:00.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-1082 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Dec 19 11:26:00.512: INFO: stderr: ""
  Dec 19 11:26:00.512: INFO: stdout: ""
  Dec 19 11:26:00.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1082" for this suite. @ 12/19/23 11:26:00.529
• [39.464 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 12/19/23 11:26:00.549
  Dec 19 11:26:00.549: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename field-validation @ 12/19/23 11:26:00.552
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:26:00.587
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:26:00.594
  Dec 19 11:26:00.602: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  W1219 11:26:03.506208      13 warnings.go:70] unknown field "alpha"
  W1219 11:26:03.506315      13 warnings.go:70] unknown field "beta"
  W1219 11:26:03.506329      13 warnings.go:70] unknown field "delta"
  W1219 11:26:03.506370      13 warnings.go:70] unknown field "epsilon"
  W1219 11:26:03.506399      13 warnings.go:70] unknown field "gamma"
  Dec 19 11:26:04.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7201" for this suite. @ 12/19/23 11:26:04.132
• [3.600 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:318
  STEP: Creating a kubernetes client @ 12/19/23 11:26:04.151
  Dec 19 11:26:04.151: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename statefulset @ 12/19/23 11:26:04.153
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:26:04.183
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:26:04.188
  STEP: Creating service test in namespace statefulset-5418 @ 12/19/23 11:26:04.196
  STEP: Creating a new StatefulSet @ 12/19/23 11:26:04.226
  Dec 19 11:26:04.260: INFO: Found 0 stateful pods, waiting for 3
  Dec 19 11:26:14.286: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Dec 19 11:26:14.287: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Dec 19 11:26:14.288: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Dec 19 11:26:14.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-5418 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Dec 19 11:26:14.723: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 19 11:26:14.723: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 19 11:26:14.723: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 12/19/23 11:26:24.778
  Dec 19 11:26:24.818: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 12/19/23 11:26:24.819
  STEP: Updating Pods in reverse ordinal order @ 12/19/23 11:26:34.875
  Dec 19 11:26:34.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-5418 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Dec 19 11:26:35.220: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Dec 19 11:26:35.220: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Dec 19 11:26:35.220: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  STEP: Rolling back to a previous revision @ 12/19/23 11:26:45.283
  Dec 19 11:26:45.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-5418 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Dec 19 11:26:45.602: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 19 11:26:45.603: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 19 11:26:45.603: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Dec 19 11:26:55.686: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 12/19/23 11:27:05.739
  Dec 19 11:27:05.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-5418 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Dec 19 11:27:06.060: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Dec 19 11:27:06.060: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Dec 19 11:27:06.060: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Dec 19 11:27:16.112: INFO: Deleting all statefulset in ns statefulset-5418
  Dec 19 11:27:16.124: INFO: Scaling statefulset ss2 to 0
  Dec 19 11:27:26.161: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 19 11:27:26.169: INFO: Deleting statefulset ss2
  Dec 19 11:27:26.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5418" for this suite. @ 12/19/23 11:27:26.216
• [82.088 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 12/19/23 11:27:26.239
  Dec 19 11:27:26.240: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename configmap @ 12/19/23 11:27:26.243
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:27:26.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:27:26.293
  STEP: Creating configMap with name configmap-test-volume-map-8b2c5f93-7f17-4c5c-928a-d672d7fc189f @ 12/19/23 11:27:26.301
  STEP: Creating a pod to test consume configMaps @ 12/19/23 11:27:26.308
  STEP: Saw pod success @ 12/19/23 11:27:30.383
  Dec 19 11:27:30.390: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-configmaps-84cc90c5-65d6-4805-aadd-58ee9b15dbc9 container agnhost-container: <nil>
  STEP: delete the pod @ 12/19/23 11:27:30.426
  Dec 19 11:27:30.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1042" for this suite. @ 12/19/23 11:27:30.47
• [4.246 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 12/19/23 11:27:30.495
  Dec 19 11:27:30.495: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename webhook @ 12/19/23 11:27:30.5
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:27:30.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:27:30.539
  STEP: Setting up server cert @ 12/19/23 11:27:30.596
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/19/23 11:27:31.757
  STEP: Deploying the webhook pod @ 12/19/23 11:27:31.779
  STEP: Wait for the deployment to be ready @ 12/19/23 11:27:31.819
  Dec 19 11:27:31.845: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 12/19/23 11:27:33.871
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 11:27:33.915
  Dec 19 11:27:34.916: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 12/19/23 11:27:34.923
  STEP: create a configmap that should be updated by the webhook @ 12/19/23 11:27:34.972
  Dec 19 11:27:34.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9118" for this suite. @ 12/19/23 11:27:35.132
  STEP: Destroying namespace "webhook-markers-5494" for this suite. @ 12/19/23 11:27:35.155
• [4.684 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 12/19/23 11:27:35.213
  Dec 19 11:27:35.214: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename deployment @ 12/19/23 11:27:35.219
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:27:35.262
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:27:35.266
  Dec 19 11:27:35.300: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  Dec 19 11:27:40.317: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 12/19/23 11:27:40.317
  Dec 19 11:27:40.317: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 12/19/23 11:27:40.372
  Dec 19 11:27:42.436: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6963  27951fd2-b0cc-4b6c-a387-707c53f6e191 33466 1 2023-12-19 11:27:40 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-12-19 11:27:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 11:27:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0019f6ad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-12-19 11:27:40 +0000 UTC,LastTransitionTime:2023-12-19 11:27:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-68b75d69f8" has successfully progressed.,LastUpdateTime:2023-12-19 11:27:42 +0000 UTC,LastTransitionTime:2023-12-19 11:27:40 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Dec 19 11:27:42.447: INFO: New ReplicaSet "test-cleanup-deployment-68b75d69f8" of Deployment "test-cleanup-deployment":
  &ReplicaSet{ObjectMeta:{test-cleanup-deployment-68b75d69f8  deployment-6963  84d318c1-6880-4eb4-bf27-debb967337f8 33456 1 2023-12-19 11:27:40 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 27951fd2-b0cc-4b6c-a387-707c53f6e191 0xc0019f6f37 0xc0019f6f38}] [] [{kube-controller-manager Update apps/v1 2023-12-19 11:27:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27951fd2-b0cc-4b6c-a387-707c53f6e191\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 11:27:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 68b75d69f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0019f6fe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Dec 19 11:27:42.455: INFO: Pod "test-cleanup-deployment-68b75d69f8-7vqd7" is available:
  &Pod{ObjectMeta:{test-cleanup-deployment-68b75d69f8-7vqd7 test-cleanup-deployment-68b75d69f8- deployment-6963  4e1335d2-a176-4d31-86c2-2d414a3e024e 33455 0 2023-12-19 11:27:40 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-68b75d69f8 84d318c1-6880-4eb4-bf27-debb967337f8 0xc003090757 0xc003090758}] [] [{kube-controller-manager Update v1 2023-12-19 11:27:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84d318c1-6880-4eb4-bf27-debb967337f8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:27:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mbld9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mbld9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:27:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:27:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:27:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:27:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.61,PodIP:10.233.66.150,StartTime:2023-12-19 11:27:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-19 11:27:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://c117b1053032ba571acb2974fad49d35002044e5db8892c1a1934fb0d0d894b9,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.150,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:27:42.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6963" for this suite. @ 12/19/23 11:27:42.466
• [7.268 seconds]
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 12/19/23 11:27:42.482
  Dec 19 11:27:42.482: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename cronjob @ 12/19/23 11:27:42.484
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:27:42.522
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:27:42.528
  STEP: Creating a suspended cronjob @ 12/19/23 11:27:42.532
  STEP: Ensuring no jobs are scheduled @ 12/19/23 11:27:42.543
  STEP: Ensuring no job exists by listing jobs explicitly @ 12/19/23 11:32:42.56
  STEP: Removing cronjob @ 12/19/23 11:32:42.569
  Dec 19 11:32:42.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2819" for this suite. @ 12/19/23 11:32:42.602
• [300.135 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 12/19/23 11:32:42.621
  Dec 19 11:32:42.621: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 11:32:42.624
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:32:42.657
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:32:42.663
  STEP: Creating configMap with name configmap-projected-all-test-volume-56706f06-c869-4894-a5cc-237da2e8bc92 @ 12/19/23 11:32:42.669
  STEP: Creating secret with name secret-projected-all-test-volume-b9c5bab9-1737-4cb9-bfc5-b198a1194c74 @ 12/19/23 11:32:42.68
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 12/19/23 11:32:42.691
  STEP: Saw pod success @ 12/19/23 11:32:46.744
  Dec 19 11:32:46.752: INFO: Trying to get logs from node cahyeife7pae-3 pod projected-volume-d7c2a8ec-8223-4379-9149-b41fa244a869 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 12/19/23 11:32:46.784
  Dec 19 11:32:46.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-97" for this suite. @ 12/19/23 11:32:46.823
• [4.214 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 12/19/23 11:32:46.841
  Dec 19 11:32:46.841: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename webhook @ 12/19/23 11:32:46.843
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:32:46.874
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:32:46.88
  STEP: Setting up server cert @ 12/19/23 11:32:46.932
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/19/23 11:32:48.489
  STEP: Deploying the webhook pod @ 12/19/23 11:32:48.506
  STEP: Wait for the deployment to be ready @ 12/19/23 11:32:48.535
  Dec 19 11:32:48.554: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 12/19/23 11:32:50.576
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 11:32:50.598
  Dec 19 11:32:51.598: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 12/19/23 11:32:51.606
  STEP: Registering slow webhook via the AdmissionRegistration API @ 12/19/23 11:32:51.606
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 12/19/23 11:32:51.643
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 12/19/23 11:32:52.667
  STEP: Registering slow webhook via the AdmissionRegistration API @ 12/19/23 11:32:52.667
  STEP: Having no error when timeout is longer than webhook latency @ 12/19/23 11:32:53.751
  STEP: Registering slow webhook via the AdmissionRegistration API @ 12/19/23 11:32:53.751
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 12/19/23 11:32:58.868
  STEP: Registering slow webhook via the AdmissionRegistration API @ 12/19/23 11:32:58.868
  Dec 19 11:33:03.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-130" for this suite. @ 12/19/23 11:33:04.115
  STEP: Destroying namespace "webhook-markers-1454" for this suite. @ 12/19/23 11:33:04.128
• [17.303 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 12/19/23 11:33:04.145
  Dec 19 11:33:04.145: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename emptydir @ 12/19/23 11:33:04.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:33:04.186
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:33:04.192
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 12/19/23 11:33:04.204
  STEP: Saw pod success @ 12/19/23 11:33:06.24
  Dec 19 11:33:06.247: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-99b5f0c0-edca-490e-9819-1ae2dbb545ef container test-container: <nil>
  STEP: delete the pod @ 12/19/23 11:33:06.263
  Dec 19 11:33:06.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6236" for this suite. @ 12/19/23 11:33:06.316
• [2.187 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 12/19/23 11:33:06.342
  Dec 19 11:33:06.342: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename events @ 12/19/23 11:33:06.345
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:33:06.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:33:06.394
  STEP: creating a test event @ 12/19/23 11:33:06.403
  STEP: listing all events in all namespaces @ 12/19/23 11:33:06.412
  STEP: patching the test event @ 12/19/23 11:33:06.421
  STEP: fetching the test event @ 12/19/23 11:33:06.438
  STEP: updating the test event @ 12/19/23 11:33:06.448
  STEP: getting the test event @ 12/19/23 11:33:06.476
  STEP: deleting the test event @ 12/19/23 11:33:06.487
  STEP: listing all events in all namespaces @ 12/19/23 11:33:06.504
  Dec 19 11:33:06.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-9952" for this suite. @ 12/19/23 11:33:06.525
• [0.197 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 12/19/23 11:33:06.544
  Dec 19 11:33:06.544: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename dns @ 12/19/23 11:33:06.546
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:33:06.584
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:33:06.589
  STEP: Creating a test externalName service @ 12/19/23 11:33:06.594
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4053.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4053.svc.cluster.local; sleep 1; done
   @ 12/19/23 11:33:06.604
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4053.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4053.svc.cluster.local; sleep 1; done
   @ 12/19/23 11:33:06.605
  STEP: creating a pod to probe DNS @ 12/19/23 11:33:06.605
  STEP: submitting the pod to kubernetes @ 12/19/23 11:33:06.606
  STEP: retrieving the pod @ 12/19/23 11:33:08.644
  STEP: looking for the results for each expected name from probers @ 12/19/23 11:33:08.651
  Dec 19 11:33:08.672: INFO: DNS probes using dns-test-085f7829-a5fe-47e7-bb04-e75020971cc4 succeeded

  STEP: changing the externalName to bar.example.com @ 12/19/23 11:33:08.672
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4053.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4053.svc.cluster.local; sleep 1; done
   @ 12/19/23 11:33:08.694
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4053.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4053.svc.cluster.local; sleep 1; done
   @ 12/19/23 11:33:08.694
  STEP: creating a second pod to probe DNS @ 12/19/23 11:33:08.694
  STEP: submitting the pod to kubernetes @ 12/19/23 11:33:08.694
  STEP: retrieving the pod @ 12/19/23 11:33:12.759
  STEP: looking for the results for each expected name from probers @ 12/19/23 11:33:12.767
  Dec 19 11:33:12.780: INFO: File wheezy_udp@dns-test-service-3.dns-4053.svc.cluster.local from pod  dns-4053/dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 19 11:33:12.791: INFO: File jessie_udp@dns-test-service-3.dns-4053.svc.cluster.local from pod  dns-4053/dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 19 11:33:12.793: INFO: Lookups using dns-4053/dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 failed for: [wheezy_udp@dns-test-service-3.dns-4053.svc.cluster.local jessie_udp@dns-test-service-3.dns-4053.svc.cluster.local]

  Dec 19 11:33:17.806: INFO: File wheezy_udp@dns-test-service-3.dns-4053.svc.cluster.local from pod  dns-4053/dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 19 11:33:17.816: INFO: File jessie_udp@dns-test-service-3.dns-4053.svc.cluster.local from pod  dns-4053/dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 19 11:33:17.816: INFO: Lookups using dns-4053/dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 failed for: [wheezy_udp@dns-test-service-3.dns-4053.svc.cluster.local jessie_udp@dns-test-service-3.dns-4053.svc.cluster.local]

  Dec 19 11:33:22.809: INFO: File wheezy_udp@dns-test-service-3.dns-4053.svc.cluster.local from pod  dns-4053/dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 19 11:33:22.818: INFO: File jessie_udp@dns-test-service-3.dns-4053.svc.cluster.local from pod  dns-4053/dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 19 11:33:22.818: INFO: Lookups using dns-4053/dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 failed for: [wheezy_udp@dns-test-service-3.dns-4053.svc.cluster.local jessie_udp@dns-test-service-3.dns-4053.svc.cluster.local]

  Dec 19 11:33:27.811: INFO: File wheezy_udp@dns-test-service-3.dns-4053.svc.cluster.local from pod  dns-4053/dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 19 11:33:27.821: INFO: File jessie_udp@dns-test-service-3.dns-4053.svc.cluster.local from pod  dns-4053/dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 19 11:33:27.821: INFO: Lookups using dns-4053/dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 failed for: [wheezy_udp@dns-test-service-3.dns-4053.svc.cluster.local jessie_udp@dns-test-service-3.dns-4053.svc.cluster.local]

  Dec 19 11:33:32.807: INFO: File wheezy_udp@dns-test-service-3.dns-4053.svc.cluster.local from pod  dns-4053/dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 19 11:33:32.819: INFO: File jessie_udp@dns-test-service-3.dns-4053.svc.cluster.local from pod  dns-4053/dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 19 11:33:32.819: INFO: Lookups using dns-4053/dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 failed for: [wheezy_udp@dns-test-service-3.dns-4053.svc.cluster.local jessie_udp@dns-test-service-3.dns-4053.svc.cluster.local]

  Dec 19 11:33:37.806: INFO: File wheezy_udp@dns-test-service-3.dns-4053.svc.cluster.local from pod  dns-4053/dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 19 11:33:37.812: INFO: File jessie_udp@dns-test-service-3.dns-4053.svc.cluster.local from pod  dns-4053/dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Dec 19 11:33:37.812: INFO: Lookups using dns-4053/dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 failed for: [wheezy_udp@dns-test-service-3.dns-4053.svc.cluster.local jessie_udp@dns-test-service-3.dns-4053.svc.cluster.local]

  Dec 19 11:33:42.820: INFO: DNS probes using dns-test-0e917958-0ccc-4c57-98b8-38708711f9d3 succeeded

  STEP: changing the service to type=ClusterIP @ 12/19/23 11:33:42.82
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4053.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4053.svc.cluster.local; sleep 1; done
   @ 12/19/23 11:33:42.858
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4053.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4053.svc.cluster.local; sleep 1; done
   @ 12/19/23 11:33:42.858
  STEP: creating a third pod to probe DNS @ 12/19/23 11:33:42.858
  STEP: submitting the pod to kubernetes @ 12/19/23 11:33:42.866
  STEP: retrieving the pod @ 12/19/23 11:34:17.089
  STEP: looking for the results for each expected name from probers @ 12/19/23 11:34:17.097
  Dec 19 11:34:17.128: INFO: DNS probes using dns-test-08e9d8c9-6d13-4201-b2f0-431a63a94b15 succeeded

  Dec 19 11:34:17.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/19/23 11:34:17.165
  STEP: deleting the pod @ 12/19/23 11:34:17.218
  STEP: deleting the pod @ 12/19/23 11:34:17.263
  STEP: deleting the test externalName service @ 12/19/23 11:34:17.31
  STEP: Destroying namespace "dns-4053" for this suite. @ 12/19/23 11:34:17.363
• [70.834 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 12/19/23 11:34:17.38
  Dec 19 11:34:17.380: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename runtimeclass @ 12/19/23 11:34:17.387
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:34:17.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:34:17.451
  STEP: Deleting RuntimeClass runtimeclass-4538-delete-me @ 12/19/23 11:34:17.47
  STEP: Waiting for the RuntimeClass to disappear @ 12/19/23 11:34:17.495
  Dec 19 11:34:17.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-4538" for this suite. @ 12/19/23 11:34:17.6
• [0.253 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 12/19/23 11:34:17.635
  Dec 19 11:34:17.635: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename webhook @ 12/19/23 11:34:17.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:34:17.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:34:17.703
  STEP: Setting up server cert @ 12/19/23 11:34:17.757
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/19/23 11:34:18.815
  STEP: Deploying the webhook pod @ 12/19/23 11:34:18.834
  STEP: Wait for the deployment to be ready @ 12/19/23 11:34:18.859
  Dec 19 11:34:18.876: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 12/19/23 11:34:20.896
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 11:34:20.914
  Dec 19 11:34:21.914: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 12/19/23 11:34:22.073
  STEP: Creating a configMap that should be mutated @ 12/19/23 11:34:22.122
  STEP: Deleting the collection of validation webhooks @ 12/19/23 11:34:22.197
  STEP: Creating a configMap that should not be mutated @ 12/19/23 11:34:22.295
  Dec 19 11:34:22.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7722" for this suite. @ 12/19/23 11:34:22.598
  STEP: Destroying namespace "webhook-markers-9034" for this suite. @ 12/19/23 11:34:22.616
• [5.008 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 12/19/23 11:34:22.651
  Dec 19 11:34:22.651: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename discovery @ 12/19/23 11:34:22.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:34:22.689
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:34:22.704
  STEP: Setting up server cert @ 12/19/23 11:34:22.715
  Dec 19 11:34:24.063: INFO: Checking APIGroup: apiregistration.k8s.io
  Dec 19 11:34:24.065: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Dec 19 11:34:24.065: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Dec 19 11:34:24.065: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Dec 19 11:34:24.065: INFO: Checking APIGroup: apps
  Dec 19 11:34:24.067: INFO: PreferredVersion.GroupVersion: apps/v1
  Dec 19 11:34:24.067: INFO: Versions found [{apps/v1 v1}]
  Dec 19 11:34:24.067: INFO: apps/v1 matches apps/v1
  Dec 19 11:34:24.067: INFO: Checking APIGroup: events.k8s.io
  Dec 19 11:34:24.069: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Dec 19 11:34:24.069: INFO: Versions found [{events.k8s.io/v1 v1}]
  Dec 19 11:34:24.069: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Dec 19 11:34:24.070: INFO: Checking APIGroup: authentication.k8s.io
  Dec 19 11:34:24.073: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Dec 19 11:34:24.073: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Dec 19 11:34:24.073: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Dec 19 11:34:24.073: INFO: Checking APIGroup: authorization.k8s.io
  Dec 19 11:34:24.075: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Dec 19 11:34:24.075: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Dec 19 11:34:24.075: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Dec 19 11:34:24.075: INFO: Checking APIGroup: autoscaling
  Dec 19 11:34:24.077: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Dec 19 11:34:24.077: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Dec 19 11:34:24.077: INFO: autoscaling/v2 matches autoscaling/v2
  Dec 19 11:34:24.078: INFO: Checking APIGroup: batch
  Dec 19 11:34:24.079: INFO: PreferredVersion.GroupVersion: batch/v1
  Dec 19 11:34:24.079: INFO: Versions found [{batch/v1 v1}]
  Dec 19 11:34:24.079: INFO: batch/v1 matches batch/v1
  Dec 19 11:34:24.079: INFO: Checking APIGroup: certificates.k8s.io
  Dec 19 11:34:24.081: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Dec 19 11:34:24.081: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Dec 19 11:34:24.081: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Dec 19 11:34:24.082: INFO: Checking APIGroup: networking.k8s.io
  Dec 19 11:34:24.083: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Dec 19 11:34:24.083: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Dec 19 11:34:24.083: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Dec 19 11:34:24.083: INFO: Checking APIGroup: policy
  Dec 19 11:34:24.085: INFO: PreferredVersion.GroupVersion: policy/v1
  Dec 19 11:34:24.085: INFO: Versions found [{policy/v1 v1}]
  Dec 19 11:34:24.085: INFO: policy/v1 matches policy/v1
  Dec 19 11:34:24.085: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Dec 19 11:34:24.087: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Dec 19 11:34:24.087: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Dec 19 11:34:24.087: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Dec 19 11:34:24.087: INFO: Checking APIGroup: storage.k8s.io
  Dec 19 11:34:24.089: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Dec 19 11:34:24.089: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Dec 19 11:34:24.089: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Dec 19 11:34:24.090: INFO: Checking APIGroup: admissionregistration.k8s.io
  Dec 19 11:34:24.092: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Dec 19 11:34:24.092: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Dec 19 11:34:24.092: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Dec 19 11:34:24.092: INFO: Checking APIGroup: apiextensions.k8s.io
  Dec 19 11:34:24.094: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Dec 19 11:34:24.094: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Dec 19 11:34:24.094: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Dec 19 11:34:24.094: INFO: Checking APIGroup: scheduling.k8s.io
  Dec 19 11:34:24.096: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Dec 19 11:34:24.096: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Dec 19 11:34:24.096: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Dec 19 11:34:24.096: INFO: Checking APIGroup: coordination.k8s.io
  Dec 19 11:34:24.098: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Dec 19 11:34:24.098: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Dec 19 11:34:24.098: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Dec 19 11:34:24.099: INFO: Checking APIGroup: node.k8s.io
  Dec 19 11:34:24.100: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Dec 19 11:34:24.100: INFO: Versions found [{node.k8s.io/v1 v1}]
  Dec 19 11:34:24.100: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Dec 19 11:34:24.100: INFO: Checking APIGroup: discovery.k8s.io
  Dec 19 11:34:24.102: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Dec 19 11:34:24.102: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Dec 19 11:34:24.102: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Dec 19 11:34:24.102: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Dec 19 11:34:24.104: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Dec 19 11:34:24.104: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Dec 19 11:34:24.104: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Dec 19 11:34:24.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-1462" for this suite. @ 12/19/23 11:34:24.114
• [1.475 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 12/19/23 11:34:24.14
  Dec 19 11:34:24.140: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename taint-multiple-pods @ 12/19/23 11:34:24.142
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:34:24.171
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:34:24.176
  Dec 19 11:34:24.181: INFO: Waiting up to 1m0s for all nodes to be ready
  Dec 19 11:35:24.220: INFO: Waiting for terminating namespaces to be deleted...
  Dec 19 11:35:24.227: INFO: Starting informer...
  STEP: Starting pods... @ 12/19/23 11:35:24.228
  Dec 19 11:35:24.471: INFO: Pod1 is running on cahyeife7pae-3. Tainting Node
  Dec 19 11:35:26.722: INFO: Pod2 is running on cahyeife7pae-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 12/19/23 11:35:26.722
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 12/19/23 11:35:26.767
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 12/19/23 11:35:26.776
  Dec 19 11:35:32.780: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  Dec 19 11:35:52.852: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Dec 19 11:35:52.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 12/19/23 11:35:52.9
  STEP: Destroying namespace "taint-multiple-pods-9260" for this suite. @ 12/19/23 11:35:52.911
• [88.790 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 12/19/23 11:35:52.932
  Dec 19 11:35:52.932: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename emptydir @ 12/19/23 11:35:52.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:35:52.999
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:35:53.014
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 12/19/23 11:35:53.041
  STEP: Saw pod success @ 12/19/23 11:35:57.077
  Dec 19 11:35:57.084: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-0d327874-25ee-47b4-b4ef-cfaafb9b7396 container test-container: <nil>
  STEP: delete the pod @ 12/19/23 11:35:57.118
  Dec 19 11:35:57.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5920" for this suite. @ 12/19/23 11:35:57.158
• [4.238 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 12/19/23 11:35:57.185
  Dec 19 11:35:57.186: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 11:35:57.188
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:35:57.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:35:57.224
  STEP: Creating configMap with name projected-configmap-test-volume-map-62a2d591-3536-4fe0-a8dc-11a0d1f3ec1f @ 12/19/23 11:35:57.229
  STEP: Creating a pod to test consume configMaps @ 12/19/23 11:35:57.238
  STEP: Saw pod success @ 12/19/23 11:36:01.275
  Dec 19 11:36:01.282: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-projected-configmaps-e7e5ae29-7877-499f-8336-21a0e8abaf5c container agnhost-container: <nil>
  STEP: delete the pod @ 12/19/23 11:36:01.295
  Dec 19 11:36:01.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-314" for this suite. @ 12/19/23 11:36:01.334
• [4.161 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 12/19/23 11:36:01.362
  Dec 19 11:36:01.362: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename crd-publish-openapi @ 12/19/23 11:36:01.372
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:36:01.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:36:01.426
  Dec 19 11:36:01.440: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 12/19/23 11:36:03.461
  Dec 19 11:36:03.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-1704 --namespace=crd-publish-openapi-1704 create -f -'
  Dec 19 11:36:05.053: INFO: stderr: ""
  Dec 19 11:36:05.053: INFO: stdout: "e2e-test-crd-publish-openapi-5790-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Dec 19 11:36:05.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-1704 --namespace=crd-publish-openapi-1704 delete e2e-test-crd-publish-openapi-5790-crds test-cr'
  Dec 19 11:36:05.219: INFO: stderr: ""
  Dec 19 11:36:05.220: INFO: stdout: "e2e-test-crd-publish-openapi-5790-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Dec 19 11:36:05.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-1704 --namespace=crd-publish-openapi-1704 apply -f -'
  Dec 19 11:36:06.483: INFO: stderr: ""
  Dec 19 11:36:06.483: INFO: stdout: "e2e-test-crd-publish-openapi-5790-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Dec 19 11:36:06.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-1704 --namespace=crd-publish-openapi-1704 delete e2e-test-crd-publish-openapi-5790-crds test-cr'
  Dec 19 11:36:06.713: INFO: stderr: ""
  Dec 19 11:36:06.713: INFO: stdout: "e2e-test-crd-publish-openapi-5790-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 12/19/23 11:36:06.713
  Dec 19 11:36:06.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=crd-publish-openapi-1704 explain e2e-test-crd-publish-openapi-5790-crds'
  Dec 19 11:36:07.214: INFO: stderr: ""
  Dec 19 11:36:07.214: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-5790-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Dec 19 11:36:09.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1704" for this suite. @ 12/19/23 11:36:09.035
• [7.684 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 12/19/23 11:36:09.048
  Dec 19 11:36:09.048: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename sysctl @ 12/19/23 11:36:09.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:36:09.078
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:36:09.083
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 12/19/23 11:36:09.089
  STEP: Watching for error events or started pod @ 12/19/23 11:36:09.106
  STEP: Waiting for pod completion @ 12/19/23 11:36:11.115
  STEP: Checking that the pod succeeded @ 12/19/23 11:36:11.131
  STEP: Getting logs from the pod @ 12/19/23 11:36:11.132
  STEP: Checking that the sysctl is actually updated @ 12/19/23 11:36:11.144
  Dec 19 11:36:11.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-3912" for this suite. @ 12/19/23 11:36:11.155
• [2.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 12/19/23 11:36:11.174
  Dec 19 11:36:11.174: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename emptydir @ 12/19/23 11:36:11.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:36:11.218
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:36:11.222
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 12/19/23 11:36:11.227
  STEP: Saw pod success @ 12/19/23 11:36:15.275
  Dec 19 11:36:15.284: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-7a1178db-c3f1-4ca1-ae1f-71591fc0d9b4 container test-container: <nil>
  STEP: delete the pod @ 12/19/23 11:36:15.298
  Dec 19 11:36:15.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5040" for this suite. @ 12/19/23 11:36:15.336
• [4.176 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 12/19/23 11:36:15.354
  Dec 19 11:36:15.354: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename var-expansion @ 12/19/23 11:36:15.356
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:36:15.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:36:15.44
  Dec 19 11:36:17.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 19 11:36:17.487: INFO: Deleting pod "var-expansion-c0bb12b6-93ba-4408-a825-2d7230f9e221" in namespace "var-expansion-520"
  Dec 19 11:36:17.516: INFO: Wait up to 5m0s for pod "var-expansion-c0bb12b6-93ba-4408-a825-2d7230f9e221" to be fully deleted
  STEP: Destroying namespace "var-expansion-520" for this suite. @ 12/19/23 11:36:19.565
• [4.226 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 12/19/23 11:36:19.594
  Dec 19 11:36:19.595: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename downward-api @ 12/19/23 11:36:19.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:36:19.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:36:19.663
  STEP: Creating a pod to test downward API volume plugin @ 12/19/23 11:36:19.671
  STEP: Saw pod success @ 12/19/23 11:36:23.732
  Dec 19 11:36:23.743: INFO: Trying to get logs from node cahyeife7pae-3 pod downwardapi-volume-ee5d1e7a-bfc9-439b-ac06-b3cf58450e6d container client-container: <nil>
  STEP: delete the pod @ 12/19/23 11:36:23.761
  Dec 19 11:36:23.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8781" for this suite. @ 12/19/23 11:36:23.803
• [4.229 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 12/19/23 11:36:23.845
  Dec 19 11:36:23.845: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename ingress @ 12/19/23 11:36:23.849
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:36:23.889
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:36:23.895
  STEP: getting /apis @ 12/19/23 11:36:23.902
  STEP: getting /apis/networking.k8s.io @ 12/19/23 11:36:23.912
  STEP: getting /apis/networking.k8s.iov1 @ 12/19/23 11:36:23.915
  STEP: creating @ 12/19/23 11:36:23.917
  STEP: getting @ 12/19/23 11:36:23.972
  STEP: listing @ 12/19/23 11:36:23.98
  STEP: watching @ 12/19/23 11:36:23.989
  Dec 19 11:36:23.990: INFO: starting watch
  STEP: cluster-wide listing @ 12/19/23 11:36:23.992
  STEP: cluster-wide watching @ 12/19/23 11:36:24.006
  Dec 19 11:36:24.006: INFO: starting watch
  STEP: patching @ 12/19/23 11:36:24.014
  STEP: updating @ 12/19/23 11:36:24.044
  Dec 19 11:36:24.069: INFO: waiting for watch events with expected annotations
  Dec 19 11:36:24.069: INFO: saw patched and updated annotations
  STEP: patching /status @ 12/19/23 11:36:24.069
  STEP: updating /status @ 12/19/23 11:36:24.083
  STEP: get /status @ 12/19/23 11:36:24.112
  STEP: deleting @ 12/19/23 11:36:24.125
  STEP: deleting a collection @ 12/19/23 11:36:24.162
  Dec 19 11:36:24.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-2396" for this suite. @ 12/19/23 11:36:24.23
• [0.400 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 12/19/23 11:36:24.249
  Dec 19 11:36:24.249: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-probe @ 12/19/23 11:36:24.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:36:24.284
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:36:24.289
  STEP: Creating pod test-webserver-513df569-2981-41a7-a529-43d6432cecfb in namespace container-probe-9297 @ 12/19/23 11:36:24.294
  Dec 19 11:36:26.328: INFO: Started pod test-webserver-513df569-2981-41a7-a529-43d6432cecfb in namespace container-probe-9297
  STEP: checking the pod's current state and verifying that restartCount is present @ 12/19/23 11:36:26.329
  Dec 19 11:36:26.338: INFO: Initial restart count of pod test-webserver-513df569-2981-41a7-a529-43d6432cecfb is 0
  Dec 19 11:40:27.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/19/23 11:40:27.684
  STEP: Destroying namespace "container-probe-9297" for this suite. @ 12/19/23 11:40:27.714
• [243.489 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 12/19/23 11:40:27.744
  Dec 19 11:40:27.744: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-probe @ 12/19/23 11:40:27.749
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:40:27.834
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:40:27.848
  STEP: Creating pod liveness-9a83221c-270f-4a4a-900d-157b63c29d00 in namespace container-probe-4616 @ 12/19/23 11:40:27.864
  Dec 19 11:40:29.943: INFO: Started pod liveness-9a83221c-270f-4a4a-900d-157b63c29d00 in namespace container-probe-4616
  STEP: checking the pod's current state and verifying that restartCount is present @ 12/19/23 11:40:29.943
  Dec 19 11:40:29.952: INFO: Initial restart count of pod liveness-9a83221c-270f-4a4a-900d-157b63c29d00 is 0
  Dec 19 11:40:50.063: INFO: Restart count of pod container-probe-4616/liveness-9a83221c-270f-4a4a-900d-157b63c29d00 is now 1 (20.111531801s elapsed)
  Dec 19 11:40:50.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/19/23 11:40:50.075
  STEP: Destroying namespace "container-probe-4616" for this suite. @ 12/19/23 11:40:50.114
• [22.399 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 12/19/23 11:40:50.154
  Dec 19 11:40:50.155: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename services @ 12/19/23 11:40:50.158
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:40:50.206
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:40:50.213
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-7839 @ 12/19/23 11:40:50.22
  STEP: changing the ExternalName service to type=NodePort @ 12/19/23 11:40:50.232
  STEP: creating replication controller externalname-service in namespace services-7839 @ 12/19/23 11:40:50.282
  I1219 11:40:50.306124      13 runners.go:194] Created replication controller with name: externalname-service, namespace: services-7839, replica count: 2
  I1219 11:40:53.359342      13 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Dec 19 11:40:53.359: INFO: Creating new exec pod
  Dec 19 11:40:56.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7839 exec execpod6psbd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Dec 19 11:40:56.774: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Dec 19 11:40:56.774: INFO: stdout: "externalname-service-sfp2l"
  Dec 19 11:40:56.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7839 exec execpod6psbd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.32.160 80'
  Dec 19 11:40:57.049: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.32.160 80\nConnection to 10.233.32.160 80 port [tcp/http] succeeded!\n"
  Dec 19 11:40:57.049: INFO: stdout: "externalname-service-sfp2l"
  Dec 19 11:40:57.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7839 exec execpod6psbd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.61 30912'
  Dec 19 11:40:57.318: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.61 30912\nConnection to 192.168.121.61 30912 port [tcp/*] succeeded!\n"
  Dec 19 11:40:57.318: INFO: stdout: "externalname-service-sfp2l"
  Dec 19 11:40:57.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7839 exec execpod6psbd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.223 30912'
  Dec 19 11:40:57.651: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.223 30912\nConnection to 192.168.121.223 30912 port [tcp/*] succeeded!\n"
  Dec 19 11:40:57.651: INFO: stdout: ""
  Dec 19 11:40:58.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=services-7839 exec execpod6psbd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.223 30912'
  Dec 19 11:40:58.968: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.223 30912\nConnection to 192.168.121.223 30912 port [tcp/*] succeeded!\n"
  Dec 19 11:40:58.968: INFO: stdout: "externalname-service-sfp2l"
  Dec 19 11:40:58.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Dec 19 11:40:58.980: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-7839" for this suite. @ 12/19/23 11:40:59.044
• [8.915 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 12/19/23 11:40:59.073
  Dec 19 11:40:59.073: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename resourcequota @ 12/19/23 11:40:59.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:40:59.117
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:40:59.124
  STEP: Creating a ResourceQuota with terminating scope @ 12/19/23 11:40:59.129
  STEP: Ensuring ResourceQuota status is calculated @ 12/19/23 11:40:59.147
  STEP: Creating a ResourceQuota with not terminating scope @ 12/19/23 11:41:01.157
  STEP: Ensuring ResourceQuota status is calculated @ 12/19/23 11:41:01.171
  STEP: Creating a long running pod @ 12/19/23 11:41:03.179
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 12/19/23 11:41:03.211
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 12/19/23 11:41:05.219
  STEP: Deleting the pod @ 12/19/23 11:41:07.226
  STEP: Ensuring resource quota status released the pod usage @ 12/19/23 11:41:07.255
  STEP: Creating a terminating pod @ 12/19/23 11:41:09.264
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 12/19/23 11:41:09.286
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 12/19/23 11:41:11.301
  STEP: Deleting the pod @ 12/19/23 11:41:13.313
  STEP: Ensuring resource quota status released the pod usage @ 12/19/23 11:41:13.339
  Dec 19 11:41:15.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4320" for this suite. @ 12/19/23 11:41:15.379
• [16.324 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 12/19/23 11:41:15.405
  Dec 19 11:41:15.406: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename security-context @ 12/19/23 11:41:15.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:41:15.45
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:41:15.458
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 12/19/23 11:41:15.466
  STEP: Saw pod success @ 12/19/23 11:41:19.536
  Dec 19 11:41:19.545: INFO: Trying to get logs from node cahyeife7pae-3 pod security-context-032b6c9f-9cee-402d-842e-dd3904fe128d container test-container: <nil>
  STEP: delete the pod @ 12/19/23 11:41:19.588
  Dec 19 11:41:19.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-7244" for this suite. @ 12/19/23 11:41:19.667
• [4.281 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 12/19/23 11:41:19.698
  Dec 19 11:41:19.698: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename downward-api @ 12/19/23 11:41:19.7
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:41:19.747
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:41:19.758
  STEP: Creating a pod to test downward API volume plugin @ 12/19/23 11:41:19.767
  STEP: Saw pod success @ 12/19/23 11:41:23.819
  Dec 19 11:41:23.826: INFO: Trying to get logs from node cahyeife7pae-3 pod downwardapi-volume-ef4ae8fc-9b37-436f-a1a5-d5e91b14cadd container client-container: <nil>
  STEP: delete the pod @ 12/19/23 11:41:23.84
  Dec 19 11:41:23.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4722" for this suite. @ 12/19/23 11:41:23.896
• [4.212 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 12/19/23 11:41:23.91
  Dec 19 11:41:23.910: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubelet-test @ 12/19/23 11:41:23.913
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:41:23.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:41:23.951
  STEP: Waiting for pod completion @ 12/19/23 11:41:23.971
  Dec 19 11:41:28.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4797" for this suite. @ 12/19/23 11:41:28.035
• [4.140 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 12/19/23 11:41:28.058
  Dec 19 11:41:28.058: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename replication-controller @ 12/19/23 11:41:28.06
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:41:28.099
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:41:28.104
  STEP: creating a ReplicationController @ 12/19/23 11:41:28.116
  STEP: waiting for RC to be added @ 12/19/23 11:41:28.127
  STEP: waiting for available Replicas @ 12/19/23 11:41:28.128
  STEP: patching ReplicationController @ 12/19/23 11:41:29.196
  STEP: waiting for RC to be modified @ 12/19/23 11:41:29.297
  STEP: patching ReplicationController status @ 12/19/23 11:41:29.297
  STEP: waiting for RC to be modified @ 12/19/23 11:41:29.337
  STEP: waiting for available Replicas @ 12/19/23 11:41:29.352
  STEP: fetching ReplicationController status @ 12/19/23 11:41:29.372
  STEP: patching ReplicationController scale @ 12/19/23 11:41:29.385
  STEP: waiting for RC to be modified @ 12/19/23 11:41:29.411
  STEP: waiting for ReplicationController's scale to be the max amount @ 12/19/23 11:41:29.422
  STEP: fetching ReplicationController; ensuring that it's patched @ 12/19/23 11:41:30.863
  STEP: updating ReplicationController status @ 12/19/23 11:41:30.874
  STEP: waiting for RC to be modified @ 12/19/23 11:41:30.894
  STEP: listing all ReplicationControllers @ 12/19/23 11:41:30.894
  STEP: checking that ReplicationController has expected values @ 12/19/23 11:41:30.902
  STEP: deleting ReplicationControllers by collection @ 12/19/23 11:41:30.902
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 12/19/23 11:41:30.933
  Dec 19 11:41:31.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E1219 11:41:31.076926      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-8103" for this suite. @ 12/19/23 11:41:31.088
• [3.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 12/19/23 11:41:31.11
  Dec 19 11:41:31.110: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename webhook @ 12/19/23 11:41:31.113
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:41:31.145
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:41:31.152
  STEP: Setting up server cert @ 12/19/23 11:41:31.202
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/19/23 11:41:31.861
  STEP: Deploying the webhook pod @ 12/19/23 11:41:31.88
  STEP: Wait for the deployment to be ready @ 12/19/23 11:41:31.906
  Dec 19 11:41:31.925: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E1219 11:41:32.078242      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:41:33.078271      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/19/23 11:41:33.951
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 11:41:33.976
  E1219 11:41:34.079379      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:41:34.976: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 12/19/23 11:41:34.984
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 12/19/23 11:41:35.028
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 12/19/23 11:41:35.045
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 12/19/23 11:41:35.068
  E1219 11:41:35.080177      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 12/19/23 11:41:35.092
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 12/19/23 11:41:35.109
  Dec 19 11:41:35.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7841" for this suite. @ 12/19/23 11:41:35.247
  STEP: Destroying namespace "webhook-markers-6946" for this suite. @ 12/19/23 11:41:35.276
• [4.205 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 12/19/23 11:41:35.319
  Dec 19 11:41:35.319: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 11:41:35.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:41:35.373
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:41:35.38
  STEP: Creating projection with secret that has name projected-secret-test-25497ea6-c94b-4796-9bc2-aa400aaceec8 @ 12/19/23 11:41:35.387
  STEP: Creating a pod to test consume secrets @ 12/19/23 11:41:35.396
  E1219 11:41:36.080670      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:41:37.080724      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:41:38.081705      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:41:39.081899      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/19/23 11:41:39.45
  Dec 19 11:41:39.458: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-projected-secrets-c4ff1981-3192-4afe-acae-a51515ac02a0 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 12/19/23 11:41:39.473
  Dec 19 11:41:39.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5421" for this suite. @ 12/19/23 11:41:39.516
• [4.211 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 12/19/23 11:41:39.536
  Dec 19 11:41:39.536: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename var-expansion @ 12/19/23 11:41:39.539
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:41:39.581
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:41:39.587
  STEP: Creating a pod to test env composition @ 12/19/23 11:41:39.594
  E1219 11:41:40.082944      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:41:41.083345      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:41:42.083490      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:41:43.083539      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/19/23 11:41:43.646
  Dec 19 11:41:43.653: INFO: Trying to get logs from node cahyeife7pae-3 pod var-expansion-3b16d156-19e6-40db-8bc3-ce95699312e7 container dapi-container: <nil>
  STEP: delete the pod @ 12/19/23 11:41:43.669
  Dec 19 11:41:43.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2570" for this suite. @ 12/19/23 11:41:43.722
• [4.201 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 12/19/23 11:41:43.739
  Dec 19 11:41:43.739: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename webhook @ 12/19/23 11:41:43.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:41:43.773
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:41:43.778
  STEP: Setting up server cert @ 12/19/23 11:41:43.835
  E1219 11:41:44.083686      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/19/23 11:41:44.475
  STEP: Deploying the webhook pod @ 12/19/23 11:41:44.485
  STEP: Wait for the deployment to be ready @ 12/19/23 11:41:44.508
  Dec 19 11:41:44.531: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E1219 11:41:45.083842      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:41:46.084295      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/19/23 11:41:46.556
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 11:41:46.58
  E1219 11:41:47.084488      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:41:47.581: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 12/19/23 11:41:47.59
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 12/19/23 11:41:47.625
  STEP: Creating a dummy validating-webhook-configuration object @ 12/19/23 11:41:47.659
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 12/19/23 11:41:47.675
  STEP: Creating a dummy mutating-webhook-configuration object @ 12/19/23 11:41:47.688
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 12/19/23 11:41:47.703
  Dec 19 11:41:47.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9041" for this suite. @ 12/19/23 11:41:47.872
  STEP: Destroying namespace "webhook-markers-6057" for this suite. @ 12/19/23 11:41:47.89
• [4.175 seconds]
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 12/19/23 11:41:47.914
  Dec 19 11:41:47.914: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename endpointslice @ 12/19/23 11:41:47.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:41:47.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:41:47.96
  Dec 19 11:41:48.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E1219 11:41:48.085196      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "endpointslice-3577" for this suite. @ 12/19/23 11:41:48.092
• [0.189 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 12/19/23 11:41:48.106
  Dec 19 11:41:48.106: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename emptydir @ 12/19/23 11:41:48.109
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:41:48.145
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:41:48.151
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 12/19/23 11:41:48.157
  E1219 11:41:49.085604      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:41:50.085993      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:41:51.086193      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:41:52.087091      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/19/23 11:41:52.209
  Dec 19 11:41:52.217: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-154d4bc8-a615-4090-9453-890db7b2ccad container test-container: <nil>
  STEP: delete the pod @ 12/19/23 11:41:52.234
  Dec 19 11:41:52.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3998" for this suite. @ 12/19/23 11:41:52.278
• [4.192 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 12/19/23 11:41:52.313
  Dec 19 11:41:52.313: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename downward-api @ 12/19/23 11:41:52.315
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:41:52.355
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:41:52.362
  STEP: Creating a pod to test downward API volume plugin @ 12/19/23 11:41:52.37
  E1219 11:41:53.088053      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:41:54.088204      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:41:55.088482      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:41:56.088747      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/19/23 11:41:56.413
  Dec 19 11:41:56.421: INFO: Trying to get logs from node cahyeife7pae-3 pod downwardapi-volume-5d995180-6e5a-4035-8d79-0324a807b02d container client-container: <nil>
  STEP: delete the pod @ 12/19/23 11:41:56.437
  Dec 19 11:41:56.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4158" for this suite. @ 12/19/23 11:41:56.48
• [4.184 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 12/19/23 11:41:56.498
  Dec 19 11:41:56.498: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename configmap @ 12/19/23 11:41:56.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:41:56.532
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:41:56.538
  STEP: Creating configMap with name cm-test-opt-del-c45ac688-fb02-4652-a89a-43b975a127c2 @ 12/19/23 11:41:56.563
  STEP: Creating configMap with name cm-test-opt-upd-7a8979d3-e507-49be-813b-f52e7771c2fc @ 12/19/23 11:41:56.576
  STEP: Creating the pod @ 12/19/23 11:41:56.583
  E1219 11:41:57.091548      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:41:58.089681      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-c45ac688-fb02-4652-a89a-43b975a127c2 @ 12/19/23 11:41:58.684
  STEP: Updating configmap cm-test-opt-upd-7a8979d3-e507-49be-813b-f52e7771c2fc @ 12/19/23 11:41:58.7
  STEP: Creating configMap with name cm-test-opt-create-dc96c534-7b6c-4764-b497-3b2b01b20193 @ 12/19/23 11:41:58.713
  STEP: waiting to observe update in volume @ 12/19/23 11:41:58.725
  E1219 11:41:59.089846      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:00.090597      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:01.091029      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:02.091309      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:42:02.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2713" for this suite. @ 12/19/23 11:42:02.825
• [6.339 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 12/19/23 11:42:02.841
  Dec 19 11:42:02.841: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename secrets @ 12/19/23 11:42:02.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:42:02.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:42:02.883
  STEP: Creating secret with name s-test-opt-del-8efb1af9-898b-4385-95c6-977cf6388e15 @ 12/19/23 11:42:02.901
  STEP: Creating secret with name s-test-opt-upd-6467ef7e-a0f2-406f-ac31-328fc1f7d91c @ 12/19/23 11:42:02.912
  STEP: Creating the pod @ 12/19/23 11:42:02.921
  E1219 11:42:03.091580      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:04.091858      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-8efb1af9-898b-4385-95c6-977cf6388e15 @ 12/19/23 11:42:05.045
  STEP: Updating secret s-test-opt-upd-6467ef7e-a0f2-406f-ac31-328fc1f7d91c @ 12/19/23 11:42:05.061
  STEP: Creating secret with name s-test-opt-create-2ae86e7a-860a-4733-966e-99db402c0e45 @ 12/19/23 11:42:05.08
  E1219 11:42:05.092219      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting to observe update in volume @ 12/19/23 11:42:05.094
  E1219 11:42:06.093298      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:07.093466      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:08.094120      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:09.094188      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:10.094703      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:11.094686      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:12.095476      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:13.096477      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:14.096748      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:15.096926      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:16.097270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:17.097914      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:18.098408      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:19.098834      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:20.099558      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:21.099477      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:22.099567      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:23.099851      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:24.100453      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:25.100443      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:26.100635      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:27.100890      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:28.100833      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:29.101039      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:30.102509      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:31.102733      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:32.103743      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:33.103980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:34.104469      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:35.105228      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:36.105387      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:37.105566      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:38.105887      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:39.106057      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:40.106274      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:41.106436      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:42.106738      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:43.107004      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:44.107270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:45.107295      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:46.107581      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:47.107907      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:48.108041      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:49.108598      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:50.109270      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:51.109537      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:52.110652      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:53.110816      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:54.110924      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:55.111290      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:56.112436      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:57.113062      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:58.113158      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:42:59.113876      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:00.114872      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:01.115991      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:02.116014      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:03.116341      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:04.116594      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:05.116753      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:06.117128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:07.117465      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:08.117550      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:09.118300      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:10.118680      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:11.119239      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:12.120384      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:13.120621      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:14.120902      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:15.121885      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:16.122922      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:17.123622      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:18.124585      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:19.125392      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:20.125603      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:21.126191      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:22.126493      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:23.126809      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:24.127292      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:25.127469      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:26.128600      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:27.129288      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:28.129866      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:29.131141      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:30.131356      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:31.131493      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:32.131933      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:43:32.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3670" for this suite. @ 12/19/23 11:43:32.175
• [89.357 seconds]
------------------------------
S
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 12/19/23 11:43:32.201
  Dec 19 11:43:32.201: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename disruption @ 12/19/23 11:43:32.205
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:43:32.242
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:43:32.249
  STEP: Waiting for the pdb to be processed @ 12/19/23 11:43:32.268
  E1219 11:43:33.132923      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:34.133492      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating PodDisruptionBudget status @ 12/19/23 11:43:34.285
  STEP: Waiting for all pods to be running @ 12/19/23 11:43:34.303
  Dec 19 11:43:34.315: INFO: running pods: 0 < 1
  E1219 11:43:35.133872      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:36.133994      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 12/19/23 11:43:36.324
  STEP: Waiting for the pdb to be processed @ 12/19/23 11:43:36.352
  STEP: Patching PodDisruptionBudget status @ 12/19/23 11:43:36.367
  STEP: Waiting for the pdb to be processed @ 12/19/23 11:43:36.391
  Dec 19 11:43:36.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-7800" for this suite. @ 12/19/23 11:43:36.43
• [4.243 seconds]
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 12/19/23 11:43:36.445
  Dec 19 11:43:36.445: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename runtimeclass @ 12/19/23 11:43:36.448
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:43:36.485
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:43:36.493
  Dec 19 11:43:36.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8463" for this suite. @ 12/19/23 11:43:36.534
• [0.103 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 12/19/23 11:43:36.55
  Dec 19 11:43:36.550: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename configmap @ 12/19/23 11:43:36.553
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:43:36.602
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:43:36.61
  STEP: Creating configMap configmap-998/configmap-test-8917e91d-e15b-425b-9173-b9b808daf7c0 @ 12/19/23 11:43:36.615
  STEP: Creating a pod to test consume configMaps @ 12/19/23 11:43:36.627
  E1219 11:43:37.134742      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:38.135116      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:39.135737      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:40.136120      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/19/23 11:43:40.68
  Dec 19 11:43:40.688: INFO: Trying to get logs from node cahyeife7pae-2 pod pod-configmaps-936029ae-947c-4724-b732-c372ea9e1b41 container env-test: <nil>
  STEP: delete the pod @ 12/19/23 11:43:40.735
  Dec 19 11:43:40.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-998" for this suite. @ 12/19/23 11:43:40.777
• [4.243 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 12/19/23 11:43:40.797
  Dec 19 11:43:40.797: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename dns @ 12/19/23 11:43:40.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:43:40.845
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:43:40.856
  STEP: Creating a test headless service @ 12/19/23 11:43:40.864
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2261.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2261.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2261.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2261.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2261.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2261.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2261.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2261.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2261.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2261.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 153.10.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.10.153_udp@PTR;check="$$(dig +tcp +noall +answer +search 153.10.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.10.153_tcp@PTR;sleep 1; done
   @ 12/19/23 11:43:40.907
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2261.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2261.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2261.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2261.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2261.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2261.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2261.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2261.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2261.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2261.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 153.10.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.10.153_udp@PTR;check="$$(dig +tcp +noall +answer +search 153.10.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.10.153_tcp@PTR;sleep 1; done
   @ 12/19/23 11:43:40.907
  STEP: creating a pod to probe DNS @ 12/19/23 11:43:40.907
  STEP: submitting the pod to kubernetes @ 12/19/23 11:43:40.907
  E1219 11:43:41.136610      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:42.136855      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 12/19/23 11:43:42.948
  STEP: looking for the results for each expected name from probers @ 12/19/23 11:43:42.956
  Dec 19 11:43:42.973: INFO: Unable to read wheezy_udp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:42.981: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:42.988: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:42.996: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:43.038: INFO: Unable to read jessie_udp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:43.046: INFO: Unable to read jessie_tcp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:43.055: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:43.062: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:43.092: INFO: Lookups using dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396 failed for: [wheezy_udp@dns-test-service.dns-2261.svc.cluster.local wheezy_tcp@dns-test-service.dns-2261.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local jessie_udp@dns-test-service.dns-2261.svc.cluster.local jessie_tcp@dns-test-service.dns-2261.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local]

  E1219 11:43:43.137615      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:44.137678      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:45.144825      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:46.143263      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:47.143413      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:43:48.120: INFO: Unable to read wheezy_udp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:48.127: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:48.135: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  E1219 11:43:48.144465      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:43:48.157: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:48.196: INFO: Unable to read jessie_udp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:48.205: INFO: Unable to read jessie_tcp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:48.211: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:48.221: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:48.271: INFO: Lookups using dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396 failed for: [wheezy_udp@dns-test-service.dns-2261.svc.cluster.local wheezy_tcp@dns-test-service.dns-2261.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local jessie_udp@dns-test-service.dns-2261.svc.cluster.local jessie_tcp@dns-test-service.dns-2261.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local]

  E1219 11:43:49.146927      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:50.147969      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:51.148023      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:52.152680      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:43:53.103: INFO: Unable to read wheezy_udp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:53.113: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:53.124: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:53.131: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  E1219 11:43:53.153298      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:43:53.166: INFO: Unable to read jessie_udp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:53.172: INFO: Unable to read jessie_tcp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:53.178: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:53.185: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:53.211: INFO: Lookups using dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396 failed for: [wheezy_udp@dns-test-service.dns-2261.svc.cluster.local wheezy_tcp@dns-test-service.dns-2261.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local jessie_udp@dns-test-service.dns-2261.svc.cluster.local jessie_tcp@dns-test-service.dns-2261.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local]

  E1219 11:43:54.153331      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:55.153576      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:56.153813      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:43:57.154307      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:43:58.113: INFO: Unable to read wheezy_udp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:58.123: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:58.131: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  E1219 11:43:58.154223      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:43:58.158: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:58.201: INFO: Unable to read jessie_udp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:58.209: INFO: Unable to read jessie_tcp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:58.215: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:58.227: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:43:58.264: INFO: Lookups using dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396 failed for: [wheezy_udp@dns-test-service.dns-2261.svc.cluster.local wheezy_tcp@dns-test-service.dns-2261.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local jessie_udp@dns-test-service.dns-2261.svc.cluster.local jessie_tcp@dns-test-service.dns-2261.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local]

  E1219 11:43:59.154515      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:00.155562      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:01.155814      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:02.155720      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:44:03.102: INFO: Unable to read wheezy_udp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:44:03.109: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:44:03.117: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:44:03.124: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  E1219 11:44:03.156153      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:44:03.163: INFO: Unable to read jessie_udp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:44:03.171: INFO: Unable to read jessie_tcp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:44:03.177: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:44:03.185: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:44:03.211: INFO: Lookups using dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396 failed for: [wheezy_udp@dns-test-service.dns-2261.svc.cluster.local wheezy_tcp@dns-test-service.dns-2261.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local jessie_udp@dns-test-service.dns-2261.svc.cluster.local jessie_tcp@dns-test-service.dns-2261.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local]

  E1219 11:44:04.156348      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:05.157147      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:06.157697      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:07.158483      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:44:08.100: INFO: Unable to read wheezy_udp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:44:08.109: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:44:08.118: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:44:08.129: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  E1219 11:44:08.159115      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:44:08.197: INFO: Unable to read jessie_udp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:44:08.204: INFO: Unable to read jessie_tcp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:44:08.212: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:44:08.221: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:44:08.260: INFO: Lookups using dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396 failed for: [wheezy_udp@dns-test-service.dns-2261.svc.cluster.local wheezy_tcp@dns-test-service.dns-2261.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local jessie_udp@dns-test-service.dns-2261.svc.cluster.local jessie_tcp@dns-test-service.dns-2261.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2261.svc.cluster.local]

  E1219 11:44:09.159339      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:10.159763      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:11.165666      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:12.165491      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:13.166740      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:44:13.175: INFO: Unable to read jessie_tcp@dns-test-service.dns-2261.svc.cluster.local from pod dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396: the server could not find the requested resource (get pods dns-test-3bdf6520-efc2-48de-9374-ce74239fe396)
  Dec 19 11:44:13.218: INFO: Lookups using dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396 failed for: [jessie_tcp@dns-test-service.dns-2261.svc.cluster.local]

  E1219 11:44:14.167188      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:15.167888      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:16.168515      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:17.168572      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:18.169621      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:44:18.227: INFO: DNS probes using dns-2261/dns-test-3bdf6520-efc2-48de-9374-ce74239fe396 succeeded

  Dec 19 11:44:18.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 12/19/23 11:44:18.24
  STEP: deleting the test service @ 12/19/23 11:44:18.288
  STEP: deleting the test headless service @ 12/19/23 11:44:18.342
  STEP: Destroying namespace "dns-2261" for this suite. @ 12/19/23 11:44:18.411
• [37.656 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 12/19/23 11:44:18.457
  Dec 19 11:44:18.457: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubectl @ 12/19/23 11:44:18.464
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:44:18.526
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:44:18.534
  STEP: starting the proxy server @ 12/19/23 11:44:18.54
  Dec 19 11:44:18.541: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-188 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 12/19/23 11:44:18.694
  Dec 19 11:44:18.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-188" for this suite. @ 12/19/23 11:44:18.735
• [0.295 seconds]
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 12/19/23 11:44:18.752
  Dec 19 11:44:18.752: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename proxy @ 12/19/23 11:44:18.755
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:44:18.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:44:18.801
  Dec 19 11:44:18.812: INFO: Creating pod...
  E1219 11:44:19.169756      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:20.171054      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:44:20.857: INFO: Creating service...
  Dec 19 11:44:20.886: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8608/pods/agnhost/proxy?method=DELETE
  Dec 19 11:44:20.902: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Dec 19 11:44:20.903: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8608/pods/agnhost/proxy?method=OPTIONS
  Dec 19 11:44:20.916: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Dec 19 11:44:20.916: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8608/pods/agnhost/proxy?method=PATCH
  Dec 19 11:44:20.927: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Dec 19 11:44:20.927: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8608/pods/agnhost/proxy?method=POST
  Dec 19 11:44:20.939: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Dec 19 11:44:20.939: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8608/pods/agnhost/proxy?method=PUT
  Dec 19 11:44:20.948: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Dec 19 11:44:20.948: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8608/services/e2e-proxy-test-service/proxy?method=DELETE
  Dec 19 11:44:20.960: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Dec 19 11:44:20.961: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8608/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Dec 19 11:44:20.989: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Dec 19 11:44:20.989: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8608/services/e2e-proxy-test-service/proxy?method=PATCH
  Dec 19 11:44:21.000: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Dec 19 11:44:21.001: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8608/services/e2e-proxy-test-service/proxy?method=POST
  Dec 19 11:44:21.019: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Dec 19 11:44:21.020: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8608/services/e2e-proxy-test-service/proxy?method=PUT
  Dec 19 11:44:21.035: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Dec 19 11:44:21.036: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8608/pods/agnhost/proxy?method=GET
  Dec 19 11:44:21.042: INFO: http.Client request:GET StatusCode:301
  Dec 19 11:44:21.043: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8608/services/e2e-proxy-test-service/proxy?method=GET
  Dec 19 11:44:21.053: INFO: http.Client request:GET StatusCode:301
  Dec 19 11:44:21.053: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8608/pods/agnhost/proxy?method=HEAD
  Dec 19 11:44:21.059: INFO: http.Client request:HEAD StatusCode:301
  Dec 19 11:44:21.059: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8608/services/e2e-proxy-test-service/proxy?method=HEAD
  Dec 19 11:44:21.072: INFO: http.Client request:HEAD StatusCode:301
  Dec 19 11:44:21.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-8608" for this suite. @ 12/19/23 11:44:21.085
• [2.364 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 12/19/23 11:44:21.123
  Dec 19 11:44:21.123: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename proxy @ 12/19/23 11:44:21.126
  E1219 11:44:21.171473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:44:21.18
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:44:21.188
  Dec 19 11:44:21.195: INFO: Creating pod...
  E1219 11:44:22.172337      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:23.172917      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:44:23.241: INFO: Creating service...
  Dec 19 11:44:23.263: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-953/pods/agnhost/proxy/some/path/with/DELETE
  Dec 19 11:44:23.292: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Dec 19 11:44:23.292: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-953/pods/agnhost/proxy/some/path/with/GET
  Dec 19 11:44:23.306: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Dec 19 11:44:23.306: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-953/pods/agnhost/proxy/some/path/with/HEAD
  Dec 19 11:44:23.313: INFO: http.Client request:HEAD | StatusCode:200
  Dec 19 11:44:23.314: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-953/pods/agnhost/proxy/some/path/with/OPTIONS
  Dec 19 11:44:23.325: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Dec 19 11:44:23.325: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-953/pods/agnhost/proxy/some/path/with/PATCH
  Dec 19 11:44:23.335: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Dec 19 11:44:23.335: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-953/pods/agnhost/proxy/some/path/with/POST
  Dec 19 11:44:23.343: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Dec 19 11:44:23.344: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-953/pods/agnhost/proxy/some/path/with/PUT
  Dec 19 11:44:23.350: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Dec 19 11:44:23.350: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-953/services/test-service/proxy/some/path/with/DELETE
  Dec 19 11:44:23.363: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Dec 19 11:44:23.363: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-953/services/test-service/proxy/some/path/with/GET
  Dec 19 11:44:23.375: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Dec 19 11:44:23.375: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-953/services/test-service/proxy/some/path/with/HEAD
  Dec 19 11:44:23.453: INFO: http.Client request:HEAD | StatusCode:200
  Dec 19 11:44:23.454: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-953/services/test-service/proxy/some/path/with/OPTIONS
  Dec 19 11:44:23.471: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Dec 19 11:44:23.472: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-953/services/test-service/proxy/some/path/with/PATCH
  Dec 19 11:44:23.487: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Dec 19 11:44:23.487: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-953/services/test-service/proxy/some/path/with/POST
  Dec 19 11:44:23.506: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Dec 19 11:44:23.506: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-953/services/test-service/proxy/some/path/with/PUT
  Dec 19 11:44:23.542: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Dec 19 11:44:23.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-953" for this suite. @ 12/19/23 11:44:23.565
• [2.470 seconds]
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 12/19/23 11:44:23.594
  Dec 19 11:44:23.594: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename svcaccounts @ 12/19/23 11:44:23.597
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:44:23.645
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:44:23.651
  Dec 19 11:44:23.695: INFO: created pod
  E1219 11:44:24.172863      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:25.173285      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:26.173534      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:27.173891      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/19/23 11:44:27.75
  E1219 11:44:28.174936      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:29.175170      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:30.175524      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:31.175707      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:32.176271      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:33.176386      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:34.176723      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:35.177089      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:36.177406      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:37.178192      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:38.178354      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:39.179119      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:40.179586      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:41.179908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:42.179873      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:43.180108      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:44.180959      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:45.181085      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:46.181447      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:47.181721      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:48.182617      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:49.182840      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:50.183204      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:51.183402      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:52.184055      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:53.184692      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:54.185147      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:55.185406      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:56.186486      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:57.187468      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:44:57.750: INFO: polling logs
  Dec 19 11:44:57.768: INFO: Pod logs: 
  I1219 11:44:24.523366       1 log.go:198] OK: Got token
  I1219 11:44:24.523974       1 log.go:198] validating with in-cluster discovery
  I1219 11:44:24.526329       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I1219 11:44:24.526506       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8564:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1702986863, NotBefore:1702986263, IssuedAt:1702986263, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8564", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"187f6a6e-3acf-4fb4-a0da-8115924fa3e4"}}}
  I1219 11:44:24.579091       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I1219 11:44:24.596650       1 log.go:198] OK: Validated signature on JWT
  I1219 11:44:24.596791       1 log.go:198] OK: Got valid claims from token!
  I1219 11:44:24.596833       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8564:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1702986863, NotBefore:1702986263, IssuedAt:1702986263, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8564", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"187f6a6e-3acf-4fb4-a0da-8115924fa3e4"}}}

  Dec 19 11:44:57.768: INFO: completed pod
  Dec 19 11:44:57.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8564" for this suite. @ 12/19/23 11:44:57.791
• [34.211 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 12/19/23 11:44:57.805
  Dec 19 11:44:57.805: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename replication-controller @ 12/19/23 11:44:57.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:44:57.842
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:44:57.848
  STEP: Given a ReplicationController is created @ 12/19/23 11:44:57.854
  STEP: When the matched label of one of its pods change @ 12/19/23 11:44:57.866
  Dec 19 11:44:57.875: INFO: Pod name pod-release: Found 0 pods out of 1
  E1219 11:44:58.187512      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:44:59.188117      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:00.189064      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:01.189031      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:02.189235      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:45:02.882: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 12/19/23 11:45:02.932
  E1219 11:45:03.189344      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:45:03.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7828" for this suite. @ 12/19/23 11:45:03.978
• [6.188 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 12/19/23 11:45:04.005
  Dec 19 11:45:04.005: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubectl @ 12/19/23 11:45:04.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:45:04.053
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:45:04.062
  STEP: creating a replication controller @ 12/19/23 11:45:04.071
  Dec 19 11:45:04.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 create -f -'
  E1219 11:45:04.190359      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:05.191188      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:45:05.805: INFO: stderr: ""
  Dec 19 11:45:05.805: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 12/19/23 11:45:05.805
  Dec 19 11:45:05.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Dec 19 11:45:06.069: INFO: stderr: ""
  Dec 19 11:45:06.069: INFO: stdout: "update-demo-nautilus-55r7b update-demo-nautilus-mrjqr "
  Dec 19 11:45:06.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 get pods update-demo-nautilus-55r7b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E1219 11:45:06.191605      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:45:06.269: INFO: stderr: ""
  Dec 19 11:45:06.269: INFO: stdout: ""
  Dec 19 11:45:06.270: INFO: update-demo-nautilus-55r7b is created but not running
  E1219 11:45:07.192140      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:08.192100      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:09.192312      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:10.192464      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:11.192770      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:45:11.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Dec 19 11:45:11.515: INFO: stderr: ""
  Dec 19 11:45:11.515: INFO: stdout: "update-demo-nautilus-55r7b update-demo-nautilus-mrjqr "
  Dec 19 11:45:11.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 get pods update-demo-nautilus-55r7b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Dec 19 11:45:11.692: INFO: stderr: ""
  Dec 19 11:45:11.692: INFO: stdout: "true"
  Dec 19 11:45:11.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 get pods update-demo-nautilus-55r7b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Dec 19 11:45:11.851: INFO: stderr: ""
  Dec 19 11:45:11.852: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Dec 19 11:45:11.852: INFO: validating pod update-demo-nautilus-55r7b
  Dec 19 11:45:11.881: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Dec 19 11:45:11.881: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Dec 19 11:45:11.881: INFO: update-demo-nautilus-55r7b is verified up and running
  Dec 19 11:45:11.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 get pods update-demo-nautilus-mrjqr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Dec 19 11:45:12.087: INFO: stderr: ""
  Dec 19 11:45:12.087: INFO: stdout: "true"
  Dec 19 11:45:12.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 get pods update-demo-nautilus-mrjqr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E1219 11:45:12.193298      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:45:12.267: INFO: stderr: ""
  Dec 19 11:45:12.267: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Dec 19 11:45:12.267: INFO: validating pod update-demo-nautilus-mrjqr
  Dec 19 11:45:12.285: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Dec 19 11:45:12.285: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Dec 19 11:45:12.285: INFO: update-demo-nautilus-mrjqr is verified up and running
  STEP: scaling down the replication controller @ 12/19/23 11:45:12.285
  Dec 19 11:45:12.304: INFO: scanned /root for discovery docs: <nil>
  Dec 19 11:45:12.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E1219 11:45:13.193494      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:45:13.627: INFO: stderr: ""
  Dec 19 11:45:13.627: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 12/19/23 11:45:13.627
  Dec 19 11:45:13.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Dec 19 11:45:13.872: INFO: stderr: ""
  Dec 19 11:45:13.872: INFO: stdout: "update-demo-nautilus-55r7b "
  Dec 19 11:45:13.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 get pods update-demo-nautilus-55r7b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Dec 19 11:45:14.055: INFO: stderr: ""
  Dec 19 11:45:14.056: INFO: stdout: "true"
  Dec 19 11:45:14.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 get pods update-demo-nautilus-55r7b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E1219 11:45:14.194308      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:45:14.240: INFO: stderr: ""
  Dec 19 11:45:14.240: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Dec 19 11:45:14.240: INFO: validating pod update-demo-nautilus-55r7b
  Dec 19 11:45:14.253: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Dec 19 11:45:14.253: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Dec 19 11:45:14.253: INFO: update-demo-nautilus-55r7b is verified up and running
  STEP: scaling up the replication controller @ 12/19/23 11:45:14.253
  Dec 19 11:45:14.266: INFO: scanned /root for discovery docs: <nil>
  Dec 19 11:45:14.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E1219 11:45:15.195913      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:45:15.525: INFO: stderr: ""
  Dec 19 11:45:15.525: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 12/19/23 11:45:15.525
  Dec 19 11:45:15.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Dec 19 11:45:15.741: INFO: stderr: ""
  Dec 19 11:45:15.741: INFO: stdout: "update-demo-nautilus-55r7b update-demo-nautilus-f56ct "
  Dec 19 11:45:15.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 get pods update-demo-nautilus-55r7b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Dec 19 11:45:15.933: INFO: stderr: ""
  Dec 19 11:45:15.933: INFO: stdout: "true"
  Dec 19 11:45:15.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 get pods update-demo-nautilus-55r7b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Dec 19 11:45:16.130: INFO: stderr: ""
  Dec 19 11:45:16.130: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Dec 19 11:45:16.130: INFO: validating pod update-demo-nautilus-55r7b
  Dec 19 11:45:16.143: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Dec 19 11:45:16.143: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Dec 19 11:45:16.143: INFO: update-demo-nautilus-55r7b is verified up and running
  Dec 19 11:45:16.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 get pods update-demo-nautilus-f56ct -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E1219 11:45:16.196207      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:45:16.317: INFO: stderr: ""
  Dec 19 11:45:16.317: INFO: stdout: "true"
  Dec 19 11:45:16.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 get pods update-demo-nautilus-f56ct -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Dec 19 11:45:16.477: INFO: stderr: ""
  Dec 19 11:45:16.477: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Dec 19 11:45:16.477: INFO: validating pod update-demo-nautilus-f56ct
  Dec 19 11:45:16.491: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Dec 19 11:45:16.491: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Dec 19 11:45:16.491: INFO: update-demo-nautilus-f56ct is verified up and running
  STEP: using delete to clean up resources @ 12/19/23 11:45:16.492
  Dec 19 11:45:16.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 delete --grace-period=0 --force -f -'
  Dec 19 11:45:16.664: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Dec 19 11:45:16.664: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Dec 19 11:45:16.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 get rc,svc -l name=update-demo --no-headers'
  Dec 19 11:45:16.949: INFO: stderr: "No resources found in kubectl-9793 namespace.\n"
  Dec 19 11:45:16.949: INFO: stdout: ""
  Dec 19 11:45:16.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-9793 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Dec 19 11:45:17.148: INFO: stderr: ""
  Dec 19 11:45:17.148: INFO: stdout: ""
  Dec 19 11:45:17.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9793" for this suite. @ 12/19/23 11:45:17.159
• [13.177 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 12/19/23 11:45:17.185
  Dec 19 11:45:17.186: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 12/19/23 11:45:17.191
  E1219 11:45:17.196068      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:45:17.248
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:45:17.263
  STEP: create the container to handle the HTTPGet hook request. @ 12/19/23 11:45:17.288
  E1219 11:45:18.196527      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:19.196882      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 12/19/23 11:45:19.372
  E1219 11:45:20.197511      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:21.197602      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 12/19/23 11:45:21.427
  STEP: delete the pod with lifecycle hook @ 12/19/23 11:45:21.483
  E1219 11:45:22.197952      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:23.198663      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:45:23.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-4066" for this suite. @ 12/19/23 11:45:23.559
• [6.392 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 12/19/23 11:45:23.578
  Dec 19 11:45:23.578: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename deployment @ 12/19/23 11:45:23.581
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:45:23.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:45:23.621
  Dec 19 11:45:23.630: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Dec 19 11:45:23.659: INFO: Pod name sample-pod: Found 0 pods out of 1
  E1219 11:45:24.198895      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:25.199788      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:26.199889      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:27.200864      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:28.201074      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:45:28.670: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 12/19/23 11:45:28.671
  Dec 19 11:45:28.672: INFO: Creating deployment "test-rolling-update-deployment"
  Dec 19 11:45:28.698: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Dec 19 11:45:28.732: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E1219 11:45:29.201260      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:30.201385      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:45:30.751: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Dec 19 11:45:30.763: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Dec 19 11:45:30.817: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2766  b56a6072-b722-4a5b-934d-ba01f01adf99 37070 1 2023-12-19 11:45:28 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-12-19 11:45:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 11:45:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a64678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-12-19 11:45:28 +0000 UTC,LastTransitionTime:2023-12-19 11:45:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-12-19 11:45:30 +0000 UTC,LastTransitionTime:2023-12-19 11:45:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Dec 19 11:45:30.833: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-2766  1586751b-667e-4730-8518-4b31cf2ff565 37060 1 2023-12-19 11:45:28 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment b56a6072-b722-4a5b-934d-ba01f01adf99 0xc0040dd007 0xc0040dd008}] [] [{kube-controller-manager Update apps/v1 2023-12-19 11:45:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b56a6072-b722-4a5b-934d-ba01f01adf99\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 11:45:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040dd0b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Dec 19 11:45:30.833: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Dec 19 11:45:30.835: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2766  a49531a9-6da2-45fd-ae1c-f06c5689d13d 37069 2 2023-12-19 11:45:23 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment b56a6072-b722-4a5b-934d-ba01f01adf99 0xc0040dced7 0xc0040dced8}] [] [{e2e.test Update apps/v1 2023-12-19 11:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-12-19 11:45:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b56a6072-b722-4a5b-934d-ba01f01adf99\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-12-19 11:45:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0040dcf98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Dec 19 11:45:30.852: INFO: Pod "test-rolling-update-deployment-656d657cd8-pd9lc" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-pd9lc test-rolling-update-deployment-656d657cd8- deployment-2766  ef4b7117-b69c-490e-8576-69b07387bee9 37059 0 2023-12-19 11:45:28 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 1586751b-667e-4730-8518-4b31cf2ff565 0xc0040dd527 0xc0040dd528}] [] [{kube-controller-manager Update v1 2023-12-19 11:45:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1586751b-667e-4730-8518-4b31cf2ff565\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-12-19 11:45:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v92k4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v92k4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cahyeife7pae-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:45:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:45:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:45:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-12-19 11:45:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.61,PodIP:10.233.66.192,StartTime:2023-12-19 11:45:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-12-19 11:45:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://4164fa017e4fc8b900f52d2a5dd8d316fe6d29ea82f623905fe52b70669a60a4,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.192,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Dec 19 11:45:30.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2766" for this suite. @ 12/19/23 11:45:30.869
• [7.307 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 12/19/23 11:45:30.888
  Dec 19 11:45:30.889: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename emptydir @ 12/19/23 11:45:30.891
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:45:30.931
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:45:30.94
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 12/19/23 11:45:30.95
  E1219 11:45:31.202305      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:32.203045      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:33.204112      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:34.205502      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/19/23 11:45:35.025
  Dec 19 11:45:35.032: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-0ed48002-ba0c-4ef4-b3dd-0fab1668c5d0 container test-container: <nil>
  STEP: delete the pod @ 12/19/23 11:45:35.051
  Dec 19 11:45:35.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5454" for this suite. @ 12/19/23 11:45:35.101
• [4.238 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 12/19/23 11:45:35.134
  Dec 19 11:45:35.134: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubectl @ 12/19/23 11:45:35.137
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:45:35.177
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:45:35.184
  STEP: creating the pod @ 12/19/23 11:45:35.192
  Dec 19 11:45:35.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-8936 create -f -'
  E1219 11:45:35.205942      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:45:36.095: INFO: stderr: ""
  Dec 19 11:45:36.095: INFO: stdout: "pod/pause created\n"
  E1219 11:45:36.206887      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:37.206862      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 12/19/23 11:45:38.126
  Dec 19 11:45:38.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-8936 label pods pause testing-label=testing-label-value'
  E1219 11:45:38.207935      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:45:38.323: INFO: stderr: ""
  Dec 19 11:45:38.324: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 12/19/23 11:45:38.325
  Dec 19 11:45:38.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-8936 get pod pause -L testing-label'
  Dec 19 11:45:38.540: INFO: stderr: ""
  Dec 19 11:45:38.540: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 12/19/23 11:45:38.541
  Dec 19 11:45:38.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-8936 label pods pause testing-label-'
  Dec 19 11:45:38.721: INFO: stderr: ""
  Dec 19 11:45:38.721: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 12/19/23 11:45:38.721
  Dec 19 11:45:38.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-8936 get pod pause -L testing-label'
  Dec 19 11:45:38.915: INFO: stderr: ""
  Dec 19 11:45:38.916: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
  STEP: using delete to clean up resources @ 12/19/23 11:45:38.916
  Dec 19 11:45:38.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-8936 delete --grace-period=0 --force -f -'
  Dec 19 11:45:39.112: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Dec 19 11:45:39.112: INFO: stdout: "pod \"pause\" force deleted\n"
  Dec 19 11:45:39.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-8936 get rc,svc -l name=pause --no-headers'
  E1219 11:45:39.208114      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:45:39.387: INFO: stderr: "No resources found in kubectl-8936 namespace.\n"
  Dec 19 11:45:39.387: INFO: stdout: ""
  Dec 19 11:45:39.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-8936 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Dec 19 11:45:39.561: INFO: stderr: ""
  Dec 19 11:45:39.562: INFO: stdout: ""
  Dec 19 11:45:39.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8936" for this suite. @ 12/19/23 11:45:39.573
• [4.456 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 12/19/23 11:45:39.591
  Dec 19 11:45:39.591: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename resourcequota @ 12/19/23 11:45:39.594
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:45:39.624
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:45:39.631
  STEP: Creating resourceQuota "e2e-rq-status-fd856" @ 12/19/23 11:45:39.646
  Dec 19 11:45:39.670: INFO: Resource quota "e2e-rq-status-fd856" reports spec: hard cpu limit of 500m
  Dec 19 11:45:39.671: INFO: Resource quota "e2e-rq-status-fd856" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-fd856" /status @ 12/19/23 11:45:39.671
  STEP: Confirm /status for "e2e-rq-status-fd856" resourceQuota via watch @ 12/19/23 11:45:39.698
  Dec 19 11:45:39.703: INFO: observed resourceQuota "e2e-rq-status-fd856" in namespace "resourcequota-3939" with hard status: v1.ResourceList(nil)
  Dec 19 11:45:39.703: INFO: Found resourceQuota "e2e-rq-status-fd856" in namespace "resourcequota-3939" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Dec 19 11:45:39.703: INFO: ResourceQuota "e2e-rq-status-fd856" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 12/19/23 11:45:39.714
  Dec 19 11:45:39.728: INFO: Resource quota "e2e-rq-status-fd856" reports spec: hard cpu limit of 1
  Dec 19 11:45:39.728: INFO: Resource quota "e2e-rq-status-fd856" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-fd856" /status @ 12/19/23 11:45:39.728
  STEP: Confirm /status for "e2e-rq-status-fd856" resourceQuota via watch @ 12/19/23 11:45:39.744
  Dec 19 11:45:39.747: INFO: observed resourceQuota "e2e-rq-status-fd856" in namespace "resourcequota-3939" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Dec 19 11:45:39.748: INFO: Found resourceQuota "e2e-rq-status-fd856" in namespace "resourcequota-3939" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Dec 19 11:45:39.748: INFO: ResourceQuota "e2e-rq-status-fd856" /status was patched
  STEP: Get "e2e-rq-status-fd856" /status @ 12/19/23 11:45:39.748
  Dec 19 11:45:39.756: INFO: Resourcequota "e2e-rq-status-fd856" reports status: hard cpu of 1
  Dec 19 11:45:39.756: INFO: Resourcequota "e2e-rq-status-fd856" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-fd856" /status before checking Spec is unchanged @ 12/19/23 11:45:39.762
  Dec 19 11:45:39.775: INFO: Resourcequota "e2e-rq-status-fd856" reports status: hard cpu of 2
  Dec 19 11:45:39.776: INFO: Resourcequota "e2e-rq-status-fd856" reports status: hard memory of 2Gi
  Dec 19 11:45:39.779: INFO: Found resourceQuota "e2e-rq-status-fd856" in namespace "resourcequota-3939" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E1219 11:45:40.208602      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:41.208944      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:42.209147      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:43.209733      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:44.210446      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:45.211161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:46.211788      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:47.212855      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:48.212951      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:49.213331      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:50.215098      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:51.214945      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:52.215808      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:53.215623      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:54.216060      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:55.215954      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:56.216219      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:57.216983      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:58.217456      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:45:59.218252      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:00.219481      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:01.219499      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:02.220235      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:03.221357      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:04.221647      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:05.223976      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:06.222819      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:07.222810      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:08.223015      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:09.223976      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:10.224299      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:11.224987      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:12.225087      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:13.225370      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:14.226455      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:15.226882      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:16.227263      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:17.227626      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:18.227874      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:19.228812      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:20.229092      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:21.229168      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:22.229392      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:23.231760      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:24.232504      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:25.232919      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:26.232540      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:27.233570      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:28.233895      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:29.234166      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:30.234372      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:31.234786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:32.235436      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:33.235916      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:34.236210      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:35.236885      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:36.237544      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:37.238148      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:38.238631      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:39.238864      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:40.239278      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:41.239942      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:42.240341      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:43.240888      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:44.241748      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:45.242128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:46.242995      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:47.243436      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:48.243645      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:49.243451      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:50.243860      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:51.244745      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:52.245450      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:53.246008      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:54.246104      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:55.246990      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:56.247714      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:57.248194      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:58.248968      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:46:59.249138      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:00.249856      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:01.249860      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:02.250751      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:03.250999      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:04.251750      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:05.252634      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:06.253127      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:07.253600      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:08.254027      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:09.254696      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:10.255247      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:11.255268      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:12.256203      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:13.256231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:14.256694      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:15.256855      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:16.257101      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:17.258074      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:18.258744      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:19.259771      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:20.259582      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:21.259984      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:22.260461      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:23.261737      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:24.261345      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:25.262045      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:26.262487      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:27.262920      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:28.262769      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:29.263489      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:30.263942      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:31.264404      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:32.264532      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:33.264904      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:34.265168      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:35.265602      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:36.266144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:37.267018      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:38.267520      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:39.267686      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:40.268869      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:41.269161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:42.269794      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:43.269591      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:44.269912      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:45.270189      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:46.270237      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:47.270668      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:48.271238      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:49.271707      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:50.272862      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:51.273003      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:52.273608      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:53.278032      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:54.277463      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:55.276919      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:56.277008      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:57.278136      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:58.278346      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:47:59.278769      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:00.279728      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:01.280340      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:02.280813      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:03.281309      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:04.281733      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:05.282231      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:06.282515      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:07.282987      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:08.283109      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:09.283217      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:10.284240      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:11.285024      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:12.285377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:13.285620      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:14.286317      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:15.286339      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:16.286910      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:17.288066      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:18.288597      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:19.288692      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:20.289094      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:21.290725      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:22.290161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:23.291588      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:24.290984      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:25.291141      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:26.291159      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:27.292095      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:28.292792      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:29.293250      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:30.293575      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:31.293719      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:32.294418      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:33.294633      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:34.294831      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:35.295528      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:36.296331      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:37.296765      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:38.297008      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:39.297170      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:40.297238      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:41.297432      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:42.298439      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:43.299162      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:44.300567      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:45.300786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:46.304395      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:47.304013      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:48.304208      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:49.304531      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:50.304829      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:51.305266      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:52.315171      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:53.309568      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:54.311151      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:55.310313      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:56.310949      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:57.311295      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:58.311941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:48:59.312377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:00.313652      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:01.313400      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:02.313600      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:03.313914      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:04.314242      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:05.314581      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:06.314946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:07.315595      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:08.315785      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:09.315887      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:10.316653      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:11.317281      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:12.317887      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:13.318300      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:14.318608      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:15.318917      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:16.319960      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:17.321106      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:18.321692      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:19.322640      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:20.322782      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:21.323467      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:22.323586      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:23.323908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:24.324108      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:25.324502      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:26.325346      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:27.325833      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:28.326158      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:29.327080      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:30.327395      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:31.328305      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:32.328594      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:33.329573      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:34.329872      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:35.330709      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:36.331021      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:37.331829      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:38.332005      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:39.332155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:40.333167      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:41.333315      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:42.333665      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:43.333935      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:44.334761      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:45.335795      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:46.335907      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:47.336191      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:48.336227      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:49.336922      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:50.337023      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:51.337370      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:52.337479      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:53.338382      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:54.338902      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:55.339111      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:56.339590      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:57.340286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:58.340845      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:49:59.341005      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:00.341501      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:01.341474      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:02.342580      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:03.342848      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:04.343587      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:05.343779      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:06.343972      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:07.344792      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:08.345009      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:09.345212      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:10.345394      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:11.346075      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:12.346229      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:13.346673      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:14.346714      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:15.346884      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:16.347382      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:17.348069      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:18.348788      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:19.348865      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:20.350097      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:21.350990      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:22.351463      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:23.351694      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:24.352019      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:50:24.815: INFO: ResourceQuota "e2e-rq-status-fd856" Spec was unchanged and /status reset
  Dec 19 11:50:24.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3939" for this suite. @ 12/19/23 11:50:24.835
• [285.264 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 12/19/23 11:50:24.86
  Dec 19 11:50:24.861: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename sched-pred @ 12/19/23 11:50:24.866
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:50:24.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:50:24.921
  Dec 19 11:50:24.928: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Dec 19 11:50:24.953: INFO: Waiting for terminating namespaces to be deleted...
  Dec 19 11:50:24.966: INFO: 
  Logging pods the apiserver thinks is on node cahyeife7pae-1 before test
  Dec 19 11:50:24.991: INFO: coredns-5d78c9869d-b55f2 from kube-system started at 2023-12-19 09:34:24 +0000 UTC (1 container statuses recorded)
  Dec 19 11:50:24.992: INFO: 	Container coredns ready: true, restart count 1
  Dec 19 11:50:24.992: INFO: coredns-5d78c9869d-pvg7c from kube-system started at 2023-12-19 09:34:24 +0000 UTC (1 container statuses recorded)
  Dec 19 11:50:24.992: INFO: 	Container coredns ready: true, restart count 1
  Dec 19 11:50:24.993: INFO: kube-addon-manager-cahyeife7pae-1 from kube-system started at 2023-12-19 09:46:25 +0000 UTC (1 container statuses recorded)
  Dec 19 11:50:24.993: INFO: 	Container kube-addon-manager ready: true, restart count 1
  Dec 19 11:50:24.993: INFO: kube-apiserver-cahyeife7pae-1 from kube-system started at 2023-12-19 09:46:25 +0000 UTC (1 container statuses recorded)
  Dec 19 11:50:24.994: INFO: 	Container kube-apiserver ready: true, restart count 1
  Dec 19 11:50:24.994: INFO: kube-controller-manager-cahyeife7pae-1 from kube-system started at 2023-12-19 09:46:25 +0000 UTC (1 container statuses recorded)
  Dec 19 11:50:24.994: INFO: 	Container kube-controller-manager ready: true, restart count 1
  Dec 19 11:50:24.994: INFO: kube-flannel-ds-84xmx from kube-system started at 2023-12-19 09:35:44 +0000 UTC (1 container statuses recorded)
  Dec 19 11:50:24.995: INFO: 	Container kube-flannel ready: true, restart count 1
  Dec 19 11:50:24.995: INFO: kube-proxy-xmh99 from kube-system started at 2023-12-19 09:34:23 +0000 UTC (1 container statuses recorded)
  Dec 19 11:50:24.995: INFO: 	Container kube-proxy ready: true, restart count 1
  Dec 19 11:50:24.996: INFO: kube-scheduler-cahyeife7pae-1 from kube-system started at 2023-12-19 09:46:25 +0000 UTC (1 container statuses recorded)
  Dec 19 11:50:24.996: INFO: 	Container kube-scheduler ready: true, restart count 1
  Dec 19 11:50:24.997: INFO: sonobuoy-systemd-logs-daemon-set-a1a4b21fb49145dd-557wq from sonobuoy started at 2023-12-19 10:01:11 +0000 UTC (2 container statuses recorded)
  Dec 19 11:50:24.998: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 19 11:50:24.999: INFO: 	Container systemd-logs ready: true, restart count 0
  Dec 19 11:50:24.999: INFO: 
  Logging pods the apiserver thinks is on node cahyeife7pae-2 before test
  Dec 19 11:50:25.029: INFO: kube-addon-manager-cahyeife7pae-2 from kube-system started at 2023-12-19 09:46:48 +0000 UTC (1 container statuses recorded)
  Dec 19 11:50:25.030: INFO: 	Container kube-addon-manager ready: true, restart count 1
  Dec 19 11:50:25.030: INFO: kube-apiserver-cahyeife7pae-2 from kube-system started at 2023-12-19 09:46:48 +0000 UTC (1 container statuses recorded)
  Dec 19 11:50:25.030: INFO: 	Container kube-apiserver ready: true, restart count 1
  Dec 19 11:50:25.031: INFO: kube-controller-manager-cahyeife7pae-2 from kube-system started at 2023-12-19 09:46:48 +0000 UTC (1 container statuses recorded)
  Dec 19 11:50:25.031: INFO: 	Container kube-controller-manager ready: true, restart count 1
  Dec 19 11:50:25.031: INFO: kube-flannel-ds-zfl5g from kube-system started at 2023-12-19 09:35:44 +0000 UTC (1 container statuses recorded)
  Dec 19 11:50:25.032: INFO: 	Container kube-flannel ready: true, restart count 1
  Dec 19 11:50:25.032: INFO: kube-proxy-qhj8b from kube-system started at 2023-12-19 09:34:50 +0000 UTC (1 container statuses recorded)
  Dec 19 11:50:25.032: INFO: 	Container kube-proxy ready: true, restart count 1
  Dec 19 11:50:25.033: INFO: kube-scheduler-cahyeife7pae-2 from kube-system started at 2023-12-19 09:46:48 +0000 UTC (1 container statuses recorded)
  Dec 19 11:50:25.033: INFO: 	Container kube-scheduler ready: true, restart count 1
  Dec 19 11:50:25.033: INFO: sonobuoy-systemd-logs-daemon-set-a1a4b21fb49145dd-rfjdj from sonobuoy started at 2023-12-19 10:01:11 +0000 UTC (2 container statuses recorded)
  Dec 19 11:50:25.034: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 19 11:50:25.034: INFO: 	Container systemd-logs ready: true, restart count 0
  Dec 19 11:50:25.034: INFO: 
  Logging pods the apiserver thinks is on node cahyeife7pae-3 before test
  Dec 19 11:50:25.054: INFO: kube-flannel-ds-swv5m from kube-system started at 2023-12-19 11:35:52 +0000 UTC (1 container statuses recorded)
  Dec 19 11:50:25.054: INFO: 	Container kube-flannel ready: true, restart count 0
  Dec 19 11:50:25.054: INFO: kube-proxy-v2qp9 from kube-system started at 2023-12-19 09:35:14 +0000 UTC (1 container statuses recorded)
  Dec 19 11:50:25.055: INFO: 	Container kube-proxy ready: true, restart count 1
  Dec 19 11:50:25.055: INFO: sonobuoy from sonobuoy started at 2023-12-19 10:01:00 +0000 UTC (1 container statuses recorded)
  Dec 19 11:50:25.055: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Dec 19 11:50:25.056: INFO: sonobuoy-e2e-job-b6558dadff6840cc from sonobuoy started at 2023-12-19 10:01:11 +0000 UTC (2 container statuses recorded)
  Dec 19 11:50:25.056: INFO: 	Container e2e ready: true, restart count 0
  Dec 19 11:50:25.057: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 19 11:50:25.057: INFO: sonobuoy-systemd-logs-daemon-set-a1a4b21fb49145dd-xsfr4 from sonobuoy started at 2023-12-19 10:01:11 +0000 UTC (2 container statuses recorded)
  Dec 19 11:50:25.057: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Dec 19 11:50:25.058: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 12/19/23 11:50:25.058
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.17a239500f3add04], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] @ 12/19/23 11:50:25.134
  E1219 11:50:25.352446      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:50:26.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-7321" for this suite. @ 12/19/23 11:50:26.14
• [1.294 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 12/19/23 11:50:26.157
  Dec 19 11:50:26.157: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 11:50:26.158
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:50:26.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:50:26.195
  STEP: Creating configMap with name projected-configmap-test-volume-f194177d-f76e-4ea2-9fb6-32751194bb7c @ 12/19/23 11:50:26.202
  STEP: Creating a pod to test consume configMaps @ 12/19/23 11:50:26.212
  E1219 11:50:26.352765      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:27.354600      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:28.354795      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:29.354970      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/19/23 11:50:30.266
  Dec 19 11:50:30.274: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-projected-configmaps-a8ebabf2-7bc8-450c-bb5e-f55da6a91ec2 container agnhost-container: <nil>
  STEP: delete the pod @ 12/19/23 11:50:30.328
  E1219 11:50:30.355250      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:50:30.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1991" for this suite. @ 12/19/23 11:50:30.375
• [4.236 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 12/19/23 11:50:30.394
  Dec 19 11:50:30.395: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename resourcequota @ 12/19/23 11:50:30.398
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:50:30.437
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:50:30.445
  STEP: Counting existing ResourceQuota @ 12/19/23 11:50:30.452
  E1219 11:50:31.356081      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:32.356717      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:33.358081      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:34.358763      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:35.359322      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 12/19/23 11:50:35.463
  STEP: Ensuring resource quota status is calculated @ 12/19/23 11:50:35.476
  E1219 11:50:36.359882      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:37.360339      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 12/19/23 11:50:37.494
  STEP: Creating a NodePort Service @ 12/19/23 11:50:37.594
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 12/19/23 11:50:37.729
  STEP: Ensuring resource quota status captures service creation @ 12/19/23 11:50:37.779
  E1219 11:50:38.360957      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:39.361788      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 12/19/23 11:50:39.788
  STEP: Ensuring resource quota status released usage @ 12/19/23 11:50:39.92
  E1219 11:50:40.361499      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:41.361719      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:50:41.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8181" for this suite. @ 12/19/23 11:50:41.937
• [11.561 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 12/19/23 11:50:41.959
  Dec 19 11:50:41.959: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename namespaces @ 12/19/23 11:50:41.965
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:50:42.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:50:42.027
  STEP: Read namespace status @ 12/19/23 11:50:42.031
  Dec 19 11:50:42.041: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 12/19/23 11:50:42.041
  Dec 19 11:50:42.063: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 12/19/23 11:50:42.063
  Dec 19 11:50:42.083: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Dec 19 11:50:42.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-357" for this suite. @ 12/19/23 11:50:42.093
• [0.149 seconds]
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 12/19/23 11:50:42.11
  Dec 19 11:50:42.110: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename secrets @ 12/19/23 11:50:42.114
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:50:42.143
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:50:42.151
  STEP: creating a secret @ 12/19/23 11:50:42.158
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 12/19/23 11:50:42.168
  STEP: patching the secret @ 12/19/23 11:50:42.175
  STEP: deleting the secret using a LabelSelector @ 12/19/23 11:50:42.195
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 12/19/23 11:50:42.214
  Dec 19 11:50:42.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4065" for this suite. @ 12/19/23 11:50:42.232
• [0.138 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 12/19/23 11:50:42.249
  Dec 19 11:50:42.249: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename webhook @ 12/19/23 11:50:42.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:50:42.284
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:50:42.29
  STEP: Setting up server cert @ 12/19/23 11:50:42.334
  E1219 11:50:42.362509      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/19/23 11:50:43.218
  STEP: Deploying the webhook pod @ 12/19/23 11:50:43.242
  STEP: Wait for the deployment to be ready @ 12/19/23 11:50:43.273
  Dec 19 11:50:43.289: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E1219 11:50:43.363391      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:44.363673      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/19/23 11:50:45.313
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 11:50:45.334
  E1219 11:50:45.364312      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:50:46.335: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 12/19/23 11:50:46.344
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 12/19/23 11:50:46.346
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 12/19/23 11:50:46.347
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 12/19/23 11:50:46.347
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 12/19/23 11:50:46.354
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 12/19/23 11:50:46.355
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 12/19/23 11:50:46.358
  Dec 19 11:50:46.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E1219 11:50:46.364856      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-6209" for this suite. @ 12/19/23 11:50:46.518
  STEP: Destroying namespace "webhook-markers-7350" for this suite. @ 12/19/23 11:50:46.543
• [4.317 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 12/19/23 11:50:46.567
  Dec 19 11:50:46.567: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename var-expansion @ 12/19/23 11:50:46.569
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:50:46.605
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:50:46.609
  STEP: Creating a pod to test substitution in container's args @ 12/19/23 11:50:46.616
  E1219 11:50:47.365394      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:48.366905      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:49.366394      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:50.366607      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/19/23 11:50:50.668
  Dec 19 11:50:50.676: INFO: Trying to get logs from node cahyeife7pae-3 pod var-expansion-8924997c-0557-45ee-96a3-782f9f191cbe container dapi-container: <nil>
  STEP: delete the pod @ 12/19/23 11:50:50.691
  Dec 19 11:50:50.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4181" for this suite. @ 12/19/23 11:50:50.749
• [4.195 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 12/19/23 11:50:50.767
  Dec 19 11:50:50.767: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename init-container @ 12/19/23 11:50:50.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:50:50.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:50:50.811
  STEP: creating the pod @ 12/19/23 11:50:50.815
  Dec 19 11:50:50.815: INFO: PodSpec: initContainers in spec.initContainers
  E1219 11:50:51.366679      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:52.367667      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:53.367859      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:54.368357      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:50:54.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-9869" for this suite. @ 12/19/23 11:50:54.719
• [3.968 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:350
  STEP: Creating a kubernetes client @ 12/19/23 11:50:54.736
  Dec 19 11:50:54.736: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename field-validation @ 12/19/23 11:50:54.739
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:50:54.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:50:54.789
  Dec 19 11:50:54.799: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  W1219 11:50:54.802079      13 field_validation.go:423] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc005186a20 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E1219 11:50:55.368051      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:56.368552      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:50:57.369585      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  W1219 11:50:57.652399      13 warnings.go:70] unknown field "alpha"
  W1219 11:50:57.652448      13 warnings.go:70] unknown field "beta"
  W1219 11:50:57.652460      13 warnings.go:70] unknown field "delta"
  W1219 11:50:57.652470      13 warnings.go:70] unknown field "epsilon"
  W1219 11:50:57.652480      13 warnings.go:70] unknown field "gamma"
  Dec 19 11:50:58.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2822" for this suite. @ 12/19/23 11:50:58.278
• [3.555 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 12/19/23 11:50:58.294
  Dec 19 11:50:58.295: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename replication-controller @ 12/19/23 11:50:58.3
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:50:58.342
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:50:58.351
  STEP: Creating ReplicationController "e2e-rc-r82gd" @ 12/19/23 11:50:58.365
  E1219 11:50:58.370414      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:50:58.390: INFO: Get Replication Controller "e2e-rc-r82gd" to confirm replicas
  E1219 11:50:59.371095      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:50:59.398: INFO: Get Replication Controller "e2e-rc-r82gd" to confirm replicas
  Dec 19 11:50:59.405: INFO: Found 1 replicas for "e2e-rc-r82gd" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-r82gd" @ 12/19/23 11:50:59.405
  STEP: Updating a scale subresource @ 12/19/23 11:50:59.418
  STEP: Verifying replicas where modified for replication controller "e2e-rc-r82gd" @ 12/19/23 11:50:59.434
  Dec 19 11:50:59.434: INFO: Get Replication Controller "e2e-rc-r82gd" to confirm replicas
  E1219 11:51:00.372586      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:51:00.444: INFO: Get Replication Controller "e2e-rc-r82gd" to confirm replicas
  Dec 19 11:51:00.453: INFO: Found 2 replicas for "e2e-rc-r82gd" replication controller
  Dec 19 11:51:00.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9727" for this suite. @ 12/19/23 11:51:00.464
• [2.187 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 12/19/23 11:51:00.482
  Dec 19 11:51:00.482: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename subpath @ 12/19/23 11:51:00.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:51:00.523
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:51:00.534
  STEP: Setting up data @ 12/19/23 11:51:00.544
  STEP: Creating pod pod-subpath-test-configmap-bd9c @ 12/19/23 11:51:00.569
  STEP: Creating a pod to test atomic-volume-subpath @ 12/19/23 11:51:00.57
  E1219 11:51:01.371902      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:02.372397      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:03.373569      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:04.373772      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:05.374659      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:06.374641      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:07.374936      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:08.377109      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:09.377643      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:10.378440      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:11.378952      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:12.379790      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:13.381593      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:14.381873      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:15.382017      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:16.382220      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:17.383320      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:18.384144      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:19.384141      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:20.384492      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:21.384834      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:22.385226      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:23.385829      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:24.386134      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/19/23 11:51:24.734
  Dec 19 11:51:24.742: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-subpath-test-configmap-bd9c container test-container-subpath-configmap-bd9c: <nil>
  STEP: delete the pod @ 12/19/23 11:51:24.795
  STEP: Deleting pod pod-subpath-test-configmap-bd9c @ 12/19/23 11:51:24.85
  Dec 19 11:51:24.851: INFO: Deleting pod "pod-subpath-test-configmap-bd9c" in namespace "subpath-4982"
  Dec 19 11:51:24.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4982" for this suite. @ 12/19/23 11:51:24.899
• [24.432 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 12/19/23 11:51:24.933
  Dec 19 11:51:24.933: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename webhook @ 12/19/23 11:51:24.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:51:25.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:51:25.029
  STEP: Setting up server cert @ 12/19/23 11:51:25.138
  E1219 11:51:25.386431      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:26.387347      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/19/23 11:51:26.608
  STEP: Deploying the webhook pod @ 12/19/23 11:51:26.622
  STEP: Wait for the deployment to be ready @ 12/19/23 11:51:26.647
  Dec 19 11:51:26.665: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E1219 11:51:27.388837      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:28.388805      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/19/23 11:51:28.685
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 11:51:28.706
  E1219 11:51:29.389090      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:51:29.707: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 12/19/23 11:51:29.716
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 12/19/23 11:51:29.765
  STEP: Creating a configMap that should not be mutated @ 12/19/23 11:51:29.779
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 12/19/23 11:51:29.8
  STEP: Creating a configMap that should be mutated @ 12/19/23 11:51:29.815
  Dec 19 11:51:29.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9468" for this suite. @ 12/19/23 11:51:29.991
  STEP: Destroying namespace "webhook-markers-4866" for this suite. @ 12/19/23 11:51:30.007
• [5.101 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 12/19/23 11:51:30.042
  Dec 19 11:51:30.042: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename secrets @ 12/19/23 11:51:30.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:51:30.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:51:30.092
  STEP: Creating secret with name secret-test-545153ba-f034-4a51-a8c5-069f10e4110b @ 12/19/23 11:51:30.101
  STEP: Creating a pod to test consume secrets @ 12/19/23 11:51:30.117
  E1219 11:51:30.390400      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:31.391122      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:32.392133      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:33.393092      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/19/23 11:51:34.163
  Dec 19 11:51:34.171: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-secrets-9bc2c188-19ed-4b0b-9b90-5b499973d991 container secret-env-test: <nil>
  STEP: delete the pod @ 12/19/23 11:51:34.19
  Dec 19 11:51:34.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-702" for this suite. @ 12/19/23 11:51:34.255
• [4.234 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 12/19/23 11:51:34.285
  Dec 19 11:51:34.285: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename emptydir-wrapper @ 12/19/23 11:51:34.287
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:51:34.321
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:51:34.327
  E1219 11:51:34.393712      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:35.393801      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:36.394459      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:51:36.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 12/19/23 11:51:36.443
  STEP: Cleaning up the configmap @ 12/19/23 11:51:36.457
  STEP: Cleaning up the pod @ 12/19/23 11:51:36.469
  STEP: Destroying namespace "emptydir-wrapper-8631" for this suite. @ 12/19/23 11:51:36.491
• [2.220 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 12/19/23 11:51:36.508
  Dec 19 11:51:36.508: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubelet-test @ 12/19/23 11:51:36.511
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:51:36.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:51:36.557
  Dec 19 11:51:36.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1713" for this suite. @ 12/19/23 11:51:36.646
• [0.153 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:289
  STEP: Creating a kubernetes client @ 12/19/23 11:51:36.663
  Dec 19 11:51:36.663: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename field-validation @ 12/19/23 11:51:36.666
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:51:36.702
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:51:36.708
  Dec 19 11:51:36.716: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  E1219 11:51:37.394606      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:38.395246      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:39.395089      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:51:40.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1505" for this suite. @ 12/19/23 11:51:40.258
• [3.613 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 12/19/23 11:51:40.281
  Dec 19 11:51:40.281: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 11:51:40.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:51:40.324
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:51:40.333
  STEP: Creating configMap with name projected-configmap-test-volume-2dcb8ff0-9f42-421e-a744-dd5569cd52e7 @ 12/19/23 11:51:40.348
  STEP: Creating a pod to test consume configMaps @ 12/19/23 11:51:40.356
  E1219 11:51:40.396458      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:41.396358      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:42.397268      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:43.399988      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:44.399443      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/19/23 11:51:44.403
  Dec 19 11:51:44.410: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-projected-configmaps-e07e4458-522c-4bf0-912d-5f4f77f94ca1 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 12/19/23 11:51:44.427
  Dec 19 11:51:44.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2295" for this suite. @ 12/19/23 11:51:44.484
• [4.226 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 12/19/23 11:51:44.516
  Dec 19 11:51:44.516: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename events @ 12/19/23 11:51:44.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:51:44.568
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:51:44.575
  STEP: Create set of events @ 12/19/23 11:51:44.589
  Dec 19 11:51:44.596: INFO: created test-event-1
  Dec 19 11:51:44.608: INFO: created test-event-2
  Dec 19 11:51:44.618: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 12/19/23 11:51:44.618
  STEP: delete collection of events @ 12/19/23 11:51:44.623
  Dec 19 11:51:44.623: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 12/19/23 11:51:44.669
  Dec 19 11:51:44.669: INFO: requesting list of events to confirm quantity
  Dec 19 11:51:44.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-1055" for this suite. @ 12/19/23 11:51:44.693
• [0.195 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 12/19/23 11:51:44.715
  Dec 19 11:51:44.715: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename webhook @ 12/19/23 11:51:44.717
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:51:44.746
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:51:44.754
  STEP: Setting up server cert @ 12/19/23 11:51:44.812
  E1219 11:51:45.401182      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:46.403484      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 12/19/23 11:51:46.639
  STEP: Deploying the webhook pod @ 12/19/23 11:51:46.654
  STEP: Wait for the deployment to be ready @ 12/19/23 11:51:46.68
  Dec 19 11:51:46.710: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E1219 11:51:47.402106      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:48.402764      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 12/19/23 11:51:48.735
  STEP: Verifying the service has paired with the endpoint @ 12/19/23 11:51:48.762
  E1219 11:51:49.403748      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:51:49.763: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 12/19/23 11:51:49.771
  STEP: create a namespace for the webhook @ 12/19/23 11:51:49.819
  STEP: create a configmap should be unconditionally rejected by the webhook @ 12/19/23 11:51:49.854
  Dec 19 11:51:49.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-72" for this suite. @ 12/19/23 11:51:50.036
  STEP: Destroying namespace "webhook-markers-8800" for this suite. @ 12/19/23 11:51:50.053
  STEP: Destroying namespace "fail-closed-namespace-5648" for this suite. @ 12/19/23 11:51:50.071
• [5.369 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 12/19/23 11:51:50.086
  Dec 19 11:51:50.086: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename security-context @ 12/19/23 11:51:50.092
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:51:50.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:51:50.141
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 12/19/23 11:51:50.157
  E1219 11:51:50.404663      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:51.404780      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:52.405423      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:53.406052      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/19/23 11:51:54.222
  Dec 19 11:51:54.229: INFO: Trying to get logs from node cahyeife7pae-3 pod security-context-b4cc5f13-6b1a-42a1-98a1-8e5747c9b826 container test-container: <nil>
  STEP: delete the pod @ 12/19/23 11:51:54.242
  Dec 19 11:51:54.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-4224" for this suite. @ 12/19/23 11:51:54.295
• [4.222 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 12/19/23 11:51:54.311
  Dec 19 11:51:54.311: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename kubectl @ 12/19/23 11:51:54.314
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:51:54.342
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:51:54.349
  STEP: Starting the proxy @ 12/19/23 11:51:54.359
  Dec 19 11:51:54.361: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=kubectl-4832 proxy --unix-socket=/tmp/kubectl-proxy-unix3331763968/test'
  E1219 11:51:54.406255      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving proxy /api/ output @ 12/19/23 11:51:54.533
  Dec 19 11:51:54.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4832" for this suite. @ 12/19/23 11:51:54.545
• [0.250 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 12/19/23 11:51:54.56
  Dec 19 11:51:54.560: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename projected @ 12/19/23 11:51:54.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:51:54.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:51:54.598
  STEP: Creating projection with secret that has name projected-secret-test-map-83e24d53-8ded-4661-9170-7fcc44be5b03 @ 12/19/23 11:51:54.605
  STEP: Creating a pod to test consume secrets @ 12/19/23 11:51:54.617
  E1219 11:51:55.406513      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:56.407581      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:57.408066      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:51:58.408533      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/19/23 11:51:58.666
  Dec 19 11:51:58.672: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-projected-secrets-98426378-c0fe-47e0-a6ec-280c9d977a46 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 12/19/23 11:51:58.689
  Dec 19 11:51:58.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7477" for this suite. @ 12/19/23 11:51:58.745
• [4.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 12/19/23 11:51:58.768
  Dec 19 11:51:58.768: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename subpath @ 12/19/23 11:51:58.77
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:51:58.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:51:58.81
  STEP: Setting up data @ 12/19/23 11:51:58.818
  STEP: Creating pod pod-subpath-test-projected-6qx6 @ 12/19/23 11:51:58.842
  STEP: Creating a pod to test atomic-volume-subpath @ 12/19/23 11:51:58.842
  E1219 11:51:59.409582      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:00.410250      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:01.410838      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:02.411474      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:03.413095      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:04.413889      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:05.414054      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:06.414430      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:07.414260      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:08.414956      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:09.415153      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:10.416019      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:11.416540      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:12.417552      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:13.418505      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:14.419195      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:15.419831      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:16.420439      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:17.420900      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:18.421621      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:19.421753      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:20.421989      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:21.422720      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:22.423656      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/19/23 11:52:22.995
  Dec 19 11:52:23.003: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-subpath-test-projected-6qx6 container test-container-subpath-projected-6qx6: <nil>
  STEP: delete the pod @ 12/19/23 11:52:23.024
  STEP: Deleting pod pod-subpath-test-projected-6qx6 @ 12/19/23 11:52:23.06
  Dec 19 11:52:23.060: INFO: Deleting pod "pod-subpath-test-projected-6qx6" in namespace "subpath-5876"
  Dec 19 11:52:23.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5876" for this suite. @ 12/19/23 11:52:23.08
• [24.324 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 12/19/23 11:52:23.094
  Dec 19 11:52:23.094: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename pods @ 12/19/23 11:52:23.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:52:23.131
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:52:23.137
  STEP: creating the pod @ 12/19/23 11:52:23.143
  STEP: submitting the pod to kubernetes @ 12/19/23 11:52:23.143
  W1219 11:52:23.170080      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E1219 11:52:23.424826      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:24.425060      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 12/19/23 11:52:25.205
  STEP: updating the pod @ 12/19/23 11:52:25.212
  E1219 11:52:25.425445      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:52:25.743: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c7e2bc1e-b226-4492-ad1e-25dccf7720ff"
  E1219 11:52:26.426707      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:27.427234      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:28.427569      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:29.428446      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:52:29.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2099" for this suite. @ 12/19/23 11:52:29.782
• [6.702 seconds]
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 12/19/23 11:52:29.797
  Dec 19 11:52:29.797: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 12/19/23 11:52:29.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:52:29.836
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:52:29.843
  STEP: Setting up the test @ 12/19/23 11:52:29.848
  STEP: Creating hostNetwork=false pod @ 12/19/23 11:52:29.848
  E1219 11:52:30.430695      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:31.431575      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 12/19/23 11:52:31.892
  E1219 11:52:32.432247      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:33.432742      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 12/19/23 11:52:33.932
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 12/19/23 11:52:33.932
  Dec 19 11:52:33.932: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9981 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:52:33.932: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:52:33.933: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:52:33.934: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9981/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Dec 19 11:52:34.088: INFO: Exec stderr: ""
  Dec 19 11:52:34.088: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9981 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:52:34.088: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:52:34.091: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:52:34.092: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9981/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Dec 19 11:52:34.190: INFO: Exec stderr: ""
  Dec 19 11:52:34.190: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9981 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:52:34.190: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:52:34.192: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:52:34.192: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9981/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Dec 19 11:52:34.341: INFO: Exec stderr: ""
  Dec 19 11:52:34.341: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9981 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:52:34.341: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:52:34.342: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:52:34.343: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9981/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  E1219 11:52:34.433325      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:52:34.473: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 12/19/23 11:52:34.473
  Dec 19 11:52:34.473: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9981 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:52:34.473: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:52:34.474: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:52:34.475: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9981/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Dec 19 11:52:34.611: INFO: Exec stderr: ""
  Dec 19 11:52:34.611: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9981 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:52:34.611: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:52:34.614: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:52:34.614: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9981/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Dec 19 11:52:34.719: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 12/19/23 11:52:34.719
  Dec 19 11:52:34.719: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9981 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:52:34.719: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:52:34.721: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:52:34.721: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9981/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Dec 19 11:52:34.847: INFO: Exec stderr: ""
  Dec 19 11:52:34.847: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9981 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:52:34.847: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:52:34.849: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:52:34.850: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9981/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Dec 19 11:52:34.965: INFO: Exec stderr: ""
  Dec 19 11:52:34.966: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9981 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:52:34.966: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:52:34.968: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:52:34.969: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9981/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Dec 19 11:52:35.113: INFO: Exec stderr: ""
  Dec 19 11:52:35.114: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9981 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Dec 19 11:52:35.114: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  Dec 19 11:52:35.116: INFO: ExecWithOptions: Clientset creation
  Dec 19 11:52:35.116: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9981/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Dec 19 11:52:35.243: INFO: Exec stderr: ""
  Dec 19 11:52:35.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-9981" for this suite. @ 12/19/23 11:52:35.262
• [5.490 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:748
  STEP: Creating a kubernetes client @ 12/19/23 11:52:35.289
  Dec 19 11:52:35.289: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename statefulset @ 12/19/23 11:52:35.291
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:52:35.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:52:35.369
  STEP: Creating service test in namespace statefulset-8866 @ 12/19/23 11:52:35.377
  STEP: Creating stateful set ss in namespace statefulset-8866 @ 12/19/23 11:52:35.393
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8866 @ 12/19/23 11:52:35.42
  Dec 19 11:52:35.431: INFO: Found 0 stateful pods, waiting for 1
  E1219 11:52:35.433693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:36.434740      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:37.435806      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:38.436280      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:39.436419      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:40.437020      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:41.437972      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:42.438762      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:43.439183      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:44.439305      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:45.439536      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:52:45.442: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 12/19/23 11:52:45.442
  Dec 19 11:52:45.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-8866 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Dec 19 11:52:45.788: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 19 11:52:45.789: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 19 11:52:45.789: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Dec 19 11:52:45.795: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E1219 11:52:46.439919      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:47.440947      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:48.441098      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:49.441407      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:50.441508      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:51.441831      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:52.442598      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:53.442851      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:54.443514      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:52:55.443959      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:52:55.805: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Dec 19 11:52:55.805: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 19 11:52:55.838: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Dec 19 11:52:55.838: INFO: ss-0  cahyeife7pae-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:52:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:52:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:52:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:52:35 +0000 UTC  }]
  Dec 19 11:52:55.839: INFO: 
  Dec 19 11:52:55.839: INFO: StatefulSet ss has not reached scale 3, at 1
  E1219 11:52:56.444807      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:52:56.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989274003s
  E1219 11:52:57.445746      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:52:57.865: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.977286227s
  E1219 11:52:58.446692      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:52:58.880: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.963421096s
  E1219 11:52:59.448258      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:52:59.889: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.949361528s
  E1219 11:53:00.448187      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:53:00.900: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.939732251s
  E1219 11:53:01.448697      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:53:01.910: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.928468813s
  E1219 11:53:02.449579      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:53:02.921: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.918524119s
  E1219 11:53:03.450196      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:53:03.933: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.908242734s
  E1219 11:53:04.450358      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:53:04.945: INFO: Verifying statefulset ss doesn't scale past 3 for another 895.302569ms
  E1219 11:53:05.451130      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8866 @ 12/19/23 11:53:05.945
  Dec 19 11:53:05.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-8866 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Dec 19 11:53:06.270: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Dec 19 11:53:06.270: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Dec 19 11:53:06.270: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Dec 19 11:53:06.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-8866 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E1219 11:53:06.451798      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:53:06.579: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Dec 19 11:53:06.579: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Dec 19 11:53:06.579: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Dec 19 11:53:06.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-8866 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Dec 19 11:53:06.884: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Dec 19 11:53:06.884: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Dec 19 11:53:06.884: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Dec 19 11:53:06.893: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  E1219 11:53:07.451948      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:08.452183      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:09.452437      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:10.452782      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:11.452852      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:12.453473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:13.453773      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:14.453852      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:15.454196      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:16.454420      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:53:16.906: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Dec 19 11:53:16.907: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Dec 19 11:53:16.907: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 12/19/23 11:53:16.907
  Dec 19 11:53:16.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-8866 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Dec 19 11:53:17.254: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 19 11:53:17.254: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 19 11:53:17.254: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Dec 19 11:53:17.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-8866 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E1219 11:53:17.455539      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:53:17.619: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 19 11:53:17.619: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 19 11:53:17.619: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Dec 19 11:53:17.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-129681620 --namespace=statefulset-8866 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Dec 19 11:53:18.081: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Dec 19 11:53:18.081: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Dec 19 11:53:18.081: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Dec 19 11:53:18.081: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 19 11:53:18.089: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  E1219 11:53:18.456695      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:19.456934      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:20.457067      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:21.457233      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:22.457445      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:23.457754      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:24.457799      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:25.458014      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:26.458216      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:27.459217      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:53:28.109: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Dec 19 11:53:28.109: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Dec 19 11:53:28.109: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Dec 19 11:53:28.148: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Dec 19 11:53:28.148: INFO: ss-0  cahyeife7pae-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:52:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:53:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:53:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:52:35 +0000 UTC  }]
  Dec 19 11:53:28.148: INFO: ss-1  cahyeife7pae-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:52:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:53:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:53:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:52:55 +0000 UTC  }]
  Dec 19 11:53:28.148: INFO: ss-2  cahyeife7pae-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:52:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:53:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:53:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:52:55 +0000 UTC  }]
  Dec 19 11:53:28.148: INFO: 
  Dec 19 11:53:28.148: INFO: StatefulSet ss has not reached scale 0, at 3
  E1219 11:53:28.460034      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:53:29.157: INFO: POD   NODE            PHASE      GRACE  CONDITIONS
  Dec 19 11:53:29.157: INFO: ss-0  cahyeife7pae-2  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:52:35 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:53:17 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:53:17 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:52:35 +0000 UTC  }]
  Dec 19 11:53:29.158: INFO: ss-1  cahyeife7pae-3  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:52:55 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:53:18 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:53:18 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:52:55 +0000 UTC  }]
  Dec 19 11:53:29.158: INFO: ss-2  cahyeife7pae-1  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:52:55 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:53:18 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:53:18 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-12-19 11:52:55 +0000 UTC  }]
  Dec 19 11:53:29.158: INFO: 
  Dec 19 11:53:29.159: INFO: StatefulSet ss has not reached scale 0, at 3
  E1219 11:53:29.460775      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:53:30.166: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.979931882s
  E1219 11:53:30.461730      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:53:31.173: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.973218826s
  E1219 11:53:31.461960      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:53:32.181: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.965886914s
  E1219 11:53:32.462664      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:53:33.189: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.957545049s
  E1219 11:53:33.463360      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:53:34.203: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.949149957s
  E1219 11:53:34.463943      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:53:35.212: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.935595124s
  E1219 11:53:35.464817      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:53:36.220: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.926519669s
  E1219 11:53:36.464887      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Dec 19 11:53:37.228: INFO: Verifying statefulset ss doesn't scale past 0 for another 918.265635ms
  E1219 11:53:37.466014      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8866 @ 12/19/23 11:53:38.229
  Dec 19 11:53:38.236: INFO: Scaling statefulset ss to 0
  Dec 19 11:53:38.268: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 19 11:53:38.275: INFO: Deleting all statefulset in ns statefulset-8866
  Dec 19 11:53:38.283: INFO: Scaling statefulset ss to 0
  Dec 19 11:53:38.308: INFO: Waiting for statefulset status.replicas updated to 0
  Dec 19 11:53:38.317: INFO: Deleting statefulset ss
  Dec 19 11:53:38.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8866" for this suite. @ 12/19/23 11:53:38.381
• [63.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 12/19/23 11:53:38.41
  Dec 19 11:53:38.410: INFO: >>> kubeConfig: /tmp/kubeconfig-129681620
  STEP: Building a namespace api object, basename emptydir @ 12/19/23 11:53:38.415
  STEP: Waiting for a default service account to be provisioned in namespace @ 12/19/23 11:53:38.462
  E1219 11:53:38.466276      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 12/19/23 11:53:38.471
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 12/19/23 11:53:38.485
  E1219 11:53:39.466692      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E1219 11:53:40.466928      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 12/19/23 11:53:40.539
  Dec 19 11:53:40.546: INFO: Trying to get logs from node cahyeife7pae-3 pod pod-c5e7a514-1953-4e61-92d9-97631baef761 container test-container: <nil>
  STEP: delete the pod @ 12/19/23 11:53:40.561
  Dec 19 11:53:40.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-556" for this suite. @ 12/19/23 11:53:40.603
• [2.206 seconds]
------------------------------
SSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Dec 19 11:53:40.623: INFO: Running AfterSuite actions on node 1
  Dec 19 11:53:40.624: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.002 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.129 seconds]
------------------------------

Ran 378 of 7209 Specs in 6702.905 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6831 Skipped
PASS

Ginkgo ran 1 suite in 1h51m43.926237328s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

