I0302 00:42:33.957203      22 e2e.go:129] Starting e2e run "07dcbd30-74e4-46f6-993c-0f84563800fa" on Ginkgo node 1
{"msg":"Test Suite starting","total":356,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1677717753 - Will randomize all specs
Will run 356 of 6973 specs

Mar  2 00:42:39.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 00:42:39.254: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar  2 00:42:39.315: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar  2 00:42:39.386: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar  2 00:42:39.387: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Mar  2 00:42:39.387: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar  2 00:42:39.403: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Mar  2 00:42:39.403: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Mar  2 00:42:39.403: INFO: e2e test version: v1.24.6
Mar  2 00:42:39.410: INFO: kube-apiserver version: v1.24.6+263df15
Mar  2 00:42:39.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 00:42:39.435: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:42:39.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
W0302 00:42:39.504697      22 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Mar  2 00:42:39.504: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-65b1518c-763d-442e-817d-9ef96b8db49e
STEP: Creating a pod to test consume secrets
Mar  2 00:42:39.611: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fb3697fa-5f6f-46bb-9abe-2bff989544dc" in namespace "projected-4935" to be "Succeeded or Failed"
Mar  2 00:42:39.622: INFO: Pod "pod-projected-secrets-fb3697fa-5f6f-46bb-9abe-2bff989544dc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.090875ms
Mar  2 00:42:41.642: INFO: Pod "pod-projected-secrets-fb3697fa-5f6f-46bb-9abe-2bff989544dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031387107s
Mar  2 00:42:43.659: INFO: Pod "pod-projected-secrets-fb3697fa-5f6f-46bb-9abe-2bff989544dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04832027s
Mar  2 00:42:45.670: INFO: Pod "pod-projected-secrets-fb3697fa-5f6f-46bb-9abe-2bff989544dc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.059193709s
Mar  2 00:42:47.696: INFO: Pod "pod-projected-secrets-fb3697fa-5f6f-46bb-9abe-2bff989544dc": Phase="Running", Reason="", readiness=false. Elapsed: 8.08532651s
Mar  2 00:42:49.710: INFO: Pod "pod-projected-secrets-fb3697fa-5f6f-46bb-9abe-2bff989544dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.099116397s
STEP: Saw pod success
Mar  2 00:42:49.710: INFO: Pod "pod-projected-secrets-fb3697fa-5f6f-46bb-9abe-2bff989544dc" satisfied condition "Succeeded or Failed"
Mar  2 00:42:49.721: INFO: Trying to get logs from node 10.123.244.39 pod pod-projected-secrets-fb3697fa-5f6f-46bb-9abe-2bff989544dc container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 00:42:49.812: INFO: Waiting for pod pod-projected-secrets-fb3697fa-5f6f-46bb-9abe-2bff989544dc to disappear
Mar  2 00:42:49.823: INFO: Pod pod-projected-secrets-fb3697fa-5f6f-46bb-9abe-2bff989544dc no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Mar  2 00:42:49.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4935" for this suite.

• [SLOW TEST:10.425 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":1,"skipped":25,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:42:49.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4162.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4162.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4162.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4162.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 00:43:02.148: INFO: DNS probes using dns-4162/dns-test-556358c5-de3c-4054-8e53-2344a8a0b6c9 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Mar  2 00:43:02.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4162" for this suite.

• [SLOW TEST:12.430 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","total":356,"completed":2,"skipped":27,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:43:02.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service multi-endpoint-test in namespace services-8995
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8995 to expose endpoints map[]
Mar  2 00:43:02.677: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Mar  2 00:43:03.712: INFO: successfully validated that service multi-endpoint-test in namespace services-8995 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8995
Mar  2 00:43:03.820: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:43:05.856: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:43:07.835: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8995 to expose endpoints map[pod1:[100]]
Mar  2 00:43:07.904: INFO: successfully validated that service multi-endpoint-test in namespace services-8995 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8995
Mar  2 00:43:08.034: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:43:10.074: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:43:12.056: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:43:14.064: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:43:16.068: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:43:18.067: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:43:20.048: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8995 to expose endpoints map[pod1:[100] pod2:[101]]
Mar  2 00:43:20.116: INFO: successfully validated that service multi-endpoint-test in namespace services-8995 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Mar  2 00:43:20.116: INFO: Creating new exec pod
Mar  2 00:43:25.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-8995 exec execpodtnjhq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Mar  2 00:43:25.749: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Mar  2 00:43:25.749: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 00:43:25.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-8995 exec execpodtnjhq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.174.31 80'
Mar  2 00:43:26.154: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.174.31 80\nConnection to 172.21.174.31 80 port [tcp/http] succeeded!\n"
Mar  2 00:43:26.154: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 00:43:26.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-8995 exec execpodtnjhq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Mar  2 00:43:26.538: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Mar  2 00:43:26.538: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 00:43:26.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-8995 exec execpodtnjhq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.174.31 81'
Mar  2 00:43:26.898: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.174.31 81\nConnection to 172.21.174.31 81 port [tcp/*] succeeded!\n"
Mar  2 00:43:26.898: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-8995
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8995 to expose endpoints map[pod2:[101]]
Mar  2 00:43:28.020: INFO: successfully validated that service multi-endpoint-test in namespace services-8995 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8995
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8995 to expose endpoints map[]
Mar  2 00:43:29.194: INFO: successfully validated that service multi-endpoint-test in namespace services-8995 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar  2 00:43:29.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8995" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:27.172 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":356,"completed":3,"skipped":116,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:43:29.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 00:43:30.723: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 00:43:32.812: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 0, 43, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 43, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 0, 43, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 43, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 00:43:35.932: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 00:43:46.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2139" for this suite.
STEP: Destroying namespace "webhook-2139-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:17.325 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":356,"completed":4,"skipped":132,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:43:46.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-55b4315c-9af0-4c1a-9910-9ff62d565b70
STEP: Creating a pod to test consume configMaps
Mar  2 00:43:47.095: INFO: Waiting up to 5m0s for pod "pod-configmaps-5dad7a9d-bfc6-4f04-b346-39cea6251268" in namespace "configmap-4" to be "Succeeded or Failed"
Mar  2 00:43:47.117: INFO: Pod "pod-configmaps-5dad7a9d-bfc6-4f04-b346-39cea6251268": Phase="Pending", Reason="", readiness=false. Elapsed: 21.691915ms
Mar  2 00:43:49.135: INFO: Pod "pod-configmaps-5dad7a9d-bfc6-4f04-b346-39cea6251268": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040212629s
Mar  2 00:43:51.158: INFO: Pod "pod-configmaps-5dad7a9d-bfc6-4f04-b346-39cea6251268": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062380505s
Mar  2 00:43:53.175: INFO: Pod "pod-configmaps-5dad7a9d-bfc6-4f04-b346-39cea6251268": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.07927937s
STEP: Saw pod success
Mar  2 00:43:53.175: INFO: Pod "pod-configmaps-5dad7a9d-bfc6-4f04-b346-39cea6251268" satisfied condition "Succeeded or Failed"
Mar  2 00:43:53.196: INFO: Trying to get logs from node 10.123.244.39 pod pod-configmaps-5dad7a9d-bfc6-4f04-b346-39cea6251268 container configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 00:43:53.287: INFO: Waiting for pod pod-configmaps-5dad7a9d-bfc6-4f04-b346-39cea6251268 to disappear
Mar  2 00:43:53.297: INFO: Pod pod-configmaps-5dad7a9d-bfc6-4f04-b346-39cea6251268 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar  2 00:43:53.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4" for this suite.

• [SLOW TEST:6.527 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":5,"skipped":135,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:43:53.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 00:43:53.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 00:43:57.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1129" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":356,"completed":6,"skipped":139,"failed":0}
S
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:43:57.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Mar  2 00:43:57.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9365" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":7,"skipped":140,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:43:57.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name projected-secret-test-2f86d8a4-e6be-4ab4-805d-5d83d8ce234a
STEP: Creating a pod to test consume secrets
Mar  2 00:43:57.840: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fd38c28e-6770-4071-b926-ab6e2e2d47c6" in namespace "projected-5799" to be "Succeeded or Failed"
Mar  2 00:43:57.862: INFO: Pod "pod-projected-secrets-fd38c28e-6770-4071-b926-ab6e2e2d47c6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.565248ms
Mar  2 00:43:59.888: INFO: Pod "pod-projected-secrets-fd38c28e-6770-4071-b926-ab6e2e2d47c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047369106s
Mar  2 00:44:01.904: INFO: Pod "pod-projected-secrets-fd38c28e-6770-4071-b926-ab6e2e2d47c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063330688s
STEP: Saw pod success
Mar  2 00:44:01.904: INFO: Pod "pod-projected-secrets-fd38c28e-6770-4071-b926-ab6e2e2d47c6" satisfied condition "Succeeded or Failed"
Mar  2 00:44:01.928: INFO: Trying to get logs from node 10.123.244.39 pod pod-projected-secrets-fd38c28e-6770-4071-b926-ab6e2e2d47c6 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 00:44:02.007: INFO: Waiting for pod pod-projected-secrets-fd38c28e-6770-4071-b926-ab6e2e2d47c6 to disappear
Mar  2 00:44:02.047: INFO: Pod pod-projected-secrets-fd38c28e-6770-4071-b926-ab6e2e2d47c6 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Mar  2 00:44:02.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5799" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":8,"skipped":145,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:44:02.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 00:44:03.487: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 00:44:05.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 0, 44, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 44, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 0, 44, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 44, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 00:44:08.622: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 00:44:08.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4596" for this suite.
STEP: Destroying namespace "webhook-4596-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.922 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":356,"completed":9,"skipped":150,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:44:09.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 00:44:09.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 00:44:09.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9087" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":356,"completed":10,"skipped":200,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:44:09.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar  2 00:44:26.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-764" for this suite.

• [SLOW TEST:16.662 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":356,"completed":11,"skipped":201,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:44:26.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 00:44:26.682: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-5cd2eff1-b07b-435a-ad81-cf717634e4e2
STEP: Creating configMap with name cm-test-opt-upd-bb45bfb5-7345-4fbe-8821-310468fcc5ee
STEP: Creating the pod
Mar  2 00:44:26.797: INFO: The status of Pod pod-projected-configmaps-91b3643c-9e97-4488-aa70-59000ce2fb81 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:44:28.875: INFO: The status of Pod pod-projected-configmaps-91b3643c-9e97-4488-aa70-59000ce2fb81 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:44:30.813: INFO: The status of Pod pod-projected-configmaps-91b3643c-9e97-4488-aa70-59000ce2fb81 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-5cd2eff1-b07b-435a-ad81-cf717634e4e2
STEP: Updating configmap cm-test-opt-upd-bb45bfb5-7345-4fbe-8821-310468fcc5ee
STEP: Creating configMap with name cm-test-opt-create-d90d581e-9c32-42b3-a503-e4625e5ef4b7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Mar  2 00:45:53.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5133" for this suite.

• [SLOW TEST:86.911 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":12,"skipped":202,"failed":0}
SSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:45:53.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Mar  2 00:45:53.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1572" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":13,"skipped":208,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:45:54.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
W0302 00:45:54.169879      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Mar  2 00:45:55.122: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Mar  2 00:45:55.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0302 00:45:55.122115      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
STEP: Destroying namespace "gc-4879" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":356,"completed":14,"skipped":226,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:45:55.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar  2 00:45:55.392: INFO: Waiting up to 5m0s for pod "downwardapi-volume-583d88b4-3751-47c7-9051-a4a6a3519921" in namespace "projected-2796" to be "Succeeded or Failed"
Mar  2 00:45:55.415: INFO: Pod "downwardapi-volume-583d88b4-3751-47c7-9051-a4a6a3519921": Phase="Pending", Reason="", readiness=false. Elapsed: 22.278434ms
Mar  2 00:45:57.448: INFO: Pod "downwardapi-volume-583d88b4-3751-47c7-9051-a4a6a3519921": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05583028s
Mar  2 00:45:59.462: INFO: Pod "downwardapi-volume-583d88b4-3751-47c7-9051-a4a6a3519921": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069245028s
STEP: Saw pod success
Mar  2 00:45:59.462: INFO: Pod "downwardapi-volume-583d88b4-3751-47c7-9051-a4a6a3519921" satisfied condition "Succeeded or Failed"
Mar  2 00:45:59.474: INFO: Trying to get logs from node 10.123.244.39 pod downwardapi-volume-583d88b4-3751-47c7-9051-a4a6a3519921 container client-container: <nil>
STEP: delete the pod
Mar  2 00:45:59.538: INFO: Waiting for pod downwardapi-volume-583d88b4-3751-47c7-9051-a4a6a3519921 to disappear
Mar  2 00:45:59.548: INFO: Pod downwardapi-volume-583d88b4-3751-47c7-9051-a4a6a3519921 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar  2 00:45:59.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2796" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":15,"skipped":273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:45:59.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-dccfed5c-e02e-4d7a-822b-a7abe9cce81c in namespace container-probe-452
Mar  2 00:46:03.890: INFO: Started pod liveness-dccfed5c-e02e-4d7a-822b-a7abe9cce81c in namespace container-probe-452
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 00:46:03.908: INFO: Initial restart count of pod liveness-dccfed5c-e02e-4d7a-822b-a7abe9cce81c is 0
Mar  2 00:46:22.097: INFO: Restart count of pod container-probe-452/liveness-dccfed5c-e02e-4d7a-822b-a7abe9cce81c is now 1 (18.189547733s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Mar  2 00:46:22.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-452" for this suite.

• [SLOW TEST:22.562 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":16,"skipped":304,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:46:22.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Mar  2 00:46:24.597: INFO: running pods: 0 < 3
Mar  2 00:46:26.612: INFO: running pods: 0 < 3
Mar  2 00:46:28.635: INFO: running pods: 2 < 3
Mar  2 00:46:30.637: INFO: running pods: 2 < 3
Mar  2 00:46:32.628: INFO: running pods: 2 < 3
Mar  2 00:46:34.610: INFO: running pods: 2 < 3
Mar  2 00:46:36.613: INFO: running pods: 2 < 3
Mar  2 00:46:38.636: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Mar  2 00:46:40.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9740" for this suite.

• [SLOW TEST:18.530 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":356,"completed":17,"skipped":319,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:46:40.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Mar  2 00:47:01.418: INFO: EndpointSlice for Service endpointslice-261/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Mar  2 00:47:11.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-261" for this suite.

• [SLOW TEST:30.796 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":356,"completed":18,"skipped":347,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:47:11.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-e2ff9605-ac65-4bfd-9728-8a43256251ad
STEP: Creating a pod to test consume configMaps
Mar  2 00:47:11.750: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-020886b9-741d-4142-b4a2-b4672b42d73b" in namespace "projected-9484" to be "Succeeded or Failed"
Mar  2 00:47:11.764: INFO: Pod "pod-projected-configmaps-020886b9-741d-4142-b4a2-b4672b42d73b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.979858ms
Mar  2 00:47:13.777: INFO: Pod "pod-projected-configmaps-020886b9-741d-4142-b4a2-b4672b42d73b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027058134s
Mar  2 00:47:15.789: INFO: Pod "pod-projected-configmaps-020886b9-741d-4142-b4a2-b4672b42d73b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039179582s
Mar  2 00:47:17.813: INFO: Pod "pod-projected-configmaps-020886b9-741d-4142-b4a2-b4672b42d73b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062748154s
STEP: Saw pod success
Mar  2 00:47:17.813: INFO: Pod "pod-projected-configmaps-020886b9-741d-4142-b4a2-b4672b42d73b" satisfied condition "Succeeded or Failed"
Mar  2 00:47:17.859: INFO: Trying to get logs from node 10.123.244.39 pod pod-projected-configmaps-020886b9-741d-4142-b4a2-b4672b42d73b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar  2 00:47:17.954: INFO: Waiting for pod pod-projected-configmaps-020886b9-741d-4142-b4a2-b4672b42d73b to disappear
Mar  2 00:47:17.963: INFO: Pod pod-projected-configmaps-020886b9-741d-4142-b4a2-b4672b42d73b no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Mar  2 00:47:17.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9484" for this suite.

• [SLOW TEST:6.507 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":19,"skipped":355,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:47:18.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2399
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2399
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2399
Mar  2 00:47:18.282: INFO: Found 0 stateful pods, waiting for 1
Mar  2 00:47:28.295: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar  2 00:47:28.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-2399 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 00:47:28.617: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 00:47:28.618: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 00:47:28.618: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 00:47:28.631: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  2 00:47:38.647: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 00:47:38.647: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 00:47:38.741: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999997437s
Mar  2 00:47:39.762: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.979442478s
Mar  2 00:47:40.776: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.959001244s
Mar  2 00:47:41.792: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.945369985s
Mar  2 00:47:42.809: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.929036801s
Mar  2 00:47:43.833: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.912263334s
Mar  2 00:47:44.849: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.886873989s
Mar  2 00:47:45.862: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.872215061s
Mar  2 00:47:46.875: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.859225799s
Mar  2 00:47:47.893: INFO: Verifying statefulset ss doesn't scale past 1 for another 846.355632ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2399
Mar  2 00:47:48.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-2399 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 00:47:49.254: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 00:47:49.254: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 00:47:49.254: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 00:47:49.269: INFO: Found 1 stateful pods, waiting for 3
Mar  2 00:47:59.288: INFO: Found 2 stateful pods, waiting for 3
Mar  2 00:48:09.287: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 00:48:09.287: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 00:48:09.287: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar  2 00:48:09.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-2399 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 00:48:09.734: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 00:48:09.734: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 00:48:09.734: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 00:48:09.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-2399 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 00:48:10.178: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 00:48:10.178: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 00:48:10.178: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 00:48:10.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-2399 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 00:48:10.565: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 00:48:10.565: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 00:48:10.565: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 00:48:10.565: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 00:48:10.589: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  2 00:48:20.628: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 00:48:20.628: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 00:48:20.629: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 00:48:20.736: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999761s
Mar  2 00:48:21.763: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.984506385s
Mar  2 00:48:22.786: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.956857971s
Mar  2 00:48:23.805: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.934002424s
Mar  2 00:48:24.825: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.915385009s
Mar  2 00:48:25.845: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.894740215s
Mar  2 00:48:26.877: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.875011413s
Mar  2 00:48:27.910: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.842544642s
Mar  2 00:48:28.925: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.80993042s
Mar  2 00:48:29.946: INFO: Verifying statefulset ss doesn't scale past 3 for another 794.647346ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2399
Mar  2 00:48:30.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-2399 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 00:48:31.368: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 00:48:31.368: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 00:48:31.368: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 00:48:31.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-2399 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 00:48:31.723: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 00:48:31.723: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 00:48:31.723: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 00:48:31.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-2399 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 00:48:32.112: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 00:48:32.112: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 00:48:32.112: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 00:48:32.112: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 00:48:42.187: INFO: Deleting all statefulset in ns statefulset-2399
Mar  2 00:48:42.202: INFO: Scaling statefulset ss to 0
Mar  2 00:48:42.243: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 00:48:42.254: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Mar  2 00:48:42.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2399" for this suite.

• [SLOW TEST:84.328 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":356,"completed":20,"skipped":444,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:48:42.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-c466d01e-9425-454d-a48d-1fe18300986a
STEP: Creating a pod to test consume secrets
Mar  2 00:48:42.563: INFO: Waiting up to 5m0s for pod "pod-secrets-db9a31df-64f6-4107-a86d-62b1a0559b10" in namespace "secrets-8805" to be "Succeeded or Failed"
Mar  2 00:48:42.575: INFO: Pod "pod-secrets-db9a31df-64f6-4107-a86d-62b1a0559b10": Phase="Pending", Reason="", readiness=false. Elapsed: 12.117748ms
Mar  2 00:48:44.593: INFO: Pod "pod-secrets-db9a31df-64f6-4107-a86d-62b1a0559b10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030318095s
Mar  2 00:48:46.609: INFO: Pod "pod-secrets-db9a31df-64f6-4107-a86d-62b1a0559b10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046448157s
Mar  2 00:48:48.623: INFO: Pod "pod-secrets-db9a31df-64f6-4107-a86d-62b1a0559b10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.06090732s
STEP: Saw pod success
Mar  2 00:48:48.624: INFO: Pod "pod-secrets-db9a31df-64f6-4107-a86d-62b1a0559b10" satisfied condition "Succeeded or Failed"
Mar  2 00:48:48.634: INFO: Trying to get logs from node 10.123.244.39 pod pod-secrets-db9a31df-64f6-4107-a86d-62b1a0559b10 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 00:48:48.750: INFO: Waiting for pod pod-secrets-db9a31df-64f6-4107-a86d-62b1a0559b10 to disappear
Mar  2 00:48:48.760: INFO: Pod pod-secrets-db9a31df-64f6-4107-a86d-62b1a0559b10 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Mar  2 00:48:48.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8805" for this suite.

• [SLOW TEST:6.455 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":21,"skipped":457,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:48:48.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Deleting RuntimeClass runtimeclass-6016-delete-me
STEP: Waiting for the RuntimeClass to disappear
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Mar  2 00:48:49.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6016" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":22,"skipped":487,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:48:49.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7211, will wait for the garbage collector to delete the pods
Mar  2 00:48:53.314: INFO: Deleting Job.batch foo took: 17.787584ms
Mar  2 00:48:53.415: INFO: Terminating Job.batch foo pods took: 101.14525ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Mar  2 00:49:27.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7211" for this suite.

• [SLOW TEST:38.393 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":356,"completed":23,"skipped":497,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:49:27.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 00:49:27.702: INFO: The status of Pod busybox-host-aliases6efa6629-3837-4b47-b780-d29dc91f4ebf is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:49:29.717: INFO: The status of Pod busybox-host-aliases6efa6629-3837-4b47-b780-d29dc91f4ebf is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Mar  2 00:49:30.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4664" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":24,"skipped":547,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:49:30.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar  2 00:49:30.336: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1070  36131b1f-e2c6-4a96-ac30-6d7982ac7e72 80815 0 2023-03-02 00:49:30 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2023-03-02 00:49:30 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lmq7g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lmq7g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c33,c32,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 00:49:30.371: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:49:32.389: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:49:34.383: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Mar  2 00:49:34.383: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1070 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 00:49:34.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 00:49:34.384: INFO: ExecWithOptions: Clientset creation
Mar  2 00:49:34.385: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-1070/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod...
Mar  2 00:49:34.609: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1070 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 00:49:34.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 00:49:34.611: INFO: ExecWithOptions: Clientset creation
Mar  2 00:49:34.611: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-1070/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 00:49:34.799: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Mar  2 00:49:34.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1070" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":356,"completed":25,"skipped":556,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:49:34.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Mar  2 00:49:35.036: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 00:50:35.314: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 00:50:35.337: INFO: Starting informer...
STEP: Starting pods...
Mar  2 00:50:35.618: INFO: Pod1 is running on 10.123.244.39. Tainting Node
Mar  2 00:50:37.922: INFO: Pod2 is running on 10.123.244.39. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Mar  2 00:50:47.883: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar  2 00:51:04.855: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:188
Mar  2 00:51:05.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-2980" for this suite.

• [SLOW TEST:90.202 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":356,"completed":26,"skipped":567,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:51:05.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-2e7f13a8-209b-4325-8be2-2f910c9eecec
STEP: Creating a pod to test consume secrets
Mar  2 00:51:05.310: INFO: Waiting up to 5m0s for pod "pod-secrets-c2c2b05b-d636-41bd-9853-07f3fe48f132" in namespace "secrets-8970" to be "Succeeded or Failed"
Mar  2 00:51:05.333: INFO: Pod "pod-secrets-c2c2b05b-d636-41bd-9853-07f3fe48f132": Phase="Pending", Reason="", readiness=false. Elapsed: 23.102759ms
Mar  2 00:51:07.372: INFO: Pod "pod-secrets-c2c2b05b-d636-41bd-9853-07f3fe48f132": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06174725s
Mar  2 00:51:09.385: INFO: Pod "pod-secrets-c2c2b05b-d636-41bd-9853-07f3fe48f132": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075106727s
Mar  2 00:51:11.403: INFO: Pod "pod-secrets-c2c2b05b-d636-41bd-9853-07f3fe48f132": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.092954233s
STEP: Saw pod success
Mar  2 00:51:11.403: INFO: Pod "pod-secrets-c2c2b05b-d636-41bd-9853-07f3fe48f132" satisfied condition "Succeeded or Failed"
Mar  2 00:51:11.419: INFO: Trying to get logs from node 10.123.244.39 pod pod-secrets-c2c2b05b-d636-41bd-9853-07f3fe48f132 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 00:51:11.512: INFO: Waiting for pod pod-secrets-c2c2b05b-d636-41bd-9853-07f3fe48f132 to disappear
Mar  2 00:51:11.526: INFO: Pod pod-secrets-c2c2b05b-d636-41bd-9853-07f3fe48f132 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Mar  2 00:51:11.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8970" for this suite.

• [SLOW TEST:6.441 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":27,"skipped":574,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:51:11.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
Mar  2 00:51:11.792: INFO: Creating simple deployment test-deployment-rj2p7
Mar  2 00:51:11.859: INFO: deployment "test-deployment-rj2p7" doesn't have the required revision set
Mar  2 00:51:13.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 0, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 51, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 0, 51, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 51, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-rj2p7-688c4d6789\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status
Mar  2 00:51:16.077: INFO: Deployment test-deployment-rj2p7 has Conditions: [{Available True 2023-03-02 00:51:13 +0000 UTC 2023-03-02 00:51:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-02 00:51:13 +0000 UTC 2023-03-02 00:51:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rj2p7-688c4d6789" has successfully progressed.}]
STEP: updating Deployment Status
Mar  2 00:51:16.121: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 0, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 51, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 0, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 51, 11, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-rj2p7-688c4d6789\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Mar  2 00:51:16.134: INFO: Observed &Deployment event: ADDED
Mar  2 00:51:16.134: INFO: Observed Deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 00:51:11 +0000 UTC 2023-03-02 00:51:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rj2p7-688c4d6789"}
Mar  2 00:51:16.134: INFO: Observed &Deployment event: MODIFIED
Mar  2 00:51:16.134: INFO: Observed Deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 00:51:11 +0000 UTC 2023-03-02 00:51:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rj2p7-688c4d6789"}
Mar  2 00:51:16.134: INFO: Observed Deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 00:51:11 +0000 UTC 2023-03-02 00:51:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  2 00:51:16.135: INFO: Observed &Deployment event: MODIFIED
Mar  2 00:51:16.135: INFO: Observed Deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 00:51:11 +0000 UTC 2023-03-02 00:51:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  2 00:51:16.135: INFO: Observed Deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 00:51:11 +0000 UTC 2023-03-02 00:51:11 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-rj2p7-688c4d6789" is progressing.}
Mar  2 00:51:16.135: INFO: Observed &Deployment event: MODIFIED
Mar  2 00:51:16.136: INFO: Observed Deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 00:51:13 +0000 UTC 2023-03-02 00:51:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  2 00:51:16.136: INFO: Observed Deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 00:51:13 +0000 UTC 2023-03-02 00:51:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rj2p7-688c4d6789" has successfully progressed.}
Mar  2 00:51:16.136: INFO: Observed &Deployment event: MODIFIED
Mar  2 00:51:16.136: INFO: Observed Deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 00:51:13 +0000 UTC 2023-03-02 00:51:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  2 00:51:16.136: INFO: Observed Deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 00:51:13 +0000 UTC 2023-03-02 00:51:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rj2p7-688c4d6789" has successfully progressed.}
Mar  2 00:51:16.136: INFO: Found Deployment test-deployment-rj2p7 in namespace deployment-634 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 00:51:16.136: INFO: Deployment test-deployment-rj2p7 has an updated status
STEP: patching the Statefulset Status
Mar  2 00:51:16.136: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar  2 00:51:16.164: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Mar  2 00:51:16.195: INFO: Observed &Deployment event: ADDED
Mar  2 00:51:16.195: INFO: Observed deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 00:51:11 +0000 UTC 2023-03-02 00:51:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rj2p7-688c4d6789"}
Mar  2 00:51:16.195: INFO: Observed &Deployment event: MODIFIED
Mar  2 00:51:16.195: INFO: Observed deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 00:51:11 +0000 UTC 2023-03-02 00:51:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rj2p7-688c4d6789"}
Mar  2 00:51:16.196: INFO: Observed deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 00:51:11 +0000 UTC 2023-03-02 00:51:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  2 00:51:16.196: INFO: Observed &Deployment event: MODIFIED
Mar  2 00:51:16.197: INFO: Observed deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-02 00:51:11 +0000 UTC 2023-03-02 00:51:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar  2 00:51:16.197: INFO: Observed deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 00:51:11 +0000 UTC 2023-03-02 00:51:11 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-rj2p7-688c4d6789" is progressing.}
Mar  2 00:51:16.198: INFO: Observed &Deployment event: MODIFIED
Mar  2 00:51:16.198: INFO: Observed deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 00:51:13 +0000 UTC 2023-03-02 00:51:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  2 00:51:16.198: INFO: Observed deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 00:51:13 +0000 UTC 2023-03-02 00:51:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rj2p7-688c4d6789" has successfully progressed.}
Mar  2 00:51:16.199: INFO: Observed &Deployment event: MODIFIED
Mar  2 00:51:16.199: INFO: Observed deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-02 00:51:13 +0000 UTC 2023-03-02 00:51:13 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar  2 00:51:16.199: INFO: Observed deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-02 00:51:13 +0000 UTC 2023-03-02 00:51:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rj2p7-688c4d6789" has successfully progressed.}
Mar  2 00:51:16.199: INFO: Observed deployment test-deployment-rj2p7 in namespace deployment-634 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 00:51:16.200: INFO: Observed &Deployment event: MODIFIED
Mar  2 00:51:16.200: INFO: Found deployment test-deployment-rj2p7 in namespace deployment-634 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Mar  2 00:51:16.201: INFO: Deployment test-deployment-rj2p7 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 00:51:16.224: INFO: Deployment "test-deployment-rj2p7":
&Deployment{ObjectMeta:{test-deployment-rj2p7  deployment-634  b1478042-c185-48c8-a390-b7b1570073b1 82467 1 2023-03-02 00:51:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2023-03-02 00:51:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-02 00:51:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-02 00:51:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0020c4b68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-rj2p7-688c4d6789",LastUpdateTime:2023-03-02 00:51:16 +0000 UTC,LastTransitionTime:2023-03-02 00:51:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 00:51:16.236: INFO: New ReplicaSet "test-deployment-rj2p7-688c4d6789" of Deployment "test-deployment-rj2p7":
&ReplicaSet{ObjectMeta:{test-deployment-rj2p7-688c4d6789  deployment-634  c7aa41a6-0e9a-4bc6-aacb-a20077d4f44f 82450 1 2023-03-02 00:51:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-rj2p7 b1478042-c185-48c8-a390-b7b1570073b1 0xc001fd8500 0xc001fd8501}] []  [{kube-controller-manager Update apps/v1 2023-03-02 00:51:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1478042-c185-48c8-a390-b7b1570073b1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 00:51:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 688c4d6789,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001fd85a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 00:51:16.248: INFO: Pod "test-deployment-rj2p7-688c4d6789-v4trw" is available:
&Pod{ObjectMeta:{test-deployment-rj2p7-688c4d6789-v4trw test-deployment-rj2p7-688c4d6789- deployment-634  8cea5418-f7cd-46a3-b0cb-f24cd7e41da2 82449 0 2023-03-02 00:51:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[cni.projectcalico.org/containerID:20dc62e614a6004ce07f24fb9fff3d1c8a848eb843cbf9fe1061dd04f0939d63 cni.projectcalico.org/podIP:172.30.88.227/32 cni.projectcalico.org/podIPs:172.30.88.227/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.227"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.227"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-rj2p7-688c4d6789 c7aa41a6-0e9a-4bc6-aacb-a20077d4f44f 0xc0020c4f87 0xc0020c4f88}] []  [{kube-controller-manager Update v1 2023-03-02 00:51:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c7aa41a6-0e9a-4bc6-aacb-a20077d4f44f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-03-02 00:51:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 00:51:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 00:51:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.88.227\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vfhk7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vfhk7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.39,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 00:51:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 00:51:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 00:51:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 00:51:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.39,PodIP:172.30.88.227,StartTime:2023-03-02 00:51:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 00:51:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://bef71ffcdf918bff4f52a618f5055d8ed813623ec012f32f0f54292349a6f25d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.88.227,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Mar  2 00:51:16.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-634" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":356,"completed":28,"skipped":589,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:51:16.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Starting the proxy
Mar  2 00:51:16.451: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-3124 proxy --unix-socket=/tmp/kubectl-proxy-unix712034972/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar  2 00:51:16.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3124" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":356,"completed":29,"skipped":595,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:51:16.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replication controller my-hostname-basic-b976d2e1-3990-4270-8b75-f4b3059c9cc4
Mar  2 00:51:17.115: INFO: Pod name my-hostname-basic-b976d2e1-3990-4270-8b75-f4b3059c9cc4: Found 0 pods out of 1
Mar  2 00:51:22.135: INFO: Pod name my-hostname-basic-b976d2e1-3990-4270-8b75-f4b3059c9cc4: Found 1 pods out of 1
Mar  2 00:51:22.135: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-b976d2e1-3990-4270-8b75-f4b3059c9cc4" are running
Mar  2 00:51:22.147: INFO: Pod "my-hostname-basic-b976d2e1-3990-4270-8b75-f4b3059c9cc4-rlxbx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 00:51:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 00:51:18 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 00:51:18 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 00:51:17 +0000 UTC Reason: Message:}])
Mar  2 00:51:22.147: INFO: Trying to dial the pod
Mar  2 00:51:27.308: INFO: Controller my-hostname-basic-b976d2e1-3990-4270-8b75-f4b3059c9cc4: Got expected result from replica 1 [my-hostname-basic-b976d2e1-3990-4270-8b75-f4b3059c9cc4-rlxbx]: "my-hostname-basic-b976d2e1-3990-4270-8b75-f4b3059c9cc4-rlxbx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Mar  2 00:51:27.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2511" for this suite.

• [SLOW TEST:10.542 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":30,"skipped":640,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:51:27.444: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2896
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2896
I0302 00:51:27.798063      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2896, replica count: 2
I0302 00:51:30.852266      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 00:51:33.855912      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 00:51:36.873250      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 00:51:39.874925      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 00:51:42.877032      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 00:51:45.883026      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 00:51:48.893773      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 00:51:51.904014      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 00:51:54.906154      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 00:51:57.913138      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 00:52:00.926: INFO: Creating new exec pod
I0302 00:52:00.926911      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 00:52:04.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-2896 exec execpods2klj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  2 00:52:04.922: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 00:52:04.922: INFO: stdout: "externalname-service-f4wsj"
Mar  2 00:52:04.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-2896 exec execpods2klj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.133.45 80'
Mar  2 00:52:05.818: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.133.45 80\nConnection to 172.21.133.45 80 port [tcp/http] succeeded!\n"
Mar  2 00:52:05.818: INFO: stdout: "externalname-service-jp5mf"
Mar  2 00:52:05.818: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar  2 00:52:05.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2896" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:38.520 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":356,"completed":31,"skipped":729,"failed":0}
SS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:52:05.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:79
Mar  2 00:52:06.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the sample API server.
Mar  2 00:52:08.674: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar  2 00:52:10.925: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 00:52:12.947: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 00:52:14.950: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 00:52:16.961: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 00:52:18.972: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 00:52:20.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 52, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-d9646c97b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 00:52:24.393: INFO: Waited 1.414862882s for the sample-apiserver to be ready to handle requests.
I0302 00:52:25.695644      22 request.go:682] Waited for 1.007930281s due to client-side throttling, not priority and fairness, request: GET:https://172.21.0.1:443/apis/whereabouts.cni.cncf.io/v1alpha1
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Mar  2 00:52:26.925: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:69
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:188
Mar  2 00:52:27.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2269" for this suite.

• [SLOW TEST:21.764 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":356,"completed":32,"skipped":731,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:52:27.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar  2 00:52:27.983: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bcd5e435-e406-4086-a535-7e1d9c46d269" in namespace "downward-api-2018" to be "Succeeded or Failed"
Mar  2 00:52:27.992: INFO: Pod "downwardapi-volume-bcd5e435-e406-4086-a535-7e1d9c46d269": Phase="Pending", Reason="", readiness=false. Elapsed: 9.585961ms
Mar  2 00:52:30.028: INFO: Pod "downwardapi-volume-bcd5e435-e406-4086-a535-7e1d9c46d269": Phase="Running", Reason="", readiness=true. Elapsed: 2.04507142s
Mar  2 00:52:32.057: INFO: Pod "downwardapi-volume-bcd5e435-e406-4086-a535-7e1d9c46d269": Phase="Running", Reason="", readiness=false. Elapsed: 4.074214281s
Mar  2 00:52:34.072: INFO: Pod "downwardapi-volume-bcd5e435-e406-4086-a535-7e1d9c46d269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.089605024s
STEP: Saw pod success
Mar  2 00:52:34.073: INFO: Pod "downwardapi-volume-bcd5e435-e406-4086-a535-7e1d9c46d269" satisfied condition "Succeeded or Failed"
Mar  2 00:52:34.146: INFO: Trying to get logs from node 10.123.244.39 pod downwardapi-volume-bcd5e435-e406-4086-a535-7e1d9c46d269 container client-container: <nil>
STEP: delete the pod
Mar  2 00:52:34.430: INFO: Waiting for pod downwardapi-volume-bcd5e435-e406-4086-a535-7e1d9c46d269 to disappear
Mar  2 00:52:34.440: INFO: Pod downwardapi-volume-bcd5e435-e406-4086-a535-7e1d9c46d269 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar  2 00:52:34.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2018" for this suite.

• [SLOW TEST:6.780 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":33,"skipped":747,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:52:34.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  2 00:52:34.813: INFO: Waiting up to 5m0s for pod "pod-1fa174c4-3e65-4251-8331-6ff7db06c266" in namespace "emptydir-205" to be "Succeeded or Failed"
Mar  2 00:52:34.833: INFO: Pod "pod-1fa174c4-3e65-4251-8331-6ff7db06c266": Phase="Pending", Reason="", readiness=false. Elapsed: 19.943041ms
Mar  2 00:52:36.872: INFO: Pod "pod-1fa174c4-3e65-4251-8331-6ff7db06c266": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059521663s
Mar  2 00:52:38.927: INFO: Pod "pod-1fa174c4-3e65-4251-8331-6ff7db06c266": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.114855434s
STEP: Saw pod success
Mar  2 00:52:38.928: INFO: Pod "pod-1fa174c4-3e65-4251-8331-6ff7db06c266" satisfied condition "Succeeded or Failed"
Mar  2 00:52:38.965: INFO: Trying to get logs from node 10.123.244.39 pod pod-1fa174c4-3e65-4251-8331-6ff7db06c266 container test-container: <nil>
STEP: delete the pod
Mar  2 00:52:39.081: INFO: Waiting for pod pod-1fa174c4-3e65-4251-8331-6ff7db06c266 to disappear
Mar  2 00:52:39.120: INFO: Pod pod-1fa174c4-3e65-4251-8331-6ff7db06c266 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar  2 00:52:39.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-205" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":34,"skipped":756,"failed":0}
SSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:52:39.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Mar  2 00:52:39.356: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Mar  2 00:52:39.400: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  2 00:52:39.401: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Mar  2 00:52:39.477: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar  2 00:52:39.477: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Mar  2 00:52:39.562: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar  2 00:52:39.562: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Mar  2 00:52:46.828: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:188
Mar  2 00:52:46.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4866" for this suite.

• [SLOW TEST:7.846 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":356,"completed":35,"skipped":760,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:52:47.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-317adbc3-7a3f-4af1-950f-e6d766eef965
STEP: Creating a pod to test consume secrets
Mar  2 00:52:47.478: INFO: Waiting up to 5m0s for pod "pod-secrets-0de338c9-ce41-4372-b127-b78c3fc18d0d" in namespace "secrets-6421" to be "Succeeded or Failed"
Mar  2 00:52:47.491: INFO: Pod "pod-secrets-0de338c9-ce41-4372-b127-b78c3fc18d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.472283ms
Mar  2 00:52:49.503: INFO: Pod "pod-secrets-0de338c9-ce41-4372-b127-b78c3fc18d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024607984s
Mar  2 00:52:51.518: INFO: Pod "pod-secrets-0de338c9-ce41-4372-b127-b78c3fc18d0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039589825s
STEP: Saw pod success
Mar  2 00:52:51.518: INFO: Pod "pod-secrets-0de338c9-ce41-4372-b127-b78c3fc18d0d" satisfied condition "Succeeded or Failed"
Mar  2 00:52:51.536: INFO: Trying to get logs from node 10.123.244.39 pod pod-secrets-0de338c9-ce41-4372-b127-b78c3fc18d0d container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 00:52:51.599: INFO: Waiting for pod pod-secrets-0de338c9-ce41-4372-b127-b78c3fc18d0d to disappear
Mar  2 00:52:51.624: INFO: Pod pod-secrets-0de338c9-ce41-4372-b127-b78c3fc18d0d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Mar  2 00:52:51.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6421" for this suite.
STEP: Destroying namespace "secret-namespace-8933" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":356,"completed":36,"skipped":766,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:52:51.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar  2 00:53:03.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9942" for this suite.

• [SLOW TEST:11.569 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":356,"completed":37,"skipped":785,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:53:03.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 00:53:05.578: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 00:53:07.700: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 0, 53, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 53, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 0, 53, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 53, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 00:53:10.767: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 00:53:11.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-547" for this suite.
STEP: Destroying namespace "webhook-547-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:8.066 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":356,"completed":38,"skipped":787,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:53:11.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:188
Mar  2 00:53:11.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8226" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":356,"completed":39,"skipped":856,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:53:11.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Mar  2 00:53:11.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-7606 create -f -'
Mar  2 00:53:21.756: INFO: stderr: ""
Mar  2 00:53:21.756: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar  2 00:53:22.979: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 00:53:22.979: INFO: Found 0 / 1
Mar  2 00:53:23.862: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 00:53:23.862: INFO: Found 1 / 1
Mar  2 00:53:23.862: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar  2 00:53:23.885: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 00:53:23.886: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 00:53:23.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-7606 patch pod agnhost-primary-vm4mn -p {"metadata":{"annotations":{"x":"y"}}}'
Mar  2 00:53:25.621: INFO: stderr: ""
Mar  2 00:53:25.621: INFO: stdout: "pod/agnhost-primary-vm4mn patched\n"
STEP: checking annotations
Mar  2 00:53:25.639: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 00:53:25.639: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar  2 00:53:25.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7606" for this suite.

• [SLOW TEST:13.943 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1486
    should add annotations for pods in rc  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":356,"completed":40,"skipped":881,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:53:25.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  2 00:53:25.943: INFO: Waiting up to 5m0s for pod "pod-3848cc29-cc9c-481b-9a5f-a694a655c11e" in namespace "emptydir-4095" to be "Succeeded or Failed"
Mar  2 00:53:25.978: INFO: Pod "pod-3848cc29-cc9c-481b-9a5f-a694a655c11e": Phase="Pending", Reason="", readiness=false. Elapsed: 34.353848ms
Mar  2 00:53:28.043: INFO: Pod "pod-3848cc29-cc9c-481b-9a5f-a694a655c11e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.099155619s
Mar  2 00:53:30.092: INFO: Pod "pod-3848cc29-cc9c-481b-9a5f-a694a655c11e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.148468953s
STEP: Saw pod success
Mar  2 00:53:30.092: INFO: Pod "pod-3848cc29-cc9c-481b-9a5f-a694a655c11e" satisfied condition "Succeeded or Failed"
Mar  2 00:53:30.118: INFO: Trying to get logs from node 10.123.244.39 pod pod-3848cc29-cc9c-481b-9a5f-a694a655c11e container test-container: <nil>
STEP: delete the pod
Mar  2 00:53:30.311: INFO: Waiting for pod pod-3848cc29-cc9c-481b-9a5f-a694a655c11e to disappear
Mar  2 00:53:30.365: INFO: Pod pod-3848cc29-cc9c-481b-9a5f-a694a655c11e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar  2 00:53:30.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4095" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":41,"skipped":889,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:53:30.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: reading a file in the container
Mar  2 00:53:33.487: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5973 pod-service-account-56c84789-30a4-49e3-9af8-3a29067638d2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar  2 00:53:43.389: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5973 pod-service-account-56c84789-30a4-49e3-9af8-3a29067638d2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar  2 00:53:44.361: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5973 pod-service-account-56c84789-30a4-49e3-9af8-3a29067638d2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Mar  2 00:53:44.781: INFO: Got root ca configmap in namespace "svcaccounts-5973"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Mar  2 00:53:44.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5973" for this suite.

• [SLOW TEST:14.487 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":356,"completed":42,"skipped":892,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:53:44.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Mar  2 00:53:45.251: INFO: Waiting up to 5m0s for pod "downward-api-38557e11-93d6-4063-b9f4-142eccd766c6" in namespace "downward-api-1126" to be "Succeeded or Failed"
Mar  2 00:53:45.263: INFO: Pod "downward-api-38557e11-93d6-4063-b9f4-142eccd766c6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.551319ms
Mar  2 00:53:47.279: INFO: Pod "downward-api-38557e11-93d6-4063-b9f4-142eccd766c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028045534s
Mar  2 00:53:49.322: INFO: Pod "downward-api-38557e11-93d6-4063-b9f4-142eccd766c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070518517s
STEP: Saw pod success
Mar  2 00:53:49.357: INFO: Pod "downward-api-38557e11-93d6-4063-b9f4-142eccd766c6" satisfied condition "Succeeded or Failed"
Mar  2 00:53:49.414: INFO: Trying to get logs from node 10.123.244.39 pod downward-api-38557e11-93d6-4063-b9f4-142eccd766c6 container dapi-container: <nil>
STEP: delete the pod
Mar  2 00:53:49.758: INFO: Waiting for pod downward-api-38557e11-93d6-4063-b9f4-142eccd766c6 to disappear
Mar  2 00:53:49.793: INFO: Pod downward-api-38557e11-93d6-4063-b9f4-142eccd766c6 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Mar  2 00:53:49.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1126" for this suite.

• [SLOW TEST:5.195 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":356,"completed":43,"skipped":895,"failed":0}
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:53:50.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 00:53:51.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar  2 00:53:51.522: INFO: The status of Pod pod-exec-websocket-a389f0ab-54eb-4595-a6b7-4ed18ec24610 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:53:53.566: INFO: The status of Pod pod-exec-websocket-a389f0ab-54eb-4595-a6b7-4ed18ec24610 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:53:55.552: INFO: The status of Pod pod-exec-websocket-a389f0ab-54eb-4595-a6b7-4ed18ec24610 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Mar  2 00:53:55.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8560" for this suite.

• [SLOW TEST:5.640 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":356,"completed":44,"skipped":895,"failed":0}
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:53:55.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  2 00:54:00.128: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Mar  2 00:54:00.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3058" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":45,"skipped":898,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:54:00.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar  2 00:54:00.525: INFO: Waiting up to 5m0s for pod "pod-c6008ea0-62b2-4de6-8a11-4879c9d2a0f8" in namespace "emptydir-9681" to be "Succeeded or Failed"
Mar  2 00:54:00.711: INFO: Pod "pod-c6008ea0-62b2-4de6-8a11-4879c9d2a0f8": Phase="Pending", Reason="", readiness=false. Elapsed: 168.995386ms
Mar  2 00:54:02.780: INFO: Pod "pod-c6008ea0-62b2-4de6-8a11-4879c9d2a0f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.237197291s
Mar  2 00:54:04.807: INFO: Pod "pod-c6008ea0-62b2-4de6-8a11-4879c9d2a0f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.264717363s
STEP: Saw pod success
Mar  2 00:54:04.807: INFO: Pod "pod-c6008ea0-62b2-4de6-8a11-4879c9d2a0f8" satisfied condition "Succeeded or Failed"
Mar  2 00:54:04.827: INFO: Trying to get logs from node 10.123.244.39 pod pod-c6008ea0-62b2-4de6-8a11-4879c9d2a0f8 container test-container: <nil>
STEP: delete the pod
Mar  2 00:54:04.915: INFO: Waiting for pod pod-c6008ea0-62b2-4de6-8a11-4879c9d2a0f8 to disappear
Mar  2 00:54:04.947: INFO: Pod pod-c6008ea0-62b2-4de6-8a11-4879c9d2a0f8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar  2 00:54:04.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9681" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":46,"skipped":912,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:54:05.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 00:54:06.586: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 0, 54, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 54, 6, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-68c7bd4684\""}}, CollisionCount:(*int32)(nil)}
Mar  2 00:54:08.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 0, 54, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 54, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 0, 54, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 54, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 00:54:11.638: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 00:54:11.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9643-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 00:54:15.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9858" for this suite.
STEP: Destroying namespace "webhook-9858-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:10.568 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":356,"completed":47,"skipped":921,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:54:15.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 00:54:15.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Creating first CR 
Mar  2 00:54:18.469: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T00:54:18Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T00:54:18Z]] name:name1 resourceVersion:85668 uid:5b4255b8-2407-466b-9c44-b55fe227a044] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar  2 00:54:28.527: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T00:54:28Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T00:54:28Z]] name:name2 resourceVersion:85775 uid:acb09161-ff0b-4fc9-b855-db574382ff55] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar  2 00:54:38.556: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T00:54:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T00:54:38Z]] name:name1 resourceVersion:85833 uid:5b4255b8-2407-466b-9c44-b55fe227a044] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar  2 00:54:48.583: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T00:54:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T00:54:48Z]] name:name2 resourceVersion:85895 uid:acb09161-ff0b-4fc9-b855-db574382ff55] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar  2 00:54:58.611: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T00:54:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T00:54:38Z]] name:name1 resourceVersion:85946 uid:5b4255b8-2407-466b-9c44-b55fe227a044] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar  2 00:55:08.642: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-02T00:54:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-02T00:54:48Z]] name:name2 resourceVersion:85991 uid:acb09161-ff0b-4fc9-b855-db574382ff55] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 00:55:19.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-4284" for this suite.

• [SLOW TEST:63.619 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":356,"completed":48,"skipped":925,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:55:19.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating api versions
Mar  2 00:55:19.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-2273 api-versions'
Mar  2 00:55:19.548: INFO: stderr: ""
Mar  2 00:55:19.548: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar  2 00:55:19.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2273" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":356,"completed":49,"skipped":926,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:55:19.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
W0302 00:55:19.922658      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 00:55:19.941: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  2 00:55:24.956: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Mar  2 00:55:24.988: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Mar  2 00:55:25.046: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Mar  2 00:55:25.054: INFO: Observed &ReplicaSet event: ADDED
Mar  2 00:55:25.054: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 00:55:25.054: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 00:55:25.055: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 00:55:25.055: INFO: Found replicaset test-rs in namespace replicaset-4094 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  2 00:55:25.055: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Mar  2 00:55:25.055: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar  2 00:55:25.070: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Mar  2 00:55:25.077: INFO: Observed &ReplicaSet event: ADDED
Mar  2 00:55:25.077: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 00:55:25.077: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 00:55:25.078: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 00:55:25.078: INFO: Observed replicaset test-rs in namespace replicaset-4094 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 00:55:25.078: INFO: Observed &ReplicaSet event: MODIFIED
Mar  2 00:55:25.078: INFO: Found replicaset test-rs in namespace replicaset-4094 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Mar  2 00:55:25.078: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Mar  2 00:55:25.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4094" for this suite.

• [SLOW TEST:5.478 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":356,"completed":50,"skipped":936,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:55:25.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
W0302 00:55:25.276724      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Mar  2 00:55:26.517: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0302 00:55:26.516984      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Mar  2 00:55:26.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-974" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":356,"completed":51,"skipped":986,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:55:26.560: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-597
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 00:55:26.663: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 00:55:26.862: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:55:28.899: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 00:55:30.874: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 00:55:32.874: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 00:55:34.904: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 00:55:36.876: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 00:55:38.880: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  2 00:55:38.942: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  2 00:55:40.965: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  2 00:55:42.961: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  2 00:55:45.030: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  2 00:55:46.960: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar  2 00:55:48.956: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  2 00:55:48.984: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar  2 00:55:53.191: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 00:55:53.191: INFO: Going to poll 172.30.88.247 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar  2 00:55:53.204: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.88.247:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-597 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 00:55:53.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 00:55:53.205: INFO: ExecWithOptions: Clientset creation
Mar  2 00:55:53.205: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-597/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.88.247%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 00:55:53.431: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  2 00:55:53.431: INFO: Going to poll 172.30.54.232 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar  2 00:55:53.445: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.54.232:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-597 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 00:55:53.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 00:55:53.447: INFO: ExecWithOptions: Clientset creation
Mar  2 00:55:53.447: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-597/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.54.232%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 00:55:53.692: INFO: Found all 1 expected endpoints: [netserver-1]
Mar  2 00:55:53.692: INFO: Going to poll 172.30.0.253 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar  2 00:55:53.710: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.0.253:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-597 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 00:55:53.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 00:55:53.711: INFO: ExecWithOptions: Clientset creation
Mar  2 00:55:53.711: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-597/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.0.253%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 00:55:53.901: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Mar  2 00:55:53.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-597" for this suite.

• [SLOW TEST:27.414 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":52,"skipped":997,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:55:53.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Mar  2 00:55:54.217: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:55:56.230: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:55:58.241: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Mar  2 00:55:58.321: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:56:00.335: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  2 00:56:00.441: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 00:56:00.469: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 00:56:02.476: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 00:56:02.535: INFO: Pod pod-with-poststart-exec-hook still exists
Mar  2 00:56:04.473: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar  2 00:56:04.492: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Mar  2 00:56:04.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6947" for this suite.

• [SLOW TEST:10.596 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":356,"completed":53,"skipped":1018,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:56:04.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Mar  2 00:56:05.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4572" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":356,"completed":54,"skipped":1019,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:56:05.161: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 00:56:05.507: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-8b775b24-9307-415d-9144-60bb9edac7e1
STEP: Creating secret with name s-test-opt-upd-a12da2ba-70c3-47c5-b400-3175c053f360
STEP: Creating the pod
Mar  2 00:56:05.652: INFO: The status of Pod pod-projected-secrets-edb90db7-a0b8-4ef9-835c-f748d4e702d6 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:56:07.664: INFO: The status of Pod pod-projected-secrets-edb90db7-a0b8-4ef9-835c-f748d4e702d6 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:56:09.667: INFO: The status of Pod pod-projected-secrets-edb90db7-a0b8-4ef9-835c-f748d4e702d6 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-8b775b24-9307-415d-9144-60bb9edac7e1
STEP: Updating secret s-test-opt-upd-a12da2ba-70c3-47c5-b400-3175c053f360
STEP: Creating secret with name s-test-opt-create-fb1549fe-48be-4fda-a275-a6fd6d04a84e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Mar  2 00:57:23.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2129" for this suite.

• [SLOW TEST:78.841 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":55,"skipped":1021,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:57:24.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-x5dwv in namespace proxy-1264
I0302 00:57:24.289246      22 runners.go:193] Created replication controller with name: proxy-service-x5dwv, namespace: proxy-1264, replica count: 1
I0302 00:57:25.341155      22 runners.go:193] proxy-service-x5dwv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 00:57:26.341505      22 runners.go:193] proxy-service-x5dwv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 00:57:27.341875      22 runners.go:193] proxy-service-x5dwv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0302 00:57:28.343046      22 runners.go:193] proxy-service-x5dwv Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 00:57:28.357: INFO: setup took 4.198917216s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar  2 00:57:28.405: INFO: (0) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 47.008205ms)
Mar  2 00:57:28.405: INFO: (0) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 46.360036ms)
Mar  2 00:57:28.406: INFO: (0) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 47.219466ms)
Mar  2 00:57:28.406: INFO: (0) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 47.971227ms)
Mar  2 00:57:28.406: INFO: (0) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 46.958428ms)
Mar  2 00:57:28.406: INFO: (0) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 47.267804ms)
Mar  2 00:57:28.406: INFO: (0) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 49.078351ms)
Mar  2 00:57:28.408: INFO: (0) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 49.910765ms)
Mar  2 00:57:28.408: INFO: (0) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 50.200426ms)
Mar  2 00:57:28.409: INFO: (0) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 50.489861ms)
Mar  2 00:57:28.442: INFO: (0) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 83.924353ms)
Mar  2 00:57:28.445: INFO: (0) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 87.542762ms)
Mar  2 00:57:28.481: INFO: (0) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 123.718136ms)
Mar  2 00:57:28.483: INFO: (0) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 126.392753ms)
Mar  2 00:57:28.484: INFO: (0) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 126.268405ms)
Mar  2 00:57:28.484: INFO: (0) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 125.126151ms)
Mar  2 00:57:28.532: INFO: (1) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 46.737922ms)
Mar  2 00:57:28.532: INFO: (1) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 46.704706ms)
Mar  2 00:57:28.532: INFO: (1) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 47.831785ms)
Mar  2 00:57:28.534: INFO: (1) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 49.67648ms)
Mar  2 00:57:28.534: INFO: (1) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 48.414763ms)
Mar  2 00:57:28.534: INFO: (1) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 49.252085ms)
Mar  2 00:57:28.534: INFO: (1) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 48.680181ms)
Mar  2 00:57:28.534: INFO: (1) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 48.792584ms)
Mar  2 00:57:28.534: INFO: (1) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 49.122069ms)
Mar  2 00:57:28.534: INFO: (1) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 49.284145ms)
Mar  2 00:57:28.535: INFO: (1) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 49.585146ms)
Mar  2 00:57:28.538: INFO: (1) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 52.976408ms)
Mar  2 00:57:28.547: INFO: (1) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 61.917621ms)
Mar  2 00:57:28.548: INFO: (1) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 62.794193ms)
Mar  2 00:57:28.548: INFO: (1) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 62.675472ms)
Mar  2 00:57:28.548: INFO: (1) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 61.206404ms)
Mar  2 00:57:28.566: INFO: (2) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 16.831224ms)
Mar  2 00:57:28.567: INFO: (2) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 17.914799ms)
Mar  2 00:57:28.567: INFO: (2) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 18.22515ms)
Mar  2 00:57:28.568: INFO: (2) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 20.448686ms)
Mar  2 00:57:28.569: INFO: (2) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 19.878715ms)
Mar  2 00:57:28.569: INFO: (2) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 18.957285ms)
Mar  2 00:57:28.569: INFO: (2) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 19.180657ms)
Mar  2 00:57:28.570: INFO: (2) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 20.986462ms)
Mar  2 00:57:28.571: INFO: (2) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 21.176671ms)
Mar  2 00:57:28.571: INFO: (2) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 22.616638ms)
Mar  2 00:57:28.571: INFO: (2) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 21.712059ms)
Mar  2 00:57:28.578: INFO: (2) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 29.832453ms)
Mar  2 00:57:28.579: INFO: (2) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 30.426732ms)
Mar  2 00:57:28.579: INFO: (2) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 31.313529ms)
Mar  2 00:57:28.579: INFO: (2) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 31.792293ms)
Mar  2 00:57:28.580: INFO: (2) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 30.327556ms)
Mar  2 00:57:28.612: INFO: (3) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 31.2959ms)
Mar  2 00:57:28.612: INFO: (3) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 29.194613ms)
Mar  2 00:57:28.612: INFO: (3) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 30.996498ms)
Mar  2 00:57:28.612: INFO: (3) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 30.083055ms)
Mar  2 00:57:28.613: INFO: (3) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 32.06247ms)
Mar  2 00:57:28.613: INFO: (3) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 32.742494ms)
Mar  2 00:57:28.613: INFO: (3) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 30.639174ms)
Mar  2 00:57:28.614: INFO: (3) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 31.927349ms)
Mar  2 00:57:28.614: INFO: (3) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 32.191339ms)
Mar  2 00:57:28.615: INFO: (3) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 33.834005ms)
Mar  2 00:57:28.615: INFO: (3) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 33.374346ms)
Mar  2 00:57:28.615: INFO: (3) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 33.195238ms)
Mar  2 00:57:28.645: INFO: (3) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 62.825206ms)
Mar  2 00:57:28.645: INFO: (3) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 63.064714ms)
Mar  2 00:57:28.652: INFO: (3) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 70.620193ms)
Mar  2 00:57:28.652: INFO: (3) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 70.256865ms)
Mar  2 00:57:28.674: INFO: (4) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 21.734267ms)
Mar  2 00:57:28.679: INFO: (4) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 27.133098ms)
Mar  2 00:57:28.679: INFO: (4) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 26.215905ms)
Mar  2 00:57:28.679: INFO: (4) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 26.086609ms)
Mar  2 00:57:28.681: INFO: (4) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 27.418711ms)
Mar  2 00:57:28.681: INFO: (4) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 27.770439ms)
Mar  2 00:57:28.682: INFO: (4) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 28.447257ms)
Mar  2 00:57:28.682: INFO: (4) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 29.117832ms)
Mar  2 00:57:28.682: INFO: (4) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 29.604191ms)
Mar  2 00:57:28.684: INFO: (4) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 29.842216ms)
Mar  2 00:57:28.684: INFO: (4) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 31.004404ms)
Mar  2 00:57:28.686: INFO: (4) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 32.418553ms)
Mar  2 00:57:28.686: INFO: (4) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 31.908698ms)
Mar  2 00:57:28.686: INFO: (4) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 32.294741ms)
Mar  2 00:57:28.687: INFO: (4) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 32.787633ms)
Mar  2 00:57:28.690: INFO: (4) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 36.117308ms)
Mar  2 00:57:28.732: INFO: (5) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 42.322708ms)
Mar  2 00:57:28.739: INFO: (5) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 47.071358ms)
Mar  2 00:57:28.739: INFO: (5) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 48.101507ms)
Mar  2 00:57:28.739: INFO: (5) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 46.697328ms)
Mar  2 00:57:28.739: INFO: (5) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 48.837714ms)
Mar  2 00:57:28.739: INFO: (5) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 48.033457ms)
Mar  2 00:57:28.739: INFO: (5) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 47.594914ms)
Mar  2 00:57:28.741: INFO: (5) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 49.226488ms)
Mar  2 00:57:28.741: INFO: (5) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 50.757576ms)
Mar  2 00:57:28.757: INFO: (5) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 65.664853ms)
Mar  2 00:57:28.757: INFO: (5) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 63.085122ms)
Mar  2 00:57:28.757: INFO: (5) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 64.453162ms)
Mar  2 00:57:28.757: INFO: (5) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 64.289639ms)
Mar  2 00:57:28.757: INFO: (5) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 64.092214ms)
Mar  2 00:57:28.757: INFO: (5) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 63.886121ms)
Mar  2 00:57:28.757: INFO: (5) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 63.549912ms)
Mar  2 00:57:28.794: INFO: (6) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 36.817101ms)
Mar  2 00:57:28.794: INFO: (6) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 34.302165ms)
Mar  2 00:57:28.794: INFO: (6) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 35.676184ms)
Mar  2 00:57:28.794: INFO: (6) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 35.058715ms)
Mar  2 00:57:28.805: INFO: (6) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 45.519495ms)
Mar  2 00:57:28.805: INFO: (6) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 45.041101ms)
Mar  2 00:57:28.805: INFO: (6) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 47.811463ms)
Mar  2 00:57:28.805: INFO: (6) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 45.200606ms)
Mar  2 00:57:28.806: INFO: (6) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 45.811465ms)
Mar  2 00:57:28.806: INFO: (6) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 47.9261ms)
Mar  2 00:57:28.807: INFO: (6) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 47.084712ms)
Mar  2 00:57:28.807: INFO: (6) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 48.541854ms)
Mar  2 00:57:28.807: INFO: (6) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 49.255356ms)
Mar  2 00:57:28.807: INFO: (6) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 49.358438ms)
Mar  2 00:57:28.807: INFO: (6) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 47.783882ms)
Mar  2 00:57:28.807: INFO: (6) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 48.651283ms)
Mar  2 00:57:28.843: INFO: (7) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 33.871892ms)
Mar  2 00:57:28.843: INFO: (7) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 35.502881ms)
Mar  2 00:57:28.844: INFO: (7) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 35.393706ms)
Mar  2 00:57:28.845: INFO: (7) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 37.160219ms)
Mar  2 00:57:28.845: INFO: (7) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 37.659099ms)
Mar  2 00:57:28.847: INFO: (7) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 37.531217ms)
Mar  2 00:57:28.846: INFO: (7) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 37.657485ms)
Mar  2 00:57:28.847: INFO: (7) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 38.178515ms)
Mar  2 00:57:28.848: INFO: (7) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 39.351504ms)
Mar  2 00:57:28.848: INFO: (7) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 38.866235ms)
Mar  2 00:57:28.849: INFO: (7) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 40.593708ms)
Mar  2 00:57:28.849: INFO: (7) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 39.500253ms)
Mar  2 00:57:28.860: INFO: (7) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 51.32516ms)
Mar  2 00:57:28.860: INFO: (7) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 51.803511ms)
Mar  2 00:57:28.860: INFO: (7) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 51.215268ms)
Mar  2 00:57:28.860: INFO: (7) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 52.303376ms)
Mar  2 00:57:28.877: INFO: (8) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 16.050582ms)
Mar  2 00:57:28.877: INFO: (8) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 15.569098ms)
Mar  2 00:57:28.880: INFO: (8) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 18.31048ms)
Mar  2 00:57:28.881: INFO: (8) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 19.422038ms)
Mar  2 00:57:28.881: INFO: (8) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 19.250296ms)
Mar  2 00:57:28.881: INFO: (8) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 19.635537ms)
Mar  2 00:57:28.882: INFO: (8) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 20.646465ms)
Mar  2 00:57:28.883: INFO: (8) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 21.738494ms)
Mar  2 00:57:28.884: INFO: (8) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 22.943867ms)
Mar  2 00:57:28.884: INFO: (8) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 23.141674ms)
Mar  2 00:57:28.884: INFO: (8) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 22.775123ms)
Mar  2 00:57:28.884: INFO: (8) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 22.927953ms)
Mar  2 00:57:28.886: INFO: (8) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 24.556241ms)
Mar  2 00:57:28.887: INFO: (8) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 25.631209ms)
Mar  2 00:57:28.887: INFO: (8) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 25.607819ms)
Mar  2 00:57:28.888: INFO: (8) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 26.680917ms)
Mar  2 00:57:28.909: INFO: (9) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 20.279588ms)
Mar  2 00:57:28.910: INFO: (9) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 21.079681ms)
Mar  2 00:57:28.910: INFO: (9) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 21.343213ms)
Mar  2 00:57:28.912: INFO: (9) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 23.574551ms)
Mar  2 00:57:28.912: INFO: (9) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 23.087331ms)
Mar  2 00:57:28.913: INFO: (9) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 22.921897ms)
Mar  2 00:57:28.914: INFO: (9) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 24.847382ms)
Mar  2 00:57:28.916: INFO: (9) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 26.679945ms)
Mar  2 00:57:28.917: INFO: (9) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 27.630273ms)
Mar  2 00:57:28.917: INFO: (9) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 28.421082ms)
Mar  2 00:57:28.918: INFO: (9) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 29.391312ms)
Mar  2 00:57:28.918: INFO: (9) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 28.115275ms)
Mar  2 00:57:28.928: INFO: (9) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 38.639077ms)
Mar  2 00:57:28.928: INFO: (9) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 38.211044ms)
Mar  2 00:57:28.928: INFO: (9) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 38.708398ms)
Mar  2 00:57:28.928: INFO: (9) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 38.464176ms)
Mar  2 00:57:28.946: INFO: (10) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 17.557381ms)
Mar  2 00:57:28.948: INFO: (10) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 19.250577ms)
Mar  2 00:57:28.948: INFO: (10) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 19.170265ms)
Mar  2 00:57:28.949: INFO: (10) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 20.109673ms)
Mar  2 00:57:28.950: INFO: (10) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 20.829772ms)
Mar  2 00:57:28.953: INFO: (10) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 23.658802ms)
Mar  2 00:57:28.956: INFO: (10) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 26.678899ms)
Mar  2 00:57:28.959: INFO: (10) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 29.898877ms)
Mar  2 00:57:28.959: INFO: (10) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 30.525141ms)
Mar  2 00:57:28.960: INFO: (10) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 30.643864ms)
Mar  2 00:57:28.960: INFO: (10) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 31.001359ms)
Mar  2 00:57:28.960: INFO: (10) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 30.72388ms)
Mar  2 00:57:28.961: INFO: (10) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 32.643121ms)
Mar  2 00:57:28.967: INFO: (10) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 38.259694ms)
Mar  2 00:57:28.968: INFO: (10) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 38.802363ms)
Mar  2 00:57:28.968: INFO: (10) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 38.405665ms)
Mar  2 00:57:28.990: INFO: (11) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 22.266822ms)
Mar  2 00:57:28.996: INFO: (11) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 27.320776ms)
Mar  2 00:57:28.996: INFO: (11) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 27.142631ms)
Mar  2 00:57:28.996: INFO: (11) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 27.64532ms)
Mar  2 00:57:28.996: INFO: (11) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 27.376612ms)
Mar  2 00:57:28.997: INFO: (11) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 27.725856ms)
Mar  2 00:57:28.997: INFO: (11) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 28.368985ms)
Mar  2 00:57:28.997: INFO: (11) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 27.824894ms)
Mar  2 00:57:28.999: INFO: (11) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 30.083632ms)
Mar  2 00:57:28.999: INFO: (11) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 30.59599ms)
Mar  2 00:57:28.999: INFO: (11) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 30.366446ms)
Mar  2 00:57:28.999: INFO: (11) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 30.524456ms)
Mar  2 00:57:29.006: INFO: (11) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 37.31643ms)
Mar  2 00:57:29.006: INFO: (11) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 38.168191ms)
Mar  2 00:57:29.008: INFO: (11) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 39.690401ms)
Mar  2 00:57:29.008: INFO: (11) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 39.749118ms)
Mar  2 00:57:29.029: INFO: (12) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 20.667684ms)
Mar  2 00:57:29.030: INFO: (12) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 19.94942ms)
Mar  2 00:57:29.038: INFO: (12) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 28.051199ms)
Mar  2 00:57:29.038: INFO: (12) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 27.793693ms)
Mar  2 00:57:29.039: INFO: (12) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 27.517745ms)
Mar  2 00:57:29.039: INFO: (12) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 30.287754ms)
Mar  2 00:57:29.039: INFO: (12) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 27.409659ms)
Mar  2 00:57:29.040: INFO: (12) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 28.653072ms)
Mar  2 00:57:29.043: INFO: (12) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 31.642612ms)
Mar  2 00:57:29.043: INFO: (12) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 32.188969ms)
Mar  2 00:57:29.055: INFO: (12) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 43.017254ms)
Mar  2 00:57:29.055: INFO: (12) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 46.306461ms)
Mar  2 00:57:29.055: INFO: (12) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 45.358213ms)
Mar  2 00:57:29.057: INFO: (12) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 45.424331ms)
Mar  2 00:57:29.057: INFO: (12) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 46.415215ms)
Mar  2 00:57:29.057: INFO: (12) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 46.380063ms)
Mar  2 00:57:29.085: INFO: (13) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 25.078859ms)
Mar  2 00:57:29.085: INFO: (13) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 25.580382ms)
Mar  2 00:57:29.085: INFO: (13) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 25.409531ms)
Mar  2 00:57:29.085: INFO: (13) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 25.571746ms)
Mar  2 00:57:29.085: INFO: (13) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 25.555516ms)
Mar  2 00:57:29.088: INFO: (13) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 31.11878ms)
Mar  2 00:57:29.090: INFO: (13) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 31.538826ms)
Mar  2 00:57:29.090: INFO: (13) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 31.117039ms)
Mar  2 00:57:29.091: INFO: (13) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 32.042962ms)
Mar  2 00:57:29.091: INFO: (13) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 31.454484ms)
Mar  2 00:57:29.092: INFO: (13) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 33.798928ms)
Mar  2 00:57:29.096: INFO: (13) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 35.789499ms)
Mar  2 00:57:29.097: INFO: (13) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 38.368302ms)
Mar  2 00:57:29.098: INFO: (13) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 37.321691ms)
Mar  2 00:57:29.098: INFO: (13) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 39.879978ms)
Mar  2 00:57:29.099: INFO: (13) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 41.050489ms)
Mar  2 00:57:29.139: INFO: (14) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 37.66397ms)
Mar  2 00:57:29.139: INFO: (14) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 38.256213ms)
Mar  2 00:57:29.139: INFO: (14) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 37.505306ms)
Mar  2 00:57:29.139: INFO: (14) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 37.952349ms)
Mar  2 00:57:29.139: INFO: (14) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 39.654916ms)
Mar  2 00:57:29.139: INFO: (14) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 38.364042ms)
Mar  2 00:57:29.139: INFO: (14) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 37.908302ms)
Mar  2 00:57:29.142: INFO: (14) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 40.379855ms)
Mar  2 00:57:29.142: INFO: (14) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 40.739045ms)
Mar  2 00:57:29.142: INFO: (14) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 40.636496ms)
Mar  2 00:57:29.142: INFO: (14) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 42.029417ms)
Mar  2 00:57:29.143: INFO: (14) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 42.776732ms)
Mar  2 00:57:29.144: INFO: (14) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 42.472992ms)
Mar  2 00:57:29.145: INFO: (14) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 43.836136ms)
Mar  2 00:57:29.145: INFO: (14) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 45.096149ms)
Mar  2 00:57:29.146: INFO: (14) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 46.274991ms)
Mar  2 00:57:29.166: INFO: (15) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 20.1576ms)
Mar  2 00:57:29.167: INFO: (15) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 20.060292ms)
Mar  2 00:57:29.167: INFO: (15) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 19.426565ms)
Mar  2 00:57:29.168: INFO: (15) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 20.097497ms)
Mar  2 00:57:29.169: INFO: (15) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 22.013301ms)
Mar  2 00:57:29.169: INFO: (15) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 20.680783ms)
Mar  2 00:57:29.169: INFO: (15) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 21.091736ms)
Mar  2 00:57:29.169: INFO: (15) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 21.585366ms)
Mar  2 00:57:29.169: INFO: (15) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 22.103172ms)
Mar  2 00:57:29.170: INFO: (15) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 22.754539ms)
Mar  2 00:57:29.171: INFO: (15) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 24.532695ms)
Mar  2 00:57:29.194: INFO: (15) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 47.408326ms)
Mar  2 00:57:29.202: INFO: (15) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 53.928648ms)
Mar  2 00:57:29.202: INFO: (15) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 54.65837ms)
Mar  2 00:57:29.204: INFO: (15) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 55.529979ms)
Mar  2 00:57:29.204: INFO: (15) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 55.754038ms)
Mar  2 00:57:29.261: INFO: (16) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 55.865593ms)
Mar  2 00:57:29.261: INFO: (16) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 56.743846ms)
Mar  2 00:57:29.261: INFO: (16) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 56.163257ms)
Mar  2 00:57:29.262: INFO: (16) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 56.702406ms)
Mar  2 00:57:29.263: INFO: (16) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 58.698027ms)
Mar  2 00:57:29.264: INFO: (16) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 58.85415ms)
Mar  2 00:57:29.265: INFO: (16) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 59.696736ms)
Mar  2 00:57:29.265: INFO: (16) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 60.18315ms)
Mar  2 00:57:29.266: INFO: (16) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 60.207345ms)
Mar  2 00:57:29.266: INFO: (16) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 60.602778ms)
Mar  2 00:57:29.267: INFO: (16) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 61.648294ms)
Mar  2 00:57:29.267: INFO: (16) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 61.497282ms)
Mar  2 00:57:29.270: INFO: (16) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 64.810243ms)
Mar  2 00:57:29.270: INFO: (16) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 65.52813ms)
Mar  2 00:57:29.271: INFO: (16) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 65.455292ms)
Mar  2 00:57:29.271: INFO: (16) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 66.245981ms)
Mar  2 00:57:29.288: INFO: (17) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 16.392551ms)
Mar  2 00:57:29.289: INFO: (17) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 16.870276ms)
Mar  2 00:57:29.289: INFO: (17) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 17.710734ms)
Mar  2 00:57:29.291: INFO: (17) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 15.693762ms)
Mar  2 00:57:29.291: INFO: (17) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 18.638163ms)
Mar  2 00:57:29.291: INFO: (17) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 16.440774ms)
Mar  2 00:57:29.291: INFO: (17) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 18.388476ms)
Mar  2 00:57:29.292: INFO: (17) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 18.144947ms)
Mar  2 00:57:29.323: INFO: (17) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 50.224676ms)
Mar  2 00:57:29.325: INFO: (17) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 49.978036ms)
Mar  2 00:57:29.325: INFO: (17) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 49.957512ms)
Mar  2 00:57:29.359: INFO: (17) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 85.754884ms)
Mar  2 00:57:29.359: INFO: (17) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 84.96214ms)
Mar  2 00:57:29.359: INFO: (17) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 85.687658ms)
Mar  2 00:57:29.361: INFO: (17) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 86.186304ms)
Mar  2 00:57:29.361: INFO: (17) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 86.769363ms)
Mar  2 00:57:29.378: INFO: (18) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 16.163951ms)
Mar  2 00:57:29.378: INFO: (18) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 16.852219ms)
Mar  2 00:57:29.381: INFO: (18) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 18.900069ms)
Mar  2 00:57:29.381: INFO: (18) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 19.065579ms)
Mar  2 00:57:29.385: INFO: (18) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 23.825287ms)
Mar  2 00:57:29.386: INFO: (18) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 23.942763ms)
Mar  2 00:57:29.386: INFO: (18) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 24.081669ms)
Mar  2 00:57:29.389: INFO: (18) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 26.938596ms)
Mar  2 00:57:29.389: INFO: (18) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 27.295155ms)
Mar  2 00:57:29.389: INFO: (18) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 27.087565ms)
Mar  2 00:57:29.389: INFO: (18) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 27.556391ms)
Mar  2 00:57:29.390: INFO: (18) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 28.133836ms)
Mar  2 00:57:29.391: INFO: (18) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 29.060893ms)
Mar  2 00:57:29.391: INFO: (18) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 29.216625ms)
Mar  2 00:57:29.391: INFO: (18) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 29.322897ms)
Mar  2 00:57:29.392: INFO: (18) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 30.388962ms)
Mar  2 00:57:29.436: INFO: (19) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">test<... (200; 43.22421ms)
Mar  2 00:57:29.437: INFO: (19) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 42.340334ms)
Mar  2 00:57:29.439: INFO: (19) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:460/proxy/: tls baz (200; 43.085773ms)
Mar  2 00:57:29.439: INFO: (19) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw/proxy/rewriteme">test</a> (200; 44.945398ms)
Mar  2 00:57:29.439: INFO: (19) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 44.545942ms)
Mar  2 00:57:29.439: INFO: (19) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:162/proxy/: bar (200; 43.122572ms)
Mar  2 00:57:29.439: INFO: (19) /api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/http:proxy-service-x5dwv-q99mw:1080/proxy/rewriteme">... (200; 42.938241ms)
Mar  2 00:57:29.439: INFO: (19) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/: <a href="/api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:443/proxy/tlsrewritem... (200; 42.037097ms)
Mar  2 00:57:29.440: INFO: (19) /api/v1/namespaces/proxy-1264/pods/proxy-service-x5dwv-q99mw:160/proxy/: foo (200; 42.283553ms)
Mar  2 00:57:29.441: INFO: (19) /api/v1/namespaces/proxy-1264/pods/https:proxy-service-x5dwv-q99mw:462/proxy/: tls qux (200; 43.90498ms)
Mar  2 00:57:29.444: INFO: (19) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname1/proxy/: foo (200; 46.301823ms)
Mar  2 00:57:29.444: INFO: (19) /api/v1/namespaces/proxy-1264/services/http:proxy-service-x5dwv:portname2/proxy/: bar (200; 48.909516ms)
Mar  2 00:57:29.444: INFO: (19) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname2/proxy/: bar (200; 46.251118ms)
Mar  2 00:57:29.446: INFO: (19) /api/v1/namespaces/proxy-1264/services/proxy-service-x5dwv:portname1/proxy/: foo (200; 49.297755ms)
Mar  2 00:57:29.446: INFO: (19) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname1/proxy/: tls baz (200; 48.723341ms)
Mar  2 00:57:29.446: INFO: (19) /api/v1/namespaces/proxy-1264/services/https:proxy-service-x5dwv:tlsportname2/proxy/: tls qux (200; 52.901504ms)
STEP: deleting ReplicationController proxy-service-x5dwv in namespace proxy-1264, will wait for the garbage collector to delete the pods
Mar  2 00:57:29.550: INFO: Deleting ReplicationController proxy-service-x5dwv took: 33.859102ms
Mar  2 00:57:29.753: INFO: Terminating ReplicationController proxy-service-x5dwv pods took: 203.193846ms
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Mar  2 00:57:31.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1264" for this suite.

• [SLOW TEST:7.806 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":356,"completed":56,"skipped":1030,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:57:31.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7800 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7800;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7800 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7800;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7800.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7800.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7800.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7800.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7800.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7800.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7800.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7800.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7800.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7800.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7800.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7800.svc;check="$$(dig +notcp +noall +answer +search 113.2.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.2.113_udp@PTR;check="$$(dig +tcp +noall +answer +search 113.2.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.2.113_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7800 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7800;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7800 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7800;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7800.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7800.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7800.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7800.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7800.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7800.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7800.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7800.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7800.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7800.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7800.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7800.svc;check="$$(dig +notcp +noall +answer +search 113.2.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.2.113_udp@PTR;check="$$(dig +tcp +noall +answer +search 113.2.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.2.113_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 00:57:36.157: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7800/dns-test-251dae35-b8f3-474e-b842-c05dc620f308: the server could not find the requested resource (get pods dns-test-251dae35-b8f3-474e-b842-c05dc620f308)
Mar  2 00:57:36.174: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7800/dns-test-251dae35-b8f3-474e-b842-c05dc620f308: the server could not find the requested resource (get pods dns-test-251dae35-b8f3-474e-b842-c05dc620f308)
Mar  2 00:57:36.187: INFO: Unable to read wheezy_udp@dns-test-service.dns-7800 from pod dns-7800/dns-test-251dae35-b8f3-474e-b842-c05dc620f308: the server could not find the requested resource (get pods dns-test-251dae35-b8f3-474e-b842-c05dc620f308)
Mar  2 00:57:36.201: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7800 from pod dns-7800/dns-test-251dae35-b8f3-474e-b842-c05dc620f308: the server could not find the requested resource (get pods dns-test-251dae35-b8f3-474e-b842-c05dc620f308)
Mar  2 00:57:36.221: INFO: Unable to read wheezy_udp@dns-test-service.dns-7800.svc from pod dns-7800/dns-test-251dae35-b8f3-474e-b842-c05dc620f308: the server could not find the requested resource (get pods dns-test-251dae35-b8f3-474e-b842-c05dc620f308)
Mar  2 00:57:36.234: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7800.svc from pod dns-7800/dns-test-251dae35-b8f3-474e-b842-c05dc620f308: the server could not find the requested resource (get pods dns-test-251dae35-b8f3-474e-b842-c05dc620f308)
Mar  2 00:57:36.249: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7800.svc from pod dns-7800/dns-test-251dae35-b8f3-474e-b842-c05dc620f308: the server could not find the requested resource (get pods dns-test-251dae35-b8f3-474e-b842-c05dc620f308)
Mar  2 00:57:36.363: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7800/dns-test-251dae35-b8f3-474e-b842-c05dc620f308: the server could not find the requested resource (get pods dns-test-251dae35-b8f3-474e-b842-c05dc620f308)
Mar  2 00:57:36.380: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7800/dns-test-251dae35-b8f3-474e-b842-c05dc620f308: the server could not find the requested resource (get pods dns-test-251dae35-b8f3-474e-b842-c05dc620f308)
Mar  2 00:57:36.396: INFO: Unable to read jessie_udp@dns-test-service.dns-7800 from pod dns-7800/dns-test-251dae35-b8f3-474e-b842-c05dc620f308: the server could not find the requested resource (get pods dns-test-251dae35-b8f3-474e-b842-c05dc620f308)
Mar  2 00:57:36.412: INFO: Unable to read jessie_tcp@dns-test-service.dns-7800 from pod dns-7800/dns-test-251dae35-b8f3-474e-b842-c05dc620f308: the server could not find the requested resource (get pods dns-test-251dae35-b8f3-474e-b842-c05dc620f308)
Mar  2 00:57:36.453: INFO: Unable to read jessie_tcp@dns-test-service.dns-7800.svc from pod dns-7800/dns-test-251dae35-b8f3-474e-b842-c05dc620f308: the server could not find the requested resource (get pods dns-test-251dae35-b8f3-474e-b842-c05dc620f308)
Mar  2 00:57:36.474: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7800.svc from pod dns-7800/dns-test-251dae35-b8f3-474e-b842-c05dc620f308: the server could not find the requested resource (get pods dns-test-251dae35-b8f3-474e-b842-c05dc620f308)
Mar  2 00:57:36.491: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7800.svc from pod dns-7800/dns-test-251dae35-b8f3-474e-b842-c05dc620f308: the server could not find the requested resource (get pods dns-test-251dae35-b8f3-474e-b842-c05dc620f308)
Mar  2 00:57:36.562: INFO: Lookups using dns-7800/dns-test-251dae35-b8f3-474e-b842-c05dc620f308 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7800 wheezy_tcp@dns-test-service.dns-7800 wheezy_udp@dns-test-service.dns-7800.svc wheezy_tcp@dns-test-service.dns-7800.svc wheezy_udp@_http._tcp.dns-test-service.dns-7800.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7800 jessie_tcp@dns-test-service.dns-7800 jessie_tcp@dns-test-service.dns-7800.svc jessie_udp@_http._tcp.dns-test-service.dns-7800.svc jessie_tcp@_http._tcp.dns-test-service.dns-7800.svc]

Mar  2 00:57:41.993: INFO: DNS probes using dns-7800/dns-test-251dae35-b8f3-474e-b842-c05dc620f308 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Mar  2 00:57:42.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7800" for this suite.

• [SLOW TEST:10.397 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":356,"completed":57,"skipped":1040,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:57:42.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:188
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar  2 00:57:42.557: INFO: starting watch
STEP: patching
STEP: updating
Mar  2 00:57:42.614: INFO: waiting for watch events with expected annotations
Mar  2 00:57:42.614: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:188
Mar  2 00:57:42.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-8247" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":356,"completed":58,"skipped":1063,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:57:42.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Mar  2 00:57:42.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-8696 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar  2 00:57:43.116: INFO: stderr: ""
Mar  2 00:57:43.117: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Mar  2 00:57:43.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-8696 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Mar  2 00:57:45.939: INFO: stderr: ""
Mar  2 00:57:45.939: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Mar  2 00:57:45.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-8696 delete pods e2e-test-httpd-pod'
Mar  2 00:57:47.786: INFO: stderr: ""
Mar  2 00:57:47.786: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar  2 00:57:47.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8696" for this suite.

• [SLOW TEST:5.034 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:927
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":356,"completed":59,"skipped":1068,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:57:47.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-3219/configmap-test-519d020f-245c-44c4-9242-943f2dbcb77b
STEP: Creating a pod to test consume configMaps
Mar  2 00:57:48.079: INFO: Waiting up to 5m0s for pod "pod-configmaps-a27f08a4-e8fa-42df-aadf-2be83e43507c" in namespace "configmap-3219" to be "Succeeded or Failed"
Mar  2 00:57:48.091: INFO: Pod "pod-configmaps-a27f08a4-e8fa-42df-aadf-2be83e43507c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.296701ms
Mar  2 00:57:50.107: INFO: Pod "pod-configmaps-a27f08a4-e8fa-42df-aadf-2be83e43507c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027317426s
Mar  2 00:57:52.119: INFO: Pod "pod-configmaps-a27f08a4-e8fa-42df-aadf-2be83e43507c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039851151s
Mar  2 00:57:54.139: INFO: Pod "pod-configmaps-a27f08a4-e8fa-42df-aadf-2be83e43507c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.059440172s
STEP: Saw pod success
Mar  2 00:57:54.139: INFO: Pod "pod-configmaps-a27f08a4-e8fa-42df-aadf-2be83e43507c" satisfied condition "Succeeded or Failed"
Mar  2 00:57:54.149: INFO: Trying to get logs from node 10.123.244.39 pod pod-configmaps-a27f08a4-e8fa-42df-aadf-2be83e43507c container env-test: <nil>
STEP: delete the pod
Mar  2 00:57:54.230: INFO: Waiting for pod pod-configmaps-a27f08a4-e8fa-42df-aadf-2be83e43507c to disappear
Mar  2 00:57:54.239: INFO: Pod pod-configmaps-a27f08a4-e8fa-42df-aadf-2be83e43507c no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Mar  2 00:57:54.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3219" for this suite.

• [SLOW TEST:6.459 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":356,"completed":60,"skipped":1070,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:57:54.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Mar  2 00:57:54.677: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:57:56.688: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:57:58.710: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Mar  2 00:57:58.884: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:58:00.899: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:58:02.901: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:58:04.899: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Mar  2 00:58:04.942: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 00:58:04.957: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 00:58:06.969: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 00:58:07.059: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 00:58:09.009: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 00:58:09.139: INFO: Pod pod-with-prestop-http-hook still exists
Mar  2 00:58:10.965: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar  2 00:58:10.996: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Mar  2 00:58:11.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8180" for this suite.

• [SLOW TEST:17.839 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":356,"completed":61,"skipped":1090,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:58:12.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Mar  2 00:58:30.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5225" for this suite.
STEP: Destroying namespace "nsdeletetest-3591" for this suite.
Mar  2 00:58:30.373: INFO: Namespace nsdeletetest-3591 was already deleted
STEP: Destroying namespace "nsdeletetest-1010" for this suite.

• [SLOW TEST:18.265 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":356,"completed":62,"skipped":1094,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:58:30.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 00:58:30.524: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar  2 00:58:32.727: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Mar  2 00:58:33.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6541" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":356,"completed":63,"skipped":1130,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:58:33.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-1284
STEP: creating service affinity-nodeport in namespace services-1284
STEP: creating replication controller affinity-nodeport in namespace services-1284
I0302 00:58:34.192054      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-1284, replica count: 3
I0302 00:58:37.244005      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 00:58:40.244636      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 00:58:40.335: INFO: Creating new exec pod
Mar  2 00:58:43.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-1284 exec execpod-affinity8j6rr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Mar  2 00:58:43.760: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar  2 00:58:43.760: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 00:58:43.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-1284 exec execpod-affinity8j6rr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.173.248 80'
Mar  2 00:58:44.138: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.173.248 80\nConnection to 172.21.173.248 80 port [tcp/http] succeeded!\n"
Mar  2 00:58:44.138: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 00:58:44.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-1284 exec execpod-affinity8j6rr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.123.244.41 30274'
Mar  2 00:58:44.448: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.123.244.41 30274\nConnection to 10.123.244.41 30274 port [tcp/*] succeeded!\n"
Mar  2 00:58:44.448: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 00:58:44.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-1284 exec execpod-affinity8j6rr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.123.244.50 30274'
Mar  2 00:58:44.899: INFO: stderr: "+ + echo hostNamenc\n -v -t -w 2 10.123.244.50 30274\nConnection to 10.123.244.50 30274 port [tcp/*] succeeded!\n"
Mar  2 00:58:44.899: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 00:58:44.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-1284 exec execpod-affinity8j6rr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.123.244.39:30274/ ; done'
Mar  2 00:58:45.647: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:30274/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:30274/\n"
Mar  2 00:58:45.647: INFO: stdout: "\naffinity-nodeport-6kcbr\naffinity-nodeport-6kcbr\naffinity-nodeport-6kcbr\naffinity-nodeport-6kcbr\naffinity-nodeport-6kcbr\naffinity-nodeport-6kcbr\naffinity-nodeport-6kcbr\naffinity-nodeport-6kcbr\naffinity-nodeport-6kcbr\naffinity-nodeport-6kcbr\naffinity-nodeport-6kcbr\naffinity-nodeport-6kcbr\naffinity-nodeport-6kcbr\naffinity-nodeport-6kcbr\naffinity-nodeport-6kcbr\naffinity-nodeport-6kcbr"
Mar  2 00:58:45.647: INFO: Received response from host: affinity-nodeport-6kcbr
Mar  2 00:58:45.647: INFO: Received response from host: affinity-nodeport-6kcbr
Mar  2 00:58:45.647: INFO: Received response from host: affinity-nodeport-6kcbr
Mar  2 00:58:45.647: INFO: Received response from host: affinity-nodeport-6kcbr
Mar  2 00:58:45.647: INFO: Received response from host: affinity-nodeport-6kcbr
Mar  2 00:58:45.647: INFO: Received response from host: affinity-nodeport-6kcbr
Mar  2 00:58:45.647: INFO: Received response from host: affinity-nodeport-6kcbr
Mar  2 00:58:45.647: INFO: Received response from host: affinity-nodeport-6kcbr
Mar  2 00:58:45.647: INFO: Received response from host: affinity-nodeport-6kcbr
Mar  2 00:58:45.647: INFO: Received response from host: affinity-nodeport-6kcbr
Mar  2 00:58:45.647: INFO: Received response from host: affinity-nodeport-6kcbr
Mar  2 00:58:45.647: INFO: Received response from host: affinity-nodeport-6kcbr
Mar  2 00:58:45.647: INFO: Received response from host: affinity-nodeport-6kcbr
Mar  2 00:58:45.647: INFO: Received response from host: affinity-nodeport-6kcbr
Mar  2 00:58:45.647: INFO: Received response from host: affinity-nodeport-6kcbr
Mar  2 00:58:45.647: INFO: Received response from host: affinity-nodeport-6kcbr
Mar  2 00:58:45.648: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1284, will wait for the garbage collector to delete the pods
Mar  2 00:58:45.776: INFO: Deleting ReplicationController affinity-nodeport took: 18.319247ms
Mar  2 00:58:45.979: INFO: Terminating ReplicationController affinity-nodeport pods took: 202.279961ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar  2 00:58:49.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1284" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:15.397 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":64,"skipped":1145,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:58:49.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Mar  2 00:58:49.492: INFO: The status of Pod annotationupdate933049f2-3d70-4066-9f0d-0c92749cf4c1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 00:58:51.512: INFO: The status of Pod annotationupdate933049f2-3d70-4066-9f0d-0c92749cf4c1 is Running (Ready = true)
Mar  2 00:58:52.185: INFO: Successfully updated pod "annotationupdate933049f2-3d70-4066-9f0d-0c92749cf4c1"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar  2 00:58:56.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6307" for this suite.

• [SLOW TEST:7.122 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":65,"skipped":1151,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:58:56.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 00:58:57.874: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 00:58:59.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 0, 58, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 58, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 0, 58, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 0, 58, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 00:59:02.992: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar  2 00:59:05.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=webhook-7337 attach --namespace=webhook-7337 to-be-attached-pod -i -c=container1'
Mar  2 00:59:05.451: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 00:59:05.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7337" for this suite.
STEP: Destroying namespace "webhook-7337-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:9.357 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":356,"completed":66,"skipped":1191,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:59:05.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 00:59:05.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties
Mar  2 00:59:25.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-2213 --namespace=crd-publish-openapi-2213 create -f -'
Mar  2 00:59:29.470: INFO: stderr: ""
Mar  2 00:59:29.470: INFO: stdout: "e2e-test-crd-publish-openapi-7258-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  2 00:59:29.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-2213 --namespace=crd-publish-openapi-2213 delete e2e-test-crd-publish-openapi-7258-crds test-foo'
Mar  2 00:59:29.678: INFO: stderr: ""
Mar  2 00:59:29.678: INFO: stdout: "e2e-test-crd-publish-openapi-7258-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar  2 00:59:29.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-2213 --namespace=crd-publish-openapi-2213 apply -f -'
Mar  2 00:59:31.832: INFO: stderr: ""
Mar  2 00:59:31.832: INFO: stdout: "e2e-test-crd-publish-openapi-7258-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar  2 00:59:31.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-2213 --namespace=crd-publish-openapi-2213 delete e2e-test-crd-publish-openapi-7258-crds test-foo'
Mar  2 00:59:32.031: INFO: stderr: ""
Mar  2 00:59:32.031: INFO: stdout: "e2e-test-crd-publish-openapi-7258-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values
Mar  2 00:59:32.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-2213 --namespace=crd-publish-openapi-2213 create -f -'
Mar  2 00:59:33.222: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar  2 00:59:33.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-2213 --namespace=crd-publish-openapi-2213 create -f -'
Mar  2 00:59:34.142: INFO: rc: 1
Mar  2 00:59:34.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-2213 --namespace=crd-publish-openapi-2213 apply -f -'
Mar  2 00:59:34.916: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties
Mar  2 00:59:34.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-2213 --namespace=crd-publish-openapi-2213 create -f -'
Mar  2 00:59:36.468: INFO: rc: 1
Mar  2 00:59:36.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-2213 --namespace=crd-publish-openapi-2213 apply -f -'
Mar  2 00:59:36.881: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar  2 00:59:36.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-2213 explain e2e-test-crd-publish-openapi-7258-crds'
Mar  2 00:59:37.298: INFO: stderr: ""
Mar  2 00:59:37.298: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7258-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar  2 00:59:37.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-2213 explain e2e-test-crd-publish-openapi-7258-crds.metadata'
Mar  2 00:59:37.802: INFO: stderr: ""
Mar  2 00:59:37.802: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7258-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     Deprecated: ClusterName is a legacy field that was always cleared by the\n     system and never used; it will be removed completely in 1.25.\n\n     The name in the go struct is changed to help clients detect accidental use.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar  2 00:59:37.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-2213 explain e2e-test-crd-publish-openapi-7258-crds.spec'
Mar  2 00:59:38.436: INFO: stderr: ""
Mar  2 00:59:38.436: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7258-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar  2 00:59:38.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-2213 explain e2e-test-crd-publish-openapi-7258-crds.spec.bars'
Mar  2 00:59:41.363: INFO: stderr: ""
Mar  2 00:59:41.363: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7258-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar  2 00:59:41.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-2213 explain e2e-test-crd-publish-openapi-7258-crds.spec.bars2'
Mar  2 00:59:42.057: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 00:59:58.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2213" for this suite.

• [SLOW TEST:52.649 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":356,"completed":67,"skipped":1192,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 00:59:58.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 00:59:58.698: INFO: created pod
Mar  2 00:59:58.698: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4303" to be "Succeeded or Failed"
Mar  2 00:59:58.709: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.193252ms
Mar  2 01:00:00.724: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0257217s
Mar  2 01:00:02.758: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059241339s
STEP: Saw pod success
Mar  2 01:00:02.758: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Mar  2 01:00:32.761: INFO: polling logs
Mar  2 01:00:32.793: INFO: Pod logs: 
I0302 00:59:59.919684       1 log.go:195] OK: Got token
I0302 00:59:59.919868       1 log.go:195] validating with in-cluster discovery
I0302 00:59:59.920556       1 log.go:195] OK: got issuer https://kubernetes.default.svc
I0302 00:59:59.920617       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4303:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677719399, NotBefore:1677718799, IssuedAt:1677718799, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4303", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"5710dfcd-21e2-43b5-8486-4d7a25e26ba4"}}}
I0302 00:59:59.942228       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0302 00:59:59.953096       1 log.go:195] OK: Validated signature on JWT
I0302 00:59:59.953246       1 log.go:195] OK: Got valid claims from token!
I0302 00:59:59.953318       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4303:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1677719399, NotBefore:1677718799, IssuedAt:1677718799, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4303", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"5710dfcd-21e2-43b5-8486-4d7a25e26ba4"}}}

Mar  2 01:00:32.793: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Mar  2 01:00:32.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4303" for this suite.

• [SLOW TEST:34.419 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":356,"completed":68,"skipped":1236,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:00:32.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-5a9a7eec-b713-4cd0-aa70-e40b1a734e3f in namespace container-probe-9305
Mar  2 01:00:34.991: INFO: Started pod liveness-5a9a7eec-b713-4cd0-aa70-e40b1a734e3f in namespace container-probe-9305
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 01:00:35.021: INFO: Initial restart count of pod liveness-5a9a7eec-b713-4cd0-aa70-e40b1a734e3f is 0
Mar  2 01:00:55.201: INFO: Restart count of pod container-probe-9305/liveness-5a9a7eec-b713-4cd0-aa70-e40b1a734e3f is now 1 (20.179842219s elapsed)
Mar  2 01:01:15.334: INFO: Restart count of pod container-probe-9305/liveness-5a9a7eec-b713-4cd0-aa70-e40b1a734e3f is now 2 (40.312702419s elapsed)
Mar  2 01:01:35.483: INFO: Restart count of pod container-probe-9305/liveness-5a9a7eec-b713-4cd0-aa70-e40b1a734e3f is now 3 (1m0.461382106s elapsed)
Mar  2 01:01:55.621: INFO: Restart count of pod container-probe-9305/liveness-5a9a7eec-b713-4cd0-aa70-e40b1a734e3f is now 4 (1m20.600178314s elapsed)
Mar  2 01:03:02.120: INFO: Restart count of pod container-probe-9305/liveness-5a9a7eec-b713-4cd0-aa70-e40b1a734e3f is now 5 (2m27.098984443s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Mar  2 01:03:02.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9305" for this suite.

• [SLOW TEST:149.364 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":356,"completed":69,"skipped":1238,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:03:02.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Mar  2 01:03:02.304: INFO: namespace kubectl-3626
Mar  2 01:03:02.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-3626 create -f -'
Mar  2 01:03:03.831: INFO: stderr: ""
Mar  2 01:03:03.832: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar  2 01:03:04.862: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 01:03:04.862: INFO: Found 0 / 1
Mar  2 01:03:05.842: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 01:03:05.842: INFO: Found 1 / 1
Mar  2 01:03:05.842: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  2 01:03:05.854: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 01:03:05.854: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 01:03:05.854: INFO: wait on agnhost-primary startup in kubectl-3626 
Mar  2 01:03:05.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-3626 logs agnhost-primary-r2sb2 agnhost-primary'
Mar  2 01:03:06.588: INFO: stderr: ""
Mar  2 01:03:06.588: INFO: stdout: "Paused\n"
STEP: exposing RC
Mar  2 01:03:06.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-3626 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar  2 01:03:06.819: INFO: stderr: ""
Mar  2 01:03:06.819: INFO: stdout: "service/rm2 exposed\n"
Mar  2 01:03:06.829: INFO: Service rm2 in namespace kubectl-3626 found.
STEP: exposing service
Mar  2 01:03:08.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-3626 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar  2 01:03:09.201: INFO: stderr: ""
Mar  2 01:03:09.201: INFO: stdout: "service/rm3 exposed\n"
Mar  2 01:03:09.217: INFO: Service rm3 in namespace kubectl-3626 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar  2 01:03:11.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3626" for this suite.

• [SLOW TEST:9.067 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1249
    should create services for rc  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":356,"completed":70,"skipped":1241,"failed":0}
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:03:11.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
W0302 01:03:11.382407      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  2 01:03:15.439: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Mar  2 01:03:15.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5908" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":71,"skipped":1243,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:03:15.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3695.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3695.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3695.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3695.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3695.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3695.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3695.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3695.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3695.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3695.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3695.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3695.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 7.67.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.67.7_udp@PTR;check="$$(dig +tcp +noall +answer +search 7.67.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.67.7_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3695.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3695.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3695.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3695.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3695.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3695.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3695.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3695.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3695.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3695.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3695.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3695.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 7.67.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.67.7_udp@PTR;check="$$(dig +tcp +noall +answer +search 7.67.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.67.7_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 01:03:19.792: INFO: Unable to read wheezy_udp@dns-test-service.dns-3695.svc.cluster.local from pod dns-3695/dns-test-253b1a1a-1c04-42d5-a34d-261efc269f2e: the server could not find the requested resource (get pods dns-test-253b1a1a-1c04-42d5-a34d-261efc269f2e)
Mar  2 01:03:19.812: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3695.svc.cluster.local from pod dns-3695/dns-test-253b1a1a-1c04-42d5-a34d-261efc269f2e: the server could not find the requested resource (get pods dns-test-253b1a1a-1c04-42d5-a34d-261efc269f2e)
Mar  2 01:03:19.838: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3695.svc.cluster.local from pod dns-3695/dns-test-253b1a1a-1c04-42d5-a34d-261efc269f2e: the server could not find the requested resource (get pods dns-test-253b1a1a-1c04-42d5-a34d-261efc269f2e)
Mar  2 01:03:19.867: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3695.svc.cluster.local from pod dns-3695/dns-test-253b1a1a-1c04-42d5-a34d-261efc269f2e: the server could not find the requested resource (get pods dns-test-253b1a1a-1c04-42d5-a34d-261efc269f2e)
Mar  2 01:03:19.931: INFO: Unable to read jessie_udp@dns-test-service.dns-3695.svc.cluster.local from pod dns-3695/dns-test-253b1a1a-1c04-42d5-a34d-261efc269f2e: the server could not find the requested resource (get pods dns-test-253b1a1a-1c04-42d5-a34d-261efc269f2e)
Mar  2 01:03:19.944: INFO: Unable to read jessie_tcp@dns-test-service.dns-3695.svc.cluster.local from pod dns-3695/dns-test-253b1a1a-1c04-42d5-a34d-261efc269f2e: the server could not find the requested resource (get pods dns-test-253b1a1a-1c04-42d5-a34d-261efc269f2e)
Mar  2 01:03:19.957: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3695.svc.cluster.local from pod dns-3695/dns-test-253b1a1a-1c04-42d5-a34d-261efc269f2e: the server could not find the requested resource (get pods dns-test-253b1a1a-1c04-42d5-a34d-261efc269f2e)
Mar  2 01:03:20.035: INFO: Lookups using dns-3695/dns-test-253b1a1a-1c04-42d5-a34d-261efc269f2e failed for: [wheezy_udp@dns-test-service.dns-3695.svc.cluster.local wheezy_tcp@dns-test-service.dns-3695.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3695.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3695.svc.cluster.local jessie_udp@dns-test-service.dns-3695.svc.cluster.local jessie_tcp@dns-test-service.dns-3695.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3695.svc.cluster.local]

Mar  2 01:03:26.300: INFO: DNS probes using dns-3695/dns-test-253b1a1a-1c04-42d5-a34d-261efc269f2e succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Mar  2 01:03:26.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3695" for this suite.

• [SLOW TEST:11.056 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":356,"completed":72,"skipped":1244,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:03:26.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-b0d21976-30c6-44b2-b170-a8363fa0e690
STEP: Creating a pod to test consume configMaps
Mar  2 01:03:26.801: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5f9cae7f-f183-4b76-bc70-8b980019ed08" in namespace "projected-1833" to be "Succeeded or Failed"
Mar  2 01:03:26.810: INFO: Pod "pod-projected-configmaps-5f9cae7f-f183-4b76-bc70-8b980019ed08": Phase="Pending", Reason="", readiness=false. Elapsed: 8.873984ms
Mar  2 01:03:28.821: INFO: Pod "pod-projected-configmaps-5f9cae7f-f183-4b76-bc70-8b980019ed08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019521373s
Mar  2 01:03:30.831: INFO: Pod "pod-projected-configmaps-5f9cae7f-f183-4b76-bc70-8b980019ed08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03018936s
Mar  2 01:03:32.847: INFO: Pod "pod-projected-configmaps-5f9cae7f-f183-4b76-bc70-8b980019ed08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046172843s
STEP: Saw pod success
Mar  2 01:03:32.847: INFO: Pod "pod-projected-configmaps-5f9cae7f-f183-4b76-bc70-8b980019ed08" satisfied condition "Succeeded or Failed"
Mar  2 01:03:32.856: INFO: Trying to get logs from node 10.123.244.39 pod pod-projected-configmaps-5f9cae7f-f183-4b76-bc70-8b980019ed08 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 01:03:33.404: INFO: Waiting for pod pod-projected-configmaps-5f9cae7f-f183-4b76-bc70-8b980019ed08 to disappear
Mar  2 01:03:33.412: INFO: Pod pod-projected-configmaps-5f9cae7f-f183-4b76-bc70-8b980019ed08 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Mar  2 01:03:33.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1833" for this suite.

• [SLOW TEST:6.882 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":73,"skipped":1281,"failed":0}
SS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:03:33.445: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Mar  2 01:03:33.635: INFO: Waiting up to 5m0s for pod "security-context-ab5e2799-e576-48d2-8fe3-9a6409068648" in namespace "security-context-5636" to be "Succeeded or Failed"
Mar  2 01:03:33.645: INFO: Pod "security-context-ab5e2799-e576-48d2-8fe3-9a6409068648": Phase="Pending", Reason="", readiness=false. Elapsed: 9.949883ms
Mar  2 01:03:35.655: INFO: Pod "security-context-ab5e2799-e576-48d2-8fe3-9a6409068648": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019982907s
Mar  2 01:03:37.668: INFO: Pod "security-context-ab5e2799-e576-48d2-8fe3-9a6409068648": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03297089s
STEP: Saw pod success
Mar  2 01:03:37.668: INFO: Pod "security-context-ab5e2799-e576-48d2-8fe3-9a6409068648" satisfied condition "Succeeded or Failed"
Mar  2 01:03:37.675: INFO: Trying to get logs from node 10.123.244.39 pod security-context-ab5e2799-e576-48d2-8fe3-9a6409068648 container test-container: <nil>
STEP: delete the pod
Mar  2 01:03:37.845: INFO: Waiting for pod security-context-ab5e2799-e576-48d2-8fe3-9a6409068648 to disappear
Mar  2 01:03:37.852: INFO: Pod security-context-ab5e2799-e576-48d2-8fe3-9a6409068648 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Mar  2 01:03:37.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-5636" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":74,"skipped":1283,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:03:37.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:188
Mar  2 01:03:37.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1129" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":356,"completed":75,"skipped":1288,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:03:38.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar  2 01:03:49.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9072" for this suite.

• [SLOW TEST:11.570 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":356,"completed":76,"skipped":1291,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests 
  should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:03:49.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename conformance-tests
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
STEP: Getting node addresses
Mar  2 01:03:49.771: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:188
Mar  2 01:03:49.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-8068" for this suite.
•{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","total":356,"completed":77,"skipped":1379,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:03:49.820: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Mar  2 01:03:50.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8219" for this suite.
STEP: Destroying namespace "nspatchtest-ffa1d865-d3a8-4098-a6e7-602cc08e72eb-6606" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":356,"completed":78,"skipped":1396,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:03:50.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  2 01:03:50.383: INFO: Waiting up to 5m0s for pod "pod-d855024b-9aa3-4d39-893b-84cf78e4bb01" in namespace "emptydir-8066" to be "Succeeded or Failed"
W0302 01:03:50.383183      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:03:50.397: INFO: Pod "pod-d855024b-9aa3-4d39-893b-84cf78e4bb01": Phase="Pending", Reason="", readiness=false. Elapsed: 14.056259ms
Mar  2 01:03:52.413: INFO: Pod "pod-d855024b-9aa3-4d39-893b-84cf78e4bb01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030260075s
Mar  2 01:03:54.425: INFO: Pod "pod-d855024b-9aa3-4d39-893b-84cf78e4bb01": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042550714s
Mar  2 01:03:56.440: INFO: Pod "pod-d855024b-9aa3-4d39-893b-84cf78e4bb01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057121549s
STEP: Saw pod success
Mar  2 01:03:56.440: INFO: Pod "pod-d855024b-9aa3-4d39-893b-84cf78e4bb01" satisfied condition "Succeeded or Failed"
Mar  2 01:03:56.448: INFO: Trying to get logs from node 10.123.244.39 pod pod-d855024b-9aa3-4d39-893b-84cf78e4bb01 container test-container: <nil>
STEP: delete the pod
Mar  2 01:03:56.715: INFO: Waiting for pod pod-d855024b-9aa3-4d39-893b-84cf78e4bb01 to disappear
Mar  2 01:03:56.741: INFO: Pod pod-d855024b-9aa3-4d39-893b-84cf78e4bb01 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar  2 01:03:56.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8066" for this suite.

• [SLOW TEST:6.567 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":79,"skipped":1400,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:03:56.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Mar  2 01:04:03.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9342" for this suite.

• [SLOW TEST:6.321 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":80,"skipped":1418,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:04:03.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1540
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Mar  2 01:04:03.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-8392 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Mar  2 01:04:03.454: INFO: stderr: ""
Mar  2 01:04:03.454: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1544
Mar  2 01:04:03.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-8392 delete pods e2e-test-httpd-pod'
Mar  2 01:04:06.592: INFO: stderr: ""
Mar  2 01:04:06.592: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar  2 01:04:06.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8392" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":356,"completed":81,"skipped":1435,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:04:06.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Mar  2 01:04:08.956: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Mar  2 01:04:11.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9972" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":356,"completed":82,"skipped":1465,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:04:11.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-3457
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 01:04:11.153: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 01:04:11.266: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:04:13.293: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:04:15.276: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:04:17.276: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:04:19.276: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:04:21.274: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:04:23.279: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:04:25.274: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:04:27.276: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:04:29.275: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:04:31.279: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:04:33.281: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  2 01:04:33.296: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  2 01:04:33.312: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar  2 01:04:37.427: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 01:04:37.427: INFO: Going to poll 172.30.88.212 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  2 01:04:37.442: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.88.212 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3457 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:04:37.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:04:37.445: INFO: ExecWithOptions: Clientset creation
Mar  2 01:04:37.445: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3457/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.88.212+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 01:04:38.676: INFO: Found all 1 expected endpoints: [netserver-0]
Mar  2 01:04:38.676: INFO: Going to poll 172.30.54.226 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  2 01:04:38.694: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.54.226 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3457 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:04:38.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:04:38.695: INFO: ExecWithOptions: Clientset creation
Mar  2 01:04:38.698: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3457/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.54.226+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 01:04:40.071: INFO: Found all 1 expected endpoints: [netserver-1]
Mar  2 01:04:40.071: INFO: Going to poll 172.30.0.204 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar  2 01:04:40.088: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.0.204 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3457 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:04:40.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:04:40.090: INFO: ExecWithOptions: Clientset creation
Mar  2 01:04:40.090: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3457/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.0.204+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar  2 01:04:41.920: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Mar  2 01:04:41.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3457" for this suite.

• [SLOW TEST:30.876 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":83,"skipped":1465,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:04:41.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar  2 01:04:42.068: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7fb4b3a2-5c9e-4c49-8e2f-30c5919106c9" in namespace "projected-1148" to be "Succeeded or Failed"
Mar  2 01:04:42.076: INFO: Pod "downwardapi-volume-7fb4b3a2-5c9e-4c49-8e2f-30c5919106c9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.669858ms
Mar  2 01:04:44.091: INFO: Pod "downwardapi-volume-7fb4b3a2-5c9e-4c49-8e2f-30c5919106c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023138225s
Mar  2 01:04:46.111: INFO: Pod "downwardapi-volume-7fb4b3a2-5c9e-4c49-8e2f-30c5919106c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042780459s
STEP: Saw pod success
Mar  2 01:04:46.111: INFO: Pod "downwardapi-volume-7fb4b3a2-5c9e-4c49-8e2f-30c5919106c9" satisfied condition "Succeeded or Failed"
Mar  2 01:04:46.118: INFO: Trying to get logs from node 10.123.244.39 pod downwardapi-volume-7fb4b3a2-5c9e-4c49-8e2f-30c5919106c9 container client-container: <nil>
STEP: delete the pod
Mar  2 01:04:46.151: INFO: Waiting for pod downwardapi-volume-7fb4b3a2-5c9e-4c49-8e2f-30c5919106c9 to disappear
Mar  2 01:04:46.169: INFO: Pod downwardapi-volume-7fb4b3a2-5c9e-4c49-8e2f-30c5919106c9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar  2 01:04:46.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1148" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":84,"skipped":1529,"failed":0}

------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:04:46.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Mar  2 01:04:48.369: INFO: pods: 0 < 3
Mar  2 01:04:50.382: INFO: running pods: 0 < 3
Mar  2 01:04:52.395: INFO: running pods: 2 < 3
Mar  2 01:04:54.379: INFO: running pods: 2 < 3
Mar  2 01:04:56.379: INFO: running pods: 2 < 3
Mar  2 01:04:58.379: INFO: running pods: 2 < 3
Mar  2 01:05:00.380: INFO: running pods: 2 < 3
Mar  2 01:05:02.383: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Mar  2 01:05:08.602: INFO: running pods: 2 < 3
Mar  2 01:05:10.615: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Mar  2 01:05:12.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9445" for this suite.

• [SLOW TEST:26.519 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":356,"completed":85,"skipped":1529,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:05:12.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Mar  2 01:05:12.841: INFO: Waiting up to 5m0s for pod "downward-api-de374e1e-6f7d-47fd-bfa2-834199d4df28" in namespace "downward-api-686" to be "Succeeded or Failed"
Mar  2 01:05:12.849: INFO: Pod "downward-api-de374e1e-6f7d-47fd-bfa2-834199d4df28": Phase="Pending", Reason="", readiness=false. Elapsed: 8.133788ms
Mar  2 01:05:14.866: INFO: Pod "downward-api-de374e1e-6f7d-47fd-bfa2-834199d4df28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025356033s
Mar  2 01:05:16.876: INFO: Pod "downward-api-de374e1e-6f7d-47fd-bfa2-834199d4df28": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035231646s
Mar  2 01:05:18.890: INFO: Pod "downward-api-de374e1e-6f7d-47fd-bfa2-834199d4df28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048962623s
STEP: Saw pod success
Mar  2 01:05:18.890: INFO: Pod "downward-api-de374e1e-6f7d-47fd-bfa2-834199d4df28" satisfied condition "Succeeded or Failed"
Mar  2 01:05:18.900: INFO: Trying to get logs from node 10.123.244.39 pod downward-api-de374e1e-6f7d-47fd-bfa2-834199d4df28 container dapi-container: <nil>
STEP: delete the pod
Mar  2 01:05:18.963: INFO: Waiting for pod downward-api-de374e1e-6f7d-47fd-bfa2-834199d4df28 to disappear
Mar  2 01:05:18.969: INFO: Pod downward-api-de374e1e-6f7d-47fd-bfa2-834199d4df28 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Mar  2 01:05:18.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-686" for this suite.

• [SLOW TEST:6.306 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":356,"completed":86,"skipped":1547,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:05:19.037: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-4096dbaa-ae1f-495d-96ce-4691a33ea908
STEP: Creating a pod to test consume secrets
Mar  2 01:05:19.238: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c2cc8cbc-c3bc-48fd-a322-5ea8d516ff03" in namespace "projected-3797" to be "Succeeded or Failed"
Mar  2 01:05:19.245: INFO: Pod "pod-projected-secrets-c2cc8cbc-c3bc-48fd-a322-5ea8d516ff03": Phase="Pending", Reason="", readiness=false. Elapsed: 7.031538ms
Mar  2 01:05:21.260: INFO: Pod "pod-projected-secrets-c2cc8cbc-c3bc-48fd-a322-5ea8d516ff03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021973484s
Mar  2 01:05:23.275: INFO: Pod "pod-projected-secrets-c2cc8cbc-c3bc-48fd-a322-5ea8d516ff03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036896928s
STEP: Saw pod success
Mar  2 01:05:23.275: INFO: Pod "pod-projected-secrets-c2cc8cbc-c3bc-48fd-a322-5ea8d516ff03" satisfied condition "Succeeded or Failed"
Mar  2 01:05:23.282: INFO: Trying to get logs from node 10.123.244.39 pod pod-projected-secrets-c2cc8cbc-c3bc-48fd-a322-5ea8d516ff03 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 01:05:23.318: INFO: Waiting for pod pod-projected-secrets-c2cc8cbc-c3bc-48fd-a322-5ea8d516ff03 to disappear
Mar  2 01:05:23.326: INFO: Pod pod-projected-secrets-c2cc8cbc-c3bc-48fd-a322-5ea8d516ff03 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Mar  2 01:05:23.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3797" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":87,"skipped":1599,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:05:23.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a ReplicationController is created
W0302 01:05:23.476028      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: When the matched label of one of its pods change
Mar  2 01:05:23.484: INFO: Pod name pod-release: Found 0 pods out of 1
Mar  2 01:05:28.498: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Mar  2 01:05:29.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1518" for this suite.

• [SLOW TEST:6.228 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":356,"completed":88,"skipped":1625,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:05:29.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1304
[It] should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-1304
W0302 01:05:29.735980      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:05:29.742: INFO: Found 0 stateful pods, waiting for 1
Mar  2 01:05:39.767: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 01:05:39.846: INFO: Deleting all statefulset in ns statefulset-1304
Mar  2 01:05:39.855: INFO: Scaling statefulset ss to 0
Mar  2 01:05:49.920: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 01:05:49.927: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Mar  2 01:05:49.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1304" for this suite.

• [SLOW TEST:20.419 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":356,"completed":89,"skipped":1640,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:05:50.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-112a6727-2f3d-4586-a6b8-8a3a31a6f529 in namespace container-probe-4274
Mar  2 01:05:52.186: INFO: Started pod liveness-112a6727-2f3d-4586-a6b8-8a3a31a6f529 in namespace container-probe-4274
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 01:05:52.198: INFO: Initial restart count of pod liveness-112a6727-2f3d-4586-a6b8-8a3a31a6f529 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Mar  2 01:09:52.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4274" for this suite.

• [SLOW TEST:242.375 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":356,"completed":90,"skipped":1690,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:09:52.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar  2 01:09:52.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6152" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":356,"completed":91,"skipped":1723,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:09:52.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1334
STEP: creating the pod
Mar  2 01:09:52.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-3567 create -f -'
Mar  2 01:09:53.460: INFO: stderr: ""
Mar  2 01:09:53.460: INFO: stdout: "pod/pause created\n"
Mar  2 01:09:53.460: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar  2 01:09:53.460: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3567" to be "running and ready"
Mar  2 01:09:53.473: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.705449ms
Mar  2 01:09:55.482: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.022193994s
Mar  2 01:09:55.482: INFO: Pod "pause" satisfied condition "running and ready"
Mar  2 01:09:55.482: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
STEP: adding the label testing-label with value testing-label-value to a pod
Mar  2 01:09:55.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-3567 label pods pause testing-label=testing-label-value'
Mar  2 01:09:55.666: INFO: stderr: ""
Mar  2 01:09:55.666: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar  2 01:09:55.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-3567 get pod pause -L testing-label'
Mar  2 01:09:55.833: INFO: stderr: ""
Mar  2 01:09:55.833: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar  2 01:09:55.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-3567 label pods pause testing-label-'
Mar  2 01:09:56.065: INFO: stderr: ""
Mar  2 01:09:56.065: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar  2 01:09:56.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-3567 get pod pause -L testing-label'
Mar  2 01:09:56.265: INFO: stderr: ""
Mar  2 01:09:56.265: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Mar  2 01:09:56.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-3567 delete --grace-period=0 --force -f -'
Mar  2 01:09:56.413: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 01:09:56.413: INFO: stdout: "pod \"pause\" force deleted\n"
Mar  2 01:09:56.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-3567 get rc,svc -l name=pause --no-headers'
Mar  2 01:09:56.629: INFO: stderr: "No resources found in kubectl-3567 namespace.\n"
Mar  2 01:09:56.630: INFO: stdout: ""
Mar  2 01:09:56.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-3567 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 01:09:56.832: INFO: stderr: ""
Mar  2 01:09:56.832: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar  2 01:09:56.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3567" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":356,"completed":92,"skipped":1736,"failed":0}
SSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:09:56.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
Mar  2 01:09:56.993: INFO: created test-event-1
Mar  2 01:09:57.008: INFO: created test-event-2
Mar  2 01:09:57.056: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Mar  2 01:09:57.071: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Mar  2 01:09:57.191: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Mar  2 01:09:57.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7227" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":356,"completed":93,"skipped":1739,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:09:57.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod with failed condition
STEP: updating the pod
Mar  2 01:11:57.926: INFO: Successfully updated pod "var-expansion-780a5d1e-b961-4252-833f-a527e99a2cdd"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Mar  2 01:11:59.945: INFO: Deleting pod "var-expansion-780a5d1e-b961-4252-833f-a527e99a2cdd" in namespace "var-expansion-9629"
Mar  2 01:11:59.961: INFO: Wait up to 5m0s for pod "var-expansion-780a5d1e-b961-4252-833f-a527e99a2cdd" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Mar  2 01:12:31.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9629" for this suite.

• [SLOW TEST:154.779 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":356,"completed":94,"skipped":1740,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:12:32.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  2 01:12:32.189: INFO: Waiting up to 5m0s for pod "pod-d80149a2-ba7d-474b-8af2-3788cf772bbc" in namespace "emptydir-3344" to be "Succeeded or Failed"
Mar  2 01:12:32.196: INFO: Pod "pod-d80149a2-ba7d-474b-8af2-3788cf772bbc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.994362ms
Mar  2 01:12:34.210: INFO: Pod "pod-d80149a2-ba7d-474b-8af2-3788cf772bbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021228106s
Mar  2 01:12:36.218: INFO: Pod "pod-d80149a2-ba7d-474b-8af2-3788cf772bbc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029713286s
Mar  2 01:12:38.229: INFO: Pod "pod-d80149a2-ba7d-474b-8af2-3788cf772bbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03999359s
STEP: Saw pod success
Mar  2 01:12:38.229: INFO: Pod "pod-d80149a2-ba7d-474b-8af2-3788cf772bbc" satisfied condition "Succeeded or Failed"
Mar  2 01:12:38.237: INFO: Trying to get logs from node 10.123.244.39 pod pod-d80149a2-ba7d-474b-8af2-3788cf772bbc container test-container: <nil>
STEP: delete the pod
Mar  2 01:12:38.305: INFO: Waiting for pod pod-d80149a2-ba7d-474b-8af2-3788cf772bbc to disappear
Mar  2 01:12:38.344: INFO: Pod pod-d80149a2-ba7d-474b-8af2-3788cf772bbc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar  2 01:12:38.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3344" for this suite.

• [SLOW TEST:6.388 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":95,"skipped":1740,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:12:38.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service endpoint-test2 in namespace services-8564
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8564 to expose endpoints map[]
Mar  2 01:12:38.598: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Mar  2 01:12:39.634: INFO: successfully validated that service endpoint-test2 in namespace services-8564 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8564
Mar  2 01:12:39.705: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:12:41.721: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8564 to expose endpoints map[pod1:[80]]
Mar  2 01:12:41.772: INFO: successfully validated that service endpoint-test2 in namespace services-8564 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Mar  2 01:12:41.772: INFO: Creating new exec pod
Mar  2 01:12:44.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-8564 exec execpodnq8lt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  2 01:12:45.581: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar  2 01:12:45.581: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:12:45.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-8564 exec execpodnq8lt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.53.19 80'
Mar  2 01:12:45.828: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.53.19 80\nConnection to 172.21.53.19 80 port [tcp/http] succeeded!\n"
Mar  2 01:12:45.829: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-8564
Mar  2 01:12:45.862: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:12:47.873: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:12:49.875: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8564 to expose endpoints map[pod1:[80] pod2:[80]]
Mar  2 01:12:49.931: INFO: successfully validated that service endpoint-test2 in namespace services-8564 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Mar  2 01:12:50.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-8564 exec execpodnq8lt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  2 01:12:51.603: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar  2 01:12:51.603: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:12:51.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-8564 exec execpodnq8lt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.53.19 80'
Mar  2 01:12:52.074: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.53.19 80\nConnection to 172.21.53.19 80 port [tcp/http] succeeded!\n"
Mar  2 01:12:52.075: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-8564
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8564 to expose endpoints map[pod2:[80]]
Mar  2 01:12:54.185: INFO: successfully validated that service endpoint-test2 in namespace services-8564 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Mar  2 01:12:55.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-8564 exec execpodnq8lt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar  2 01:12:55.640: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar  2 01:12:55.640: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:12:55.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-8564 exec execpodnq8lt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.53.19 80'
Mar  2 01:12:56.032: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.53.19 80\nConnection to 172.21.53.19 80 port [tcp/http] succeeded!\n"
Mar  2 01:12:56.032: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-8564
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8564 to expose endpoints map[]
Mar  2 01:12:57.129: INFO: successfully validated that service endpoint-test2 in namespace services-8564 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar  2 01:12:57.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8564" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:18.899 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":356,"completed":96,"skipped":1764,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:12:57.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating pod
W0302 01:12:57.537285      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:12:57.605: INFO: The status of Pod pod-hostip-2b48dfdd-1158-4b61-a55d-f5b340adc196 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:12:59.622: INFO: The status of Pod pod-hostip-2b48dfdd-1158-4b61-a55d-f5b340adc196 is Running (Ready = true)
Mar  2 01:12:59.648: INFO: Pod pod-hostip-2b48dfdd-1158-4b61-a55d-f5b340adc196 has hostIP: 10.123.244.39
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Mar  2 01:12:59.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1946" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":356,"completed":97,"skipped":1803,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:12:59.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on tmpfs
W0302 01:12:59.824730      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:12:59.826: INFO: Waiting up to 5m0s for pod "pod-e409a20f-fa2f-4391-a258-6808d3bb03c3" in namespace "emptydir-5370" to be "Succeeded or Failed"
Mar  2 01:12:59.835: INFO: Pod "pod-e409a20f-fa2f-4391-a258-6808d3bb03c3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.982167ms
Mar  2 01:13:01.845: INFO: Pod "pod-e409a20f-fa2f-4391-a258-6808d3bb03c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019001571s
Mar  2 01:13:03.889: INFO: Pod "pod-e409a20f-fa2f-4391-a258-6808d3bb03c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062804844s
STEP: Saw pod success
Mar  2 01:13:03.889: INFO: Pod "pod-e409a20f-fa2f-4391-a258-6808d3bb03c3" satisfied condition "Succeeded or Failed"
Mar  2 01:13:03.908: INFO: Trying to get logs from node 10.123.244.39 pod pod-e409a20f-fa2f-4391-a258-6808d3bb03c3 container test-container: <nil>
STEP: delete the pod
Mar  2 01:13:03.990: INFO: Waiting for pod pod-e409a20f-fa2f-4391-a258-6808d3bb03c3 to disappear
Mar  2 01:13:03.998: INFO: Pod pod-e409a20f-fa2f-4391-a258-6808d3bb03c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar  2 01:13:03.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5370" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":98,"skipped":1817,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:13:04.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Mar  2 01:13:08.207: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3004 PodName:pod-sharedvolume-7d1abfb2-b1ed-44d4-80f7-31e46783d788 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:13:08.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:13:08.208: INFO: ExecWithOptions: Clientset creation
Mar  2 01:13:08.208: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-3004/pods/pod-sharedvolume-7d1abfb2-b1ed-44d4-80f7-31e46783d788/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Mar  2 01:13:08.374: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar  2 01:13:08.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3004" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":356,"completed":99,"skipped":1839,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:13:08.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar  2 01:13:08.567: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c394cc02-4f6b-4fcf-aa92-343f3aa906dd" in namespace "projected-6566" to be "Succeeded or Failed"
Mar  2 01:13:08.585: INFO: Pod "downwardapi-volume-c394cc02-4f6b-4fcf-aa92-343f3aa906dd": Phase="Pending", Reason="", readiness=false. Elapsed: 17.673353ms
Mar  2 01:13:10.595: INFO: Pod "downwardapi-volume-c394cc02-4f6b-4fcf-aa92-343f3aa906dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027583544s
Mar  2 01:13:12.604: INFO: Pod "downwardapi-volume-c394cc02-4f6b-4fcf-aa92-343f3aa906dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036247617s
Mar  2 01:13:14.613: INFO: Pod "downwardapi-volume-c394cc02-4f6b-4fcf-aa92-343f3aa906dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045260028s
STEP: Saw pod success
Mar  2 01:13:14.613: INFO: Pod "downwardapi-volume-c394cc02-4f6b-4fcf-aa92-343f3aa906dd" satisfied condition "Succeeded or Failed"
Mar  2 01:13:14.620: INFO: Trying to get logs from node 10.123.244.39 pod downwardapi-volume-c394cc02-4f6b-4fcf-aa92-343f3aa906dd container client-container: <nil>
STEP: delete the pod
Mar  2 01:13:14.658: INFO: Waiting for pod downwardapi-volume-c394cc02-4f6b-4fcf-aa92-343f3aa906dd to disappear
Mar  2 01:13:14.666: INFO: Pod downwardapi-volume-c394cc02-4f6b-4fcf-aa92-343f3aa906dd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar  2 01:13:14.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6566" for this suite.

• [SLOW TEST:6.275 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":100,"skipped":1842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:13:14.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:13:14.870: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f48d4806-6117-4127-af23-7854ae34ec92
STEP: Creating the pod
Mar  2 01:13:14.944: INFO: The status of Pod pod-projected-configmaps-7950d0c9-3b57-49e2-810e-a60fe27d8c8b is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:13:16.955: INFO: The status of Pod pod-projected-configmaps-7950d0c9-3b57-49e2-810e-a60fe27d8c8b is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-f48d4806-6117-4127-af23-7854ae34ec92
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Mar  2 01:13:19.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5390" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":101,"skipped":1932,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:13:19.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 01:13:23.528: INFO: DNS probes using dns-1966/dns-test-df96879d-1b5b-4a34-bbaf-a1093a9be75d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Mar  2 01:13:23.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1966" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":356,"completed":102,"skipped":1987,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:13:23.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-7939
Mar  2 01:13:24.528: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:13:26.563: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar  2 01:13:26.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-7939 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  2 01:13:27.079: INFO: rc: 7
Mar  2 01:13:27.103: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  2 01:13:27.113: INFO: Pod kube-proxy-mode-detector no longer exists
Mar  2 01:13:27.113: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-7939 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-7939
STEP: creating replication controller affinity-nodeport-timeout in namespace services-7939
I0302 01:13:27.209531      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-7939, replica count: 3
I0302 01:13:30.261159      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 01:13:33.262046      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 01:13:33.307: INFO: Creating new exec pod
Mar  2 01:13:36.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-7939 exec execpod-affinityv2pvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Mar  2 01:13:36.948: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar  2 01:13:36.948: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:13:36.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-7939 exec execpod-affinityv2pvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.190.182 80'
Mar  2 01:13:37.297: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.190.182 80\nConnection to 172.21.190.182 80 port [tcp/http] succeeded!\n"
Mar  2 01:13:37.297: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:13:37.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-7939 exec execpod-affinityv2pvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.123.244.39 31110'
Mar  2 01:13:37.605: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.123.244.39 31110\nConnection to 10.123.244.39 31110 port [tcp/*] succeeded!\n"
Mar  2 01:13:37.605: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:13:37.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-7939 exec execpod-affinityv2pvw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.123.244.41 31110'
Mar  2 01:13:37.903: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.123.244.41 31110\nConnection to 10.123.244.41 31110 port [tcp/*] succeeded!\n"
Mar  2 01:13:37.903: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:13:37.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-7939 exec execpod-affinityv2pvw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.123.244.39:31110/ ; done'
Mar  2 01:13:38.366: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:31110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:31110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:31110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:31110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:31110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:31110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:31110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:31110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:31110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:31110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:31110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:31110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:31110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:31110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:31110/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:31110/\n"
Mar  2 01:13:38.366: INFO: stdout: "\naffinity-nodeport-timeout-dqkvh\naffinity-nodeport-timeout-dqkvh\naffinity-nodeport-timeout-dqkvh\naffinity-nodeport-timeout-dqkvh\naffinity-nodeport-timeout-dqkvh\naffinity-nodeport-timeout-dqkvh\naffinity-nodeport-timeout-dqkvh\naffinity-nodeport-timeout-dqkvh\naffinity-nodeport-timeout-dqkvh\naffinity-nodeport-timeout-dqkvh\naffinity-nodeport-timeout-dqkvh\naffinity-nodeport-timeout-dqkvh\naffinity-nodeport-timeout-dqkvh\naffinity-nodeport-timeout-dqkvh\naffinity-nodeport-timeout-dqkvh\naffinity-nodeport-timeout-dqkvh"
Mar  2 01:13:38.366: INFO: Received response from host: affinity-nodeport-timeout-dqkvh
Mar  2 01:13:38.366: INFO: Received response from host: affinity-nodeport-timeout-dqkvh
Mar  2 01:13:38.366: INFO: Received response from host: affinity-nodeport-timeout-dqkvh
Mar  2 01:13:38.366: INFO: Received response from host: affinity-nodeport-timeout-dqkvh
Mar  2 01:13:38.366: INFO: Received response from host: affinity-nodeport-timeout-dqkvh
Mar  2 01:13:38.366: INFO: Received response from host: affinity-nodeport-timeout-dqkvh
Mar  2 01:13:38.366: INFO: Received response from host: affinity-nodeport-timeout-dqkvh
Mar  2 01:13:38.366: INFO: Received response from host: affinity-nodeport-timeout-dqkvh
Mar  2 01:13:38.366: INFO: Received response from host: affinity-nodeport-timeout-dqkvh
Mar  2 01:13:38.366: INFO: Received response from host: affinity-nodeport-timeout-dqkvh
Mar  2 01:13:38.366: INFO: Received response from host: affinity-nodeport-timeout-dqkvh
Mar  2 01:13:38.366: INFO: Received response from host: affinity-nodeport-timeout-dqkvh
Mar  2 01:13:38.366: INFO: Received response from host: affinity-nodeport-timeout-dqkvh
Mar  2 01:13:38.366: INFO: Received response from host: affinity-nodeport-timeout-dqkvh
Mar  2 01:13:38.366: INFO: Received response from host: affinity-nodeport-timeout-dqkvh
Mar  2 01:13:38.366: INFO: Received response from host: affinity-nodeport-timeout-dqkvh
Mar  2 01:13:38.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-7939 exec execpod-affinityv2pvw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.123.244.39:31110/'
Mar  2 01:13:38.791: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.123.244.39:31110/\n"
Mar  2 01:13:38.791: INFO: stdout: "affinity-nodeport-timeout-dqkvh"
Mar  2 01:13:58.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-7939 exec execpod-affinityv2pvw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.123.244.39:31110/'
Mar  2 01:13:59.259: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.123.244.39:31110/\n"
Mar  2 01:13:59.259: INFO: stdout: "affinity-nodeport-timeout-7pw72"
Mar  2 01:13:59.259: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-7939, will wait for the garbage collector to delete the pods
Mar  2 01:13:59.394: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 34.798556ms
Mar  2 01:13:59.526: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 131.478561ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar  2 01:14:03.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7939" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:39.256 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":103,"skipped":2050,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:14:03.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:14:05.495: INFO: Deleting pod "var-expansion-de2b13fe-64ee-4497-8b0e-1fbb8aa09665" in namespace "var-expansion-4900"
Mar  2 01:14:05.510: INFO: Wait up to 5m0s for pod "var-expansion-de2b13fe-64ee-4497-8b0e-1fbb8aa09665" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Mar  2 01:14:09.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4900" for this suite.

• [SLOW TEST:6.367 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":356,"completed":104,"skipped":2098,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:14:09.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
W0302 01:14:09.747421      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:14:09.747: INFO: Waiting up to 5m0s for pod "downward-api-b625d1bc-b72c-4d3d-bd5b-404f4f53d3ce" in namespace "downward-api-4593" to be "Succeeded or Failed"
Mar  2 01:14:09.760: INFO: Pod "downward-api-b625d1bc-b72c-4d3d-bd5b-404f4f53d3ce": Phase="Pending", Reason="", readiness=false. Elapsed: 13.289218ms
Mar  2 01:14:11.771: INFO: Pod "downward-api-b625d1bc-b72c-4d3d-bd5b-404f4f53d3ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023570023s
Mar  2 01:14:13.803: INFO: Pod "downward-api-b625d1bc-b72c-4d3d-bd5b-404f4f53d3ce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056298007s
Mar  2 01:14:15.824: INFO: Pod "downward-api-b625d1bc-b72c-4d3d-bd5b-404f4f53d3ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.076742403s
STEP: Saw pod success
Mar  2 01:14:15.824: INFO: Pod "downward-api-b625d1bc-b72c-4d3d-bd5b-404f4f53d3ce" satisfied condition "Succeeded or Failed"
Mar  2 01:14:15.837: INFO: Trying to get logs from node 10.123.244.39 pod downward-api-b625d1bc-b72c-4d3d-bd5b-404f4f53d3ce container dapi-container: <nil>
STEP: delete the pod
Mar  2 01:14:15.884: INFO: Waiting for pod downward-api-b625d1bc-b72c-4d3d-bd5b-404f4f53d3ce to disappear
Mar  2 01:14:15.893: INFO: Pod downward-api-b625d1bc-b72c-4d3d-bd5b-404f4f53d3ce no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Mar  2 01:14:15.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4593" for this suite.

• [SLOW TEST:6.300 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":356,"completed":105,"skipped":2109,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:14:15.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:14:16.103: INFO: Creating daemon "daemon-set" with a node selector
W0302 01:14:16.115197      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Initially, daemon pods should not be running on any nodes.
Mar  2 01:14:16.122: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:14:16.122: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Mar  2 01:14:16.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:14:16.177: INFO: Node 10.123.244.41 is running 0 daemon pod, expected 1
Mar  2 01:14:17.192: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:14:17.192: INFO: Node 10.123.244.41 is running 0 daemon pod, expected 1
Mar  2 01:14:18.189: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:14:18.189: INFO: Node 10.123.244.41 is running 0 daemon pod, expected 1
Mar  2 01:14:19.194: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  2 01:14:19.194: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar  2 01:14:19.259: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  2 01:14:19.259: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Mar  2 01:14:20.272: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:14:20.272: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar  2 01:14:20.310: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:14:20.310: INFO: Node 10.123.244.41 is running 0 daemon pod, expected 1
Mar  2 01:14:21.323: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:14:21.323: INFO: Node 10.123.244.41 is running 0 daemon pod, expected 1
Mar  2 01:14:22.319: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:14:22.319: INFO: Node 10.123.244.41 is running 0 daemon pod, expected 1
Mar  2 01:14:23.320: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:14:23.320: INFO: Node 10.123.244.41 is running 0 daemon pod, expected 1
Mar  2 01:14:24.322: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  2 01:14:24.322: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6385, will wait for the garbage collector to delete the pods
Mar  2 01:14:24.414: INFO: Deleting DaemonSet.extensions daemon-set took: 12.117811ms
Mar  2 01:14:24.515: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.227739ms
Mar  2 01:14:27.324: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:14:27.324: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  2 01:14:27.331: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"98213"},"items":null}

Mar  2 01:14:27.337: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"98213"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Mar  2 01:14:27.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6385" for this suite.

• [SLOW TEST:11.483 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":356,"completed":106,"skipped":2149,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:14:27.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar  2 01:14:27.590: INFO: The status of Pod pod-update-3905c17d-c9fd-4234-81ed-e4e837b62007 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:14:29.599: INFO: The status of Pod pod-update-3905c17d-c9fd-4234-81ed-e4e837b62007 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:14:31.608: INFO: The status of Pod pod-update-3905c17d-c9fd-4234-81ed-e4e837b62007 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  2 01:14:32.173: INFO: Successfully updated pod "pod-update-3905c17d-c9fd-4234-81ed-e4e837b62007"
STEP: verifying the updated pod is in kubernetes
Mar  2 01:14:32.188: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Mar  2 01:14:32.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5488" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":356,"completed":107,"skipped":2151,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:14:32.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-secret-xfxq
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 01:14:32.468: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-xfxq" in namespace "subpath-4035" to be "Succeeded or Failed"
Mar  2 01:14:32.485: INFO: Pod "pod-subpath-test-secret-xfxq": Phase="Pending", Reason="", readiness=false. Elapsed: 17.067943ms
Mar  2 01:14:34.501: INFO: Pod "pod-subpath-test-secret-xfxq": Phase="Running", Reason="", readiness=true. Elapsed: 2.032713125s
Mar  2 01:14:36.522: INFO: Pod "pod-subpath-test-secret-xfxq": Phase="Running", Reason="", readiness=true. Elapsed: 4.054121111s
Mar  2 01:14:38.549: INFO: Pod "pod-subpath-test-secret-xfxq": Phase="Running", Reason="", readiness=true. Elapsed: 6.080939937s
Mar  2 01:14:40.590: INFO: Pod "pod-subpath-test-secret-xfxq": Phase="Running", Reason="", readiness=true. Elapsed: 8.121243896s
Mar  2 01:14:42.616: INFO: Pod "pod-subpath-test-secret-xfxq": Phase="Running", Reason="", readiness=true. Elapsed: 10.147308696s
Mar  2 01:14:44.630: INFO: Pod "pod-subpath-test-secret-xfxq": Phase="Running", Reason="", readiness=true. Elapsed: 12.161257573s
Mar  2 01:14:46.638: INFO: Pod "pod-subpath-test-secret-xfxq": Phase="Running", Reason="", readiness=true. Elapsed: 14.169789162s
Mar  2 01:14:48.649: INFO: Pod "pod-subpath-test-secret-xfxq": Phase="Running", Reason="", readiness=true. Elapsed: 16.180353895s
Mar  2 01:14:50.659: INFO: Pod "pod-subpath-test-secret-xfxq": Phase="Running", Reason="", readiness=true. Elapsed: 18.190462506s
Mar  2 01:14:52.704: INFO: Pod "pod-subpath-test-secret-xfxq": Phase="Running", Reason="", readiness=true. Elapsed: 20.235732224s
Mar  2 01:14:54.714: INFO: Pod "pod-subpath-test-secret-xfxq": Phase="Running", Reason="", readiness=false. Elapsed: 22.246132923s
Mar  2 01:14:56.725: INFO: Pod "pod-subpath-test-secret-xfxq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.257068564s
STEP: Saw pod success
Mar  2 01:14:56.726: INFO: Pod "pod-subpath-test-secret-xfxq" satisfied condition "Succeeded or Failed"
Mar  2 01:14:56.735: INFO: Trying to get logs from node 10.123.244.39 pod pod-subpath-test-secret-xfxq container test-container-subpath-secret-xfxq: <nil>
STEP: delete the pod
Mar  2 01:14:56.894: INFO: Waiting for pod pod-subpath-test-secret-xfxq to disappear
Mar  2 01:14:56.917: INFO: Pod pod-subpath-test-secret-xfxq no longer exists
STEP: Deleting pod pod-subpath-test-secret-xfxq
Mar  2 01:14:56.917: INFO: Deleting pod "pod-subpath-test-secret-xfxq" in namespace "subpath-4035"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Mar  2 01:14:56.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4035" for this suite.

• [SLOW TEST:24.772 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","total":356,"completed":108,"skipped":2164,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:14:56.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Mar  2 01:14:57.150: INFO: PodSpec: initContainers in spec.initContainers
Mar  2 01:15:42.312: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-8f297d41-3375-476d-9e76-25f6a143ed84", GenerateName:"", Namespace:"init-container-5373", SelfLink:"", UID:"39da8a40-e1d3-4b4d-bcb2-07e54903bd96", ResourceVersion:"98875", Generation:0, CreationTimestamp:time.Date(2023, time.March, 2, 1, 14, 57, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"150122166"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"b239e6e4347bc1a2729280c7008eb8b230b55f9600bd00d2961cfa727861729a", "cni.projectcalico.org/podIP":"172.30.88.251/32", "cni.projectcalico.org/podIPs":"172.30.88.251/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.88.251\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.88.251\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 2, 1, 14, 57, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00235dde8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 2, 1, 14, 58, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00235de18), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 2, 1, 14, 58, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00235de48), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 2, 1, 14, 59, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00235de78), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-jwswg", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0017acaa0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jwswg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0041f8660), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jwswg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0041f86c0), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.7", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jwswg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0041f8600), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00180e270), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.123.244.39", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00308f570), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00180e330)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00180e350)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00180e36c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00180e370), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000f75460), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 1, 14, 57, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 1, 14, 57, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 1, 14, 57, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 2, 1, 14, 57, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.123.244.39", PodIP:"172.30.88.251", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.88.251"}}, StartTime:time.Date(2023, time.March, 2, 1, 14, 57, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00308f650)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00308f730)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://c8014616dbee4b7c4ce66322a0f6dddfd5a2dc3a9cbd24a053aa0994387f7aab", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0017acc20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0017acbc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.7", ImageID:"", ContainerID:"", Started:(*bool)(0xc00180e3e4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Mar  2 01:15:42.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5373" for this suite.

• [SLOW TEST:45.364 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":356,"completed":109,"skipped":2174,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:15:42.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Mar  2 01:15:42.543: INFO: Waiting up to 5m0s for pod "downward-api-ef6bfeca-6995-42fe-bce8-4992f9f471a4" in namespace "downward-api-9924" to be "Succeeded or Failed"
W0302 01:15:42.543252      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:15:42.552: INFO: Pod "downward-api-ef6bfeca-6995-42fe-bce8-4992f9f471a4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.044585ms
Mar  2 01:15:44.575: INFO: Pod "downward-api-ef6bfeca-6995-42fe-bce8-4992f9f471a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031955657s
Mar  2 01:15:46.584: INFO: Pod "downward-api-ef6bfeca-6995-42fe-bce8-4992f9f471a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041057766s
STEP: Saw pod success
Mar  2 01:15:46.584: INFO: Pod "downward-api-ef6bfeca-6995-42fe-bce8-4992f9f471a4" satisfied condition "Succeeded or Failed"
Mar  2 01:15:46.594: INFO: Trying to get logs from node 10.123.244.39 pod downward-api-ef6bfeca-6995-42fe-bce8-4992f9f471a4 container dapi-container: <nil>
STEP: delete the pod
Mar  2 01:15:46.630: INFO: Waiting for pod downward-api-ef6bfeca-6995-42fe-bce8-4992f9f471a4 to disappear
Mar  2 01:15:46.636: INFO: Pod downward-api-ef6bfeca-6995-42fe-bce8-4992f9f471a4 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Mar  2 01:15:46.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9924" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":356,"completed":110,"skipped":2183,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:15:46.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-7eebb8aa-1ec6-46a5-b41a-c983993d04b2
STEP: Creating a pod to test consume secrets
Mar  2 01:15:46.879: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c6333e13-9311-4618-9966-ed6ffddb8e93" in namespace "projected-6953" to be "Succeeded or Failed"
Mar  2 01:15:46.898: INFO: Pod "pod-projected-secrets-c6333e13-9311-4618-9966-ed6ffddb8e93": Phase="Pending", Reason="", readiness=false. Elapsed: 18.932845ms
Mar  2 01:15:48.908: INFO: Pod "pod-projected-secrets-c6333e13-9311-4618-9966-ed6ffddb8e93": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028738718s
Mar  2 01:15:50.919: INFO: Pod "pod-projected-secrets-c6333e13-9311-4618-9966-ed6ffddb8e93": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039892621s
Mar  2 01:15:52.933: INFO: Pod "pod-projected-secrets-c6333e13-9311-4618-9966-ed6ffddb8e93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053963525s
STEP: Saw pod success
Mar  2 01:15:52.933: INFO: Pod "pod-projected-secrets-c6333e13-9311-4618-9966-ed6ffddb8e93" satisfied condition "Succeeded or Failed"
Mar  2 01:15:52.946: INFO: Trying to get logs from node 10.123.244.39 pod pod-projected-secrets-c6333e13-9311-4618-9966-ed6ffddb8e93 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 01:15:52.992: INFO: Waiting for pod pod-projected-secrets-c6333e13-9311-4618-9966-ed6ffddb8e93 to disappear
Mar  2 01:15:52.999: INFO: Pod pod-projected-secrets-c6333e13-9311-4618-9966-ed6ffddb8e93 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Mar  2 01:15:52.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6953" for this suite.

• [SLOW TEST:6.346 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":111,"skipped":2198,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:15:53.029: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating server pod server in namespace prestop-9705
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9705
STEP: Deleting pre-stop pod
Mar  2 01:16:06.314: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:188
Mar  2 01:16:06.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9705" for this suite.

• [SLOW TEST:13.348 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":356,"completed":112,"skipped":2221,"failed":0}
SSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:16:06.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
STEP: mirroring a new custom Endpoint
Mar  2 01:16:06.716: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Mar  2 01:16:08.784: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Mar  2 01:16:10.854: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:188
Mar  2 01:16:12.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-5108" for this suite.

• [SLOW TEST:6.518 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":356,"completed":113,"skipped":2228,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:16:12.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:16:12.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-7786
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:188
Mar  2 01:16:19.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-3516" for this suite.
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Mar  2 01:16:19.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7786" for this suite.

• [SLOW TEST:6.657 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":356,"completed":114,"skipped":2248,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:16:19.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:16:19.733: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
Mar  2 01:16:19.733: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Mar  2 01:16:19.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1474" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":356,"completed":115,"skipped":2288,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:16:19.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 01:16:21.521: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 01:16:23.567: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 16, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 16, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 16, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 16, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 01:16:26.651: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 01:16:26.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5065" for this suite.
STEP: Destroying namespace "webhook-5065-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.311 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":356,"completed":116,"skipped":2316,"failed":0}
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:16:27.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3024
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Mar  2 01:16:27.258: INFO: Found 0 stateful pods, waiting for 3
Mar  2 01:16:37.273: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 01:16:37.273: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 01:16:37.273: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 01:16:37.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-3024 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 01:16:37.831: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 01:16:37.831: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 01:16:37.831: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Mar  2 01:16:47.944: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar  2 01:16:57.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-3024 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 01:16:58.344: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 01:16:58.344: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 01:16:58.344: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 01:17:08.406: INFO: Waiting for StatefulSet statefulset-3024/ss2 to complete update
Mar  2 01:17:08.407: INFO: Waiting for Pod statefulset-3024/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Mar  2 01:17:08.407: INFO: Waiting for Pod statefulset-3024/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Mar  2 01:17:18.423: INFO: Waiting for StatefulSet statefulset-3024/ss2 to complete update
Mar  2 01:17:18.423: INFO: Waiting for Pod statefulset-3024/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Mar  2 01:17:28.454: INFO: Waiting for StatefulSet statefulset-3024/ss2 to complete update
STEP: Rolling back to a previous revision
Mar  2 01:17:38.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-3024 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 01:17:39.065: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 01:17:39.065: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 01:17:39.065: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 01:17:49.169: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar  2 01:17:59.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-3024 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 01:17:59.569: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 01:17:59.570: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 01:17:59.570: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 01:18:19.629: INFO: Deleting all statefulset in ns statefulset-3024
Mar  2 01:18:19.637: INFO: Scaling statefulset ss2 to 0
Mar  2 01:18:29.745: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 01:18:29.775: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Mar  2 01:18:29.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3024" for this suite.

• [SLOW TEST:122.968 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":356,"completed":117,"skipped":2316,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:18:30.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar  2 01:18:30.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:18:50.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 01:19:59.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6206" for this suite.

• [SLOW TEST:89.653 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":356,"completed":118,"skipped":2322,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:19:59.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pod templates
Mar  2 01:19:59.847: INFO: created test-podtemplate-1
Mar  2 01:19:59.860: INFO: created test-podtemplate-2
Mar  2 01:19:59.872: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Mar  2 01:19:59.881: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Mar  2 01:19:59.926: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Mar  2 01:19:59.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7121" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":356,"completed":119,"skipped":2434,"failed":0}
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:19:59.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Mar  2 01:20:00.158: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:20:02.196: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:20:04.167: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Mar  2 01:20:04.208: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:20:06.223: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:20:08.216: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar  2 01:20:08.345: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 01:20:08.351: INFO: Pod pod-with-poststart-http-hook still exists
Mar  2 01:20:10.352: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar  2 01:20:10.361: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Mar  2 01:20:10.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8340" for this suite.

• [SLOW TEST:10.424 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":356,"completed":120,"skipped":2441,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:20:10.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:20:10.535: INFO: Creating pod...
Mar  2 01:20:12.593: INFO: Creating service...
Mar  2 01:20:12.634: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9640/pods/agnhost/proxy/some/path/with/DELETE
Mar  2 01:20:12.714: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  2 01:20:12.715: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9640/pods/agnhost/proxy/some/path/with/GET
Mar  2 01:20:12.812: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar  2 01:20:12.812: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9640/pods/agnhost/proxy/some/path/with/HEAD
Mar  2 01:20:12.881: INFO: http.Client request:HEAD | StatusCode:200
Mar  2 01:20:12.881: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9640/pods/agnhost/proxy/some/path/with/OPTIONS
Mar  2 01:20:12.920: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  2 01:20:12.920: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9640/pods/agnhost/proxy/some/path/with/PATCH
Mar  2 01:20:12.962: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  2 01:20:12.962: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9640/pods/agnhost/proxy/some/path/with/POST
Mar  2 01:20:13.021: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  2 01:20:13.021: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9640/pods/agnhost/proxy/some/path/with/PUT
Mar  2 01:20:13.101: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar  2 01:20:13.117: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9640/services/test-service/proxy/some/path/with/DELETE
Mar  2 01:20:13.177: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  2 01:20:13.177: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9640/services/test-service/proxy/some/path/with/GET
Mar  2 01:20:13.237: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar  2 01:20:13.238: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9640/services/test-service/proxy/some/path/with/HEAD
Mar  2 01:20:13.544: INFO: http.Client request:HEAD | StatusCode:200
Mar  2 01:20:13.544: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9640/services/test-service/proxy/some/path/with/OPTIONS
Mar  2 01:20:13.580: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  2 01:20:13.580: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9640/services/test-service/proxy/some/path/with/PATCH
Mar  2 01:20:13.623: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  2 01:20:13.623: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9640/services/test-service/proxy/some/path/with/POST
Mar  2 01:20:13.671: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  2 01:20:13.671: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9640/services/test-service/proxy/some/path/with/PUT
Mar  2 01:20:13.734: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Mar  2 01:20:13.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9640" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":356,"completed":121,"skipped":2459,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:20:13.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting the proxy server
Mar  2 01:20:13.839: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-3430 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar  2 01:20:14.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3430" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":356,"completed":122,"skipped":2486,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:20:14.024: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar  2 01:20:14.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7986" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":356,"completed":123,"skipped":2501,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:20:14.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar  2 01:20:14.470: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6253  079a1355-99dc-4021-9973-819abe8a9627 101873 0 2023-03-02 01:20:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-03-02 01:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 01:20:14.471: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6253  079a1355-99dc-4021-9973-819abe8a9627 101875 0 2023-03-02 01:20:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-03-02 01:20:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar  2 01:20:14.561: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6253  079a1355-99dc-4021-9973-819abe8a9627 101880 0 2023-03-02 01:20:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-03-02 01:20:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 01:20:14.562: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6253  079a1355-99dc-4021-9973-819abe8a9627 101885 0 2023-03-02 01:20:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-03-02 01:20:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Mar  2 01:20:14.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6253" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":356,"completed":124,"skipped":2503,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:20:14.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:20:14.693: INFO: Got root ca configmap in namespace "svcaccounts-3435"
Mar  2 01:20:14.728: INFO: Deleted root ca configmap in namespace "svcaccounts-3435"
STEP: waiting for a new root ca configmap created
Mar  2 01:20:15.247: INFO: Recreated root ca configmap in namespace "svcaccounts-3435"
Mar  2 01:20:15.260: INFO: Updated root ca configmap in namespace "svcaccounts-3435"
STEP: waiting for the root ca configmap reconciled
Mar  2 01:20:15.772: INFO: Reconciled root ca configmap in namespace "svcaccounts-3435"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Mar  2 01:20:15.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3435" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":356,"completed":125,"skipped":2518,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:20:15.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod test-webserver-deaab83f-6404-4aa9-b196-0bf36ef3cfcb in namespace container-probe-6351
W0302 01:20:15.903013      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:20:17.918: INFO: Started pod test-webserver-deaab83f-6404-4aa9-b196-0bf36ef3cfcb in namespace container-probe-6351
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 01:20:17.925: INFO: Initial restart count of pod test-webserver-deaab83f-6404-4aa9-b196-0bf36ef3cfcb is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Mar  2 01:24:18.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6351" for this suite.

• [SLOW TEST:242.315 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":126,"skipped":2549,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:24:18.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Mar  2 01:24:20.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1359" for this suite.
•{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":356,"completed":127,"skipped":2618,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:24:20.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-58b32921-9951-4baa-a86e-2ad1343054e5
STEP: Creating a pod to test consume secrets
Mar  2 01:24:20.520: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-346d120e-09ae-42e8-9af1-e982f55738fd" in namespace "projected-2464" to be "Succeeded or Failed"
Mar  2 01:24:20.526: INFO: Pod "pod-projected-secrets-346d120e-09ae-42e8-9af1-e982f55738fd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.336405ms
Mar  2 01:24:22.537: INFO: Pod "pod-projected-secrets-346d120e-09ae-42e8-9af1-e982f55738fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017028606s
Mar  2 01:24:24.559: INFO: Pod "pod-projected-secrets-346d120e-09ae-42e8-9af1-e982f55738fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038165521s
STEP: Saw pod success
Mar  2 01:24:24.559: INFO: Pod "pod-projected-secrets-346d120e-09ae-42e8-9af1-e982f55738fd" satisfied condition "Succeeded or Failed"
Mar  2 01:24:24.572: INFO: Trying to get logs from node 10.123.244.39 pod pod-projected-secrets-346d120e-09ae-42e8-9af1-e982f55738fd container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 01:24:24.618: INFO: Waiting for pod pod-projected-secrets-346d120e-09ae-42e8-9af1-e982f55738fd to disappear
Mar  2 01:24:24.624: INFO: Pod pod-projected-secrets-346d120e-09ae-42e8-9af1-e982f55738fd no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Mar  2 01:24:24.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2464" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":128,"skipped":2628,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:24:24.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar  2 01:24:24.896: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Mar  2 01:24:29.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8961" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":356,"completed":129,"skipped":2670,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:24:29.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0302 01:24:29.560830      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:24:29.561: INFO: Waiting up to 5m0s for pod "downwardapi-volume-097e84c3-e9d4-4ef1-befe-8231bdd062ef" in namespace "projected-5513" to be "Succeeded or Failed"
Mar  2 01:24:29.566: INFO: Pod "downwardapi-volume-097e84c3-e9d4-4ef1-befe-8231bdd062ef": Phase="Pending", Reason="", readiness=false. Elapsed: 5.555766ms
Mar  2 01:24:31.574: INFO: Pod "downwardapi-volume-097e84c3-e9d4-4ef1-befe-8231bdd062ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01322081s
Mar  2 01:24:33.602: INFO: Pod "downwardapi-volume-097e84c3-e9d4-4ef1-befe-8231bdd062ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041305211s
STEP: Saw pod success
Mar  2 01:24:33.602: INFO: Pod "downwardapi-volume-097e84c3-e9d4-4ef1-befe-8231bdd062ef" satisfied condition "Succeeded or Failed"
Mar  2 01:24:33.613: INFO: Trying to get logs from node 10.123.244.39 pod downwardapi-volume-097e84c3-e9d4-4ef1-befe-8231bdd062ef container client-container: <nil>
STEP: delete the pod
Mar  2 01:24:33.705: INFO: Waiting for pod downwardapi-volume-097e84c3-e9d4-4ef1-befe-8231bdd062ef to disappear
Mar  2 01:24:33.713: INFO: Pod downwardapi-volume-097e84c3-e9d4-4ef1-befe-8231bdd062ef no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar  2 01:24:33.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5513" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":130,"skipped":2756,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:24:33.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  2 01:24:39.082: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Mar  2 01:24:39.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8081" for this suite.

• [SLOW TEST:5.377 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":356,"completed":131,"skipped":2768,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:24:39.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-3442
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 01:24:39.194: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 01:24:39.355: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:24:41.366: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:24:43.363: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:24:45.365: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:24:47.365: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:24:49.362: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:24:51.365: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:24:53.364: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:24:55.364: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:24:57.364: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:24:59.364: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:25:01.371: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  2 01:25:01.385: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  2 01:25:01.426: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar  2 01:25:03.494: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 01:25:03.494: INFO: Breadth first check of 172.30.88.214 on host 10.123.244.39...
Mar  2 01:25:03.504: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.88.195:9080/dial?request=hostname&protocol=http&host=172.30.88.214&port=8083&tries=1'] Namespace:pod-network-test-3442 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:25:03.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:25:03.506: INFO: ExecWithOptions: Clientset creation
Mar  2 01:25:03.506: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3442/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.88.195%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.88.214%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 01:25:03.757: INFO: Waiting for responses: map[]
Mar  2 01:25:03.757: INFO: reached 172.30.88.214 after 0/1 tries
Mar  2 01:25:03.757: INFO: Breadth first check of 172.30.54.252 on host 10.123.244.41...
Mar  2 01:25:03.764: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.88.195:9080/dial?request=hostname&protocol=http&host=172.30.54.252&port=8083&tries=1'] Namespace:pod-network-test-3442 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:25:03.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:25:03.765: INFO: ExecWithOptions: Clientset creation
Mar  2 01:25:03.766: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3442/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.88.195%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.54.252%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 01:25:04.072: INFO: Waiting for responses: map[]
Mar  2 01:25:04.072: INFO: reached 172.30.54.252 after 0/1 tries
Mar  2 01:25:04.072: INFO: Breadth first check of 172.30.0.208 on host 10.123.244.50...
Mar  2 01:25:04.086: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.88.195:9080/dial?request=hostname&protocol=http&host=172.30.0.208&port=8083&tries=1'] Namespace:pod-network-test-3442 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:25:04.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:25:04.088: INFO: ExecWithOptions: Clientset creation
Mar  2 01:25:04.088: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3442/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.88.195%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.0.208%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 01:25:04.244: INFO: Waiting for responses: map[]
Mar  2 01:25:04.244: INFO: reached 172.30.0.208 after 0/1 tries
Mar  2 01:25:04.244: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Mar  2 01:25:04.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3442" for this suite.

• [SLOW TEST:25.147 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":356,"completed":132,"skipped":2772,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:25:04.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Mar  2 01:25:04.454: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:25:06.469: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar  2 01:25:07.548: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Mar  2 01:25:08.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3535" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":356,"completed":133,"skipped":2798,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:25:08.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pods
Mar  2 01:25:08.874: INFO: created test-pod-1
Mar  2 01:25:08.913: INFO: created test-pod-2
Mar  2 01:25:08.949: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running
Mar  2 01:25:08.949: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7151' to be running and ready
Mar  2 01:25:09.046: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 01:25:09.046: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 01:25:09.046: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 01:25:09.046: INFO: 0 / 3 pods in namespace 'pods-7151' are running and ready (0 seconds elapsed)
Mar  2 01:25:09.046: INFO: expected 0 pod replicas in namespace 'pods-7151', 0 are Running and Ready.
Mar  2 01:25:09.046: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
Mar  2 01:25:09.046: INFO: test-pod-1  10.123.244.39  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:08 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:08 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:08 +0000 UTC  }]
Mar  2 01:25:09.046: INFO: test-pod-2  10.123.244.39  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:08 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:08 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:08 +0000 UTC  }]
Mar  2 01:25:09.046: INFO: test-pod-3  10.123.244.39  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:08 +0000 UTC  }]
Mar  2 01:25:09.046: INFO: 
Mar  2 01:25:11.088: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 01:25:11.088: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 01:25:11.088: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar  2 01:25:11.088: INFO: 0 / 3 pods in namespace 'pods-7151' are running and ready (2 seconds elapsed)
Mar  2 01:25:11.088: INFO: expected 0 pod replicas in namespace 'pods-7151', 0 are Running and Ready.
Mar  2 01:25:11.088: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
Mar  2 01:25:11.088: INFO: test-pod-1  10.123.244.39  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:08 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:08 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:08 +0000 UTC  }]
Mar  2 01:25:11.089: INFO: test-pod-2  10.123.244.39  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:08 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:08 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:08 +0000 UTC  }]
Mar  2 01:25:11.089: INFO: test-pod-3  10.123.244.39  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:09 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 01:25:08 +0000 UTC  }]
Mar  2 01:25:11.089: INFO: 
Mar  2 01:25:13.075: INFO: 3 / 3 pods in namespace 'pods-7151' are running and ready (4 seconds elapsed)
Mar  2 01:25:13.075: INFO: expected 0 pod replicas in namespace 'pods-7151', 0 are Running and Ready.
STEP: waiting for all pods to be deleted
Mar  2 01:25:13.149: INFO: Pod quantity 3 is different from expected quantity 0
Mar  2 01:25:14.156: INFO: Pod quantity 3 is different from expected quantity 0
Mar  2 01:25:15.158: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Mar  2 01:25:16.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7151" for this suite.

• [SLOW TEST:7.539 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":356,"completed":134,"skipped":2852,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:25:16.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-8b727eb0-aa2f-478f-b327-c02a8f7b59b9
STEP: Creating a pod to test consume configMaps
W0302 01:25:16.348680      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:25:16.349: INFO: Waiting up to 5m0s for pod "pod-configmaps-5f409560-4536-487a-8371-cd01944925f9" in namespace "configmap-2818" to be "Succeeded or Failed"
Mar  2 01:25:16.362: INFO: Pod "pod-configmaps-5f409560-4536-487a-8371-cd01944925f9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.375535ms
Mar  2 01:25:18.370: INFO: Pod "pod-configmaps-5f409560-4536-487a-8371-cd01944925f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021238763s
Mar  2 01:25:20.378: INFO: Pod "pod-configmaps-5f409560-4536-487a-8371-cd01944925f9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028919652s
Mar  2 01:25:22.385: INFO: Pod "pod-configmaps-5f409560-4536-487a-8371-cd01944925f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035840784s
STEP: Saw pod success
Mar  2 01:25:22.385: INFO: Pod "pod-configmaps-5f409560-4536-487a-8371-cd01944925f9" satisfied condition "Succeeded or Failed"
Mar  2 01:25:22.392: INFO: Trying to get logs from node 10.123.244.39 pod pod-configmaps-5f409560-4536-487a-8371-cd01944925f9 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 01:25:22.466: INFO: Waiting for pod pod-configmaps-5f409560-4536-487a-8371-cd01944925f9 to disappear
Mar  2 01:25:22.473: INFO: Pod pod-configmaps-5f409560-4536-487a-8371-cd01944925f9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar  2 01:25:22.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2818" for this suite.

• [SLOW TEST:6.327 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":135,"skipped":2869,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:25:22.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Mar  2 01:25:24.734: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2867 PodName:var-expansion-1b08c2d7-09bb-47c3-ae73-6b6f4b96ea87 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:25:24.737: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:25:24.739: INFO: ExecWithOptions: Clientset creation
Mar  2 01:25:24.739: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-2867/pods/var-expansion-1b08c2d7-09bb-47c3-ae73-6b6f4b96ea87/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path
Mar  2 01:25:24.913: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2867 PodName:var-expansion-1b08c2d7-09bb-47c3-ae73-6b6f4b96ea87 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:25:24.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:25:24.916: INFO: ExecWithOptions: Clientset creation
Mar  2 01:25:24.916: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-2867/pods/var-expansion-1b08c2d7-09bb-47c3-ae73-6b6f4b96ea87/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value
Mar  2 01:25:25.638: INFO: Successfully updated pod "var-expansion-1b08c2d7-09bb-47c3-ae73-6b6f4b96ea87"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Mar  2 01:25:25.644: INFO: Deleting pod "var-expansion-1b08c2d7-09bb-47c3-ae73-6b6f4b96ea87" in namespace "var-expansion-2867"
Mar  2 01:25:25.656: INFO: Wait up to 5m0s for pod "var-expansion-1b08c2d7-09bb-47c3-ae73-6b6f4b96ea87" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Mar  2 01:25:59.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2867" for this suite.

• [SLOW TEST:37.206 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":356,"completed":136,"skipped":2871,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:25:59.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:25:59.926: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-47b17344-1413-4cfa-abc2-82ced632f27b
STEP: Creating the pod
Mar  2 01:26:00.104: INFO: The status of Pod pod-configmaps-0063b0e7-d784-4634-89d8-f22390f4828e is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:26:02.116: INFO: The status of Pod pod-configmaps-0063b0e7-d784-4634-89d8-f22390f4828e is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-47b17344-1413-4cfa-abc2-82ced632f27b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar  2 01:26:04.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6955" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":137,"skipped":2879,"failed":0}
SSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:26:04.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
W0302 01:26:04.352872      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "terminate-cmd-rpa" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "terminate-cmd-rpa" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "terminate-cmd-rpa" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "terminate-cmd-rpa" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Mar  2 01:26:32.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1081" for this suite.

• [SLOW TEST:28.680 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":356,"completed":138,"skipped":2882,"failed":0}
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:26:32.921: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:26:33.085: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-ad62f28f-fa56-41e8-9ca7-bb4c344bcccb" in namespace "security-context-test-4983" to be "Succeeded or Failed"
Mar  2 01:26:33.092: INFO: Pod "busybox-readonly-false-ad62f28f-fa56-41e8-9ca7-bb4c344bcccb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.448726ms
Mar  2 01:26:35.112: INFO: Pod "busybox-readonly-false-ad62f28f-fa56-41e8-9ca7-bb4c344bcccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026835846s
Mar  2 01:26:37.119: INFO: Pod "busybox-readonly-false-ad62f28f-fa56-41e8-9ca7-bb4c344bcccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034117416s
Mar  2 01:26:37.119: INFO: Pod "busybox-readonly-false-ad62f28f-fa56-41e8-9ca7-bb4c344bcccb" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Mar  2 01:26:37.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4983" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":356,"completed":139,"skipped":2889,"failed":0}
SSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:26:37.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override all
Mar  2 01:26:37.252: INFO: Waiting up to 5m0s for pod "client-containers-b001a017-623d-4a22-98bf-155d8309c277" in namespace "containers-9259" to be "Succeeded or Failed"
Mar  2 01:26:37.257: INFO: Pod "client-containers-b001a017-623d-4a22-98bf-155d8309c277": Phase="Pending", Reason="", readiness=false. Elapsed: 5.305436ms
Mar  2 01:26:39.269: INFO: Pod "client-containers-b001a017-623d-4a22-98bf-155d8309c277": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017742784s
Mar  2 01:26:41.278: INFO: Pod "client-containers-b001a017-623d-4a22-98bf-155d8309c277": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026436176s
STEP: Saw pod success
Mar  2 01:26:41.278: INFO: Pod "client-containers-b001a017-623d-4a22-98bf-155d8309c277" satisfied condition "Succeeded or Failed"
Mar  2 01:26:41.283: INFO: Trying to get logs from node 10.123.244.39 pod client-containers-b001a017-623d-4a22-98bf-155d8309c277 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 01:26:41.323: INFO: Waiting for pod client-containers-b001a017-623d-4a22-98bf-155d8309c277 to disappear
Mar  2 01:26:41.329: INFO: Pod client-containers-b001a017-623d-4a22-98bf-155d8309c277 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Mar  2 01:26:41.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9259" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":356,"completed":140,"skipped":2893,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:26:41.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
W0302 01:26:41.463984      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:26:41.472: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  2 01:26:46.481: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Mar  2 01:26:46.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8004" for this suite.

• [SLOW TEST:5.213 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":356,"completed":141,"skipped":2902,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:26:46.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1142
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1142
STEP: Waiting until pod test-pod will start running in namespace statefulset-1142
STEP: Creating statefulset with conflicting port in namespace statefulset-1142
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1142
Mar  2 01:26:50.868: INFO: Observed stateful pod in namespace: statefulset-1142, name: ss-0, uid: 939721e5-8c8e-4105-bb7b-ef4d1af7d4b8, status phase: Pending. Waiting for statefulset controller to delete.
Mar  2 01:26:50.923: INFO: Observed stateful pod in namespace: statefulset-1142, name: ss-0, uid: 939721e5-8c8e-4105-bb7b-ef4d1af7d4b8, status phase: Failed. Waiting for statefulset controller to delete.
Mar  2 01:26:50.977: INFO: Observed stateful pod in namespace: statefulset-1142, name: ss-0, uid: 939721e5-8c8e-4105-bb7b-ef4d1af7d4b8, status phase: Failed. Waiting for statefulset controller to delete.
Mar  2 01:26:50.986: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1142
STEP: Removing pod with conflicting port in namespace statefulset-1142
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1142 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 01:26:57.082: INFO: Deleting all statefulset in ns statefulset-1142
Mar  2 01:26:57.088: INFO: Scaling statefulset ss to 0
Mar  2 01:27:07.119: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 01:27:07.126: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Mar  2 01:27:07.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1142" for this suite.

• [SLOW TEST:20.658 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":356,"completed":142,"skipped":2942,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:27:07.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating all guestbook components
Mar  2 01:27:07.355: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar  2 01:27:07.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-8072 create -f -'
Mar  2 01:27:08.290: INFO: stderr: ""
Mar  2 01:27:08.290: INFO: stdout: "service/agnhost-replica created\n"
Mar  2 01:27:08.291: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar  2 01:27:08.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-8072 create -f -'
Mar  2 01:27:09.882: INFO: stderr: ""
Mar  2 01:27:09.882: INFO: stdout: "service/agnhost-primary created\n"
Mar  2 01:27:09.882: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar  2 01:27:09.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-8072 create -f -'
Mar  2 01:27:10.846: INFO: stderr: ""
Mar  2 01:27:10.846: INFO: stdout: "service/frontend created\n"
Mar  2 01:27:10.847: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar  2 01:27:10.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-8072 create -f -'
Mar  2 01:27:11.951: INFO: stderr: ""
Mar  2 01:27:11.951: INFO: stdout: "deployment.apps/frontend created\n"
Mar  2 01:27:11.951: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  2 01:27:11.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-8072 create -f -'
Mar  2 01:27:12.829: INFO: stderr: ""
Mar  2 01:27:12.829: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar  2 01:27:12.830: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar  2 01:27:12.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-8072 create -f -'
Mar  2 01:27:13.986: INFO: stderr: ""
Mar  2 01:27:13.986: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Mar  2 01:27:13.986: INFO: Waiting for all frontend pods to be Running.
Mar  2 01:27:19.061: INFO: Waiting for frontend to serve content.
Mar  2 01:27:19.090: INFO: Trying to add a new entry to the guestbook.
Mar  2 01:27:19.113: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar  2 01:27:19.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-8072 delete --grace-period=0 --force -f -'
Mar  2 01:27:19.387: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 01:27:19.387: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 01:27:19.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-8072 delete --grace-period=0 --force -f -'
Mar  2 01:27:19.653: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 01:27:19.653: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 01:27:19.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-8072 delete --grace-period=0 --force -f -'
Mar  2 01:27:19.803: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 01:27:19.803: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 01:27:19.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-8072 delete --grace-period=0 --force -f -'
Mar  2 01:27:19.985: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 01:27:19.985: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 01:27:19.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-8072 delete --grace-period=0 --force -f -'
Mar  2 01:27:20.186: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 01:27:20.186: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar  2 01:27:20.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-8072 delete --grace-period=0 --force -f -'
Mar  2 01:27:20.317: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 01:27:20.317: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar  2 01:27:20.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8072" for this suite.

• [SLOW TEST:13.094 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:340
    should create and stop a working application  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":356,"completed":143,"skipped":2996,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:27:20.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar  2 01:27:20.441: INFO: Waiting up to 5m0s for pod "pod-38ee8d48-a13c-4a54-9a88-af9ea6af94a4" in namespace "emptydir-1341" to be "Succeeded or Failed"
Mar  2 01:27:20.446: INFO: Pod "pod-38ee8d48-a13c-4a54-9a88-af9ea6af94a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.642723ms
Mar  2 01:27:22.468: INFO: Pod "pod-38ee8d48-a13c-4a54-9a88-af9ea6af94a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026310372s
Mar  2 01:27:24.484: INFO: Pod "pod-38ee8d48-a13c-4a54-9a88-af9ea6af94a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042608804s
STEP: Saw pod success
Mar  2 01:27:24.484: INFO: Pod "pod-38ee8d48-a13c-4a54-9a88-af9ea6af94a4" satisfied condition "Succeeded or Failed"
Mar  2 01:27:24.492: INFO: Trying to get logs from node 10.123.244.39 pod pod-38ee8d48-a13c-4a54-9a88-af9ea6af94a4 container test-container: <nil>
STEP: delete the pod
Mar  2 01:27:24.529: INFO: Waiting for pod pod-38ee8d48-a13c-4a54-9a88-af9ea6af94a4 to disappear
Mar  2 01:27:24.538: INFO: Pod pod-38ee8d48-a13c-4a54-9a88-af9ea6af94a4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar  2 01:27:24.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1341" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":144,"skipped":3067,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:27:24.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0302 01:27:36.249815      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar  2 01:27:36.250: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Mar  2 01:27:36.250: INFO: Deleting pod "simpletest-rc-to-be-deleted-267pc" in namespace "gc-3624"
Mar  2 01:27:36.269: INFO: Deleting pod "simpletest-rc-to-be-deleted-279tw" in namespace "gc-3624"
Mar  2 01:27:36.289: INFO: Deleting pod "simpletest-rc-to-be-deleted-27gc6" in namespace "gc-3624"
Mar  2 01:27:36.305: INFO: Deleting pod "simpletest-rc-to-be-deleted-28mk8" in namespace "gc-3624"
Mar  2 01:27:36.321: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fsnt" in namespace "gc-3624"
Mar  2 01:27:36.345: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lz6r" in namespace "gc-3624"
Mar  2 01:27:36.382: INFO: Deleting pod "simpletest-rc-to-be-deleted-2w8l8" in namespace "gc-3624"
Mar  2 01:27:36.428: INFO: Deleting pod "simpletest-rc-to-be-deleted-4tgm4" in namespace "gc-3624"
Mar  2 01:27:36.454: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zmt8" in namespace "gc-3624"
Mar  2 01:27:36.470: INFO: Deleting pod "simpletest-rc-to-be-deleted-574ms" in namespace "gc-3624"
Mar  2 01:27:36.490: INFO: Deleting pod "simpletest-rc-to-be-deleted-5h5sx" in namespace "gc-3624"
Mar  2 01:27:36.509: INFO: Deleting pod "simpletest-rc-to-be-deleted-5m82k" in namespace "gc-3624"
Mar  2 01:27:36.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-6485j" in namespace "gc-3624"
Mar  2 01:27:36.543: INFO: Deleting pod "simpletest-rc-to-be-deleted-686wj" in namespace "gc-3624"
Mar  2 01:27:36.561: INFO: Deleting pod "simpletest-rc-to-be-deleted-6bmlj" in namespace "gc-3624"
Mar  2 01:27:36.605: INFO: Deleting pod "simpletest-rc-to-be-deleted-6bpf6" in namespace "gc-3624"
Mar  2 01:27:36.631: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gr7l" in namespace "gc-3624"
Mar  2 01:27:36.656: INFO: Deleting pod "simpletest-rc-to-be-deleted-6r4zd" in namespace "gc-3624"
Mar  2 01:27:36.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-6w2cl" in namespace "gc-3624"
Mar  2 01:27:36.780: INFO: Deleting pod "simpletest-rc-to-be-deleted-74hnn" in namespace "gc-3624"
Mar  2 01:27:36.871: INFO: Deleting pod "simpletest-rc-to-be-deleted-7npbn" in namespace "gc-3624"
Mar  2 01:27:36.890: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pgj7" in namespace "gc-3624"
Mar  2 01:27:36.917: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qv5k" in namespace "gc-3624"
Mar  2 01:27:36.943: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rwqr" in namespace "gc-3624"
Mar  2 01:27:36.968: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rxdw" in namespace "gc-3624"
Mar  2 01:27:36.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wsb2" in namespace "gc-3624"
Mar  2 01:27:37.009: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xw5l" in namespace "gc-3624"
Mar  2 01:27:37.037: INFO: Deleting pod "simpletest-rc-to-be-deleted-965l7" in namespace "gc-3624"
Mar  2 01:27:37.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7jhr" in namespace "gc-3624"
Mar  2 01:27:37.195: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9lcx" in namespace "gc-3624"
Mar  2 01:27:37.240: INFO: Deleting pod "simpletest-rc-to-be-deleted-bhxm9" in namespace "gc-3624"
Mar  2 01:27:37.272: INFO: Deleting pod "simpletest-rc-to-be-deleted-bjh8t" in namespace "gc-3624"
Mar  2 01:27:37.296: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmnbk" in namespace "gc-3624"
Mar  2 01:27:37.321: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnrkw" in namespace "gc-3624"
Mar  2 01:27:37.346: INFO: Deleting pod "simpletest-rc-to-be-deleted-c6fh5" in namespace "gc-3624"
Mar  2 01:27:37.387: INFO: Deleting pod "simpletest-rc-to-be-deleted-c987w" in namespace "gc-3624"
Mar  2 01:27:37.421: INFO: Deleting pod "simpletest-rc-to-be-deleted-ck445" in namespace "gc-3624"
Mar  2 01:27:37.445: INFO: Deleting pod "simpletest-rc-to-be-deleted-crwds" in namespace "gc-3624"
Mar  2 01:27:37.466: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2zp5" in namespace "gc-3624"
Mar  2 01:27:37.510: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6qss" in namespace "gc-3624"
Mar  2 01:27:37.656: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfdht" in namespace "gc-3624"
Mar  2 01:27:37.675: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjq8l" in namespace "gc-3624"
Mar  2 01:27:37.692: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftzjj" in namespace "gc-3624"
Mar  2 01:27:37.706: INFO: Deleting pod "simpletest-rc-to-be-deleted-gftfx" in namespace "gc-3624"
Mar  2 01:27:37.725: INFO: Deleting pod "simpletest-rc-to-be-deleted-hk2zz" in namespace "gc-3624"
Mar  2 01:27:37.743: INFO: Deleting pod "simpletest-rc-to-be-deleted-hkpgf" in namespace "gc-3624"
Mar  2 01:27:37.774: INFO: Deleting pod "simpletest-rc-to-be-deleted-hs7tf" in namespace "gc-3624"
Mar  2 01:27:37.794: INFO: Deleting pod "simpletest-rc-to-be-deleted-htvm2" in namespace "gc-3624"
Mar  2 01:27:37.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-hx2z5" in namespace "gc-3624"
Mar  2 01:27:37.844: INFO: Deleting pod "simpletest-rc-to-be-deleted-hz9c7" in namespace "gc-3624"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Mar  2 01:27:37.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3624" for this suite.

• [SLOW TEST:13.306 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":356,"completed":145,"skipped":3069,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:27:37.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-2569
STEP: creating service affinity-clusterip-transition in namespace services-2569
STEP: creating replication controller affinity-clusterip-transition in namespace services-2569
I0302 01:27:38.047631      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2569, replica count: 3
I0302 01:27:41.119253      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 01:27:44.121887      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 01:27:47.122992      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 01:27:50.124857      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 01:27:53.126028      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 01:27:53.163: INFO: Creating new exec pod
Mar  2 01:27:56.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-2569 exec execpod-affinity5tkwc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Mar  2 01:27:56.604: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar  2 01:27:56.604: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:27:56.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-2569 exec execpod-affinity5tkwc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.77.8 80'
Mar  2 01:27:56.971: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.77.8 80\nConnection to 172.21.77.8 80 port [tcp/http] succeeded!\n"
Mar  2 01:27:56.971: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:27:57.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-2569 exec execpod-affinity5tkwc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.77.8:80/ ; done'
Mar  2 01:27:57.645: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n"
Mar  2 01:27:57.645: INFO: stdout: "\naffinity-clusterip-transition-v6xtd\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-v6xtd\naffinity-clusterip-transition-v6xtd\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-mhsw4\naffinity-clusterip-transition-mhsw4\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-mhsw4\naffinity-clusterip-transition-v6xtd\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-v6xtd\naffinity-clusterip-transition-mhsw4\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-tvw5b"
Mar  2 01:27:57.645: INFO: Received response from host: affinity-clusterip-transition-v6xtd
Mar  2 01:27:57.645: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:57.645: INFO: Received response from host: affinity-clusterip-transition-v6xtd
Mar  2 01:27:57.645: INFO: Received response from host: affinity-clusterip-transition-v6xtd
Mar  2 01:27:57.645: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:57.645: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:57.645: INFO: Received response from host: affinity-clusterip-transition-mhsw4
Mar  2 01:27:57.645: INFO: Received response from host: affinity-clusterip-transition-mhsw4
Mar  2 01:27:57.645: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:57.645: INFO: Received response from host: affinity-clusterip-transition-mhsw4
Mar  2 01:27:57.645: INFO: Received response from host: affinity-clusterip-transition-v6xtd
Mar  2 01:27:57.645: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:57.645: INFO: Received response from host: affinity-clusterip-transition-v6xtd
Mar  2 01:27:57.645: INFO: Received response from host: affinity-clusterip-transition-mhsw4
Mar  2 01:27:57.645: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:57.645: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:57.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-2569 exec execpod-affinity5tkwc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.77.8:80/ ; done'
Mar  2 01:27:58.259: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.77.8:80/\n"
Mar  2 01:27:58.259: INFO: stdout: "\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-tvw5b\naffinity-clusterip-transition-tvw5b"
Mar  2 01:27:58.259: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:58.259: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:58.259: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:58.259: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:58.259: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:58.259: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:58.259: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:58.259: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:58.259: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:58.259: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:58.259: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:58.259: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:58.259: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:58.259: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:58.259: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:58.259: INFO: Received response from host: affinity-clusterip-transition-tvw5b
Mar  2 01:27:58.259: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2569, will wait for the garbage collector to delete the pods
Mar  2 01:27:58.360: INFO: Deleting ReplicationController affinity-clusterip-transition took: 16.326054ms
Mar  2 01:27:58.561: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 201.240389ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar  2 01:28:01.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2569" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:23.798 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":146,"skipped":3122,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:28:01.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-81c6f1a0-3606-4a86-b1ce-1678db77369a
STEP: Creating a pod to test consume configMaps
Mar  2 01:28:01.857: INFO: Waiting up to 5m0s for pod "pod-configmaps-23b64b04-d2a9-4c1d-913e-b827bde08758" in namespace "configmap-6154" to be "Succeeded or Failed"
Mar  2 01:28:01.868: INFO: Pod "pod-configmaps-23b64b04-d2a9-4c1d-913e-b827bde08758": Phase="Pending", Reason="", readiness=false. Elapsed: 11.25802ms
Mar  2 01:28:03.876: INFO: Pod "pod-configmaps-23b64b04-d2a9-4c1d-913e-b827bde08758": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019623162s
Mar  2 01:28:05.886: INFO: Pod "pod-configmaps-23b64b04-d2a9-4c1d-913e-b827bde08758": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029622144s
Mar  2 01:28:07.897: INFO: Pod "pod-configmaps-23b64b04-d2a9-4c1d-913e-b827bde08758": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04026417s
STEP: Saw pod success
Mar  2 01:28:07.897: INFO: Pod "pod-configmaps-23b64b04-d2a9-4c1d-913e-b827bde08758" satisfied condition "Succeeded or Failed"
Mar  2 01:28:07.923: INFO: Trying to get logs from node 10.123.244.39 pod pod-configmaps-23b64b04-d2a9-4c1d-913e-b827bde08758 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 01:28:07.977: INFO: Waiting for pod pod-configmaps-23b64b04-d2a9-4c1d-913e-b827bde08758 to disappear
Mar  2 01:28:07.986: INFO: Pod pod-configmaps-23b64b04-d2a9-4c1d-913e-b827bde08758 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar  2 01:28:07.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6154" for this suite.

• [SLOW TEST:6.298 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":147,"skipped":3175,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:28:08.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:28:08.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Mar  2 01:28:30.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-6874 --namespace=crd-publish-openapi-6874 create -f -'
Mar  2 01:28:33.926: INFO: stderr: ""
Mar  2 01:28:33.926: INFO: stdout: "e2e-test-crd-publish-openapi-5790-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  2 01:28:33.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-6874 --namespace=crd-publish-openapi-6874 delete e2e-test-crd-publish-openapi-5790-crds test-cr'
Mar  2 01:28:34.178: INFO: stderr: ""
Mar  2 01:28:34.178: INFO: stdout: "e2e-test-crd-publish-openapi-5790-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar  2 01:28:34.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-6874 --namespace=crd-publish-openapi-6874 apply -f -'
Mar  2 01:28:37.153: INFO: stderr: ""
Mar  2 01:28:37.153: INFO: stdout: "e2e-test-crd-publish-openapi-5790-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar  2 01:28:37.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-6874 --namespace=crd-publish-openapi-6874 delete e2e-test-crd-publish-openapi-5790-crds test-cr'
Mar  2 01:28:37.589: INFO: stderr: ""
Mar  2 01:28:37.589: INFO: stdout: "e2e-test-crd-publish-openapi-5790-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar  2 01:28:37.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-6874 explain e2e-test-crd-publish-openapi-5790-crds'
Mar  2 01:28:38.382: INFO: stderr: ""
Mar  2 01:28:38.382: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5790-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 01:28:59.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6874" for this suite.

• [SLOW TEST:51.449 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":356,"completed":148,"skipped":3180,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:28:59.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-1889
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar  2 01:28:59.819: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar  2 01:28:59.970: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:29:02.021: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:29:03.980: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:29:05.979: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:29:07.990: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:29:09.980: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:29:11.979: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:29:13.981: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:29:15.981: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:29:17.981: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:29:19.992: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar  2 01:29:21.980: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar  2 01:29:21.995: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar  2 01:29:22.040: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar  2 01:29:24.125: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar  2 01:29:24.125: INFO: Breadth first check of 172.30.88.222 on host 10.123.244.39...
Mar  2 01:29:24.134: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.88.212:9080/dial?request=hostname&protocol=udp&host=172.30.88.222&port=8081&tries=1'] Namespace:pod-network-test-1889 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:29:24.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:29:24.135: INFO: ExecWithOptions: Clientset creation
Mar  2 01:29:24.135: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-1889/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.88.212%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.88.222%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 01:29:24.312: INFO: Waiting for responses: map[]
Mar  2 01:29:24.312: INFO: reached 172.30.88.222 after 0/1 tries
Mar  2 01:29:24.312: INFO: Breadth first check of 172.30.54.249 on host 10.123.244.41...
Mar  2 01:29:24.326: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.88.212:9080/dial?request=hostname&protocol=udp&host=172.30.54.249&port=8081&tries=1'] Namespace:pod-network-test-1889 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:29:24.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:29:24.327: INFO: ExecWithOptions: Clientset creation
Mar  2 01:29:24.327: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-1889/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.88.212%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.54.249%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 01:29:24.482: INFO: Waiting for responses: map[]
Mar  2 01:29:24.483: INFO: reached 172.30.54.249 after 0/1 tries
Mar  2 01:29:24.483: INFO: Breadth first check of 172.30.0.228 on host 10.123.244.50...
Mar  2 01:29:24.491: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.88.212:9080/dial?request=hostname&protocol=udp&host=172.30.0.228&port=8081&tries=1'] Namespace:pod-network-test-1889 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:29:24.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:29:24.494: INFO: ExecWithOptions: Clientset creation
Mar  2 01:29:24.494: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-1889/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.88.212%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.0.228%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar  2 01:29:24.758: INFO: Waiting for responses: map[]
Mar  2 01:29:24.758: INFO: reached 172.30.0.228 after 0/1 tries
Mar  2 01:29:24.758: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Mar  2 01:29:24.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1889" for this suite.

• [SLOW TEST:25.400 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":356,"completed":149,"skipped":3183,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:29:24.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar  2 01:29:25.107: INFO: Waiting up to 5m0s for pod "downwardapi-volume-49a7b43c-2ae2-4e04-9b32-67eedc8d0265" in namespace "downward-api-4497" to be "Succeeded or Failed"
Mar  2 01:29:25.115: INFO: Pod "downwardapi-volume-49a7b43c-2ae2-4e04-9b32-67eedc8d0265": Phase="Pending", Reason="", readiness=false. Elapsed: 7.894032ms
Mar  2 01:29:27.124: INFO: Pod "downwardapi-volume-49a7b43c-2ae2-4e04-9b32-67eedc8d0265": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01659726s
Mar  2 01:29:29.160: INFO: Pod "downwardapi-volume-49a7b43c-2ae2-4e04-9b32-67eedc8d0265": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052830638s
STEP: Saw pod success
Mar  2 01:29:29.160: INFO: Pod "downwardapi-volume-49a7b43c-2ae2-4e04-9b32-67eedc8d0265" satisfied condition "Succeeded or Failed"
Mar  2 01:29:29.168: INFO: Trying to get logs from node 10.123.244.39 pod downwardapi-volume-49a7b43c-2ae2-4e04-9b32-67eedc8d0265 container client-container: <nil>
STEP: delete the pod
Mar  2 01:29:29.208: INFO: Waiting for pod downwardapi-volume-49a7b43c-2ae2-4e04-9b32-67eedc8d0265 to disappear
Mar  2 01:29:29.215: INFO: Pod downwardapi-volume-49a7b43c-2ae2-4e04-9b32-67eedc8d0265 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar  2 01:29:29.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4497" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":356,"completed":150,"skipped":3206,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:29:29.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 01:29:31.011: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 01:29:34.072: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 01:29:34.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-611" for this suite.
STEP: Destroying namespace "webhook-611-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.198 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":356,"completed":151,"skipped":3220,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:29:34.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
W0302 01:29:34.620265      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:29:34.629: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar  2 01:29:39.653: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  2 01:29:39.653: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar  2 01:29:41.669: INFO: Creating deployment "test-rollover-deployment"
Mar  2 01:29:41.691: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar  2 01:29:43.713: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar  2 01:29:43.748: INFO: Ensure that both replica sets have 1 created replica
Mar  2 01:29:43.779: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar  2 01:29:43.801: INFO: Updating deployment test-rollover-deployment
Mar  2 01:29:43.802: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar  2 01:29:45.821: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar  2 01:29:45.873: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar  2 01:29:45.896: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 01:29:45.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 29, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 29, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 29, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 29, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:29:47.953: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 01:29:47.954: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 29, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 29, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 29, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 29, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:29:49.944: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 01:29:49.944: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 29, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 29, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 29, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 29, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:29:51.921: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 01:29:51.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 29, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 29, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 29, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 29, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:29:53.922: INFO: all replica sets need to contain the pod-template-hash label
Mar  2 01:29:53.923: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 29, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 29, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 29, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 29, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 01:29:55.924: INFO: 
Mar  2 01:29:55.924: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 01:29:55.969: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3052  da64458a-0bc3-43c6-ba72-2638b6d86299 109888 2 2023-03-02 01:29:41 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-03-02 01:29:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:29:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ea65c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-02 01:29:41 +0000 UTC,LastTransitionTime:2023-03-02 01:29:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-779c67f4f8" has successfully progressed.,LastUpdateTime:2023-03-02 01:29:55 +0000 UTC,LastTransitionTime:2023-03-02 01:29:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 01:29:55.983: INFO: New ReplicaSet "test-rollover-deployment-779c67f4f8" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-779c67f4f8  deployment-3052  9374a241-9065-4da8-8671-3587afababf8 109877 2 2023-03-02 01:29:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment da64458a-0bc3-43c6-ba72-2638b6d86299 0xc004ea7667 0xc004ea7668}] []  [{kube-controller-manager Update apps/v1 2023-03-02 01:29:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da64458a-0bc3-43c6-ba72-2638b6d86299\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:29:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 779c67f4f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ea7858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:29:55.983: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar  2 01:29:55.984: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3052  c6418102-ffb3-4937-8792-b043e52feae2 109887 2 2023-03-02 01:29:34 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment da64458a-0bc3-43c6-ba72-2638b6d86299 0xc004ea7287 0xc004ea7288}] []  [{e2e.test Update apps/v1 2023-03-02 01:29:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:29:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da64458a-0bc3-43c6-ba72-2638b6d86299\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:29:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004ea74f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:29:55.984: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-87f8f6dcf  deployment-3052  0841c2c4-a9a1-4f79-a822-6846baee31f7 109794 2 2023-03-02 01:29:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment da64458a-0bc3-43c6-ba72-2638b6d86299 0xc004ea79d0 0xc004ea79d1}] []  [{kube-controller-manager Update apps/v1 2023-03-02 01:29:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da64458a-0bc3-43c6-ba72-2638b6d86299\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:29:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 87f8f6dcf,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ea7b58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:29:55.993: INFO: Pod "test-rollover-deployment-779c67f4f8-5n88z" is available:
&Pod{ObjectMeta:{test-rollover-deployment-779c67f4f8-5n88z test-rollover-deployment-779c67f4f8- deployment-3052  ae313f92-7fb1-421f-8d64-6c9d6c54184e 109820 0 2023-03-02 01:29:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[cni.projectcalico.org/containerID:676bf39745faa3ed90f3a6ba1a90f88727b59795ece4ffb0aa31950645e4691d cni.projectcalico.org/podIP:172.30.88.227/32 cni.projectcalico.org/podIPs:172.30.88.227/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.227"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.227"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-779c67f4f8 9374a241-9065-4da8-8671-3587afababf8 0xc004e84cf7 0xc004e84cf8}] []  [{kube-controller-manager Update v1 2023-03-02 01:29:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9374a241-9065-4da8-8671-3587afababf8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-03-02 01:29:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:29:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:29:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.88.227\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s76rz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s76rz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.39,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c50,c20,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fdglw,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:29:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:29:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.39,PodIP:172.30.88.227,StartTime:2023-03-02 01:29:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:29:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:cri-o://cb9e557a0db7afb68fb397fcadb01fb8f589f339bcdcb232834b7d2b2dd7213c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.88.227,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Mar  2 01:29:55.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3052" for this suite.

• [SLOW TEST:21.573 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":356,"completed":152,"skipped":3265,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:29:56.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-downwardapi-bjb6
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 01:29:56.310: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-bjb6" in namespace "subpath-1941" to be "Succeeded or Failed"
Mar  2 01:29:56.317: INFO: Pod "pod-subpath-test-downwardapi-bjb6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.973798ms
Mar  2 01:29:58.328: INFO: Pod "pod-subpath-test-downwardapi-bjb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018654629s
Mar  2 01:30:00.341: INFO: Pod "pod-subpath-test-downwardapi-bjb6": Phase="Running", Reason="", readiness=true. Elapsed: 4.031329577s
Mar  2 01:30:02.352: INFO: Pod "pod-subpath-test-downwardapi-bjb6": Phase="Running", Reason="", readiness=true. Elapsed: 6.042569891s
Mar  2 01:30:04.361: INFO: Pod "pod-subpath-test-downwardapi-bjb6": Phase="Running", Reason="", readiness=true. Elapsed: 8.051201159s
Mar  2 01:30:06.370: INFO: Pod "pod-subpath-test-downwardapi-bjb6": Phase="Running", Reason="", readiness=true. Elapsed: 10.059834235s
Mar  2 01:30:08.379: INFO: Pod "pod-subpath-test-downwardapi-bjb6": Phase="Running", Reason="", readiness=true. Elapsed: 12.06967338s
Mar  2 01:30:10.389: INFO: Pod "pod-subpath-test-downwardapi-bjb6": Phase="Running", Reason="", readiness=true. Elapsed: 14.0789339s
Mar  2 01:30:12.400: INFO: Pod "pod-subpath-test-downwardapi-bjb6": Phase="Running", Reason="", readiness=true. Elapsed: 16.090123866s
Mar  2 01:30:14.410: INFO: Pod "pod-subpath-test-downwardapi-bjb6": Phase="Running", Reason="", readiness=true. Elapsed: 18.100093649s
Mar  2 01:30:16.419: INFO: Pod "pod-subpath-test-downwardapi-bjb6": Phase="Running", Reason="", readiness=true. Elapsed: 20.109346824s
Mar  2 01:30:18.432: INFO: Pod "pod-subpath-test-downwardapi-bjb6": Phase="Running", Reason="", readiness=false. Elapsed: 22.121712093s
Mar  2 01:30:20.450: INFO: Pod "pod-subpath-test-downwardapi-bjb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.140508186s
STEP: Saw pod success
Mar  2 01:30:20.451: INFO: Pod "pod-subpath-test-downwardapi-bjb6" satisfied condition "Succeeded or Failed"
Mar  2 01:30:20.461: INFO: Trying to get logs from node 10.123.244.39 pod pod-subpath-test-downwardapi-bjb6 container test-container-subpath-downwardapi-bjb6: <nil>
STEP: delete the pod
Mar  2 01:30:20.630: INFO: Waiting for pod pod-subpath-test-downwardapi-bjb6 to disappear
Mar  2 01:30:20.640: INFO: Pod pod-subpath-test-downwardapi-bjb6 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-bjb6
Mar  2 01:30:20.640: INFO: Deleting pod "pod-subpath-test-downwardapi-bjb6" in namespace "subpath-1941"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Mar  2 01:30:20.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1941" for this suite.

• [SLOW TEST:24.639 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","total":356,"completed":153,"skipped":3323,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:30:20.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:30:20.882: INFO: The status of Pod pod-secrets-1bc318e0-0ca1-4878-8a5f-6c21b6c4782a is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:30:22.896: INFO: The status of Pod pod-secrets-1bc318e0-0ca1-4878-8a5f-6c21b6c4782a is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:30:24.893: INFO: The status of Pod pod-secrets-1bc318e0-0ca1-4878-8a5f-6c21b6c4782a is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Mar  2 01:30:24.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9470" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":356,"completed":154,"skipped":3357,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:30:25.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar  2 01:30:25.208: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4397  fc7164c9-0ec2-4f46-87bc-412b90416f51 110261 0 2023-03-02 01:30:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-03-02 01:30:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 01:30:25.209: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4397  fc7164c9-0ec2-4f46-87bc-412b90416f51 110261 0 2023-03-02 01:30:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-03-02 01:30:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar  2 01:30:25.265: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4397  fc7164c9-0ec2-4f46-87bc-412b90416f51 110266 0 2023-03-02 01:30:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-03-02 01:30:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 01:30:25.265: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4397  fc7164c9-0ec2-4f46-87bc-412b90416f51 110266 0 2023-03-02 01:30:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-03-02 01:30:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar  2 01:30:25.297: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4397  fc7164c9-0ec2-4f46-87bc-412b90416f51 110268 0 2023-03-02 01:30:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-03-02 01:30:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 01:30:25.297: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4397  fc7164c9-0ec2-4f46-87bc-412b90416f51 110268 0 2023-03-02 01:30:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-03-02 01:30:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar  2 01:30:25.317: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4397  fc7164c9-0ec2-4f46-87bc-412b90416f51 110270 0 2023-03-02 01:30:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-03-02 01:30:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 01:30:25.317: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4397  fc7164c9-0ec2-4f46-87bc-412b90416f51 110270 0 2023-03-02 01:30:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-03-02 01:30:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar  2 01:30:25.331: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4397  4f87667a-1d7c-4d23-9abf-c21286364208 110271 0 2023-03-02 01:30:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-03-02 01:30:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 01:30:25.331: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4397  4f87667a-1d7c-4d23-9abf-c21286364208 110271 0 2023-03-02 01:30:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-03-02 01:30:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar  2 01:30:35.355: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4397  4f87667a-1d7c-4d23-9abf-c21286364208 110370 0 2023-03-02 01:30:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-03-02 01:30:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 01:30:35.356: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4397  4f87667a-1d7c-4d23-9abf-c21286364208 110370 0 2023-03-02 01:30:25 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-03-02 01:30:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Mar  2 01:30:45.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4397" for this suite.

• [SLOW TEST:20.460 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":356,"completed":155,"skipped":3359,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity 
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:30:45.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename csistoragecapacity
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/storage.k8s.io
STEP: getting /apis/storage.k8s.io/v1
STEP: creating
STEP: watching
Mar  2 01:30:45.775: INFO: starting watch
STEP: getting
STEP: listing in namespace
STEP: listing across namespaces
STEP: patching
STEP: updating
Mar  2 01:30:45.861: INFO: waiting for watch events with expected annotations in namespace
Mar  2 01:30:45.862: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:188
Mar  2 01:30:45.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-5787" for this suite.
•{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","total":356,"completed":156,"skipped":3372,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:30:45.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-3553ee85-7ed4-4bb9-91e3-fc17b56e40a8
STEP: Creating a pod to test consume secrets
Mar  2 01:30:46.127: INFO: Waiting up to 5m0s for pod "pod-secrets-78a22968-ea18-4eb5-81cd-257ef5e5498e" in namespace "secrets-9204" to be "Succeeded or Failed"
Mar  2 01:30:46.134: INFO: Pod "pod-secrets-78a22968-ea18-4eb5-81cd-257ef5e5498e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.70319ms
Mar  2 01:30:48.146: INFO: Pod "pod-secrets-78a22968-ea18-4eb5-81cd-257ef5e5498e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018310503s
Mar  2 01:30:50.193: INFO: Pod "pod-secrets-78a22968-ea18-4eb5-81cd-257ef5e5498e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06571907s
STEP: Saw pod success
Mar  2 01:30:50.193: INFO: Pod "pod-secrets-78a22968-ea18-4eb5-81cd-257ef5e5498e" satisfied condition "Succeeded or Failed"
Mar  2 01:30:50.205: INFO: Trying to get logs from node 10.123.244.39 pod pod-secrets-78a22968-ea18-4eb5-81cd-257ef5e5498e container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 01:30:50.285: INFO: Waiting for pod pod-secrets-78a22968-ea18-4eb5-81cd-257ef5e5498e to disappear
Mar  2 01:30:50.319: INFO: Pod pod-secrets-78a22968-ea18-4eb5-81cd-257ef5e5498e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Mar  2 01:30:50.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9204" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":157,"skipped":3406,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:30:50.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
W0302 01:30:50.463146      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:30:50.470: INFO: The status of Pod labelsupdate464245c2-66df-4287-8ee8-903cb0aa5d94 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:30:52.503: INFO: The status of Pod labelsupdate464245c2-66df-4287-8ee8-903cb0aa5d94 is Running (Ready = true)
Mar  2 01:30:53.103: INFO: Successfully updated pod "labelsupdate464245c2-66df-4287-8ee8-903cb0aa5d94"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar  2 01:30:57.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9077" for this suite.

• [SLOW TEST:6.918 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":158,"skipped":3451,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:30:57.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar  2 01:30:57.386: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c94d2b8-d61b-4d0f-804c-2f841c7ab5bb" in namespace "downward-api-3251" to be "Succeeded or Failed"
Mar  2 01:30:57.394: INFO: Pod "downwardapi-volume-0c94d2b8-d61b-4d0f-804c-2f841c7ab5bb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.371031ms
Mar  2 01:30:59.404: INFO: Pod "downwardapi-volume-0c94d2b8-d61b-4d0f-804c-2f841c7ab5bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017436822s
Mar  2 01:31:01.413: INFO: Pod "downwardapi-volume-0c94d2b8-d61b-4d0f-804c-2f841c7ab5bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027347205s
STEP: Saw pod success
Mar  2 01:31:01.414: INFO: Pod "downwardapi-volume-0c94d2b8-d61b-4d0f-804c-2f841c7ab5bb" satisfied condition "Succeeded or Failed"
Mar  2 01:31:01.420: INFO: Trying to get logs from node 10.123.244.39 pod downwardapi-volume-0c94d2b8-d61b-4d0f-804c-2f841c7ab5bb container client-container: <nil>
STEP: delete the pod
Mar  2 01:31:01.487: INFO: Waiting for pod downwardapi-volume-0c94d2b8-d61b-4d0f-804c-2f841c7ab5bb to disappear
Mar  2 01:31:01.495: INFO: Pod downwardapi-volume-0c94d2b8-d61b-4d0f-804c-2f841c7ab5bb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar  2 01:31:01.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3251" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":159,"skipped":3491,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:31:01.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:31:01.740: INFO: Waiting up to 5m0s for pod "busybox-user-65534-77cc27cf-65f3-4129-9d28-c8d86d2afb79" in namespace "security-context-test-8774" to be "Succeeded or Failed"
Mar  2 01:31:01.759: INFO: Pod "busybox-user-65534-77cc27cf-65f3-4129-9d28-c8d86d2afb79": Phase="Pending", Reason="", readiness=false. Elapsed: 19.607332ms
Mar  2 01:31:03.791: INFO: Pod "busybox-user-65534-77cc27cf-65f3-4129-9d28-c8d86d2afb79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05099523s
Mar  2 01:31:05.805: INFO: Pod "busybox-user-65534-77cc27cf-65f3-4129-9d28-c8d86d2afb79": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06465451s
Mar  2 01:31:07.813: INFO: Pod "busybox-user-65534-77cc27cf-65f3-4129-9d28-c8d86d2afb79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073177748s
Mar  2 01:31:07.813: INFO: Pod "busybox-user-65534-77cc27cf-65f3-4129-9d28-c8d86d2afb79" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Mar  2 01:31:07.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8774" for this suite.

• [SLOW TEST:6.320 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:52
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":160,"skipped":3504,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:31:07.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 01:31:08.204: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 01:31:11.268: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:31:11.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5930-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 01:31:14.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6508" for this suite.
STEP: Destroying namespace "webhook-6508-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.150 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":356,"completed":161,"skipped":3547,"failed":0}
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:31:14.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar  2 01:31:15.647: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 01:31:18.773: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:31:18.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 01:31:22.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9264" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:7.840 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":356,"completed":162,"skipped":3547,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:31:22.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 01:31:24.571: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 01:31:27.762: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 01:31:28.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7484" for this suite.
STEP: Destroying namespace "webhook-7484-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.775 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":356,"completed":163,"skipped":3547,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:31:28.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar  2 01:31:45.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9930" for this suite.

• [SLOW TEST:16.532 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":356,"completed":164,"skipped":3596,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:31:45.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar  2 01:31:58.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6479" for this suite.

• [SLOW TEST:13.346 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":356,"completed":165,"skipped":3606,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:31:58.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
W0302 01:31:58.633907      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:31:58.643: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar  2 01:32:03.657: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  2 01:32:03.657: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 01:32:03.719: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6555  49d857b9-87a0-418d-8b4b-c8f53c74e522 111828 1 2023-03-02 01:32:03 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2023-03-02 01:32:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a3ac88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar  2 01:32:03.731: INFO: New ReplicaSet "test-cleanup-deployment-6755c7b765" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-6755c7b765  deployment-6555  db9c84f4-a500-46b1-a2dd-a5e900d12510 111830 1 2023-03-02 01:32:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:6755c7b765] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 49d857b9-87a0-418d-8b4b-c8f53c74e522 0xc002544397 0xc002544398}] []  [{kube-controller-manager Update apps/v1 2023-03-02 01:32:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49d857b9-87a0-418d-8b4b-c8f53c74e522\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 6755c7b765,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:6755c7b765] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002544428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:32:03.731: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar  2 01:32:03.731: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-6555  21dace80-5061-4b57-b4e1-23223a73f3a7 111829 1 2023-03-02 01:31:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 49d857b9-87a0-418d-8b4b-c8f53c74e522 0xc002544267 0xc002544268}] []  [{e2e.test Update apps/v1 2023-03-02 01:31:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:32:00 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-03-02 01:32:03 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"49d857b9-87a0-418d-8b4b-c8f53c74e522\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002544328 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:32:03.776: INFO: Pod "test-cleanup-controller-thzl4" is available:
&Pod{ObjectMeta:{test-cleanup-controller-thzl4 test-cleanup-controller- deployment-6555  4d3eecb6-8d14-428a-9433-551f0e465e69 111811 0 2023-03-02 01:31:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:0bd5ae4d49a64428c72d5be0791113275f443250b6f43b5c2951b84c8bfb748c cni.projectcalico.org/podIP:172.30.88.255/32 cni.projectcalico.org/podIPs:172.30.88.255/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.255"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.255"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller 21dace80-5061-4b57-b4e1-23223a73f3a7 0xc002544a37 0xc002544a38}] []  [{kube-controller-manager Update v1 2023-03-02 01:31:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"21dace80-5061-4b57-b4e1-23223a73f3a7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-03-02 01:31:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:31:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:32:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.88.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fkk9r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fkk9r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.39,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c51,c50,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:31:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:32:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:32:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:31:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.39,PodIP:172.30.88.255,StartTime:2023-03-02 01:31:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:31:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://39b10bb6b3263dddc8c9c806fc5e4bf45e6a283324cb64ad5730f889ae61bce4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.88.255,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Mar  2 01:32:03.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6555" for this suite.

• [SLOW TEST:5.362 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":356,"completed":166,"skipped":3618,"failed":0}
SSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:32:03.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Mar  2 01:32:06.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5314" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","total":356,"completed":167,"skipped":3621,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:32:06.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-b76a61ea-8606-4c5e-9068-62d4ef981093
STEP: Creating a pod to test consume configMaps
Mar  2 01:32:06.290: INFO: Waiting up to 5m0s for pod "pod-configmaps-817d9835-dfab-40e4-a5fd-bd9cd2ef91b2" in namespace "configmap-8410" to be "Succeeded or Failed"
Mar  2 01:32:06.297: INFO: Pod "pod-configmaps-817d9835-dfab-40e4-a5fd-bd9cd2ef91b2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.975569ms
Mar  2 01:32:08.306: INFO: Pod "pod-configmaps-817d9835-dfab-40e4-a5fd-bd9cd2ef91b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015471021s
Mar  2 01:32:10.313: INFO: Pod "pod-configmaps-817d9835-dfab-40e4-a5fd-bd9cd2ef91b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023272977s
Mar  2 01:32:12.322: INFO: Pod "pod-configmaps-817d9835-dfab-40e4-a5fd-bd9cd2ef91b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031471788s
STEP: Saw pod success
Mar  2 01:32:12.322: INFO: Pod "pod-configmaps-817d9835-dfab-40e4-a5fd-bd9cd2ef91b2" satisfied condition "Succeeded or Failed"
Mar  2 01:32:12.329: INFO: Trying to get logs from node 10.123.244.39 pod pod-configmaps-817d9835-dfab-40e4-a5fd-bd9cd2ef91b2 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 01:32:12.361: INFO: Waiting for pod pod-configmaps-817d9835-dfab-40e4-a5fd-bd9cd2ef91b2 to disappear
Mar  2 01:32:12.368: INFO: Pod pod-configmaps-817d9835-dfab-40e4-a5fd-bd9cd2ef91b2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar  2 01:32:12.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8410" for this suite.

• [SLOW TEST:6.318 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":168,"skipped":3626,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:32:12.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-7dc5e1f6-6a45-4ed1-a384-b4e28b9cd006
STEP: Creating a pod to test consume secrets
Mar  2 01:32:12.632: INFO: Waiting up to 5m0s for pod "pod-secrets-9be51e4a-f738-4205-bff3-0c80a935a273" in namespace "secrets-7669" to be "Succeeded or Failed"
Mar  2 01:32:12.640: INFO: Pod "pod-secrets-9be51e4a-f738-4205-bff3-0c80a935a273": Phase="Pending", Reason="", readiness=false. Elapsed: 8.153073ms
Mar  2 01:32:14.650: INFO: Pod "pod-secrets-9be51e4a-f738-4205-bff3-0c80a935a273": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018181457s
Mar  2 01:32:16.671: INFO: Pod "pod-secrets-9be51e4a-f738-4205-bff3-0c80a935a273": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039161772s
STEP: Saw pod success
Mar  2 01:32:16.671: INFO: Pod "pod-secrets-9be51e4a-f738-4205-bff3-0c80a935a273" satisfied condition "Succeeded or Failed"
Mar  2 01:32:16.680: INFO: Trying to get logs from node 10.123.244.39 pod pod-secrets-9be51e4a-f738-4205-bff3-0c80a935a273 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 01:32:16.720: INFO: Waiting for pod pod-secrets-9be51e4a-f738-4205-bff3-0c80a935a273 to disappear
Mar  2 01:32:16.746: INFO: Pod pod-secrets-9be51e4a-f738-4205-bff3-0c80a935a273 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Mar  2 01:32:16.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7669" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":169,"skipped":3646,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:32:16.843: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption is created
Mar  2 01:32:17.067: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:32:19.082: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Mar  2 01:32:20.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9640" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":356,"completed":170,"skipped":3652,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:32:20.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 01:32:20.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4692" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":356,"completed":171,"skipped":3654,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:32:20.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-ae3fd571-b98d-46f1-879d-12fd040ff6ca
STEP: Creating a pod to test consume secrets
Mar  2 01:32:20.499: INFO: Waiting up to 5m0s for pod "pod-secrets-88689b25-45e9-4fb1-a3c4-ca805b775f0c" in namespace "secrets-5254" to be "Succeeded or Failed"
Mar  2 01:32:20.506: INFO: Pod "pod-secrets-88689b25-45e9-4fb1-a3c4-ca805b775f0c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.032019ms
Mar  2 01:32:22.516: INFO: Pod "pod-secrets-88689b25-45e9-4fb1-a3c4-ca805b775f0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016955803s
Mar  2 01:32:24.527: INFO: Pod "pod-secrets-88689b25-45e9-4fb1-a3c4-ca805b775f0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02799857s
STEP: Saw pod success
Mar  2 01:32:24.527: INFO: Pod "pod-secrets-88689b25-45e9-4fb1-a3c4-ca805b775f0c" satisfied condition "Succeeded or Failed"
Mar  2 01:32:24.539: INFO: Trying to get logs from node 10.123.244.39 pod pod-secrets-88689b25-45e9-4fb1-a3c4-ca805b775f0c container secret-env-test: <nil>
STEP: delete the pod
Mar  2 01:32:24.589: INFO: Waiting for pod pod-secrets-88689b25-45e9-4fb1-a3c4-ca805b775f0c to disappear
Mar  2 01:32:24.596: INFO: Pod pod-secrets-88689b25-45e9-4fb1-a3c4-ca805b775f0c no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Mar  2 01:32:24.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5254" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":356,"completed":172,"skipped":3685,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:32:24.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:32:24.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar  2 01:32:24.767: INFO: The status of Pod pod-logs-websocket-ede15435-8fe0-4b1e-9df2-be22363468e3 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:32:26.776: INFO: The status of Pod pod-logs-websocket-ede15435-8fe0-4b1e-9df2-be22363468e3 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Mar  2 01:32:27.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6266" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":356,"completed":173,"skipped":3700,"failed":0}

------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:32:27.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Mar  2 01:32:27.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6506" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":356,"completed":174,"skipped":3700,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:32:27.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 01:32:28.241: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:32:28.241: INFO: Node 10.123.244.39 is running 0 daemon pod, expected 1
Mar  2 01:32:29.310: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:32:29.310: INFO: Node 10.123.244.39 is running 0 daemon pod, expected 1
Mar  2 01:32:30.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  2 01:32:30.268: INFO: Node 10.123.244.39 is running 0 daemon pod, expected 1
Mar  2 01:32:31.260: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 01:32:31.260: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Mar  2 01:32:31.324: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"112602"},"items":null}

Mar  2 01:32:31.333: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"112602"},"items":[{"metadata":{"name":"daemon-set-8h28t","generateName":"daemon-set-","namespace":"daemonsets-25","uid":"15125963-2f4a-4729-9d28-88a651965116","resourceVersion":"112582","creationTimestamp":"2023-03-02T01:32:28Z","labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"1502dd5f978d1c1be4850075ba0e01b5272291eea61256e132a3c18ce03466fd","cni.projectcalico.org/podIP":"172.30.88.251/32","cni.projectcalico.org/podIPs":"172.30.88.251/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.88.251\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.88.251\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"fc108825-1ceb-417d-a906-06a63c0f2e43","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-02T01:32:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc108825-1ceb-417d-a906-06a63c0f2e43\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-03-02T01:32:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-03-02T01:32:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-02T01:32:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.88.251\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-lz5mf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-lz5mf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.123.244.39","securityContext":{"seLinuxOptions":{"level":"s0:c52,c44"}},"imagePullSecrets":[{"name":"default-dockercfg-7hwfc"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.123.244.39"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T01:32:28Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T01:32:30Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T01:32:30Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T01:32:28Z"}],"hostIP":"10.123.244.39","podIP":"172.30.88.251","podIPs":[{"ip":"172.30.88.251"}],"startTime":"2023-03-02T01:32:28Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-02T01:32:29Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://2ab4583bdab75046a09d1a6979110685ef935db625cc9d59ab9d2ad9ebe30d25","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fwcf2","generateName":"daemon-set-","namespace":"daemonsets-25","uid":"12761587-b98c-44da-9a98-de18dc9c7690","resourceVersion":"112574","creationTimestamp":"2023-03-02T01:32:28Z","labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"af60f39993e1ec521d889cb66fbbb4ed210958f162bab3efdb3fa413f0e37e9d","cni.projectcalico.org/podIP":"172.30.54.206/32","cni.projectcalico.org/podIPs":"172.30.54.206/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.54.206\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.54.206\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"fc108825-1ceb-417d-a906-06a63c0f2e43","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-02T01:32:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc108825-1ceb-417d-a906-06a63c0f2e43\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-03-02T01:32:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-03-02T01:32:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-02T01:32:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.54.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-96wgf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-96wgf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.123.244.41","securityContext":{"seLinuxOptions":{"level":"s0:c52,c44"}},"imagePullSecrets":[{"name":"default-dockercfg-7hwfc"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.123.244.41"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T01:32:28Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T01:32:30Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T01:32:30Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T01:32:28Z"}],"hostIP":"10.123.244.41","podIP":"172.30.54.206","podIPs":[{"ip":"172.30.54.206"}],"startTime":"2023-03-02T01:32:28Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-02T01:32:29Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://8aa96f73ad98ae4851a52986336a5b92cb4e2c80758fa7b6a4a51aa7ed3416e8","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vfpf4","generateName":"daemon-set-","namespace":"daemonsets-25","uid":"07d116a9-f670-4090-aa0c-6b8a56915cee","resourceVersion":"112592","creationTimestamp":"2023-03-02T01:32:28Z","labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"fdc9c11fe71e9fd6917eeeef66863a064224e20afb0c3b98b2b6d69056fd6ab2","cni.projectcalico.org/podIP":"172.30.0.229/32","cni.projectcalico.org/podIPs":"172.30.0.229/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.0.229\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.0.229\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"fc108825-1ceb-417d-a906-06a63c0f2e43","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-02T01:32:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc108825-1ceb-417d-a906-06a63c0f2e43\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-03-02T01:32:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-03-02T01:32:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-02T01:32:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.0.229\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9c6vd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9c6vd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.123.244.50","securityContext":{"seLinuxOptions":{"level":"s0:c52,c44"}},"imagePullSecrets":[{"name":"default-dockercfg-7hwfc"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.123.244.50"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T01:32:28Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T01:32:30Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T01:32:30Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-02T01:32:28Z"}],"hostIP":"10.123.244.50","podIP":"172.30.0.229","podIPs":[{"ip":"172.30.0.229"}],"startTime":"2023-03-02T01:32:28Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-02T01:32:30Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://71f976ccd10bb92f871016d17d3499c3a46b0d597f0ed25bdcbce23aabd59686","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Mar  2 01:32:31.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-25" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":356,"completed":175,"skipped":3705,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:32:31.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar  2 01:32:31.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:32:51.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 01:34:03.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-795" for this suite.

• [SLOW TEST:92.546 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":356,"completed":176,"skipped":3712,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:34:03.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar  2 01:34:04.207: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9ed4b05-4c4b-4399-a2ee-2b3e14abb836" in namespace "downward-api-2257" to be "Succeeded or Failed"
Mar  2 01:34:04.219: INFO: Pod "downwardapi-volume-f9ed4b05-4c4b-4399-a2ee-2b3e14abb836": Phase="Pending", Reason="", readiness=false. Elapsed: 12.287818ms
Mar  2 01:34:06.236: INFO: Pod "downwardapi-volume-f9ed4b05-4c4b-4399-a2ee-2b3e14abb836": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02858984s
Mar  2 01:34:08.249: INFO: Pod "downwardapi-volume-f9ed4b05-4c4b-4399-a2ee-2b3e14abb836": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041423481s
STEP: Saw pod success
Mar  2 01:34:08.249: INFO: Pod "downwardapi-volume-f9ed4b05-4c4b-4399-a2ee-2b3e14abb836" satisfied condition "Succeeded or Failed"
Mar  2 01:34:08.262: INFO: Trying to get logs from node 10.123.244.39 pod downwardapi-volume-f9ed4b05-4c4b-4399-a2ee-2b3e14abb836 container client-container: <nil>
STEP: delete the pod
Mar  2 01:34:08.327: INFO: Waiting for pod downwardapi-volume-f9ed4b05-4c4b-4399-a2ee-2b3e14abb836 to disappear
Mar  2 01:34:08.346: INFO: Pod downwardapi-volume-f9ed4b05-4c4b-4399-a2ee-2b3e14abb836 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar  2 01:34:08.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2257" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":177,"skipped":3714,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:34:08.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service nodeport-test with type=NodePort in namespace services-5377
STEP: creating replication controller nodeport-test in namespace services-5377
I0302 01:34:08.588261      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-5377, replica count: 2
I0302 01:34:11.650991      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 01:34:11.651: INFO: Creating new exec pod
Mar  2 01:34:16.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-5377 exec execpodsglwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Mar  2 01:34:17.237: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar  2 01:34:17.237: INFO: stdout: "nodeport-test-89m82"
Mar  2 01:34:17.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-5377 exec execpodsglwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.109.255 80'
Mar  2 01:34:17.641: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.109.255 80\nConnection to 172.21.109.255 80 port [tcp/http] succeeded!\n"
Mar  2 01:34:17.641: INFO: stdout: ""
Mar  2 01:34:18.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-5377 exec execpodsglwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.109.255 80'
Mar  2 01:34:18.995: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.109.255 80\nConnection to 172.21.109.255 80 port [tcp/http] succeeded!\n"
Mar  2 01:34:18.995: INFO: stdout: ""
Mar  2 01:34:19.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-5377 exec execpodsglwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.109.255 80'
Mar  2 01:34:19.968: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.109.255 80\nConnection to 172.21.109.255 80 port [tcp/http] succeeded!\n"
Mar  2 01:34:19.969: INFO: stdout: ""
Mar  2 01:34:20.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-5377 exec execpodsglwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.109.255 80'
Mar  2 01:34:21.000: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.109.255 80\nConnection to 172.21.109.255 80 port [tcp/http] succeeded!\n"
Mar  2 01:34:21.000: INFO: stdout: "nodeport-test-62cph"
Mar  2 01:34:21.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-5377 exec execpodsglwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.123.244.50 32558'
Mar  2 01:34:21.377: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.123.244.50 32558\nConnection to 10.123.244.50 32558 port [tcp/*] succeeded!\n"
Mar  2 01:34:21.377: INFO: stdout: ""
Mar  2 01:34:22.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-5377 exec execpodsglwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.123.244.50 32558'
Mar  2 01:34:22.952: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.123.244.50 32558\nConnection to 10.123.244.50 32558 port [tcp/*] succeeded!\n"
Mar  2 01:34:22.952: INFO: stdout: ""
Mar  2 01:34:23.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-5377 exec execpodsglwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.123.244.50 32558'
Mar  2 01:34:23.697: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.123.244.50 32558\nConnection to 10.123.244.50 32558 port [tcp/*] succeeded!\n"
Mar  2 01:34:23.698: INFO: stdout: ""
Mar  2 01:34:24.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-5377 exec execpodsglwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.123.244.50 32558'
Mar  2 01:34:24.713: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.123.244.50 32558\nConnection to 10.123.244.50 32558 port [tcp/*] succeeded!\n"
Mar  2 01:34:24.713: INFO: stdout: ""
Mar  2 01:34:25.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-5377 exec execpodsglwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.123.244.50 32558'
Mar  2 01:34:25.785: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.123.244.50 32558\nConnection to 10.123.244.50 32558 port [tcp/*] succeeded!\n"
Mar  2 01:34:25.786: INFO: stdout: "nodeport-test-62cph"
Mar  2 01:34:25.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-5377 exec execpodsglwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.123.244.39 32558'
Mar  2 01:34:26.978: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.123.244.39 32558\nConnection to 10.123.244.39 32558 port [tcp/*] succeeded!\n"
Mar  2 01:34:26.978: INFO: stdout: "nodeport-test-89m82"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar  2 01:34:26.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5377" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:18.666 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":356,"completed":178,"skipped":3720,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:34:27.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar  2 01:34:27.167: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar  2 01:34:27.180: INFO: starting watch
STEP: patching
STEP: updating
Mar  2 01:34:27.223: INFO: waiting for watch events with expected annotations
Mar  2 01:34:27.223: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Mar  2 01:34:27.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5779" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":356,"completed":179,"skipped":3730,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:34:27.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:34:27.695: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"6bdb28a0-49e8-4b97-9c71-c5da84bfff56", Controller:(*bool)(0xc00ab0c212), BlockOwnerDeletion:(*bool)(0xc00ab0c213)}}
Mar  2 01:34:27.722: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3d5a6460-5e21-4d57-aae8-31160de74f0f", Controller:(*bool)(0xc00aaac216), BlockOwnerDeletion:(*bool)(0xc00aaac217)}}
Mar  2 01:34:27.755: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3f546baf-418a-40b2-8151-d50ba818032c", Controller:(*bool)(0xc00ab0c506), BlockOwnerDeletion:(*bool)(0xc00ab0c507)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Mar  2 01:34:32.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3882" for this suite.

• [SLOW TEST:5.514 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":356,"completed":180,"skipped":3755,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:34:32.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
W0302 01:34:33.084506      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0302 01:34:43.209324      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar  2 01:34:43.209: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Mar  2 01:34:43.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6350" for this suite.

• [SLOW TEST:10.352 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":356,"completed":181,"skipped":3760,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:34:43.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:34:43.341: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-de389333-be3e-4ebe-a573-ca182e5667d0
STEP: Creating configMap with name cm-test-opt-upd-5ea0e0cf-21d6-4e8e-823f-f7a1051931c6
STEP: Creating the pod
Mar  2 01:34:43.494: INFO: The status of Pod pod-configmaps-2c4be33b-705e-4817-ba00-e7d7757625a9 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:34:45.523: INFO: The status of Pod pod-configmaps-2c4be33b-705e-4817-ba00-e7d7757625a9 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:34:47.510: INFO: The status of Pod pod-configmaps-2c4be33b-705e-4817-ba00-e7d7757625a9 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-de389333-be3e-4ebe-a573-ca182e5667d0
STEP: Updating configmap cm-test-opt-upd-5ea0e0cf-21d6-4e8e-823f-f7a1051931c6
STEP: Creating configMap with name cm-test-opt-create-18197516-bace-43f1-a93d-c697d0580f04
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar  2 01:36:09.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9183" for this suite.

• [SLOW TEST:86.652 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":182,"skipped":3786,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:36:09.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar  2 01:36:11.414: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 01:36:14.485: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:36:14.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 01:36:17.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5567" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:8.275 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":356,"completed":183,"skipped":3799,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:36:18.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:36:18.437: INFO: The status of Pod test-webserver-5c26e888-aa64-4340-9fca-11bd4ea35884 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:36:20.473: INFO: The status of Pod test-webserver-5c26e888-aa64-4340-9fca-11bd4ea35884 is Running (Ready = false)
Mar  2 01:36:22.470: INFO: The status of Pod test-webserver-5c26e888-aa64-4340-9fca-11bd4ea35884 is Running (Ready = false)
Mar  2 01:36:24.461: INFO: The status of Pod test-webserver-5c26e888-aa64-4340-9fca-11bd4ea35884 is Running (Ready = false)
Mar  2 01:36:26.450: INFO: The status of Pod test-webserver-5c26e888-aa64-4340-9fca-11bd4ea35884 is Running (Ready = false)
Mar  2 01:36:28.452: INFO: The status of Pod test-webserver-5c26e888-aa64-4340-9fca-11bd4ea35884 is Running (Ready = false)
Mar  2 01:36:30.453: INFO: The status of Pod test-webserver-5c26e888-aa64-4340-9fca-11bd4ea35884 is Running (Ready = false)
Mar  2 01:36:32.456: INFO: The status of Pod test-webserver-5c26e888-aa64-4340-9fca-11bd4ea35884 is Running (Ready = false)
Mar  2 01:36:34.455: INFO: The status of Pod test-webserver-5c26e888-aa64-4340-9fca-11bd4ea35884 is Running (Ready = false)
Mar  2 01:36:36.452: INFO: The status of Pod test-webserver-5c26e888-aa64-4340-9fca-11bd4ea35884 is Running (Ready = false)
Mar  2 01:36:38.449: INFO: The status of Pod test-webserver-5c26e888-aa64-4340-9fca-11bd4ea35884 is Running (Ready = false)
Mar  2 01:36:40.455: INFO: The status of Pod test-webserver-5c26e888-aa64-4340-9fca-11bd4ea35884 is Running (Ready = true)
Mar  2 01:36:40.463: INFO: Container started at 2023-03-02 01:36:19 +0000 UTC, pod became ready at 2023-03-02 01:36:38 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Mar  2 01:36:40.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9871" for this suite.

• [SLOW TEST:22.332 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":356,"completed":184,"skipped":3801,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:36:40.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar  2 01:36:40.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7502" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":356,"completed":185,"skipped":3826,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:36:40.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Mar  2 01:37:41.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2222" for this suite.

• [SLOW TEST:60.362 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":356,"completed":186,"skipped":3865,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:37:41.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:37:41.483: INFO: Creating ReplicaSet my-hostname-basic-12404b7a-152e-405d-938d-0de4cd2296e6
Mar  2 01:37:41.535: INFO: Pod name my-hostname-basic-12404b7a-152e-405d-938d-0de4cd2296e6: Found 0 pods out of 1
Mar  2 01:37:46.547: INFO: Pod name my-hostname-basic-12404b7a-152e-405d-938d-0de4cd2296e6: Found 1 pods out of 1
Mar  2 01:37:46.547: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-12404b7a-152e-405d-938d-0de4cd2296e6" is running
Mar  2 01:37:46.556: INFO: Pod "my-hostname-basic-12404b7a-152e-405d-938d-0de4cd2296e6-4ggsl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 01:37:41 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 01:37:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 01:37:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-02 01:37:41 +0000 UTC Reason: Message:}])
Mar  2 01:37:46.556: INFO: Trying to dial the pod
Mar  2 01:37:51.596: INFO: Controller my-hostname-basic-12404b7a-152e-405d-938d-0de4cd2296e6: Got expected result from replica 1 [my-hostname-basic-12404b7a-152e-405d-938d-0de4cd2296e6-4ggsl]: "my-hostname-basic-12404b7a-152e-405d-938d-0de4cd2296e6-4ggsl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Mar  2 01:37:51.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4319" for this suite.

• [SLOW TEST:10.321 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":187,"skipped":3870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:37:51.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar  2 01:38:19.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-930" for this suite.

• [SLOW TEST:28.355 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":356,"completed":188,"skipped":3943,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:38:20.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a ReplicaSet
W0302 01:38:20.150210      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Verify that the required pods have come up
Mar  2 01:38:20.159: INFO: Pod name sample-pod: Found 0 pods out of 3
Mar  2 01:38:25.291: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Mar  2 01:38:27.429: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Mar  2 01:38:27.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6386" for this suite.

• [SLOW TEST:7.712 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":356,"completed":189,"skipped":3975,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:38:27.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar  2 01:38:27.910: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3beea576-6e16-49b6-afd2-69fc6a8dc954" in namespace "projected-7134" to be "Succeeded or Failed"
Mar  2 01:38:27.920: INFO: Pod "downwardapi-volume-3beea576-6e16-49b6-afd2-69fc6a8dc954": Phase="Pending", Reason="", readiness=false. Elapsed: 9.846668ms
Mar  2 01:38:29.936: INFO: Pod "downwardapi-volume-3beea576-6e16-49b6-afd2-69fc6a8dc954": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025181163s
Mar  2 01:38:31.947: INFO: Pod "downwardapi-volume-3beea576-6e16-49b6-afd2-69fc6a8dc954": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036401786s
STEP: Saw pod success
Mar  2 01:38:31.947: INFO: Pod "downwardapi-volume-3beea576-6e16-49b6-afd2-69fc6a8dc954" satisfied condition "Succeeded or Failed"
Mar  2 01:38:31.956: INFO: Trying to get logs from node 10.123.244.39 pod downwardapi-volume-3beea576-6e16-49b6-afd2-69fc6a8dc954 container client-container: <nil>
STEP: delete the pod
Mar  2 01:38:32.017: INFO: Waiting for pod downwardapi-volume-3beea576-6e16-49b6-afd2-69fc6a8dc954 to disappear
Mar  2 01:38:32.025: INFO: Pod downwardapi-volume-3beea576-6e16-49b6-afd2-69fc6a8dc954 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar  2 01:38:32.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7134" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":190,"skipped":3979,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:38:32.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
W0302 01:38:32.209573      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Mar  2 01:38:46.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1886" for this suite.

• [SLOW TEST:14.185 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":356,"completed":191,"skipped":4047,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:38:46.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 01:38:48.790: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 01:38:50.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 38, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 38, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 38, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 38, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 01:38:53.857: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:38:53.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2013-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 01:38:57.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8472" for this suite.
STEP: Destroying namespace "webhook-8472-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:11.343 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":356,"completed":192,"skipped":4049,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:38:57.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
STEP: create deployment with httpd image
Mar  2 01:38:57.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-2594 create -f -'
Mar  2 01:38:59.292: INFO: stderr: ""
Mar  2 01:38:59.292: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Mar  2 01:38:59.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-2594 diff -f -'
Mar  2 01:39:02.335: INFO: rc: 1
Mar  2 01:39:02.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-2594 delete -f -'
Mar  2 01:39:02.558: INFO: stderr: ""
Mar  2 01:39:02.558: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar  2 01:39:02.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2594" for this suite.

• [SLOW TEST:5.097 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:896
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":356,"completed":193,"skipped":4055,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:39:02.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:188
Mar  2 01:39:03.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-9437" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":356,"completed":194,"skipped":4071,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:39:03.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar  2 01:39:03.660: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2959  721d254e-634e-4524-a58f-b9ed8b68d6c6 116711 0 2023-03-02 01:39:03 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2023-03-02 01:39:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 01:39:03.661: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2959  721d254e-634e-4524-a58f-b9ed8b68d6c6 116717 0 2023-03-02 01:39:03 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2023-03-02 01:39:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Mar  2 01:39:03.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2959" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":356,"completed":195,"skipped":4072,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:39:03.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:39:03.874: INFO: Creating simple deployment test-new-deployment
Mar  2 01:39:03.926: INFO: deployment "test-new-deployment" doesn't have the required revision set
Mar  2 01:39:06.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 39, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 39, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 39, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 39, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-55df494869\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 01:39:08.198: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-129  61df8d12-be2c-4251-b917-28525e5617f0 116866 3 2023-03-02 01:39:03 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-02 01:39:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:39:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00264fa98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-02 01:39:06 +0000 UTC,LastTransitionTime:2023-03-02 01:39:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-55df494869" has successfully progressed.,LastUpdateTime:2023-03-02 01:39:06 +0000 UTC,LastTransitionTime:2023-03-02 01:39:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 01:39:08.225: INFO: New ReplicaSet "test-new-deployment-55df494869" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-55df494869  deployment-129  b94ec9c8-f226-4bec-93b4-8add01f7592b 116869 2 2023-03-02 01:39:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 61df8d12-be2c-4251-b917-28525e5617f0 0xc000077827 0xc000077828}] []  [{kube-controller-manager Update apps/v1 2023-03-02 01:39:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61df8d12-be2c-4251-b917-28525e5617f0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:39:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0000778b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:39:08.237: INFO: Pod "test-new-deployment-55df494869-wcgfp" is available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-wcgfp test-new-deployment-55df494869- deployment-129  ebaba53c-0bc3-4c27-8fa5-b802f8e3471f 116837 0 2023-03-02 01:39:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:e2ae301d97594463b4c8fb6666dca9f6de21de6df6d7a93ea533a1078e5ea887 cni.projectcalico.org/podIP:172.30.88.239/32 cni.projectcalico.org/podIPs:172.30.88.239/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.239"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.239"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-55df494869 b94ec9c8-f226-4bec-93b4-8add01f7592b 0xc000077d47 0xc000077d48}] []  [{kube-controller-manager Update v1 2023-03-02 01:39:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b94ec9c8-f226-4bec-93b4-8add01f7592b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-03-02 01:39:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:39:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:39:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.88.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7gw54,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7gw54,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.39,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:39:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:39:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:39:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:39:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.39,PodIP:172.30.88.239,StartTime:2023-03-02 01:39:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:39:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://abdb2853ea4113af13d315bdba03073b9821089b1b2ca042772f4d709b8bd09d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.88.239,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Mar  2 01:39:08.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-129" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":356,"completed":196,"skipped":4102,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:39:08.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 01:39:08.762: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:39:08.762: INFO: Node 10.123.244.39 is running 0 daemon pod, expected 1
Mar  2 01:39:09.815: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:39:09.815: INFO: Node 10.123.244.39 is running 0 daemon pod, expected 1
Mar  2 01:39:10.836: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:39:10.836: INFO: Node 10.123.244.39 is running 0 daemon pod, expected 1
Mar  2 01:39:11.795: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 01:39:11.795: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status
Mar  2 01:39:11.813: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Mar  2 01:39:11.839: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Mar  2 01:39:11.844: INFO: Observed &DaemonSet event: ADDED
Mar  2 01:39:11.844: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 01:39:11.845: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 01:39:11.845: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 01:39:11.845: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 01:39:11.845: INFO: Found daemon set daemon-set in namespace daemonsets-871 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  2 01:39:11.846: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Mar  2 01:39:11.877: INFO: Observed &DaemonSet event: ADDED
Mar  2 01:39:11.878: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 01:39:11.878: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 01:39:11.878: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 01:39:11.879: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 01:39:11.879: INFO: Observed daemon set daemon-set in namespace daemonsets-871 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  2 01:39:11.879: INFO: Observed &DaemonSet event: MODIFIED
Mar  2 01:39:11.879: INFO: Found daemon set daemon-set in namespace daemonsets-871 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Mar  2 01:39:11.879: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-871, will wait for the garbage collector to delete the pods
Mar  2 01:39:11.962: INFO: Deleting DaemonSet.extensions daemon-set took: 13.956019ms
Mar  2 01:39:12.063: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.263656ms
Mar  2 01:39:15.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:39:15.479: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  2 01:39:15.492: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"117137"},"items":null}

Mar  2 01:39:15.500: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"117137"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Mar  2 01:39:15.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-871" for this suite.

• [SLOW TEST:7.289 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":356,"completed":197,"skipped":4124,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:39:15.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test env composition
Mar  2 01:39:15.733: INFO: Waiting up to 5m0s for pod "var-expansion-140800e1-bbf9-4a3e-af9a-6962ae77d8a2" in namespace "var-expansion-2878" to be "Succeeded or Failed"
Mar  2 01:39:15.742: INFO: Pod "var-expansion-140800e1-bbf9-4a3e-af9a-6962ae77d8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.36657ms
Mar  2 01:39:17.756: INFO: Pod "var-expansion-140800e1-bbf9-4a3e-af9a-6962ae77d8a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023873446s
Mar  2 01:39:19.776: INFO: Pod "var-expansion-140800e1-bbf9-4a3e-af9a-6962ae77d8a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043402933s
STEP: Saw pod success
Mar  2 01:39:19.776: INFO: Pod "var-expansion-140800e1-bbf9-4a3e-af9a-6962ae77d8a2" satisfied condition "Succeeded or Failed"
Mar  2 01:39:19.788: INFO: Trying to get logs from node 10.123.244.39 pod var-expansion-140800e1-bbf9-4a3e-af9a-6962ae77d8a2 container dapi-container: <nil>
STEP: delete the pod
Mar  2 01:39:19.944: INFO: Waiting for pod var-expansion-140800e1-bbf9-4a3e-af9a-6962ae77d8a2 to disappear
Mar  2 01:39:19.953: INFO: Pod var-expansion-140800e1-bbf9-4a3e-af9a-6962ae77d8a2 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Mar  2 01:39:19.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2878" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":356,"completed":198,"skipped":4134,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:39:20.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching services
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar  2 01:39:20.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3128" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":356,"completed":199,"skipped":4155,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:39:20.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Indexed job
STEP: Ensuring job reaches completions
STEP: Ensuring pods with index for job exist
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Mar  2 01:39:30.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4939" for this suite.

• [SLOW TEST:10.206 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","total":356,"completed":200,"skipped":4194,"failed":0}
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:39:30.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's command
Mar  2 01:39:30.739: INFO: Waiting up to 5m0s for pod "var-expansion-7114cf4d-2aba-4de5-b270-89d5b23e11d0" in namespace "var-expansion-3362" to be "Succeeded or Failed"
Mar  2 01:39:30.779: INFO: Pod "var-expansion-7114cf4d-2aba-4de5-b270-89d5b23e11d0": Phase="Pending", Reason="", readiness=false. Elapsed: 40.10547ms
Mar  2 01:39:32.822: INFO: Pod "var-expansion-7114cf4d-2aba-4de5-b270-89d5b23e11d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083255352s
Mar  2 01:39:34.837: INFO: Pod "var-expansion-7114cf4d-2aba-4de5-b270-89d5b23e11d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.098523276s
STEP: Saw pod success
Mar  2 01:39:34.837: INFO: Pod "var-expansion-7114cf4d-2aba-4de5-b270-89d5b23e11d0" satisfied condition "Succeeded or Failed"
Mar  2 01:39:34.846: INFO: Trying to get logs from node 10.123.244.39 pod var-expansion-7114cf4d-2aba-4de5-b270-89d5b23e11d0 container dapi-container: <nil>
STEP: delete the pod
Mar  2 01:39:34.953: INFO: Waiting for pod var-expansion-7114cf4d-2aba-4de5-b270-89d5b23e11d0 to disappear
Mar  2 01:39:34.973: INFO: Pod var-expansion-7114cf4d-2aba-4de5-b270-89d5b23e11d0 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Mar  2 01:39:34.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3362" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":356,"completed":201,"skipped":4194,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:39:35.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 01:39:36.833: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 01:39:39.965: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 01:39:40.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6526" for this suite.
STEP: Destroying namespace "webhook-6526-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.289 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":356,"completed":202,"skipped":4218,"failed":0}
SSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:39:40.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating secret secrets-8662/secret-test-805136b0-a986-4e43-93bc-287e38344394
STEP: Creating a pod to test consume secrets
Mar  2 01:39:40.585: INFO: Waiting up to 5m0s for pod "pod-configmaps-1c03e16c-abac-4755-8dfb-93a64faf2908" in namespace "secrets-8662" to be "Succeeded or Failed"
Mar  2 01:39:40.631: INFO: Pod "pod-configmaps-1c03e16c-abac-4755-8dfb-93a64faf2908": Phase="Pending", Reason="", readiness=false. Elapsed: 29.805945ms
Mar  2 01:39:42.644: INFO: Pod "pod-configmaps-1c03e16c-abac-4755-8dfb-93a64faf2908": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042514641s
Mar  2 01:39:44.659: INFO: Pod "pod-configmaps-1c03e16c-abac-4755-8dfb-93a64faf2908": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057978711s
STEP: Saw pod success
Mar  2 01:39:44.659: INFO: Pod "pod-configmaps-1c03e16c-abac-4755-8dfb-93a64faf2908" satisfied condition "Succeeded or Failed"
Mar  2 01:39:44.670: INFO: Trying to get logs from node 10.123.244.39 pod pod-configmaps-1c03e16c-abac-4755-8dfb-93a64faf2908 container env-test: <nil>
STEP: delete the pod
Mar  2 01:39:44.743: INFO: Waiting for pod pod-configmaps-1c03e16c-abac-4755-8dfb-93a64faf2908 to disappear
Mar  2 01:39:44.753: INFO: Pod pod-configmaps-1c03e16c-abac-4755-8dfb-93a64faf2908 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Mar  2 01:39:44.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8662" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":203,"skipped":4223,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:39:44.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Mar  2 01:39:45.112: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:39:47.127: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:39:49.128: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.123.244.41 on the node which pod1 resides and expect scheduled
Mar  2 01:39:49.188: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:39:51.216: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:39:53.200: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.123.244.41 but use UDP protocol on the node which pod2 resides
Mar  2 01:39:53.258: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:39:55.269: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:39:57.275: INFO: The status of Pod pod3 is Running (Ready = true)
Mar  2 01:39:57.329: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:39:59.342: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Mar  2 01:39:59.376: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.123.244.41 http://127.0.0.1:54323/hostname] Namespace:hostport-5777 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:39:59.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:39:59.377: INFO: ExecWithOptions: Clientset creation
Mar  2 01:39:59.377: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-5777/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.123.244.41+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.123.244.41, port: 54323
Mar  2 01:39:59.609: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.123.244.41:54323/hostname] Namespace:hostport-5777 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:39:59.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:39:59.611: INFO: ExecWithOptions: Clientset creation
Mar  2 01:39:59.611: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-5777/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.123.244.41%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.123.244.41, port: 54323 UDP
Mar  2 01:39:59.817: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.123.244.41 54323] Namespace:hostport-5777 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:39:59.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:39:59.818: INFO: ExecWithOptions: Clientset creation
Mar  2 01:39:59.818: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-5777/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+10.123.244.41+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:188
Mar  2 01:40:05.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-5777" for this suite.

• [SLOW TEST:20.308 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":356,"completed":204,"skipped":4274,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:40:05.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  2 01:40:05.324: INFO: Waiting up to 5m0s for pod "pod-ead984da-e6a7-4ebe-96ea-2344ff345bf8" in namespace "emptydir-5377" to be "Succeeded or Failed"
Mar  2 01:40:05.334: INFO: Pod "pod-ead984da-e6a7-4ebe-96ea-2344ff345bf8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.082511ms
Mar  2 01:40:07.360: INFO: Pod "pod-ead984da-e6a7-4ebe-96ea-2344ff345bf8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036025542s
Mar  2 01:40:09.374: INFO: Pod "pod-ead984da-e6a7-4ebe-96ea-2344ff345bf8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050352035s
Mar  2 01:40:11.392: INFO: Pod "pod-ead984da-e6a7-4ebe-96ea-2344ff345bf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067839019s
STEP: Saw pod success
Mar  2 01:40:11.392: INFO: Pod "pod-ead984da-e6a7-4ebe-96ea-2344ff345bf8" satisfied condition "Succeeded or Failed"
Mar  2 01:40:11.400: INFO: Trying to get logs from node 10.123.244.39 pod pod-ead984da-e6a7-4ebe-96ea-2344ff345bf8 container test-container: <nil>
STEP: delete the pod
Mar  2 01:40:11.453: INFO: Waiting for pod pod-ead984da-e6a7-4ebe-96ea-2344ff345bf8 to disappear
Mar  2 01:40:11.462: INFO: Pod pod-ead984da-e6a7-4ebe-96ea-2344ff345bf8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar  2 01:40:11.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5377" for this suite.

• [SLOW TEST:6.411 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":205,"skipped":4302,"failed":0}
SSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:40:11.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Mar  2 01:40:15.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3668" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":356,"completed":206,"skipped":4307,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:40:16.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:40:16.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 01:40:17.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-347" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":356,"completed":207,"skipped":4309,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:40:17.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
W0302 01:40:17.471278      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar  2 01:40:23.689: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0302 01:40:23.689369      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Mar  2 01:40:23.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-599" for this suite.

• [SLOW TEST:6.511 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":356,"completed":208,"skipped":4327,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:40:23.763: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar  2 01:40:23.953: INFO: Waiting up to 5m0s for pod "downwardapi-volume-09acd42f-9c43-418a-b167-f9ffbb60bcda" in namespace "projected-5399" to be "Succeeded or Failed"
Mar  2 01:40:23.973: INFO: Pod "downwardapi-volume-09acd42f-9c43-418a-b167-f9ffbb60bcda": Phase="Pending", Reason="", readiness=false. Elapsed: 20.252403ms
Mar  2 01:40:25.994: INFO: Pod "downwardapi-volume-09acd42f-9c43-418a-b167-f9ffbb60bcda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041171634s
Mar  2 01:40:28.008: INFO: Pod "downwardapi-volume-09acd42f-9c43-418a-b167-f9ffbb60bcda": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055244154s
Mar  2 01:40:30.018: INFO: Pod "downwardapi-volume-09acd42f-9c43-418a-b167-f9ffbb60bcda": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065685693s
Mar  2 01:40:32.070: INFO: Pod "downwardapi-volume-09acd42f-9c43-418a-b167-f9ffbb60bcda": Phase="Pending", Reason="", readiness=false. Elapsed: 8.116844007s
Mar  2 01:40:34.093: INFO: Pod "downwardapi-volume-09acd42f-9c43-418a-b167-f9ffbb60bcda": Phase="Pending", Reason="", readiness=false. Elapsed: 10.139971527s
Mar  2 01:40:36.154: INFO: Pod "downwardapi-volume-09acd42f-9c43-418a-b167-f9ffbb60bcda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.200938791s
STEP: Saw pod success
Mar  2 01:40:36.154: INFO: Pod "downwardapi-volume-09acd42f-9c43-418a-b167-f9ffbb60bcda" satisfied condition "Succeeded or Failed"
Mar  2 01:40:36.171: INFO: Trying to get logs from node 10.123.244.39 pod downwardapi-volume-09acd42f-9c43-418a-b167-f9ffbb60bcda container client-container: <nil>
STEP: delete the pod
Mar  2 01:40:36.221: INFO: Waiting for pod downwardapi-volume-09acd42f-9c43-418a-b167-f9ffbb60bcda to disappear
Mar  2 01:40:36.237: INFO: Pod downwardapi-volume-09acd42f-9c43-418a-b167-f9ffbb60bcda no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar  2 01:40:36.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5399" for this suite.

• [SLOW TEST:12.519 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":356,"completed":209,"skipped":4350,"failed":0}
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:40:36.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:40:36.503: INFO: The status of Pod busybox-readonly-fsdca8b215-0fed-47dc-a8c9-dc15e71becac is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:40:38.545: INFO: The status of Pod busybox-readonly-fsdca8b215-0fed-47dc-a8c9-dc15e71becac is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Mar  2 01:40:38.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8927" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":210,"skipped":4350,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:40:38.665: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  2 01:40:38.897: INFO: Waiting up to 5m0s for pod "pod-f6c43ee4-0b7b-4e20-8323-11b35e474b59" in namespace "emptydir-3225" to be "Succeeded or Failed"
Mar  2 01:40:38.908: INFO: Pod "pod-f6c43ee4-0b7b-4e20-8323-11b35e474b59": Phase="Pending", Reason="", readiness=false. Elapsed: 10.836707ms
Mar  2 01:40:40.950: INFO: Pod "pod-f6c43ee4-0b7b-4e20-8323-11b35e474b59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052733695s
Mar  2 01:40:42.964: INFO: Pod "pod-f6c43ee4-0b7b-4e20-8323-11b35e474b59": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066818845s
Mar  2 01:40:45.008: INFO: Pod "pod-f6c43ee4-0b7b-4e20-8323-11b35e474b59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.110401153s
STEP: Saw pod success
Mar  2 01:40:45.008: INFO: Pod "pod-f6c43ee4-0b7b-4e20-8323-11b35e474b59" satisfied condition "Succeeded or Failed"
Mar  2 01:40:45.031: INFO: Trying to get logs from node 10.123.244.39 pod pod-f6c43ee4-0b7b-4e20-8323-11b35e474b59 container test-container: <nil>
STEP: delete the pod
Mar  2 01:40:45.095: INFO: Waiting for pod pod-f6c43ee4-0b7b-4e20-8323-11b35e474b59 to disappear
Mar  2 01:40:45.112: INFO: Pod pod-f6c43ee4-0b7b-4e20-8323-11b35e474b59 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar  2 01:40:45.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3225" for this suite.

• [SLOW TEST:6.549 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":211,"skipped":4368,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:40:45.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Mar  2 01:40:45.620: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Mar  2 01:40:49.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9198" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":356,"completed":212,"skipped":4384,"failed":0}

------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:40:49.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:40:49.405: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-76a30c7c-01c8-49ab-8dfc-1fa1b00946c1
STEP: Creating secret with name s-test-opt-upd-d7dcbcde-6ca7-4c85-83a4-c3910ffff120
STEP: Creating the pod
Mar  2 01:40:49.548: INFO: The status of Pod pod-secrets-924e1e78-4e60-4538-9eac-863ad820d97f is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:40:51.565: INFO: The status of Pod pod-secrets-924e1e78-4e60-4538-9eac-863ad820d97f is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-76a30c7c-01c8-49ab-8dfc-1fa1b00946c1
STEP: Updating secret s-test-opt-upd-d7dcbcde-6ca7-4c85-83a4-c3910ffff120
STEP: Creating secret with name s-test-opt-create-88eeb75d-9a70-4021-8de0-33ce781b216a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Mar  2 01:40:53.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4072" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":213,"skipped":4384,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:40:53.830: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
W0302 01:40:54.004884      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:40:54.005: INFO: Waiting up to 5m0s for pod "downwardapi-volume-289fedd7-ba90-4297-aaa1-7a87963cb3dd" in namespace "downward-api-5234" to be "Succeeded or Failed"
Mar  2 01:40:54.015: INFO: Pod "downwardapi-volume-289fedd7-ba90-4297-aaa1-7a87963cb3dd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.818991ms
Mar  2 01:40:56.030: INFO: Pod "downwardapi-volume-289fedd7-ba90-4297-aaa1-7a87963cb3dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025732017s
Mar  2 01:40:58.047: INFO: Pod "downwardapi-volume-289fedd7-ba90-4297-aaa1-7a87963cb3dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04266827s
Mar  2 01:41:00.061: INFO: Pod "downwardapi-volume-289fedd7-ba90-4297-aaa1-7a87963cb3dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056097796s
STEP: Saw pod success
Mar  2 01:41:00.061: INFO: Pod "downwardapi-volume-289fedd7-ba90-4297-aaa1-7a87963cb3dd" satisfied condition "Succeeded or Failed"
Mar  2 01:41:00.070: INFO: Trying to get logs from node 10.123.244.39 pod downwardapi-volume-289fedd7-ba90-4297-aaa1-7a87963cb3dd container client-container: <nil>
STEP: delete the pod
Mar  2 01:41:00.124: INFO: Waiting for pod downwardapi-volume-289fedd7-ba90-4297-aaa1-7a87963cb3dd to disappear
Mar  2 01:41:00.133: INFO: Pod downwardapi-volume-289fedd7-ba90-4297-aaa1-7a87963cb3dd no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar  2 01:41:00.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5234" for this suite.

• [SLOW TEST:6.344 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":214,"skipped":4387,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:41:00.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar  2 01:41:00.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar  2 01:42:12.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:42:32.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 01:43:49.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2431" for this suite.

• [SLOW TEST:169.455 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":356,"completed":215,"skipped":4416,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:43:49.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Mar  2 01:43:49.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-5481 create -f -'
Mar  2 01:43:52.188: INFO: stderr: ""
Mar  2 01:43:52.188: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 01:43:52.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-5481 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 01:43:52.543: INFO: stderr: ""
Mar  2 01:43:52.543: INFO: stdout: "update-demo-nautilus-44jkn update-demo-nautilus-b6tz6 "
Mar  2 01:43:52.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-5481 get pods update-demo-nautilus-44jkn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 01:43:52.946: INFO: stderr: ""
Mar  2 01:43:52.946: INFO: stdout: ""
Mar  2 01:43:52.946: INFO: update-demo-nautilus-44jkn is created but not running
Mar  2 01:43:57.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-5481 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 01:43:58.149: INFO: stderr: ""
Mar  2 01:43:58.149: INFO: stdout: "update-demo-nautilus-44jkn update-demo-nautilus-b6tz6 "
Mar  2 01:43:58.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-5481 get pods update-demo-nautilus-44jkn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 01:43:58.312: INFO: stderr: ""
Mar  2 01:43:58.312: INFO: stdout: ""
Mar  2 01:43:58.312: INFO: update-demo-nautilus-44jkn is created but not running
Mar  2 01:44:03.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-5481 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 01:44:03.515: INFO: stderr: ""
Mar  2 01:44:03.515: INFO: stdout: "update-demo-nautilus-44jkn update-demo-nautilus-b6tz6 "
Mar  2 01:44:03.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-5481 get pods update-demo-nautilus-44jkn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 01:44:03.812: INFO: stderr: ""
Mar  2 01:44:03.812: INFO: stdout: "true"
Mar  2 01:44:03.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-5481 get pods update-demo-nautilus-44jkn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 01:44:04.247: INFO: stderr: ""
Mar  2 01:44:04.247: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar  2 01:44:04.247: INFO: validating pod update-demo-nautilus-44jkn
Mar  2 01:44:04.514: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 01:44:04.514: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 01:44:04.514: INFO: update-demo-nautilus-44jkn is verified up and running
Mar  2 01:44:04.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-5481 get pods update-demo-nautilus-b6tz6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 01:44:04.831: INFO: stderr: ""
Mar  2 01:44:04.831: INFO: stdout: "true"
Mar  2 01:44:04.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-5481 get pods update-demo-nautilus-b6tz6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 01:44:05.206: INFO: stderr: ""
Mar  2 01:44:05.207: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar  2 01:44:05.207: INFO: validating pod update-demo-nautilus-b6tz6
Mar  2 01:44:05.243: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 01:44:05.243: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 01:44:05.243: INFO: update-demo-nautilus-b6tz6 is verified up and running
STEP: using delete to clean up resources
Mar  2 01:44:05.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-5481 delete --grace-period=0 --force -f -'
Mar  2 01:44:05.443: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 01:44:05.443: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  2 01:44:05.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-5481 get rc,svc -l name=update-demo --no-headers'
Mar  2 01:44:05.682: INFO: stderr: "No resources found in kubectl-5481 namespace.\n"
Mar  2 01:44:05.682: INFO: stdout: ""
Mar  2 01:44:05.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-5481 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 01:44:05.881: INFO: stderr: ""
Mar  2 01:44:05.881: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar  2 01:44:05.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5481" for this suite.

• [SLOW TEST:16.287 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should create and stop a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":356,"completed":216,"skipped":4442,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:44:05.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-h26k
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 01:44:06.197: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-h26k" in namespace "subpath-4945" to be "Succeeded or Failed"
Mar  2 01:44:06.228: INFO: Pod "pod-subpath-test-configmap-h26k": Phase="Pending", Reason="", readiness=false. Elapsed: 31.326508ms
Mar  2 01:44:08.240: INFO: Pod "pod-subpath-test-configmap-h26k": Phase="Running", Reason="", readiness=true. Elapsed: 2.043315543s
Mar  2 01:44:10.255: INFO: Pod "pod-subpath-test-configmap-h26k": Phase="Running", Reason="", readiness=true. Elapsed: 4.058167041s
Mar  2 01:44:12.267: INFO: Pod "pod-subpath-test-configmap-h26k": Phase="Running", Reason="", readiness=true. Elapsed: 6.070016876s
Mar  2 01:44:14.281: INFO: Pod "pod-subpath-test-configmap-h26k": Phase="Running", Reason="", readiness=true. Elapsed: 8.084516324s
Mar  2 01:44:16.315: INFO: Pod "pod-subpath-test-configmap-h26k": Phase="Running", Reason="", readiness=true. Elapsed: 10.117873036s
Mar  2 01:44:18.338: INFO: Pod "pod-subpath-test-configmap-h26k": Phase="Running", Reason="", readiness=true. Elapsed: 12.141308196s
Mar  2 01:44:20.357: INFO: Pod "pod-subpath-test-configmap-h26k": Phase="Running", Reason="", readiness=true. Elapsed: 14.159809771s
Mar  2 01:44:22.370: INFO: Pod "pod-subpath-test-configmap-h26k": Phase="Running", Reason="", readiness=true. Elapsed: 16.172738805s
Mar  2 01:44:24.391: INFO: Pod "pod-subpath-test-configmap-h26k": Phase="Running", Reason="", readiness=true. Elapsed: 18.194196451s
Mar  2 01:44:26.403: INFO: Pod "pod-subpath-test-configmap-h26k": Phase="Running", Reason="", readiness=true. Elapsed: 20.206660363s
Mar  2 01:44:28.418: INFO: Pod "pod-subpath-test-configmap-h26k": Phase="Running", Reason="", readiness=false. Elapsed: 22.220727828s
Mar  2 01:44:30.435: INFO: Pod "pod-subpath-test-configmap-h26k": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.238200352s
STEP: Saw pod success
Mar  2 01:44:30.435: INFO: Pod "pod-subpath-test-configmap-h26k" satisfied condition "Succeeded or Failed"
Mar  2 01:44:30.444: INFO: Trying to get logs from node 10.123.244.39 pod pod-subpath-test-configmap-h26k container test-container-subpath-configmap-h26k: <nil>
STEP: delete the pod
Mar  2 01:44:30.521: INFO: Waiting for pod pod-subpath-test-configmap-h26k to disappear
Mar  2 01:44:30.530: INFO: Pod pod-subpath-test-configmap-h26k no longer exists
STEP: Deleting pod pod-subpath-test-configmap-h26k
Mar  2 01:44:30.530: INFO: Deleting pod "pod-subpath-test-configmap-h26k" in namespace "subpath-4945"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Mar  2 01:44:30.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4945" for this suite.

• [SLOW TEST:24.683 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","total":356,"completed":217,"skipped":4452,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:44:30.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  2 01:44:30.863: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 01:45:31.113: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Mar  2 01:45:31.268: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar  2 01:45:31.309: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar  2 01:45:31.373: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar  2 01:45:31.410: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar  2 01:45:31.482: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar  2 01:45:31.546: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Mar  2 01:45:49.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7600" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:79.467 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":356,"completed":218,"skipped":4507,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:45:50.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:45:50.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Mar  2 01:46:08.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-6390 --namespace=crd-publish-openapi-6390 create -f -'
Mar  2 01:46:11.435: INFO: stderr: ""
Mar  2 01:46:11.435: INFO: stdout: "e2e-test-crd-publish-openapi-8001-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  2 01:46:11.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-6390 --namespace=crd-publish-openapi-6390 delete e2e-test-crd-publish-openapi-8001-crds test-cr'
Mar  2 01:46:11.628: INFO: stderr: ""
Mar  2 01:46:11.628: INFO: stdout: "e2e-test-crd-publish-openapi-8001-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar  2 01:46:11.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-6390 --namespace=crd-publish-openapi-6390 apply -f -'
Mar  2 01:46:14.248: INFO: stderr: ""
Mar  2 01:46:14.248: INFO: stdout: "e2e-test-crd-publish-openapi-8001-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar  2 01:46:14.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-6390 --namespace=crd-publish-openapi-6390 delete e2e-test-crd-publish-openapi-8001-crds test-cr'
Mar  2 01:46:14.388: INFO: stderr: ""
Mar  2 01:46:14.388: INFO: stdout: "e2e-test-crd-publish-openapi-8001-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar  2 01:46:14.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-6390 explain e2e-test-crd-publish-openapi-8001-crds'
Mar  2 01:46:15.093: INFO: stderr: ""
Mar  2 01:46:15.093: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8001-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 01:46:34.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6390" for this suite.

• [SLOW TEST:44.989 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":356,"completed":219,"skipped":4508,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:46:35.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap that has name configmap-test-emptyKey-2af546cf-a41e-4464-9fa5-bc656b567191
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Mar  2 01:46:35.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1138" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":356,"completed":220,"skipped":4511,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:46:35.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar  2 01:46:35.583: INFO: Waiting up to 5m0s for pod "pod-117317ea-82a5-40bc-9af6-3d067ed52c1c" in namespace "emptydir-2972" to be "Succeeded or Failed"
Mar  2 01:46:35.628: INFO: Pod "pod-117317ea-82a5-40bc-9af6-3d067ed52c1c": Phase="Pending", Reason="", readiness=false. Elapsed: 45.466689ms
Mar  2 01:46:37.698: INFO: Pod "pod-117317ea-82a5-40bc-9af6-3d067ed52c1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.11489602s
Mar  2 01:46:39.728: INFO: Pod "pod-117317ea-82a5-40bc-9af6-3d067ed52c1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.14555973s
Mar  2 01:46:41.759: INFO: Pod "pod-117317ea-82a5-40bc-9af6-3d067ed52c1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.176605197s
STEP: Saw pod success
Mar  2 01:46:41.759: INFO: Pod "pod-117317ea-82a5-40bc-9af6-3d067ed52c1c" satisfied condition "Succeeded or Failed"
Mar  2 01:46:41.767: INFO: Trying to get logs from node 10.123.244.39 pod pod-117317ea-82a5-40bc-9af6-3d067ed52c1c container test-container: <nil>
STEP: delete the pod
Mar  2 01:46:42.235: INFO: Waiting for pod pod-117317ea-82a5-40bc-9af6-3d067ed52c1c to disappear
Mar  2 01:46:42.247: INFO: Pod pod-117317ea-82a5-40bc-9af6-3d067ed52c1c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar  2 01:46:42.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2972" for this suite.

• [SLOW TEST:6.927 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":221,"skipped":4517,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:46:42.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-26s4
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 01:46:42.544: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-26s4" in namespace "subpath-2243" to be "Succeeded or Failed"
Mar  2 01:46:42.595: INFO: Pod "pod-subpath-test-configmap-26s4": Phase="Pending", Reason="", readiness=false. Elapsed: 50.23686ms
Mar  2 01:46:44.609: INFO: Pod "pod-subpath-test-configmap-26s4": Phase="Running", Reason="", readiness=true. Elapsed: 2.064234729s
Mar  2 01:46:46.663: INFO: Pod "pod-subpath-test-configmap-26s4": Phase="Running", Reason="", readiness=true. Elapsed: 4.118224075s
Mar  2 01:46:48.682: INFO: Pod "pod-subpath-test-configmap-26s4": Phase="Running", Reason="", readiness=true. Elapsed: 6.137360451s
Mar  2 01:46:50.695: INFO: Pod "pod-subpath-test-configmap-26s4": Phase="Running", Reason="", readiness=true. Elapsed: 8.15030574s
Mar  2 01:46:52.703: INFO: Pod "pod-subpath-test-configmap-26s4": Phase="Running", Reason="", readiness=true. Elapsed: 10.158934967s
Mar  2 01:46:54.713: INFO: Pod "pod-subpath-test-configmap-26s4": Phase="Running", Reason="", readiness=true. Elapsed: 12.168626361s
Mar  2 01:46:56.725: INFO: Pod "pod-subpath-test-configmap-26s4": Phase="Running", Reason="", readiness=true. Elapsed: 14.180387157s
Mar  2 01:46:58.741: INFO: Pod "pod-subpath-test-configmap-26s4": Phase="Running", Reason="", readiness=true. Elapsed: 16.196504334s
Mar  2 01:47:00.751: INFO: Pod "pod-subpath-test-configmap-26s4": Phase="Running", Reason="", readiness=true. Elapsed: 18.206935589s
Mar  2 01:47:02.765: INFO: Pod "pod-subpath-test-configmap-26s4": Phase="Running", Reason="", readiness=true. Elapsed: 20.220821498s
Mar  2 01:47:04.793: INFO: Pod "pod-subpath-test-configmap-26s4": Phase="Running", Reason="", readiness=false. Elapsed: 22.248336805s
Mar  2 01:47:06.803: INFO: Pod "pod-subpath-test-configmap-26s4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.258316369s
STEP: Saw pod success
Mar  2 01:47:06.803: INFO: Pod "pod-subpath-test-configmap-26s4" satisfied condition "Succeeded or Failed"
Mar  2 01:47:06.813: INFO: Trying to get logs from node 10.123.244.39 pod pod-subpath-test-configmap-26s4 container test-container-subpath-configmap-26s4: <nil>
STEP: delete the pod
Mar  2 01:47:06.878: INFO: Waiting for pod pod-subpath-test-configmap-26s4 to disappear
Mar  2 01:47:06.889: INFO: Pod pod-subpath-test-configmap-26s4 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-26s4
Mar  2 01:47:06.889: INFO: Deleting pod "pod-subpath-test-configmap-26s4" in namespace "subpath-2243"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Mar  2 01:47:06.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2243" for this suite.

• [SLOW TEST:24.634 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","total":356,"completed":222,"skipped":4521,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:47:06.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  2 01:47:07.023: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 01:47:07.109: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 01:47:07.169: INFO: 
Logging pods the apiserver thinks is on node 10.123.244.39 before test
Mar  2 01:47:07.204: INFO: calico-node-vkllz from calico-system started at 2023-03-01 22:01:04 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.204: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 01:47:07.204: INFO: calico-typha-db9579c55-nnjdl from calico-system started at 2023-03-01 22:01:04 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.204: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 01:47:07.204: INFO: ibm-keepalived-watcher-hf2lb from kube-system started at 2023-03-01 22:00:56 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.204: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 01:47:07.205: INFO: ibm-master-proxy-static-10.123.244.39 from kube-system started at 2023-03-01 22:00:52 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 01:47:07.205: INFO: 	Container pause ready: true, restart count 0
Mar  2 01:47:07.205: INFO: ibmcloud-block-storage-driver-qqdnc from kube-system started at 2023-03-01 22:01:08 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 01:47:07.205: INFO: tuned-7g6rx from openshift-cluster-node-tuning-operator started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container tuned ready: true, restart count 0
Mar  2 01:47:07.205: INFO: dns-default-rsb27 from openshift-dns started at 2023-03-02 00:51:04 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container dns ready: true, restart count 0
Mar  2 01:47:07.205: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.205: INFO: node-resolver-678rp from openshift-dns started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 01:47:07.205: INFO: node-ca-2dknm from openshift-image-registry started at 2023-03-01 22:02:13 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 01:47:07.205: INFO: ingress-canary-kfdpg from openshift-ingress-canary started at 2023-03-02 00:51:05 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 01:47:07.205: INFO: openshift-kube-proxy-tzkhx from openshift-kube-proxy started at 2023-03-01 22:00:56 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 01:47:07.205: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.205: INFO: redhat-marketplace-ffkpx from openshift-marketplace started at 2023-03-02 00:52:46 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 01:47:07.205: INFO: node-exporter-9hccw from openshift-monitoring started at 2023-03-01 22:06:02 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.205: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 01:47:07.205: INFO: multus-additional-cni-plugins-h44x5 from openshift-multus started at 2023-03-01 22:00:56 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 01:47:07.205: INFO: multus-admission-controller-mtp8j from openshift-multus started at 2023-03-02 00:51:09 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.205: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 01:47:07.205: INFO: multus-tcchz from openshift-multus started at 2023-03-01 22:00:56 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 01:47:07.205: INFO: network-metrics-daemon-865xx from openshift-multus started at 2023-03-01 22:00:56 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.205: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 01:47:07.205: INFO: network-check-target-2gwsp from openshift-network-diagnostics started at 2023-03-01 22:00:56 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 01:47:07.205: INFO: collect-profiles-27961995-x9dnq from openshift-operator-lifecycle-manager started at 2023-03-02 01:15:00 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 01:47:07.205: INFO: collect-profiles-27962010-s8z59 from openshift-operator-lifecycle-manager started at 2023-03-02 01:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 01:47:07.205: INFO: collect-profiles-27962025-tqwrx from openshift-operator-lifecycle-manager started at 2023-03-02 01:45:00 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 01:47:07.205: INFO: sonobuoy-systemd-logs-daemon-set-f72afd02755d449d-vlqt6 from sonobuoy started at 2023-03-02 00:42:02 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.205: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 01:47:07.205: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 01:47:07.205: INFO: 
Logging pods the apiserver thinks is on node 10.123.244.41 before test
Mar  2 01:47:07.252: INFO: calico-kube-controllers-589f57b7f-tm9tz from calico-system started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.252: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  2 01:47:07.252: INFO: calico-node-6flhq from calico-system started at 2023-03-01 22:01:04 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.252: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 01:47:07.252: INFO: managed-storage-validation-webhooks-85f57b66cf-79986 from ibm-odf-validation-webhook started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.252: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Mar  2 01:47:07.252: INFO: managed-storage-validation-webhooks-85f57b66cf-mqr5f from ibm-odf-validation-webhook started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.252: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 01:47:07.252: INFO: managed-storage-validation-webhooks-85f57b66cf-pwdx9 from ibm-odf-validation-webhook started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.252: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 01:47:07.252: INFO: ibm-cloud-provider-ip-149-81-197-212-d64d8f476-bks7g from ibm-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.252: INFO: 	Container ibm-cloud-provider-ip-149-81-197-212 ready: true, restart count 0
Mar  2 01:47:07.252: INFO: ibm-file-plugin-5fdd985647-mldf5 from kube-system started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.252: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar  2 01:47:07.253: INFO: ibm-keepalived-watcher-mjldf from kube-system started at 2023-03-01 22:00:13 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.253: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 01:47:07.253: INFO: ibm-master-proxy-static-10.123.244.41 from kube-system started at 2023-03-01 22:00:11 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.253: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 01:47:07.253: INFO: 	Container pause ready: true, restart count 0
Mar  2 01:47:07.253: INFO: ibm-storage-metrics-agent-6b696986d4-jwmxc from kube-system started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.253: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Mar  2 01:47:07.253: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 01:47:07.253: INFO: ibm-storage-watcher-64987648df-vl847 from kube-system started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.253: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar  2 01:47:07.253: INFO: ibmcloud-block-storage-driver-h5vmt from kube-system started at 2023-03-01 22:00:19 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.253: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 01:47:07.253: INFO: ibmcloud-block-storage-plugin-7cc7c95d6d-tw99v from kube-system started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.253: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar  2 01:47:07.253: INFO: cluster-node-tuning-operator-74fbdd5d47-scz8r from openshift-cluster-node-tuning-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.253: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Mar  2 01:47:07.253: INFO: tuned-bqzdt from openshift-cluster-node-tuning-operator started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.253: INFO: 	Container tuned ready: true, restart count 0
Mar  2 01:47:07.253: INFO: cluster-samples-operator-86c59f694d-klcrd from openshift-cluster-samples-operator started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.253: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Mar  2 01:47:07.253: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Mar  2 01:47:07.253: INFO: cluster-storage-operator-5b746f999f-vv6t5 from openshift-cluster-storage-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.253: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Mar  2 01:47:07.253: INFO: csi-snapshot-controller-8576577866-4lrgg from openshift-cluster-storage-operator started at 2023-03-01 22:01:48 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.253: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 01:47:07.253: INFO: csi-snapshot-controller-8576577866-8cdgr from openshift-cluster-storage-operator started at 2023-03-01 22:01:48 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.253: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 01:47:07.253: INFO: csi-snapshot-controller-operator-75849b8ccf-nmpqq from openshift-cluster-storage-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.253: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Mar  2 01:47:07.253: INFO: csi-snapshot-webhook-786787f645-wxmqk from openshift-cluster-storage-operator started at 2023-03-01 22:01:45 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.253: INFO: 	Container webhook ready: true, restart count 0
Mar  2 01:47:07.253: INFO: csi-snapshot-webhook-786787f645-zbq67 from openshift-cluster-storage-operator started at 2023-03-01 22:01:45 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.253: INFO: 	Container webhook ready: true, restart count 0
Mar  2 01:47:07.253: INFO: console-operator-67666f4bd-45tdk from openshift-console-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.253: INFO: 	Container console-operator ready: true, restart count 1
Mar  2 01:47:07.253: INFO: console-5549dcfdd9-gf5fk from openshift-console started at 2023-03-01 22:08:36 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.253: INFO: 	Container console ready: true, restart count 0
Mar  2 01:47:07.253: INFO: downloads-5b4f566bc5-hlvxl from openshift-console started at 2023-03-01 22:02:08 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.254: INFO: 	Container download-server ready: true, restart count 0
Mar  2 01:47:07.254: INFO: dns-operator-6b74679c6d-xqlfl from openshift-dns-operator started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.254: INFO: 	Container dns-operator ready: true, restart count 0
Mar  2 01:47:07.254: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.254: INFO: dns-default-c4wx7 from openshift-dns started at 2023-03-01 22:05:24 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.254: INFO: 	Container dns ready: true, restart count 0
Mar  2 01:47:07.254: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.254: INFO: node-resolver-dgx88 from openshift-dns started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.254: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 01:47:07.254: INFO: cluster-image-registry-operator-5846b9d966-sj4bv from openshift-image-registry started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.254: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Mar  2 01:47:07.254: INFO: node-ca-9vk8c from openshift-image-registry started at 2023-03-01 22:02:13 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.254: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 01:47:07.254: INFO: ingress-canary-5gtz6 from openshift-ingress-canary started at 2023-03-01 22:02:18 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.254: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 01:47:07.254: INFO: ingress-operator-5fdf7c4bb-8jgbn from openshift-ingress-operator started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.254: INFO: 	Container ingress-operator ready: true, restart count 0
Mar  2 01:47:07.254: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.254: INFO: router-default-555588874-8ffhk from openshift-ingress started at 2023-03-01 22:27:19 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.254: INFO: 	Container router ready: true, restart count 0
Mar  2 01:47:07.254: INFO: insights-operator-5449cdb4b9-7xt8p from openshift-insights started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.254: INFO: 	Container insights-operator ready: true, restart count 1
Mar  2 01:47:07.254: INFO: openshift-kube-proxy-j4ff5 from openshift-kube-proxy started at 2023-03-01 22:00:51 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.254: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 01:47:07.254: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.254: INFO: kube-storage-version-migrator-operator-66dd7b9865-9lrlp from openshift-kube-storage-version-migrator-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.254: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Mar  2 01:47:07.254: INFO: migrator-767b585794-kkngg from openshift-kube-storage-version-migrator started at 2023-03-01 22:02:08 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.254: INFO: 	Container migrator ready: true, restart count 0
Mar  2 01:47:07.254: INFO: marketplace-operator-d98c65969-9hhnp from openshift-marketplace started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.254: INFO: 	Container marketplace-operator ready: true, restart count 0
Mar  2 01:47:07.254: INFO: redhat-operators-mxdgk from openshift-marketplace started at 2023-03-02 00:50:46 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.254: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 01:47:07.254: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-03-01 22:06:09 +0000 UTC (6 container statuses recorded)
Mar  2 01:47:07.254: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 01:47:07.254: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 01:47:07.254: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 01:47:07.254: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.254: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Mar  2 01:47:07.254: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 01:47:07.254: INFO: cluster-monitoring-operator-8584844f6f-8bkqg from openshift-monitoring started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.254: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Mar  2 01:47:07.254: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.254: INFO: node-exporter-z8kbd from openshift-monitoring started at 2023-03-01 22:06:02 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.254: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.255: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 01:47:07.255: INFO: prometheus-adapter-646cdcc6f7-vb8tx from openshift-monitoring started at 2023-03-01 22:07:14 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.255: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 01:47:07.255: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-03-01 22:06:25 +0000 UTC (6 container statuses recorded)
Mar  2 01:47:07.255: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 01:47:07.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.255: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 01:47:07.255: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 01:47:07.255: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 01:47:07.255: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 01:47:07.255: INFO: prometheus-operator-admission-webhook-78c85948cd-7dxdr from openshift-monitoring started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.255: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Mar  2 01:47:07.255: INFO: telemeter-client-7b8cf9859c-5jxpn from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (3 container statuses recorded)
Mar  2 01:47:07.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.255: INFO: 	Container reload ready: true, restart count 0
Mar  2 01:47:07.255: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 01:47:07.255: INFO: thanos-querier-697f97f554-wxl4x from openshift-monitoring started at 2023-03-01 22:06:15 +0000 UTC (6 container statuses recorded)
Mar  2 01:47:07.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.255: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Mar  2 01:47:07.255: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 01:47:07.255: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 01:47:07.255: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 01:47:07.255: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 01:47:07.255: INFO: multus-additional-cni-plugins-mj5tl from openshift-multus started at 2023-03-01 22:00:46 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.255: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 01:47:07.255: INFO: multus-admission-controller-tbdnc from openshift-multus started at 2023-03-01 22:05:24 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.255: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 01:47:07.255: INFO: multus-dktpl from openshift-multus started at 2023-03-01 22:00:46 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.255: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 01:47:07.255: INFO: network-metrics-daemon-zl9qv from openshift-multus started at 2023-03-01 22:00:47 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.255: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.255: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 01:47:07.255: INFO: network-check-source-59cbfb554c-7kpmg from openshift-network-diagnostics started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.255: INFO: 	Container check-endpoints ready: true, restart count 0
Mar  2 01:47:07.255: INFO: network-check-target-nvxbw from openshift-network-diagnostics started at 2023-03-01 22:00:54 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.255: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 01:47:07.255: INFO: network-operator-7b666665c4-c8w4p from openshift-network-operator started at 2023-03-01 22:00:23 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.255: INFO: 	Container network-operator ready: true, restart count 1
Mar  2 01:47:07.255: INFO: catalog-operator-5b55d75455-w899d from openshift-operator-lifecycle-manager started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.255: INFO: 	Container catalog-operator ready: true, restart count 0
Mar  2 01:47:07.255: INFO: olm-operator-d785dddf9-v54mf from openshift-operator-lifecycle-manager started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.255: INFO: 	Container olm-operator ready: true, restart count 0
Mar  2 01:47:07.255: INFO: package-server-manager-5c8469f7c6-prjpw from openshift-operator-lifecycle-manager started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.255: INFO: 	Container package-server-manager ready: true, restart count 0
Mar  2 01:47:07.255: INFO: packageserver-bb46bbdb7-dr4v9 from openshift-operator-lifecycle-manager started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.255: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 01:47:07.256: INFO: metrics-5d9985b7b6-zpwc2 from openshift-roks-metrics started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.256: INFO: 	Container metrics ready: true, restart count 2
Mar  2 01:47:07.256: INFO: push-gateway-6fcf467c7d-8fxh8 from openshift-roks-metrics started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.256: INFO: 	Container push-gateway ready: true, restart count 0
Mar  2 01:47:07.256: INFO: service-ca-operator-865b774c95-b82zk from openshift-service-ca-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.256: INFO: 	Container service-ca-operator ready: true, restart count 1
Mar  2 01:47:07.256: INFO: service-ca-5995968f9b-25cgt from openshift-service-ca started at 2023-03-01 22:01:47 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.256: INFO: 	Container service-ca-controller ready: true, restart count 0
Mar  2 01:47:07.256: INFO: sonobuoy-systemd-logs-daemon-set-f72afd02755d449d-kddl5 from sonobuoy started at 2023-03-02 00:42:02 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.256: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 01:47:07.256: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 01:47:07.256: INFO: tigera-operator-f58c87f48-c9x9k from tigera-operator started at 2023-03-01 22:00:23 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.256: INFO: 	Container tigera-operator ready: true, restart count 1
Mar  2 01:47:07.256: INFO: 
Logging pods the apiserver thinks is on node 10.123.244.50 before test
Mar  2 01:47:07.330: INFO: calico-node-drfb9 from calico-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.331: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 01:47:07.331: INFO: calico-typha-db9579c55-7mslq from calico-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.331: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 01:47:07.331: INFO: ibm-cloud-provider-ip-149-81-197-212-d64d8f476-rq24c from ibm-system started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.331: INFO: 	Container ibm-cloud-provider-ip-149-81-197-212 ready: true, restart count 0
Mar  2 01:47:07.331: INFO: ibm-keepalived-watcher-bmgn9 from kube-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.332: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 01:47:07.332: INFO: ibm-master-proxy-static-10.123.244.50 from kube-system started at 2023-03-01 22:03:27 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.333: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 01:47:07.333: INFO: 	Container pause ready: true, restart count 0
Mar  2 01:47:07.333: INFO: ibmcloud-block-storage-driver-qm9pz from kube-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.333: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 01:47:07.333: INFO: vpn-5bbdb546b8-9w74v from kube-system started at 2023-03-01 22:19:58 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.333: INFO: 	Container vpn ready: true, restart count 0
Mar  2 01:47:07.333: INFO: tuned-6k6k4 from openshift-cluster-node-tuning-operator started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.334: INFO: 	Container tuned ready: true, restart count 0
Mar  2 01:47:07.334: INFO: console-5549dcfdd9-w5zs4 from openshift-console started at 2023-03-01 22:08:10 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.334: INFO: 	Container console ready: true, restart count 0
Mar  2 01:47:07.334: INFO: downloads-5b4f566bc5-wpgn5 from openshift-console started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.334: INFO: 	Container download-server ready: true, restart count 1
Mar  2 01:47:07.334: INFO: dns-default-q4vfw from openshift-dns started at 2023-03-01 22:08:04 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.334: INFO: 	Container dns ready: true, restart count 0
Mar  2 01:47:07.334: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.335: INFO: node-resolver-r6ktd from openshift-dns started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.335: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 01:47:07.335: INFO: image-pruner-27961920-bg4sx from openshift-image-registry started at 2023-03-02 00:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.335: INFO: 	Container image-pruner ready: false, restart count 0
Mar  2 01:47:07.335: INFO: image-registry-86657c9d77-wtb4h from openshift-image-registry started at 2023-03-01 22:48:17 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.335: INFO: 	Container registry ready: true, restart count 0
Mar  2 01:47:07.335: INFO: node-ca-cjkk7 from openshift-image-registry started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.335: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 01:47:07.336: INFO: registry-pvc-permissions-6jq4m from openshift-image-registry started at 2023-03-01 22:48:17 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.336: INFO: 	Container pvc-permissions ready: false, restart count 0
Mar  2 01:47:07.336: INFO: ingress-canary-nkd4d from openshift-ingress-canary started at 2023-03-01 22:08:04 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.336: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 01:47:07.336: INFO: router-default-555588874-w7r9r from openshift-ingress started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.336: INFO: 	Container router ready: true, restart count 0
Mar  2 01:47:07.336: INFO: openshift-kube-proxy-hnz52 from openshift-kube-proxy started at 2023-03-01 22:05:24 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.336: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 01:47:07.337: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.337: INFO: certified-operators-rdltn from openshift-marketplace started at 2023-03-02 00:50:44 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.337: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 01:47:07.337: INFO: community-operators-zrr7v from openshift-marketplace started at 2023-03-02 00:50:44 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.337: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 01:47:07.337: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-03-02 00:50:47 +0000 UTC (6 container statuses recorded)
Mar  2 01:47:07.337: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 01:47:07.337: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 01:47:07.337: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 01:47:07.337: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.338: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Mar  2 01:47:07.338: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 01:47:07.338: INFO: kube-state-metrics-84464bb775-hgv8b from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (3 container statuses recorded)
Mar  2 01:47:07.338: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 01:47:07.338: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 01:47:07.338: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 01:47:07.338: INFO: node-exporter-mlwcg from openshift-monitoring started at 2023-03-01 22:06:02 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.338: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.338: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 01:47:07.339: INFO: openshift-state-metrics-6486d6b674-g7xp7 from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (3 container statuses recorded)
Mar  2 01:47:07.339: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 01:47:07.339: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 01:47:07.339: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar  2 01:47:07.339: INFO: prometheus-adapter-646cdcc6f7-9m9ls from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.339: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 01:47:07.339: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-03-02 00:50:46 +0000 UTC (6 container statuses recorded)
Mar  2 01:47:07.339: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 01:47:07.340: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.340: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 01:47:07.340: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 01:47:07.340: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 01:47:07.340: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 01:47:07.340: INFO: prometheus-operator-5d8ddb4f-cs9vq from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.340: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.340: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 01:47:07.340: INFO: prometheus-operator-admission-webhook-78c85948cd-kxbf8 from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.341: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Mar  2 01:47:07.341: INFO: thanos-querier-697f97f554-49n2l from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (6 container statuses recorded)
Mar  2 01:47:07.341: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.341: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Mar  2 01:47:07.341: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 01:47:07.341: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 01:47:07.341: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 01:47:07.341: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 01:47:07.342: INFO: multus-78s9d from openshift-multus started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.342: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 01:47:07.342: INFO: multus-additional-cni-plugins-x67r8 from openshift-multus started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.342: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 01:47:07.342: INFO: multus-admission-controller-8d7qw from openshift-multus started at 2023-03-01 22:08:04 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.342: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.342: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 01:47:07.343: INFO: network-metrics-daemon-9m6hn from openshift-multus started at 2023-03-01 22:05:25 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.343: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:47:07.343: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 01:47:07.343: INFO: network-check-target-ckmqs from openshift-network-diagnostics started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.343: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 01:47:07.343: INFO: packageserver-bb46bbdb7-2kq2k from openshift-operator-lifecycle-manager started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.343: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 01:47:07.343: INFO: sonobuoy from sonobuoy started at 2023-03-02 00:41:53 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.344: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 01:47:07.344: INFO: sonobuoy-e2e-job-370728edd32948a1 from sonobuoy started at 2023-03-02 00:42:02 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.344: INFO: 	Container e2e ready: true, restart count 0
Mar  2 01:47:07.344: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 01:47:07.344: INFO: sonobuoy-systemd-logs-daemon-set-f72afd02755d449d-pjpm6 from sonobuoy started at 2023-03-02 00:42:02 +0000 UTC (2 container statuses recorded)
Mar  2 01:47:07.344: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 01:47:07.344: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 01:47:07.345: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-03-01 22:20:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:47:07.345: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
STEP: verifying the node has the label node 10.123.244.39
STEP: verifying the node has the label node 10.123.244.41
STEP: verifying the node has the label node 10.123.244.50
Mar  2 01:47:07.636: INFO: Pod calico-kube-controllers-589f57b7f-tm9tz requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.636: INFO: Pod calico-node-6flhq requesting resource cpu=250m on Node 10.123.244.41
Mar  2 01:47:07.636: INFO: Pod calico-node-drfb9 requesting resource cpu=250m on Node 10.123.244.50
Mar  2 01:47:07.636: INFO: Pod calico-node-vkllz requesting resource cpu=250m on Node 10.123.244.39
Mar  2 01:47:07.636: INFO: Pod calico-typha-db9579c55-7mslq requesting resource cpu=250m on Node 10.123.244.50
Mar  2 01:47:07.636: INFO: Pod calico-typha-db9579c55-nnjdl requesting resource cpu=250m on Node 10.123.244.39
Mar  2 01:47:07.636: INFO: Pod managed-storage-validation-webhooks-85f57b66cf-79986 requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.636: INFO: Pod managed-storage-validation-webhooks-85f57b66cf-mqr5f requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.636: INFO: Pod managed-storage-validation-webhooks-85f57b66cf-pwdx9 requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod ibm-cloud-provider-ip-149-81-197-212-d64d8f476-bks7g requesting resource cpu=5m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod ibm-cloud-provider-ip-149-81-197-212-d64d8f476-rq24c requesting resource cpu=5m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod ibm-file-plugin-5fdd985647-mldf5 requesting resource cpu=50m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod ibm-keepalived-watcher-bmgn9 requesting resource cpu=5m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod ibm-keepalived-watcher-hf2lb requesting resource cpu=5m on Node 10.123.244.39
Mar  2 01:47:07.637: INFO: Pod ibm-keepalived-watcher-mjldf requesting resource cpu=5m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod ibm-master-proxy-static-10.123.244.39 requesting resource cpu=26m on Node 10.123.244.39
Mar  2 01:47:07.637: INFO: Pod ibm-master-proxy-static-10.123.244.41 requesting resource cpu=26m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod ibm-master-proxy-static-10.123.244.50 requesting resource cpu=26m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod ibm-storage-metrics-agent-6b696986d4-jwmxc requesting resource cpu=60m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod ibm-storage-watcher-64987648df-vl847 requesting resource cpu=50m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod ibmcloud-block-storage-driver-h5vmt requesting resource cpu=50m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod ibmcloud-block-storage-driver-qm9pz requesting resource cpu=50m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod ibmcloud-block-storage-driver-qqdnc requesting resource cpu=50m on Node 10.123.244.39
Mar  2 01:47:07.637: INFO: Pod ibmcloud-block-storage-plugin-7cc7c95d6d-tw99v requesting resource cpu=50m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod vpn-5bbdb546b8-9w74v requesting resource cpu=5m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod cluster-node-tuning-operator-74fbdd5d47-scz8r requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod tuned-6k6k4 requesting resource cpu=10m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod tuned-7g6rx requesting resource cpu=10m on Node 10.123.244.39
Mar  2 01:47:07.637: INFO: Pod tuned-bqzdt requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod cluster-samples-operator-86c59f694d-klcrd requesting resource cpu=20m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod cluster-storage-operator-5b746f999f-vv6t5 requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod csi-snapshot-controller-8576577866-4lrgg requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod csi-snapshot-controller-8576577866-8cdgr requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod csi-snapshot-controller-operator-75849b8ccf-nmpqq requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod csi-snapshot-webhook-786787f645-wxmqk requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod csi-snapshot-webhook-786787f645-zbq67 requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod console-operator-67666f4bd-45tdk requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod console-5549dcfdd9-gf5fk requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod console-5549dcfdd9-w5zs4 requesting resource cpu=10m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod downloads-5b4f566bc5-hlvxl requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod downloads-5b4f566bc5-wpgn5 requesting resource cpu=10m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod dns-operator-6b74679c6d-xqlfl requesting resource cpu=20m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod dns-default-c4wx7 requesting resource cpu=60m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod dns-default-q4vfw requesting resource cpu=60m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod dns-default-rsb27 requesting resource cpu=60m on Node 10.123.244.39
Mar  2 01:47:07.637: INFO: Pod node-resolver-678rp requesting resource cpu=5m on Node 10.123.244.39
Mar  2 01:47:07.637: INFO: Pod node-resolver-dgx88 requesting resource cpu=5m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod node-resolver-r6ktd requesting resource cpu=5m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod cluster-image-registry-operator-5846b9d966-sj4bv requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod image-registry-86657c9d77-wtb4h requesting resource cpu=100m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod node-ca-2dknm requesting resource cpu=10m on Node 10.123.244.39
Mar  2 01:47:07.637: INFO: Pod node-ca-9vk8c requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod node-ca-cjkk7 requesting resource cpu=10m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod ingress-canary-5gtz6 requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod ingress-canary-kfdpg requesting resource cpu=10m on Node 10.123.244.39
Mar  2 01:47:07.637: INFO: Pod ingress-canary-nkd4d requesting resource cpu=10m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod ingress-operator-5fdf7c4bb-8jgbn requesting resource cpu=20m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod router-default-555588874-8ffhk requesting resource cpu=100m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod router-default-555588874-w7r9r requesting resource cpu=100m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod insights-operator-5449cdb4b9-7xt8p requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod openshift-kube-proxy-hnz52 requesting resource cpu=110m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod openshift-kube-proxy-j4ff5 requesting resource cpu=110m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod openshift-kube-proxy-tzkhx requesting resource cpu=110m on Node 10.123.244.39
Mar  2 01:47:07.637: INFO: Pod kube-storage-version-migrator-operator-66dd7b9865-9lrlp requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod migrator-767b585794-kkngg requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod certified-operators-rdltn requesting resource cpu=10m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod community-operators-zrr7v requesting resource cpu=10m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod marketplace-operator-d98c65969-9hhnp requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod redhat-marketplace-ffkpx requesting resource cpu=10m on Node 10.123.244.39
Mar  2 01:47:07.637: INFO: Pod redhat-operators-mxdgk requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod cluster-monitoring-operator-8584844f6f-8bkqg requesting resource cpu=11m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod kube-state-metrics-84464bb775-hgv8b requesting resource cpu=4m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod node-exporter-9hccw requesting resource cpu=9m on Node 10.123.244.39
Mar  2 01:47:07.637: INFO: Pod node-exporter-mlwcg requesting resource cpu=9m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod node-exporter-z8kbd requesting resource cpu=9m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod openshift-state-metrics-6486d6b674-g7xp7 requesting resource cpu=3m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod prometheus-adapter-646cdcc6f7-9m9ls requesting resource cpu=1m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod prometheus-adapter-646cdcc6f7-vb8tx requesting resource cpu=1m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod prometheus-operator-5d8ddb4f-cs9vq requesting resource cpu=6m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod prometheus-operator-admission-webhook-78c85948cd-7dxdr requesting resource cpu=5m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod prometheus-operator-admission-webhook-78c85948cd-kxbf8 requesting resource cpu=5m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod telemeter-client-7b8cf9859c-5jxpn requesting resource cpu=3m on Node 10.123.244.41
Mar  2 01:47:07.637: INFO: Pod thanos-querier-697f97f554-49n2l requesting resource cpu=15m on Node 10.123.244.50
Mar  2 01:47:07.637: INFO: Pod thanos-querier-697f97f554-wxl4x requesting resource cpu=15m on Node 10.123.244.41
Mar  2 01:47:07.638: INFO: Pod multus-78s9d requesting resource cpu=10m on Node 10.123.244.50
Mar  2 01:47:07.638: INFO: Pod multus-additional-cni-plugins-h44x5 requesting resource cpu=10m on Node 10.123.244.39
Mar  2 01:47:07.638: INFO: Pod multus-additional-cni-plugins-mj5tl requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.638: INFO: Pod multus-additional-cni-plugins-x67r8 requesting resource cpu=10m on Node 10.123.244.50
Mar  2 01:47:07.638: INFO: Pod multus-admission-controller-8d7qw requesting resource cpu=20m on Node 10.123.244.50
Mar  2 01:47:07.638: INFO: Pod multus-admission-controller-mtp8j requesting resource cpu=20m on Node 10.123.244.39
Mar  2 01:47:07.638: INFO: Pod multus-admission-controller-tbdnc requesting resource cpu=20m on Node 10.123.244.41
Mar  2 01:47:07.638: INFO: Pod multus-dktpl requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.638: INFO: Pod multus-tcchz requesting resource cpu=10m on Node 10.123.244.39
Mar  2 01:47:07.638: INFO: Pod network-metrics-daemon-865xx requesting resource cpu=20m on Node 10.123.244.39
Mar  2 01:47:07.638: INFO: Pod network-metrics-daemon-9m6hn requesting resource cpu=20m on Node 10.123.244.50
Mar  2 01:47:07.638: INFO: Pod network-metrics-daemon-zl9qv requesting resource cpu=20m on Node 10.123.244.41
Mar  2 01:47:07.638: INFO: Pod network-check-source-59cbfb554c-7kpmg requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.638: INFO: Pod network-check-target-2gwsp requesting resource cpu=10m on Node 10.123.244.39
Mar  2 01:47:07.638: INFO: Pod network-check-target-ckmqs requesting resource cpu=10m on Node 10.123.244.50
Mar  2 01:47:07.638: INFO: Pod network-check-target-nvxbw requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.638: INFO: Pod network-operator-7b666665c4-c8w4p requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.638: INFO: Pod catalog-operator-5b55d75455-w899d requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.638: INFO: Pod olm-operator-d785dddf9-v54mf requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.638: INFO: Pod package-server-manager-5c8469f7c6-prjpw requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.638: INFO: Pod packageserver-bb46bbdb7-2kq2k requesting resource cpu=10m on Node 10.123.244.50
Mar  2 01:47:07.638: INFO: Pod packageserver-bb46bbdb7-dr4v9 requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.638: INFO: Pod metrics-5d9985b7b6-zpwc2 requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.638: INFO: Pod push-gateway-6fcf467c7d-8fxh8 requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.638: INFO: Pod service-ca-operator-865b774c95-b82zk requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.638: INFO: Pod service-ca-5995968f9b-25cgt requesting resource cpu=10m on Node 10.123.244.41
Mar  2 01:47:07.638: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.123.244.50
Mar  2 01:47:07.638: INFO: Pod sonobuoy-e2e-job-370728edd32948a1 requesting resource cpu=0m on Node 10.123.244.50
Mar  2 01:47:07.638: INFO: Pod sonobuoy-systemd-logs-daemon-set-f72afd02755d449d-kddl5 requesting resource cpu=0m on Node 10.123.244.41
Mar  2 01:47:07.638: INFO: Pod sonobuoy-systemd-logs-daemon-set-f72afd02755d449d-pjpm6 requesting resource cpu=0m on Node 10.123.244.50
Mar  2 01:47:07.638: INFO: Pod sonobuoy-systemd-logs-daemon-set-f72afd02755d449d-vlqt6 requesting resource cpu=0m on Node 10.123.244.39
Mar  2 01:47:07.638: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.123.244.50
Mar  2 01:47:07.638: INFO: Pod tigera-operator-f58c87f48-c9x9k requesting resource cpu=100m on Node 10.123.244.41
STEP: Starting Pods to consume most of the cluster CPU.
Mar  2 01:47:07.638: INFO: Creating a pod which consumes cpu=1680m on Node 10.123.244.41
Mar  2 01:47:07.665: INFO: Creating a pod which consumes cpu=1866m on Node 10.123.244.50
Mar  2 01:47:07.733: INFO: Creating a pod which consumes cpu=2124m on Node 10.123.244.39
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53cfd93d-1061-4886-ae1f-6ec5ba0251d3.174876ee7691a6b1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-765/filler-pod-53cfd93d-1061-4886-ae1f-6ec5ba0251d3 to 10.123.244.50 by kube-scheduler-749c95db5d-t4qhz]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53cfd93d-1061-4886-ae1f-6ec5ba0251d3.174876eeccf27403], Reason = [AddedInterface], Message = [Add eth0 [172.30.0.254/32] from k8s-pod-network]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53cfd93d-1061-4886-ae1f-6ec5ba0251d3.174876eeefc19e76], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53cfd93d-1061-4886-ae1f-6ec5ba0251d3.174876ef08b9ee73], Reason = [Created], Message = [Created container filler-pod-53cfd93d-1061-4886-ae1f-6ec5ba0251d3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53cfd93d-1061-4886-ae1f-6ec5ba0251d3.174876ef0d5618e9], Reason = [Started], Message = [Started container filler-pod-53cfd93d-1061-4886-ae1f-6ec5ba0251d3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-63fe9719-c234-4277-a6b6-d630e12ead3e.174876ee799c280c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-765/filler-pod-63fe9719-c234-4277-a6b6-d630e12ead3e to 10.123.244.39 by kube-scheduler-749c95db5d-t4qhz]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-63fe9719-c234-4277-a6b6-d630e12ead3e.174876eeab527048], Reason = [AddedInterface], Message = [Add eth0 [172.30.88.203/32] from k8s-pod-network]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-63fe9719-c234-4277-a6b6-d630e12ead3e.174876eeb79ed211], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-63fe9719-c234-4277-a6b6-d630e12ead3e.174876eec319919d], Reason = [Created], Message = [Created container filler-pod-63fe9719-c234-4277-a6b6-d630e12ead3e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-63fe9719-c234-4277-a6b6-d630e12ead3e.174876eec4b5c594], Reason = [Started], Message = [Started container filler-pod-63fe9719-c234-4277-a6b6-d630e12ead3e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f7418d01-38a6-4769-a60c-2925d821a9c7.174876ee7348fa3e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-765/filler-pod-f7418d01-38a6-4769-a60c-2925d821a9c7 to 10.123.244.41 by kube-scheduler-749c95db5d-t4qhz]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f7418d01-38a6-4769-a60c-2925d821a9c7.174876eeaf64b744], Reason = [AddedInterface], Message = [Add eth0 [172.30.54.249/32] from k8s-pod-network]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f7418d01-38a6-4769-a60c-2925d821a9c7.174876eebece5446], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f7418d01-38a6-4769-a60c-2925d821a9c7.174876eec9c08060], Reason = [Created], Message = [Created container filler-pod-f7418d01-38a6-4769-a60c-2925d821a9c7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f7418d01-38a6-4769-a60c-2925d821a9c7.174876eecb8392ce], Reason = [Started], Message = [Started container filler-pod-f7418d01-38a6-4769-a60c-2925d821a9c7]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.174876ef6c580a68], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.]
STEP: removing the label node off the node 10.123.244.39
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.123.244.41
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.123.244.50
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Mar  2 01:47:13.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-765" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:6.159 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":356,"completed":223,"skipped":4617,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:47:13.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:47:13.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-9478 version'
Mar  2 01:47:13.408: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Mar  2 01:47:13.408: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.6\", GitCommit:\"b39bf148cd654599a52e867485c02c4f9d28b312\", GitTreeState:\"clean\", BuildDate:\"2022-09-21T13:19:24Z\", GoVersion:\"go1.18.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.4\nServer Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.6+263df15\", GitCommit:\"eddac29feb4bb46b99fb570999324e582d761a66\", GitTreeState:\"clean\", BuildDate:\"2023-01-23T21:00:20Z\", GoVersion:\"go1.18.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar  2 01:47:13.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9478" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":356,"completed":224,"skipped":4649,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:47:13.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-60824ec1-979f-4889-bd58-2c8606e15e46
STEP: Creating a pod to test consume configMaps
Mar  2 01:47:13.750: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-18f5e48c-947b-4587-b8c9-9cc2688dcbd1" in namespace "projected-5228" to be "Succeeded or Failed"
W0302 01:47:13.750231      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:47:13.762: INFO: Pod "pod-projected-configmaps-18f5e48c-947b-4587-b8c9-9cc2688dcbd1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.24219ms
Mar  2 01:47:15.772: INFO: Pod "pod-projected-configmaps-18f5e48c-947b-4587-b8c9-9cc2688dcbd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02144586s
Mar  2 01:47:17.783: INFO: Pod "pod-projected-configmaps-18f5e48c-947b-4587-b8c9-9cc2688dcbd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032550016s
STEP: Saw pod success
Mar  2 01:47:17.783: INFO: Pod "pod-projected-configmaps-18f5e48c-947b-4587-b8c9-9cc2688dcbd1" satisfied condition "Succeeded or Failed"
Mar  2 01:47:17.791: INFO: Trying to get logs from node 10.123.244.39 pod pod-projected-configmaps-18f5e48c-947b-4587-b8c9-9cc2688dcbd1 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 01:47:17.860: INFO: Waiting for pod pod-projected-configmaps-18f5e48c-947b-4587-b8c9-9cc2688dcbd1 to disappear
Mar  2 01:47:17.868: INFO: Pod pod-projected-configmaps-18f5e48c-947b-4587-b8c9-9cc2688dcbd1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Mar  2 01:47:17.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5228" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":225,"skipped":4664,"failed":0}
SSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:47:17.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name secret-emptykey-test-d4efe1b8-1c77-4991-9a96-df11d69a4c66
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Mar  2 01:47:18.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1422" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":356,"completed":226,"skipped":4668,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:47:18.068: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar  2 01:47:19.121: INFO: Pod name wrapped-volume-race-f08ddd6d-3afd-4866-b82d-ecfd09c9aa62: Found 0 pods out of 5
Mar  2 01:47:24.143: INFO: Pod name wrapped-volume-race-f08ddd6d-3afd-4866-b82d-ecfd09c9aa62: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f08ddd6d-3afd-4866-b82d-ecfd09c9aa62 in namespace emptydir-wrapper-2548, will wait for the garbage collector to delete the pods
Mar  2 01:47:24.273: INFO: Deleting ReplicationController wrapped-volume-race-f08ddd6d-3afd-4866-b82d-ecfd09c9aa62 took: 29.102491ms
Mar  2 01:47:24.374: INFO: Terminating ReplicationController wrapped-volume-race-f08ddd6d-3afd-4866-b82d-ecfd09c9aa62 pods took: 100.736085ms
STEP: Creating RC which spawns configmap-volume pods
Mar  2 01:47:28.220: INFO: Pod name wrapped-volume-race-12ae1700-370b-4b6c-9009-05deae1fbbeb: Found 0 pods out of 5
Mar  2 01:47:33.255: INFO: Pod name wrapped-volume-race-12ae1700-370b-4b6c-9009-05deae1fbbeb: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-12ae1700-370b-4b6c-9009-05deae1fbbeb in namespace emptydir-wrapper-2548, will wait for the garbage collector to delete the pods
Mar  2 01:47:35.488: INFO: Deleting ReplicationController wrapped-volume-race-12ae1700-370b-4b6c-9009-05deae1fbbeb took: 23.094456ms
Mar  2 01:47:35.690: INFO: Terminating ReplicationController wrapped-volume-race-12ae1700-370b-4b6c-9009-05deae1fbbeb pods took: 201.983776ms
STEP: Creating RC which spawns configmap-volume pods
Mar  2 01:47:41.547: INFO: Pod name wrapped-volume-race-6a11a479-5a4e-4fba-8852-0e892e38f09c: Found 0 pods out of 5
Mar  2 01:47:46.662: INFO: Pod name wrapped-volume-race-6a11a479-5a4e-4fba-8852-0e892e38f09c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6a11a479-5a4e-4fba-8852-0e892e38f09c in namespace emptydir-wrapper-2548, will wait for the garbage collector to delete the pods
Mar  2 01:47:50.846: INFO: Deleting ReplicationController wrapped-volume-race-6a11a479-5a4e-4fba-8852-0e892e38f09c took: 17.067618ms
Mar  2 01:47:51.047: INFO: Terminating ReplicationController wrapped-volume-race-6a11a479-5a4e-4fba-8852-0e892e38f09c pods took: 201.476156ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Mar  2 01:47:57.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2548" for this suite.

• [SLOW TEST:39.614 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":356,"completed":227,"skipped":4678,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:47:57.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Mar  2 01:47:57.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3686" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":356,"completed":228,"skipped":4694,"failed":0}
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:47:57.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Mar  2 01:47:58.070: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:48:00.081: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Mar  2 01:48:00.131: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:48:02.142: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Mar  2 01:48:02.178: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 01:48:02.189: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 01:48:04.190: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 01:48:04.199: INFO: Pod pod-with-prestop-exec-hook still exists
Mar  2 01:48:06.190: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar  2 01:48:06.199: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Mar  2 01:48:06.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-498" for this suite.

• [SLOW TEST:8.484 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":356,"completed":229,"skipped":4695,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:48:06.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar  2 01:48:06.501: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57957f3f-cc53-4780-a75c-3ce579017440" in namespace "projected-6582" to be "Succeeded or Failed"
Mar  2 01:48:06.511: INFO: Pod "downwardapi-volume-57957f3f-cc53-4780-a75c-3ce579017440": Phase="Pending", Reason="", readiness=false. Elapsed: 9.992624ms
Mar  2 01:48:08.527: INFO: Pod "downwardapi-volume-57957f3f-cc53-4780-a75c-3ce579017440": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026011177s
Mar  2 01:48:10.536: INFO: Pod "downwardapi-volume-57957f3f-cc53-4780-a75c-3ce579017440": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035110121s
Mar  2 01:48:12.548: INFO: Pod "downwardapi-volume-57957f3f-cc53-4780-a75c-3ce579017440": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047197869s
STEP: Saw pod success
Mar  2 01:48:12.548: INFO: Pod "downwardapi-volume-57957f3f-cc53-4780-a75c-3ce579017440" satisfied condition "Succeeded or Failed"
Mar  2 01:48:12.560: INFO: Trying to get logs from node 10.123.244.39 pod downwardapi-volume-57957f3f-cc53-4780-a75c-3ce579017440 container client-container: <nil>
STEP: delete the pod
Mar  2 01:48:12.608: INFO: Waiting for pod downwardapi-volume-57957f3f-cc53-4780-a75c-3ce579017440 to disappear
Mar  2 01:48:12.614: INFO: Pod downwardapi-volume-57957f3f-cc53-4780-a75c-3ce579017440 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar  2 01:48:12.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6582" for this suite.

• [SLOW TEST:6.286 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":230,"skipped":4702,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:48:12.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-a9b132f2-c802-45ff-9d9d-f1292e4789f0
STEP: Creating a pod to test consume configMaps
Mar  2 01:48:12.852: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bc8f695-abbc-405f-90b4-c514a0f743eb" in namespace "configmap-4077" to be "Succeeded or Failed"
Mar  2 01:48:12.860: INFO: Pod "pod-configmaps-0bc8f695-abbc-405f-90b4-c514a0f743eb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.738271ms
Mar  2 01:48:14.906: INFO: Pod "pod-configmaps-0bc8f695-abbc-405f-90b4-c514a0f743eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053527602s
Mar  2 01:48:16.915: INFO: Pod "pod-configmaps-0bc8f695-abbc-405f-90b4-c514a0f743eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062462286s
Mar  2 01:48:18.924: INFO: Pod "pod-configmaps-0bc8f695-abbc-405f-90b4-c514a0f743eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071259233s
STEP: Saw pod success
Mar  2 01:48:18.924: INFO: Pod "pod-configmaps-0bc8f695-abbc-405f-90b4-c514a0f743eb" satisfied condition "Succeeded or Failed"
Mar  2 01:48:18.931: INFO: Trying to get logs from node 10.123.244.39 pod pod-configmaps-0bc8f695-abbc-405f-90b4-c514a0f743eb container agnhost-container: <nil>
STEP: delete the pod
Mar  2 01:48:18.969: INFO: Waiting for pod pod-configmaps-0bc8f695-abbc-405f-90b4-c514a0f743eb to disappear
Mar  2 01:48:18.975: INFO: Pod pod-configmaps-0bc8f695-abbc-405f-90b4-c514a0f743eb no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar  2 01:48:18.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4077" for this suite.

• [SLOW TEST:6.351 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":231,"skipped":4713,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:48:19.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  test/e2e/framework/framework.go:652
STEP: Request ServerVersion
STEP: Confirm major version
Mar  2 01:48:19.083: INFO: Major version: 1
STEP: Confirm minor version
Mar  2 01:48:19.083: INFO: cleanMinorVersion: 24
Mar  2 01:48:19.083: INFO: Minor version: 24
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:188
Mar  2 01:48:19.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-5701" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":356,"completed":232,"skipped":4720,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:48:19.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Service
STEP: watching for the Service to be added
Mar  2 01:48:19.404: INFO: Found Service test-service-79srr in namespace services-9833 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Mar  2 01:48:19.404: INFO: Service test-service-79srr created
STEP: Getting /status
Mar  2 01:48:19.415: INFO: Service test-service-79srr has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Mar  2 01:48:19.441: INFO: observed Service test-service-79srr in namespace services-9833 with annotations: map[] & LoadBalancer: {[]}
Mar  2 01:48:19.441: INFO: Found Service test-service-79srr in namespace services-9833 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Mar  2 01:48:19.442: INFO: Service test-service-79srr has service status patched
STEP: updating the ServiceStatus
Mar  2 01:48:19.467: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Mar  2 01:48:19.479: INFO: Observed Service test-service-79srr in namespace services-9833 with annotations: map[] & Conditions: {[]}
Mar  2 01:48:19.479: INFO: Observed event: &Service{ObjectMeta:{test-service-79srr  services-9833  183c14f7-8017-489f-8e25-e2a28bad4b91 124888 0 2023-03-02 01:48:19 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2023-03-02 01:48:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-02 01:48:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.216.52,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.216.52],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Mar  2 01:48:19.480: INFO: Found Service test-service-79srr in namespace services-9833 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar  2 01:48:19.480: INFO: Service test-service-79srr has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Mar  2 01:48:19.535: INFO: observed Service test-service-79srr in namespace services-9833 with labels: map[test-service-static:true]
Mar  2 01:48:19.535: INFO: observed Service test-service-79srr in namespace services-9833 with labels: map[test-service-static:true]
Mar  2 01:48:19.535: INFO: observed Service test-service-79srr in namespace services-9833 with labels: map[test-service-static:true]
Mar  2 01:48:19.535: INFO: Found Service test-service-79srr in namespace services-9833 with labels: map[test-service:patched test-service-static:true]
Mar  2 01:48:19.535: INFO: Service test-service-79srr patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Mar  2 01:48:19.590: INFO: Observed event: ADDED
Mar  2 01:48:19.590: INFO: Observed event: MODIFIED
Mar  2 01:48:19.590: INFO: Observed event: MODIFIED
Mar  2 01:48:19.590: INFO: Observed event: MODIFIED
Mar  2 01:48:19.591: INFO: Found Service test-service-79srr in namespace services-9833 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Mar  2 01:48:19.591: INFO: Service test-service-79srr deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar  2 01:48:19.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9833" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":356,"completed":233,"skipped":4727,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:48:19.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar  2 01:48:19.729: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b52cceb8-b388-492f-b18e-16f558a7c661" in namespace "downward-api-1446" to be "Succeeded or Failed"
Mar  2 01:48:19.748: INFO: Pod "downwardapi-volume-b52cceb8-b388-492f-b18e-16f558a7c661": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006101ms
Mar  2 01:48:21.830: INFO: Pod "downwardapi-volume-b52cceb8-b388-492f-b18e-16f558a7c661": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09223907s
Mar  2 01:48:24.000: INFO: Pod "downwardapi-volume-b52cceb8-b388-492f-b18e-16f558a7c661": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.261643175s
STEP: Saw pod success
Mar  2 01:48:24.000: INFO: Pod "downwardapi-volume-b52cceb8-b388-492f-b18e-16f558a7c661" satisfied condition "Succeeded or Failed"
Mar  2 01:48:24.037: INFO: Trying to get logs from node 10.123.244.39 pod downwardapi-volume-b52cceb8-b388-492f-b18e-16f558a7c661 container client-container: <nil>
STEP: delete the pod
Mar  2 01:48:24.176: INFO: Waiting for pod downwardapi-volume-b52cceb8-b388-492f-b18e-16f558a7c661 to disappear
Mar  2 01:48:24.343: INFO: Pod downwardapi-volume-b52cceb8-b388-492f-b18e-16f558a7c661 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar  2 01:48:24.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1446" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":234,"skipped":4733,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:48:24.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 01:48:26.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:48:26.066: INFO: Node 10.123.244.39 is running 0 daemon pod, expected 1
Mar  2 01:48:27.102: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:48:27.102: INFO: Node 10.123.244.39 is running 0 daemon pod, expected 1
Mar  2 01:48:28.099: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  2 01:48:28.099: INFO: Node 10.123.244.39 is running 0 daemon pod, expected 1
Mar  2 01:48:29.117: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 01:48:29.117: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar  2 01:48:29.166: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 01:48:29.167: INFO: Node 10.123.244.41 is running 0 daemon pod, expected 1
Mar  2 01:48:30.192: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 01:48:30.192: INFO: Node 10.123.244.41 is running 0 daemon pod, expected 1
Mar  2 01:48:31.192: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 01:48:31.193: INFO: Node 10.123.244.41 is running 0 daemon pod, expected 1
Mar  2 01:48:32.201: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 01:48:32.201: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7423, will wait for the garbage collector to delete the pods
Mar  2 01:48:32.284: INFO: Deleting DaemonSet.extensions daemon-set took: 10.76526ms
Mar  2 01:48:32.385: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.657756ms
Mar  2 01:48:35.795: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 01:48:35.795: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  2 01:48:35.802: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"125241"},"items":null}

Mar  2 01:48:35.809: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"125241"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Mar  2 01:48:35.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7423" for this suite.

• [SLOW TEST:11.270 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":356,"completed":235,"skipped":4747,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:48:35.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Mar  2 01:49:16.166: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0302 01:49:16.166103      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Mar  2 01:49:16.166: INFO: Deleting pod "simpletest.rc-26q2m" in namespace "gc-4261"
Mar  2 01:49:16.189: INFO: Deleting pod "simpletest.rc-28s2d" in namespace "gc-4261"
Mar  2 01:49:16.213: INFO: Deleting pod "simpletest.rc-2dshv" in namespace "gc-4261"
Mar  2 01:49:16.273: INFO: Deleting pod "simpletest.rc-2kfn4" in namespace "gc-4261"
Mar  2 01:49:16.307: INFO: Deleting pod "simpletest.rc-2n22z" in namespace "gc-4261"
Mar  2 01:49:16.341: INFO: Deleting pod "simpletest.rc-48r8j" in namespace "gc-4261"
Mar  2 01:49:16.376: INFO: Deleting pod "simpletest.rc-4dj4r" in namespace "gc-4261"
Mar  2 01:49:16.409: INFO: Deleting pod "simpletest.rc-4g4g4" in namespace "gc-4261"
Mar  2 01:49:16.485: INFO: Deleting pod "simpletest.rc-4nx96" in namespace "gc-4261"
Mar  2 01:49:16.514: INFO: Deleting pod "simpletest.rc-4px9d" in namespace "gc-4261"
Mar  2 01:49:16.541: INFO: Deleting pod "simpletest.rc-4r22s" in namespace "gc-4261"
Mar  2 01:49:16.562: INFO: Deleting pod "simpletest.rc-4s7wx" in namespace "gc-4261"
Mar  2 01:49:16.585: INFO: Deleting pod "simpletest.rc-4t4dk" in namespace "gc-4261"
Mar  2 01:49:16.611: INFO: Deleting pod "simpletest.rc-4vvxm" in namespace "gc-4261"
Mar  2 01:49:16.639: INFO: Deleting pod "simpletest.rc-4xkd7" in namespace "gc-4261"
Mar  2 01:49:16.667: INFO: Deleting pod "simpletest.rc-56ndp" in namespace "gc-4261"
Mar  2 01:49:16.711: INFO: Deleting pod "simpletest.rc-5sbhv" in namespace "gc-4261"
Mar  2 01:49:16.749: INFO: Deleting pod "simpletest.rc-5t6mt" in namespace "gc-4261"
Mar  2 01:49:16.924: INFO: Deleting pod "simpletest.rc-6bpv8" in namespace "gc-4261"
Mar  2 01:49:16.949: INFO: Deleting pod "simpletest.rc-6dx94" in namespace "gc-4261"
Mar  2 01:49:17.103: INFO: Deleting pod "simpletest.rc-6ltfk" in namespace "gc-4261"
Mar  2 01:49:17.146: INFO: Deleting pod "simpletest.rc-6qw27" in namespace "gc-4261"
Mar  2 01:49:17.271: INFO: Deleting pod "simpletest.rc-6rts4" in namespace "gc-4261"
Mar  2 01:49:17.419: INFO: Deleting pod "simpletest.rc-6vpz6" in namespace "gc-4261"
Mar  2 01:49:17.819: INFO: Deleting pod "simpletest.rc-7jftz" in namespace "gc-4261"
Mar  2 01:49:18.044: INFO: Deleting pod "simpletest.rc-7nskm" in namespace "gc-4261"
Mar  2 01:49:18.085: INFO: Deleting pod "simpletest.rc-7vg42" in namespace "gc-4261"
Mar  2 01:49:18.195: INFO: Deleting pod "simpletest.rc-82spb" in namespace "gc-4261"
Mar  2 01:49:18.275: INFO: Deleting pod "simpletest.rc-87fvs" in namespace "gc-4261"
Mar  2 01:49:18.311: INFO: Deleting pod "simpletest.rc-8b977" in namespace "gc-4261"
Mar  2 01:49:18.347: INFO: Deleting pod "simpletest.rc-8kvks" in namespace "gc-4261"
Mar  2 01:49:18.396: INFO: Deleting pod "simpletest.rc-8xj4x" in namespace "gc-4261"
Mar  2 01:49:18.451: INFO: Deleting pod "simpletest.rc-8z7n5" in namespace "gc-4261"
Mar  2 01:49:18.535: INFO: Deleting pod "simpletest.rc-8z8w9" in namespace "gc-4261"
Mar  2 01:49:18.588: INFO: Deleting pod "simpletest.rc-9b56s" in namespace "gc-4261"
Mar  2 01:49:18.658: INFO: Deleting pod "simpletest.rc-9jwtx" in namespace "gc-4261"
Mar  2 01:49:18.847: INFO: Deleting pod "simpletest.rc-9xzrw" in namespace "gc-4261"
Mar  2 01:49:18.893: INFO: Deleting pod "simpletest.rc-b8b7b" in namespace "gc-4261"
Mar  2 01:49:18.918: INFO: Deleting pod "simpletest.rc-bbrbm" in namespace "gc-4261"
Mar  2 01:49:19.062: INFO: Deleting pod "simpletest.rc-bgvh9" in namespace "gc-4261"
Mar  2 01:49:19.097: INFO: Deleting pod "simpletest.rc-bqf95" in namespace "gc-4261"
Mar  2 01:49:19.220: INFO: Deleting pod "simpletest.rc-bsgsl" in namespace "gc-4261"
Mar  2 01:49:19.272: INFO: Deleting pod "simpletest.rc-c48qw" in namespace "gc-4261"
Mar  2 01:49:19.298: INFO: Deleting pod "simpletest.rc-c4zgx" in namespace "gc-4261"
Mar  2 01:49:19.376: INFO: Deleting pod "simpletest.rc-clstb" in namespace "gc-4261"
Mar  2 01:49:19.497: INFO: Deleting pod "simpletest.rc-ctmt9" in namespace "gc-4261"
Mar  2 01:49:19.527: INFO: Deleting pod "simpletest.rc-czbxc" in namespace "gc-4261"
Mar  2 01:49:19.561: INFO: Deleting pod "simpletest.rc-czpnz" in namespace "gc-4261"
Mar  2 01:49:19.627: INFO: Deleting pod "simpletest.rc-d44n2" in namespace "gc-4261"
Mar  2 01:49:19.667: INFO: Deleting pod "simpletest.rc-d5v5s" in namespace "gc-4261"
Mar  2 01:49:19.689: INFO: Deleting pod "simpletest.rc-dlp4x" in namespace "gc-4261"
Mar  2 01:49:19.738: INFO: Deleting pod "simpletest.rc-dp7qq" in namespace "gc-4261"
Mar  2 01:49:19.850: INFO: Deleting pod "simpletest.rc-f9vlr" in namespace "gc-4261"
Mar  2 01:49:19.905: INFO: Deleting pod "simpletest.rc-f9zcm" in namespace "gc-4261"
Mar  2 01:49:19.935: INFO: Deleting pod "simpletest.rc-gl79z" in namespace "gc-4261"
Mar  2 01:49:20.018: INFO: Deleting pod "simpletest.rc-h5brt" in namespace "gc-4261"
Mar  2 01:49:20.147: INFO: Deleting pod "simpletest.rc-hb5bt" in namespace "gc-4261"
Mar  2 01:49:20.356: INFO: Deleting pod "simpletest.rc-hzls8" in namespace "gc-4261"
Mar  2 01:49:20.410: INFO: Deleting pod "simpletest.rc-jd72s" in namespace "gc-4261"
Mar  2 01:49:20.445: INFO: Deleting pod "simpletest.rc-knv6v" in namespace "gc-4261"
Mar  2 01:49:20.475: INFO: Deleting pod "simpletest.rc-kzzk5" in namespace "gc-4261"
Mar  2 01:49:20.909: INFO: Deleting pod "simpletest.rc-lr5p4" in namespace "gc-4261"
Mar  2 01:49:20.960: INFO: Deleting pod "simpletest.rc-lvjnd" in namespace "gc-4261"
Mar  2 01:49:20.990: INFO: Deleting pod "simpletest.rc-m5rpp" in namespace "gc-4261"
Mar  2 01:49:21.037: INFO: Deleting pod "simpletest.rc-m8fw7" in namespace "gc-4261"
Mar  2 01:49:21.074: INFO: Deleting pod "simpletest.rc-mbbx9" in namespace "gc-4261"
Mar  2 01:49:21.122: INFO: Deleting pod "simpletest.rc-mdjkd" in namespace "gc-4261"
Mar  2 01:49:21.190: INFO: Deleting pod "simpletest.rc-mrrrh" in namespace "gc-4261"
Mar  2 01:49:21.217: INFO: Deleting pod "simpletest.rc-mwcwk" in namespace "gc-4261"
Mar  2 01:49:21.292: INFO: Deleting pod "simpletest.rc-mxpth" in namespace "gc-4261"
Mar  2 01:49:21.321: INFO: Deleting pod "simpletest.rc-n4gsc" in namespace "gc-4261"
Mar  2 01:49:21.353: INFO: Deleting pod "simpletest.rc-n55z6" in namespace "gc-4261"
Mar  2 01:49:21.385: INFO: Deleting pod "simpletest.rc-n768z" in namespace "gc-4261"
Mar  2 01:49:21.415: INFO: Deleting pod "simpletest.rc-ns8tn" in namespace "gc-4261"
Mar  2 01:49:21.441: INFO: Deleting pod "simpletest.rc-nswx4" in namespace "gc-4261"
Mar  2 01:49:21.467: INFO: Deleting pod "simpletest.rc-pkjqw" in namespace "gc-4261"
Mar  2 01:49:21.497: INFO: Deleting pod "simpletest.rc-pn55w" in namespace "gc-4261"
Mar  2 01:49:21.545: INFO: Deleting pod "simpletest.rc-pnp4f" in namespace "gc-4261"
Mar  2 01:49:21.690: INFO: Deleting pod "simpletest.rc-ptbvq" in namespace "gc-4261"
Mar  2 01:49:21.864: INFO: Deleting pod "simpletest.rc-q8f5g" in namespace "gc-4261"
Mar  2 01:49:21.917: INFO: Deleting pod "simpletest.rc-r7ms2" in namespace "gc-4261"
Mar  2 01:49:22.074: INFO: Deleting pod "simpletest.rc-rbkqt" in namespace "gc-4261"
Mar  2 01:49:22.101: INFO: Deleting pod "simpletest.rc-s8lt4" in namespace "gc-4261"
Mar  2 01:49:22.128: INFO: Deleting pod "simpletest.rc-srjqt" in namespace "gc-4261"
Mar  2 01:49:22.261: INFO: Deleting pod "simpletest.rc-t49z7" in namespace "gc-4261"
Mar  2 01:49:22.453: INFO: Deleting pod "simpletest.rc-t5sq9" in namespace "gc-4261"
Mar  2 01:49:22.687: INFO: Deleting pod "simpletest.rc-txpw5" in namespace "gc-4261"
Mar  2 01:49:22.755: INFO: Deleting pod "simpletest.rc-v6spv" in namespace "gc-4261"
Mar  2 01:49:22.811: INFO: Deleting pod "simpletest.rc-vgft2" in namespace "gc-4261"
Mar  2 01:49:22.852: INFO: Deleting pod "simpletest.rc-vp6wr" in namespace "gc-4261"
Mar  2 01:49:22.903: INFO: Deleting pod "simpletest.rc-vw2zh" in namespace "gc-4261"
Mar  2 01:49:22.928: INFO: Deleting pod "simpletest.rc-w2pjb" in namespace "gc-4261"
Mar  2 01:49:22.964: INFO: Deleting pod "simpletest.rc-w627s" in namespace "gc-4261"
Mar  2 01:49:23.017: INFO: Deleting pod "simpletest.rc-x77l4" in namespace "gc-4261"
Mar  2 01:49:23.043: INFO: Deleting pod "simpletest.rc-z2cpq" in namespace "gc-4261"
Mar  2 01:49:23.089: INFO: Deleting pod "simpletest.rc-zklhx" in namespace "gc-4261"
Mar  2 01:49:23.108: INFO: Deleting pod "simpletest.rc-zlnff" in namespace "gc-4261"
Mar  2 01:49:23.125: INFO: Deleting pod "simpletest.rc-zn64w" in namespace "gc-4261"
Mar  2 01:49:23.161: INFO: Deleting pod "simpletest.rc-zq994" in namespace "gc-4261"
Mar  2 01:49:23.186: INFO: Deleting pod "simpletest.rc-zxm8h" in namespace "gc-4261"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Mar  2 01:49:23.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4261" for this suite.

• [SLOW TEST:47.359 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":356,"completed":236,"skipped":4752,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:49:23.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5153
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-5153
Mar  2 01:49:23.556: INFO: Found 0 stateful pods, waiting for 1
Mar  2 01:49:33.586: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Mar  2 01:49:33.635: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Mar  2 01:49:33.664: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Mar  2 01:49:33.669: INFO: Observed &StatefulSet event: ADDED
Mar  2 01:49:33.669: INFO: Found Statefulset ss in namespace statefulset-5153 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 01:49:33.669: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Mar  2 01:49:33.669: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar  2 01:49:33.692: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Mar  2 01:49:33.704: INFO: Observed &StatefulSet event: ADDED
Mar  2 01:49:33.704: INFO: Observed Statefulset ss in namespace statefulset-5153 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar  2 01:49:33.704: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 01:49:33.704: INFO: Deleting all statefulset in ns statefulset-5153
Mar  2 01:49:33.711: INFO: Scaling statefulset ss to 0
Mar  2 01:49:43.763: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 01:49:43.770: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Mar  2 01:49:43.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5153" for this suite.

• [SLOW TEST:20.618 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":356,"completed":237,"skipped":4759,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:49:43.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Mar  2 01:49:44.011: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Mar  2 01:49:50.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-250" for this suite.

• [SLOW TEST:6.481 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":356,"completed":238,"skipped":4781,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:49:50.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  2 01:49:50.490: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 01:49:50.531: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 01:49:50.577: INFO: 
Logging pods the apiserver thinks is on node 10.123.244.39 before test
Mar  2 01:49:50.610: INFO: calico-node-vkllz from calico-system started at 2023-03-01 22:01:04 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.610: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 01:49:50.610: INFO: calico-typha-db9579c55-nnjdl from calico-system started at 2023-03-01 22:01:04 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.610: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 01:49:50.610: INFO: pod-init-94c77a1b-f1fb-4ef1-a1f7-019cabc07ee5 from init-container-250 started at 2023-03-02 01:49:44 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.610: INFO: 	Container run1 ready: false, restart count 0
Mar  2 01:49:50.610: INFO: ibm-keepalived-watcher-hf2lb from kube-system started at 2023-03-01 22:00:56 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.610: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 01:49:50.610: INFO: ibm-master-proxy-static-10.123.244.39 from kube-system started at 2023-03-01 22:00:52 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.610: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 01:49:50.610: INFO: 	Container pause ready: true, restart count 0
Mar  2 01:49:50.610: INFO: ibmcloud-block-storage-driver-qqdnc from kube-system started at 2023-03-01 22:01:08 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.610: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 01:49:50.610: INFO: tuned-7g6rx from openshift-cluster-node-tuning-operator started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.610: INFO: 	Container tuned ready: true, restart count 0
Mar  2 01:49:50.610: INFO: dns-default-rsb27 from openshift-dns started at 2023-03-02 00:51:04 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.610: INFO: 	Container dns ready: true, restart count 0
Mar  2 01:49:50.610: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.610: INFO: node-resolver-678rp from openshift-dns started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.610: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 01:49:50.610: INFO: node-ca-2dknm from openshift-image-registry started at 2023-03-01 22:02:13 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.610: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 01:49:50.610: INFO: ingress-canary-kfdpg from openshift-ingress-canary started at 2023-03-02 00:51:05 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.610: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 01:49:50.610: INFO: openshift-kube-proxy-tzkhx from openshift-kube-proxy started at 2023-03-01 22:00:56 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.610: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 01:49:50.610: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.610: INFO: redhat-marketplace-ffkpx from openshift-marketplace started at 2023-03-02 00:52:46 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.610: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 01:49:50.610: INFO: node-exporter-9hccw from openshift-monitoring started at 2023-03-01 22:06:02 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.610: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.610: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 01:49:50.610: INFO: multus-additional-cni-plugins-h44x5 from openshift-multus started at 2023-03-01 22:00:56 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.610: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 01:49:50.610: INFO: multus-admission-controller-mtp8j from openshift-multus started at 2023-03-02 00:51:09 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.610: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.611: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 01:49:50.611: INFO: multus-tcchz from openshift-multus started at 2023-03-01 22:00:56 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.611: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 01:49:50.611: INFO: network-metrics-daemon-865xx from openshift-multus started at 2023-03-01 22:00:56 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.611: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.611: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 01:49:50.611: INFO: network-check-target-2gwsp from openshift-network-diagnostics started at 2023-03-01 22:00:56 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.611: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 01:49:50.611: INFO: collect-profiles-27961995-x9dnq from openshift-operator-lifecycle-manager started at 2023-03-02 01:15:00 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.611: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 01:49:50.611: INFO: collect-profiles-27962010-s8z59 from openshift-operator-lifecycle-manager started at 2023-03-02 01:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.611: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 01:49:50.611: INFO: collect-profiles-27962025-tqwrx from openshift-operator-lifecycle-manager started at 2023-03-02 01:45:00 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.611: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 01:49:50.611: INFO: sonobuoy-systemd-logs-daemon-set-f72afd02755d449d-vlqt6 from sonobuoy started at 2023-03-02 00:42:02 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.611: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 01:49:50.611: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 01:49:50.611: INFO: 
Logging pods the apiserver thinks is on node 10.123.244.41 before test
Mar  2 01:49:50.672: INFO: calico-kube-controllers-589f57b7f-tm9tz from calico-system started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  2 01:49:50.672: INFO: calico-node-6flhq from calico-system started at 2023-03-01 22:01:04 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 01:49:50.672: INFO: managed-storage-validation-webhooks-85f57b66cf-79986 from ibm-odf-validation-webhook started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Mar  2 01:49:50.672: INFO: managed-storage-validation-webhooks-85f57b66cf-mqr5f from ibm-odf-validation-webhook started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 01:49:50.672: INFO: managed-storage-validation-webhooks-85f57b66cf-pwdx9 from ibm-odf-validation-webhook started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 01:49:50.672: INFO: ibm-cloud-provider-ip-149-81-197-212-d64d8f476-bks7g from ibm-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container ibm-cloud-provider-ip-149-81-197-212 ready: true, restart count 0
Mar  2 01:49:50.672: INFO: ibm-file-plugin-5fdd985647-mldf5 from kube-system started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar  2 01:49:50.672: INFO: ibm-keepalived-watcher-mjldf from kube-system started at 2023-03-01 22:00:13 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 01:49:50.672: INFO: ibm-master-proxy-static-10.123.244.41 from kube-system started at 2023-03-01 22:00:11 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 01:49:50.672: INFO: 	Container pause ready: true, restart count 0
Mar  2 01:49:50.672: INFO: ibm-storage-metrics-agent-6b696986d4-jwmxc from kube-system started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Mar  2 01:49:50.672: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 01:49:50.672: INFO: ibm-storage-watcher-64987648df-vl847 from kube-system started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar  2 01:49:50.672: INFO: ibmcloud-block-storage-driver-h5vmt from kube-system started at 2023-03-01 22:00:19 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 01:49:50.672: INFO: ibmcloud-block-storage-plugin-7cc7c95d6d-tw99v from kube-system started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar  2 01:49:50.672: INFO: cluster-node-tuning-operator-74fbdd5d47-scz8r from openshift-cluster-node-tuning-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Mar  2 01:49:50.672: INFO: tuned-bqzdt from openshift-cluster-node-tuning-operator started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container tuned ready: true, restart count 0
Mar  2 01:49:50.672: INFO: cluster-samples-operator-86c59f694d-klcrd from openshift-cluster-samples-operator started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Mar  2 01:49:50.672: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Mar  2 01:49:50.672: INFO: cluster-storage-operator-5b746f999f-vv6t5 from openshift-cluster-storage-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Mar  2 01:49:50.672: INFO: csi-snapshot-controller-8576577866-4lrgg from openshift-cluster-storage-operator started at 2023-03-01 22:01:48 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 01:49:50.672: INFO: csi-snapshot-controller-8576577866-8cdgr from openshift-cluster-storage-operator started at 2023-03-01 22:01:48 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 01:49:50.672: INFO: csi-snapshot-controller-operator-75849b8ccf-nmpqq from openshift-cluster-storage-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Mar  2 01:49:50.672: INFO: csi-snapshot-webhook-786787f645-wxmqk from openshift-cluster-storage-operator started at 2023-03-01 22:01:45 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container webhook ready: true, restart count 0
Mar  2 01:49:50.672: INFO: csi-snapshot-webhook-786787f645-zbq67 from openshift-cluster-storage-operator started at 2023-03-01 22:01:45 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container webhook ready: true, restart count 0
Mar  2 01:49:50.672: INFO: console-operator-67666f4bd-45tdk from openshift-console-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container console-operator ready: true, restart count 1
Mar  2 01:49:50.672: INFO: console-5549dcfdd9-gf5fk from openshift-console started at 2023-03-01 22:08:36 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container console ready: true, restart count 0
Mar  2 01:49:50.672: INFO: downloads-5b4f566bc5-hlvxl from openshift-console started at 2023-03-01 22:02:08 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container download-server ready: true, restart count 0
Mar  2 01:49:50.672: INFO: dns-operator-6b74679c6d-xqlfl from openshift-dns-operator started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container dns-operator ready: true, restart count 0
Mar  2 01:49:50.672: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.672: INFO: dns-default-c4wx7 from openshift-dns started at 2023-03-01 22:05:24 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container dns ready: true, restart count 0
Mar  2 01:49:50.672: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.672: INFO: node-resolver-dgx88 from openshift-dns started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 01:49:50.672: INFO: cluster-image-registry-operator-5846b9d966-sj4bv from openshift-image-registry started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Mar  2 01:49:50.672: INFO: node-ca-9vk8c from openshift-image-registry started at 2023-03-01 22:02:13 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.672: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 01:49:50.673: INFO: ingress-canary-5gtz6 from openshift-ingress-canary started at 2023-03-01 22:02:18 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 01:49:50.673: INFO: ingress-operator-5fdf7c4bb-8jgbn from openshift-ingress-operator started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container ingress-operator ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.673: INFO: router-default-555588874-8ffhk from openshift-ingress started at 2023-03-01 22:27:19 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container router ready: true, restart count 0
Mar  2 01:49:50.673: INFO: insights-operator-5449cdb4b9-7xt8p from openshift-insights started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container insights-operator ready: true, restart count 1
Mar  2 01:49:50.673: INFO: openshift-kube-proxy-j4ff5 from openshift-kube-proxy started at 2023-03-01 22:00:51 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.673: INFO: kube-storage-version-migrator-operator-66dd7b9865-9lrlp from openshift-kube-storage-version-migrator-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Mar  2 01:49:50.673: INFO: migrator-767b585794-kkngg from openshift-kube-storage-version-migrator started at 2023-03-01 22:02:08 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container migrator ready: true, restart count 0
Mar  2 01:49:50.673: INFO: marketplace-operator-d98c65969-9hhnp from openshift-marketplace started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container marketplace-operator ready: true, restart count 0
Mar  2 01:49:50.673: INFO: redhat-operators-mxdgk from openshift-marketplace started at 2023-03-02 00:50:46 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 01:49:50.673: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-03-01 22:06:09 +0000 UTC (6 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 01:49:50.673: INFO: cluster-monitoring-operator-8584844f6f-8bkqg from openshift-monitoring started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.673: INFO: node-exporter-z8kbd from openshift-monitoring started at 2023-03-01 22:06:02 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 01:49:50.673: INFO: prometheus-adapter-646cdcc6f7-vb8tx from openshift-monitoring started at 2023-03-01 22:07:14 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 01:49:50.673: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-03-01 22:06:25 +0000 UTC (6 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 01:49:50.673: INFO: prometheus-operator-admission-webhook-78c85948cd-7dxdr from openshift-monitoring started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Mar  2 01:49:50.673: INFO: telemeter-client-7b8cf9859c-5jxpn from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (3 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container reload ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 01:49:50.673: INFO: thanos-querier-697f97f554-wxl4x from openshift-monitoring started at 2023-03-01 22:06:15 +0000 UTC (6 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 01:49:50.673: INFO: multus-additional-cni-plugins-mj5tl from openshift-multus started at 2023-03-01 22:00:46 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 01:49:50.673: INFO: multus-admission-controller-tbdnc from openshift-multus started at 2023-03-01 22:05:24 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 01:49:50.673: INFO: multus-dktpl from openshift-multus started at 2023-03-01 22:00:46 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 01:49:50.673: INFO: network-metrics-daemon-zl9qv from openshift-multus started at 2023-03-01 22:00:47 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.673: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 01:49:50.673: INFO: network-check-source-59cbfb554c-7kpmg from openshift-network-diagnostics started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container check-endpoints ready: true, restart count 0
Mar  2 01:49:50.673: INFO: network-check-target-nvxbw from openshift-network-diagnostics started at 2023-03-01 22:00:54 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 01:49:50.673: INFO: network-operator-7b666665c4-c8w4p from openshift-network-operator started at 2023-03-01 22:00:23 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container network-operator ready: true, restart count 1
Mar  2 01:49:50.673: INFO: catalog-operator-5b55d75455-w899d from openshift-operator-lifecycle-manager started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container catalog-operator ready: true, restart count 0
Mar  2 01:49:50.673: INFO: olm-operator-d785dddf9-v54mf from openshift-operator-lifecycle-manager started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container olm-operator ready: true, restart count 0
Mar  2 01:49:50.673: INFO: package-server-manager-5c8469f7c6-prjpw from openshift-operator-lifecycle-manager started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container package-server-manager ready: true, restart count 0
Mar  2 01:49:50.673: INFO: packageserver-bb46bbdb7-dr4v9 from openshift-operator-lifecycle-manager started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 01:49:50.673: INFO: metrics-5d9985b7b6-zpwc2 from openshift-roks-metrics started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container metrics ready: true, restart count 2
Mar  2 01:49:50.673: INFO: push-gateway-6fcf467c7d-8fxh8 from openshift-roks-metrics started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.673: INFO: 	Container push-gateway ready: true, restart count 0
Mar  2 01:49:50.673: INFO: service-ca-operator-865b774c95-b82zk from openshift-service-ca-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.674: INFO: 	Container service-ca-operator ready: true, restart count 1
Mar  2 01:49:50.674: INFO: service-ca-5995968f9b-25cgt from openshift-service-ca started at 2023-03-01 22:01:47 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.674: INFO: 	Container service-ca-controller ready: true, restart count 0
Mar  2 01:49:50.674: INFO: sonobuoy-systemd-logs-daemon-set-f72afd02755d449d-kddl5 from sonobuoy started at 2023-03-02 00:42:02 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.674: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 01:49:50.674: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 01:49:50.674: INFO: tigera-operator-f58c87f48-c9x9k from tigera-operator started at 2023-03-01 22:00:23 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.674: INFO: 	Container tigera-operator ready: true, restart count 1
Mar  2 01:49:50.674: INFO: 
Logging pods the apiserver thinks is on node 10.123.244.50 before test
Mar  2 01:49:50.712: INFO: calico-node-drfb9 from calico-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.712: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 01:49:50.712: INFO: calico-typha-db9579c55-7mslq from calico-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.712: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 01:49:50.713: INFO: ibm-cloud-provider-ip-149-81-197-212-d64d8f476-rq24c from ibm-system started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.713: INFO: 	Container ibm-cloud-provider-ip-149-81-197-212 ready: true, restart count 0
Mar  2 01:49:50.713: INFO: ibm-keepalived-watcher-bmgn9 from kube-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.713: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 01:49:50.713: INFO: ibm-master-proxy-static-10.123.244.50 from kube-system started at 2023-03-01 22:03:27 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.713: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 01:49:50.713: INFO: 	Container pause ready: true, restart count 0
Mar  2 01:49:50.714: INFO: ibmcloud-block-storage-driver-qm9pz from kube-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.714: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 01:49:50.714: INFO: vpn-5bbdb546b8-9w74v from kube-system started at 2023-03-01 22:19:58 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.714: INFO: 	Container vpn ready: true, restart count 0
Mar  2 01:49:50.714: INFO: tuned-6k6k4 from openshift-cluster-node-tuning-operator started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.715: INFO: 	Container tuned ready: true, restart count 0
Mar  2 01:49:50.715: INFO: console-5549dcfdd9-w5zs4 from openshift-console started at 2023-03-01 22:08:10 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.715: INFO: 	Container console ready: true, restart count 0
Mar  2 01:49:50.715: INFO: downloads-5b4f566bc5-wpgn5 from openshift-console started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.715: INFO: 	Container download-server ready: true, restart count 1
Mar  2 01:49:50.716: INFO: dns-default-q4vfw from openshift-dns started at 2023-03-01 22:08:04 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.716: INFO: 	Container dns ready: true, restart count 0
Mar  2 01:49:50.716: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.716: INFO: node-resolver-r6ktd from openshift-dns started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.716: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 01:49:50.716: INFO: image-pruner-27961920-bg4sx from openshift-image-registry started at 2023-03-02 00:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.716: INFO: 	Container image-pruner ready: false, restart count 0
Mar  2 01:49:50.717: INFO: image-registry-86657c9d77-wtb4h from openshift-image-registry started at 2023-03-01 22:48:17 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.717: INFO: 	Container registry ready: true, restart count 0
Mar  2 01:49:50.717: INFO: node-ca-cjkk7 from openshift-image-registry started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.717: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 01:49:50.717: INFO: registry-pvc-permissions-6jq4m from openshift-image-registry started at 2023-03-01 22:48:17 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.717: INFO: 	Container pvc-permissions ready: false, restart count 0
Mar  2 01:49:50.718: INFO: ingress-canary-nkd4d from openshift-ingress-canary started at 2023-03-01 22:08:04 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.718: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 01:49:50.718: INFO: router-default-555588874-w7r9r from openshift-ingress started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.718: INFO: 	Container router ready: true, restart count 0
Mar  2 01:49:50.718: INFO: openshift-kube-proxy-hnz52 from openshift-kube-proxy started at 2023-03-01 22:05:24 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.718: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 01:49:50.718: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.719: INFO: certified-operators-rdltn from openshift-marketplace started at 2023-03-02 00:50:44 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.719: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 01:49:50.719: INFO: community-operators-zrr7v from openshift-marketplace started at 2023-03-02 00:50:44 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.719: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 01:49:50.720: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-03-02 00:50:47 +0000 UTC (6 container statuses recorded)
Mar  2 01:49:50.720: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 01:49:50.720: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 01:49:50.720: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 01:49:50.720: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.720: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Mar  2 01:49:50.720: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 01:49:50.720: INFO: kube-state-metrics-84464bb775-hgv8b from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (3 container statuses recorded)
Mar  2 01:49:50.721: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 01:49:50.721: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 01:49:50.721: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 01:49:50.721: INFO: node-exporter-mlwcg from openshift-monitoring started at 2023-03-01 22:06:02 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.721: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.721: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 01:49:50.722: INFO: openshift-state-metrics-6486d6b674-g7xp7 from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (3 container statuses recorded)
Mar  2 01:49:50.722: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 01:49:50.722: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 01:49:50.722: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar  2 01:49:50.722: INFO: prometheus-adapter-646cdcc6f7-9m9ls from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.722: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 01:49:50.723: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-03-02 00:50:46 +0000 UTC (6 container statuses recorded)
Mar  2 01:49:50.723: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 01:49:50.723: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.723: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 01:49:50.723: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 01:49:50.723: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 01:49:50.724: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 01:49:50.724: INFO: prometheus-operator-5d8ddb4f-cs9vq from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.724: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.724: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 01:49:50.724: INFO: prometheus-operator-admission-webhook-78c85948cd-kxbf8 from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.724: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Mar  2 01:49:50.724: INFO: thanos-querier-697f97f554-49n2l from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (6 container statuses recorded)
Mar  2 01:49:50.725: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.725: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Mar  2 01:49:50.725: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 01:49:50.725: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 01:49:50.725: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 01:49:50.725: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 01:49:50.725: INFO: multus-78s9d from openshift-multus started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.726: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 01:49:50.726: INFO: multus-additional-cni-plugins-x67r8 from openshift-multus started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.726: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 01:49:50.726: INFO: multus-admission-controller-8d7qw from openshift-multus started at 2023-03-01 22:08:04 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.726: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.726: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 01:49:50.726: INFO: network-metrics-daemon-9m6hn from openshift-multus started at 2023-03-01 22:05:25 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.727: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 01:49:50.727: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 01:49:50.727: INFO: network-check-target-ckmqs from openshift-network-diagnostics started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.727: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 01:49:50.727: INFO: packageserver-bb46bbdb7-2kq2k from openshift-operator-lifecycle-manager started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.727: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 01:49:50.727: INFO: sonobuoy from sonobuoy started at 2023-03-02 00:41:53 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.728: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 01:49:50.728: INFO: sonobuoy-e2e-job-370728edd32948a1 from sonobuoy started at 2023-03-02 00:42:02 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.728: INFO: 	Container e2e ready: true, restart count 0
Mar  2 01:49:50.728: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 01:49:50.728: INFO: sonobuoy-systemd-logs-daemon-set-f72afd02755d449d-pjpm6 from sonobuoy started at 2023-03-02 00:42:02 +0000 UTC (2 container statuses recorded)
Mar  2 01:49:50.728: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 01:49:50.728: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 01:49:50.729: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-03-01 22:20:41 +0000 UTC (1 container statuses recorded)
Mar  2 01:49:50.729: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.174877147157bf05], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Mar  2 01:49:51.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2948" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":356,"completed":239,"skipped":4790,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:49:51.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-89797d37-577b-44ba-89e6-1aa9a4c89513
STEP: Creating a pod to test consume configMaps
Mar  2 01:49:52.101: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5e843960-9f02-441d-8228-5b85f5d4c608" in namespace "projected-2963" to be "Succeeded or Failed"
Mar  2 01:49:52.109: INFO: Pod "pod-projected-configmaps-5e843960-9f02-441d-8228-5b85f5d4c608": Phase="Pending", Reason="", readiness=false. Elapsed: 7.952711ms
Mar  2 01:49:54.119: INFO: Pod "pod-projected-configmaps-5e843960-9f02-441d-8228-5b85f5d4c608": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0173375s
Mar  2 01:49:56.131: INFO: Pod "pod-projected-configmaps-5e843960-9f02-441d-8228-5b85f5d4c608": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030076709s
Mar  2 01:49:58.147: INFO: Pod "pod-projected-configmaps-5e843960-9f02-441d-8228-5b85f5d4c608": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045326784s
STEP: Saw pod success
Mar  2 01:49:58.147: INFO: Pod "pod-projected-configmaps-5e843960-9f02-441d-8228-5b85f5d4c608" satisfied condition "Succeeded or Failed"
Mar  2 01:49:58.159: INFO: Trying to get logs from node 10.123.244.39 pod pod-projected-configmaps-5e843960-9f02-441d-8228-5b85f5d4c608 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 01:49:58.209: INFO: Waiting for pod pod-projected-configmaps-5e843960-9f02-441d-8228-5b85f5d4c608 to disappear
Mar  2 01:49:58.216: INFO: Pod pod-projected-configmaps-5e843960-9f02-441d-8228-5b85f5d4c608 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Mar  2 01:49:58.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2963" for this suite.

• [SLOW TEST:6.301 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":240,"skipped":4790,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:49:58.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Mar  2 01:49:58.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 01:51:37.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6993" for this suite.

• [SLOW TEST:99.586 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":356,"completed":241,"skipped":4819,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:51:37.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-projected-all-test-volume-f7a28ffc-1ea1-4cfa-ba47-c344c104ae72
STEP: Creating secret with name secret-projected-all-test-volume-23b7238d-d6de-4a2d-880d-10d73e027415
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar  2 01:51:38.064: INFO: Waiting up to 5m0s for pod "projected-volume-17e3f317-15f5-4fb6-bb05-95299fd86f38" in namespace "projected-2122" to be "Succeeded or Failed"
Mar  2 01:51:38.077: INFO: Pod "projected-volume-17e3f317-15f5-4fb6-bb05-95299fd86f38": Phase="Pending", Reason="", readiness=false. Elapsed: 13.081046ms
Mar  2 01:51:40.122: INFO: Pod "projected-volume-17e3f317-15f5-4fb6-bb05-95299fd86f38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057874217s
Mar  2 01:51:42.144: INFO: Pod "projected-volume-17e3f317-15f5-4fb6-bb05-95299fd86f38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079670612s
STEP: Saw pod success
Mar  2 01:51:42.144: INFO: Pod "projected-volume-17e3f317-15f5-4fb6-bb05-95299fd86f38" satisfied condition "Succeeded or Failed"
Mar  2 01:51:42.169: INFO: Trying to get logs from node 10.123.244.39 pod projected-volume-17e3f317-15f5-4fb6-bb05-95299fd86f38 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar  2 01:51:42.320: INFO: Waiting for pod projected-volume-17e3f317-15f5-4fb6-bb05-95299fd86f38 to disappear
Mar  2 01:51:42.360: INFO: Pod projected-volume-17e3f317-15f5-4fb6-bb05-95299fd86f38 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:188
Mar  2 01:51:42.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2122" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":356,"completed":242,"skipped":4855,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:51:42.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-8280/configmap-test-cad4426b-07f6-48e2-a197-22e6d8a763d8
STEP: Creating a pod to test consume configMaps
Mar  2 01:51:42.887: INFO: Waiting up to 5m0s for pod "pod-configmaps-2329bad8-c054-4f14-a92d-03f6571f6e8f" in namespace "configmap-8280" to be "Succeeded or Failed"
Mar  2 01:51:42.923: INFO: Pod "pod-configmaps-2329bad8-c054-4f14-a92d-03f6571f6e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 36.121853ms
Mar  2 01:51:44.965: INFO: Pod "pod-configmaps-2329bad8-c054-4f14-a92d-03f6571f6e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078176533s
Mar  2 01:51:47.012: INFO: Pod "pod-configmaps-2329bad8-c054-4f14-a92d-03f6571f6e8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.125168591s
STEP: Saw pod success
Mar  2 01:51:47.012: INFO: Pod "pod-configmaps-2329bad8-c054-4f14-a92d-03f6571f6e8f" satisfied condition "Succeeded or Failed"
Mar  2 01:51:47.034: INFO: Trying to get logs from node 10.123.244.39 pod pod-configmaps-2329bad8-c054-4f14-a92d-03f6571f6e8f container env-test: <nil>
STEP: delete the pod
Mar  2 01:51:47.128: INFO: Waiting for pod pod-configmaps-2329bad8-c054-4f14-a92d-03f6571f6e8f to disappear
Mar  2 01:51:47.145: INFO: Pod pod-configmaps-2329bad8-c054-4f14-a92d-03f6571f6e8f no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Mar  2 01:51:47.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8280" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":243,"skipped":4865,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:51:47.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar  2 01:52:04.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6327" for this suite.

• [SLOW TEST:17.531 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":356,"completed":244,"skipped":4884,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:52:04.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Mar  2 01:52:04.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9038" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":245,"skipped":4895,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:52:05.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Mar  2 01:52:11.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6451" for this suite.

• [SLOW TEST:6.404 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":356,"completed":246,"skipped":4927,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:52:11.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7633.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7633.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7633.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7633.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7633.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7633.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7633.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7633.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7633.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7633.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7633.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7633.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7633.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7633.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7633.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7633.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 01:52:15.818: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7633.svc.cluster.local from pod dns-7633/dns-test-8485d444-9f88-4ec3-83b1-b1de31143b02: the server could not find the requested resource (get pods dns-test-8485d444-9f88-4ec3-83b1-b1de31143b02)
Mar  2 01:52:15.839: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7633.svc.cluster.local from pod dns-7633/dns-test-8485d444-9f88-4ec3-83b1-b1de31143b02: the server could not find the requested resource (get pods dns-test-8485d444-9f88-4ec3-83b1-b1de31143b02)
Mar  2 01:52:15.853: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7633.svc.cluster.local from pod dns-7633/dns-test-8485d444-9f88-4ec3-83b1-b1de31143b02: the server could not find the requested resource (get pods dns-test-8485d444-9f88-4ec3-83b1-b1de31143b02)
Mar  2 01:52:15.939: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7633.svc.cluster.local from pod dns-7633/dns-test-8485d444-9f88-4ec3-83b1-b1de31143b02: the server could not find the requested resource (get pods dns-test-8485d444-9f88-4ec3-83b1-b1de31143b02)
Mar  2 01:52:16.049: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7633.svc.cluster.local from pod dns-7633/dns-test-8485d444-9f88-4ec3-83b1-b1de31143b02: the server could not find the requested resource (get pods dns-test-8485d444-9f88-4ec3-83b1-b1de31143b02)
Mar  2 01:52:16.075: INFO: Lookups using dns-7633/dns-test-8485d444-9f88-4ec3-83b1-b1de31143b02 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7633.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7633.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7633.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7633.svc.cluster.local jessie_udp@dns-test-service-2.dns-7633.svc.cluster.local]

Mar  2 01:52:21.531: INFO: DNS probes using dns-7633/dns-test-8485d444-9f88-4ec3-83b1-b1de31143b02 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Mar  2 01:52:21.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7633" for this suite.

• [SLOW TEST:10.282 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":356,"completed":247,"skipped":4934,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:52:21.737: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Mar  2 01:54:02.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1586" for this suite.

• [SLOW TEST:100.400 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":356,"completed":248,"skipped":4985,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:54:02.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  2 01:54:02.405: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 01:55:02.706: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:55:02.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Mar  2 01:55:05.073: INFO: found a healthy node: 10.123.244.39
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:55:17.513: INFO: pods created so far: [1 1 1]
Mar  2 01:55:17.513: INFO: length of pods created so far: 3
Mar  2 01:55:21.594: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:188
Mar  2 01:55:28.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1943" for this suite.
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Mar  2 01:55:28.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2809" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:86.760 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":356,"completed":249,"skipped":5023,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:55:28.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-1777
STEP: creating service affinity-nodeport-transition in namespace services-1777
STEP: creating replication controller affinity-nodeport-transition in namespace services-1777
I0302 01:55:29.088627      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-1777, replica count: 3
I0302 01:55:32.141206      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 01:55:35.143036      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 01:55:35.224: INFO: Creating new exec pod
Mar  2 01:55:40.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-1777 exec execpod-affinityj229g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Mar  2 01:55:41.516: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar  2 01:55:41.516: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:55:41.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-1777 exec execpod-affinityj229g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.37.79 80'
Mar  2 01:55:41.913: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.37.79 80\nConnection to 172.21.37.79 80 port [tcp/http] succeeded!\n"
Mar  2 01:55:41.913: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:55:41.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-1777 exec execpod-affinityj229g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.123.244.41 32054'
Mar  2 01:55:42.284: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.123.244.41 32054\nConnection to 10.123.244.41 32054 port [tcp/*] succeeded!\n"
Mar  2 01:55:42.284: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:55:42.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-1777 exec execpod-affinityj229g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.123.244.50 32054'
Mar  2 01:55:42.768: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.123.244.50 32054\nConnection to 10.123.244.50 32054 port [tcp/*] succeeded!\n"
Mar  2 01:55:42.768: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 01:55:42.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-1777 exec execpod-affinityj229g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.123.244.39:32054/ ; done'
Mar  2 01:55:43.504: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n"
Mar  2 01:55:43.504: INFO: stdout: "\naffinity-nodeport-transition-tztgh\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-msc62\naffinity-nodeport-transition-tztgh\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-tztgh\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-tztgh\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-msc62\naffinity-nodeport-transition-tztgh\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-tztgh\naffinity-nodeport-transition-tztgh\naffinity-nodeport-transition-msc62\naffinity-nodeport-transition-jlkt8"
Mar  2 01:55:43.504: INFO: Received response from host: affinity-nodeport-transition-tztgh
Mar  2 01:55:43.504: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:43.504: INFO: Received response from host: affinity-nodeport-transition-msc62
Mar  2 01:55:43.504: INFO: Received response from host: affinity-nodeport-transition-tztgh
Mar  2 01:55:43.504: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:43.504: INFO: Received response from host: affinity-nodeport-transition-tztgh
Mar  2 01:55:43.504: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:43.504: INFO: Received response from host: affinity-nodeport-transition-tztgh
Mar  2 01:55:43.504: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:43.504: INFO: Received response from host: affinity-nodeport-transition-msc62
Mar  2 01:55:43.504: INFO: Received response from host: affinity-nodeport-transition-tztgh
Mar  2 01:55:43.504: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:43.504: INFO: Received response from host: affinity-nodeport-transition-tztgh
Mar  2 01:55:43.504: INFO: Received response from host: affinity-nodeport-transition-tztgh
Mar  2 01:55:43.504: INFO: Received response from host: affinity-nodeport-transition-msc62
Mar  2 01:55:43.504: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:43.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-1777 exec execpod-affinityj229g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.123.244.39:32054/ ; done'
Mar  2 01:55:44.111: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.123.244.39:32054/\n"
Mar  2 01:55:44.111: INFO: stdout: "\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-jlkt8\naffinity-nodeport-transition-jlkt8"
Mar  2 01:55:44.111: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:44.111: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:44.111: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:44.111: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:44.111: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:44.111: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:44.111: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:44.111: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:44.111: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:44.111: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:44.111: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:44.111: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:44.111: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:44.111: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:44.111: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:44.111: INFO: Received response from host: affinity-nodeport-transition-jlkt8
Mar  2 01:55:44.112: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1777, will wait for the garbage collector to delete the pods
Mar  2 01:55:44.248: INFO: Deleting ReplicationController affinity-nodeport-transition took: 20.651622ms
Mar  2 01:55:44.448: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 200.239412ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar  2 01:55:47.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1777" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:18.907 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":250,"skipped":5031,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:55:47.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:55:48.111: INFO: created pod pod-service-account-defaultsa
Mar  2 01:55:48.112: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar  2 01:55:48.251: INFO: created pod pod-service-account-mountsa
Mar  2 01:55:48.251: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar  2 01:55:48.302: INFO: created pod pod-service-account-nomountsa
Mar  2 01:55:48.302: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar  2 01:55:48.353: INFO: created pod pod-service-account-defaultsa-mountspec
Mar  2 01:55:48.353: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar  2 01:55:48.489: INFO: created pod pod-service-account-mountsa-mountspec
Mar  2 01:55:48.489: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar  2 01:55:48.554: INFO: created pod pod-service-account-nomountsa-mountspec
Mar  2 01:55:48.554: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar  2 01:55:48.638: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar  2 01:55:48.638: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar  2 01:55:48.684: INFO: created pod pod-service-account-mountsa-nomountspec
Mar  2 01:55:48.684: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar  2 01:55:48.751: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar  2 01:55:48.751: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Mar  2 01:55:48.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3814" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":356,"completed":251,"skipped":5035,"failed":0}
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:55:48.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar  2 01:55:49.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4137" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":356,"completed":252,"skipped":5036,"failed":0}
SSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:55:49.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar  2 01:55:49.450: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar  2 01:55:49.479: INFO: starting watch
STEP: patching
STEP: updating
Mar  2 01:55:49.622: INFO: waiting for watch events with expected annotations
Mar  2 01:55:49.622: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Mar  2 01:55:49.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6872" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":356,"completed":253,"skipped":5043,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:55:49.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 01:55:51.879: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 01:55:53.961: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 1, 55, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 55, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 1, 55, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 1, 55, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 01:55:57.144: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 01:56:10.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5110" for this suite.
STEP: Destroying namespace "webhook-5110-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:20.695 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":356,"completed":254,"skipped":5058,"failed":0}
S
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:56:10.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
W0302 01:56:10.778475      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0302 01:56:10.826920      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Mar  2 01:56:10.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9824" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":356,"completed":255,"skipped":5059,"failed":0}
SS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:56:11.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Mar  2 01:56:11.386: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Mar  2 01:56:11.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1159" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":356,"completed":256,"skipped":5061,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:56:11.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:56:11.848: INFO: Creating deployment "webserver-deployment"
W0302 01:56:11.890550      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 01:56:11.890: INFO: Waiting for observed generation 1
Mar  2 01:56:13.947: INFO: Waiting for all required pods to come up
Mar  2 01:56:13.971: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar  2 01:56:18.020: INFO: Waiting for deployment "webserver-deployment" to complete
Mar  2 01:56:18.043: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar  2 01:56:18.126: INFO: Updating deployment webserver-deployment
Mar  2 01:56:18.126: INFO: Waiting for observed generation 2
Mar  2 01:56:20.148: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar  2 01:56:20.160: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar  2 01:56:20.172: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  2 01:56:20.226: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar  2 01:56:20.226: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar  2 01:56:20.244: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar  2 01:56:20.271: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar  2 01:56:20.271: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar  2 01:56:20.297: INFO: Updating deployment webserver-deployment
Mar  2 01:56:20.297: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar  2 01:56:20.343: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar  2 01:56:20.364: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 01:56:20.452: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5808  c3c6ab75-badb-4bdf-b210-2345ef307bd5 132856 3 2023-03-02 01:56:11 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-03-02 01:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:56:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006bb8bc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-57ccb67bb8" is progressing.,LastUpdateTime:2023-03-02 01:56:18 +0000 UTC,LastTransitionTime:2023-03-02 01:56:11 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-02 01:56:20 +0000 UTC,LastTransitionTime:2023-03-02 01:56:20 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar  2 01:56:20.497: INFO: New ReplicaSet "webserver-deployment-57ccb67bb8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-57ccb67bb8  deployment-5808  a6fef589-99a1-4e5f-9d15-12336df22891 132854 3 2023-03-02 01:56:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment c3c6ab75-badb-4bdf-b210-2345ef307bd5 0xc0057066a7 0xc0057066a8}] []  [{kube-controller-manager Update apps/v1 2023-03-02 01:56:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3c6ab75-badb-4bdf-b210-2345ef307bd5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:56:18 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 57ccb67bb8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005706748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:56:20.497: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar  2 01:56:20.497: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-55df494869  deployment-5808  3f9b2627-fa47-4c43-8621-f2cdc18eb40a 132853 3 2023-03-02 01:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment c3c6ab75-badb-4bdf-b210-2345ef307bd5 0xc0057065b7 0xc0057065b8}] []  [{kube-controller-manager Update apps/v1 2023-03-02 01:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3c6ab75-badb-4bdf-b210-2345ef307bd5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:56:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005706648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar  2 01:56:20.521: INFO: Pod "webserver-deployment-55df494869-c554d" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-c554d webserver-deployment-55df494869- deployment-5808  3a4510de-25bd-44e9-ab52-3c40ce730114 132861 0 2023-03-02 01:56:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 3f9b2627-fa47-4c43-8621-f2cdc18eb40a 0xc005706c77 0xc005706c78}] []  [{kube-controller-manager Update v1 2023-03-02 01:56:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3f9b2627-fa47-4c43-8621-f2cdc18eb40a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qdfmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qdfmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.39,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n54rl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:56:20.521: INFO: Pod "webserver-deployment-55df494869-ddwwp" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-ddwwp webserver-deployment-55df494869- deployment-5808  d5e9abf5-4454-471f-857a-3dc053ac8463 132612 0 2023-03-02 01:56:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:3d4868c962596ce776acc5cc6e36e891f41ca76e99ee208b9d076e98a582d3ce cni.projectcalico.org/podIP:172.30.88.233/32 cni.projectcalico.org/podIPs:172.30.88.233/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.233"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.233"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 3f9b2627-fa47-4c43-8621-f2cdc18eb40a 0xc005706e47 0xc005706e48}] []  [{kube-controller-manager Update v1 2023-03-02 01:56:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3f9b2627-fa47-4c43-8621-f2cdc18eb40a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-03-02 01:56:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:56:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:56:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.88.233\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b9mds,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b9mds,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.39,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.39,PodIP:172.30.88.233,StartTime:2023-03-02 01:56:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:56:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://b7ead7c1b2f4650eb30229594c931a39365f983cabe6d5e0e362b3a88923ad14,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.88.233,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:56:20.522: INFO: Pod "webserver-deployment-55df494869-fdcg8" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-fdcg8 webserver-deployment-55df494869- deployment-5808  814c0897-0002-4231-a333-fa3d9646c82e 132635 0 2023-03-02 01:56:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:620c698015e5f98aa89526ba98341570896eb0cd6c77eed405177bc2a680e9a3 cni.projectcalico.org/podIP:172.30.54.250/32 cni.projectcalico.org/podIPs:172.30.54.250/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.54.250"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.54.250"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 3f9b2627-fa47-4c43-8621-f2cdc18eb40a 0xc0057070f7 0xc0057070f8}] []  [{kube-controller-manager Update v1 2023-03-02 01:56:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3f9b2627-fa47-4c43-8621-f2cdc18eb40a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-03-02 01:56:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:56:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:56:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.54.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dk52p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dk52p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.41,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n54rl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.41,PodIP:172.30.54.250,StartTime:2023-03-02 01:56:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:56:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://5883b08d7e9a8ce7996fcad331276de91a2ca244415f092c9dbfa73c69ec8b6c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.54.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:56:20.522: INFO: Pod "webserver-deployment-55df494869-kszzp" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-kszzp webserver-deployment-55df494869- deployment-5808  dc11ae1f-67f8-46ab-9187-84c40d7e0635 132636 0 2023-03-02 01:56:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:8c8950489e0488b4bba6e85423b5b470149d7558a2fbbf9ec2bd75f59136d095 cni.projectcalico.org/podIP:172.30.54.245/32 cni.projectcalico.org/podIPs:172.30.54.245/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.54.245"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.54.245"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 3f9b2627-fa47-4c43-8621-f2cdc18eb40a 0xc0057073a7 0xc0057073a8}] []  [{kube-controller-manager Update v1 2023-03-02 01:56:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3f9b2627-fa47-4c43-8621-f2cdc18eb40a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-03-02 01:56:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:56:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:56:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.54.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ccgng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ccgng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.41,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n54rl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.41,PodIP:172.30.54.245,StartTime:2023-03-02 01:56:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:56:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://e257c5b42d6002f9bee51a5c3e0ee88af8b5c389cdc9cdd9846d467f5207aa54,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.54.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:56:20.523: INFO: Pod "webserver-deployment-55df494869-tgrx2" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-tgrx2 webserver-deployment-55df494869- deployment-5808  e1acefbd-d322-4825-8dd1-7c5eba5e40ed 132671 0 2023-03-02 01:56:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:565af7741819732cb373b86e33c632c75eb3f18f7feaa13353fbd4ce44b0b43b cni.projectcalico.org/podIP:172.30.0.209/32 cni.projectcalico.org/podIPs:172.30.0.209/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.0.209"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.0.209"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 3f9b2627-fa47-4c43-8621-f2cdc18eb40a 0xc005707667 0xc005707668}] []  [{kube-controller-manager Update v1 2023-03-02 01:56:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3f9b2627-fa47-4c43-8621-f2cdc18eb40a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-03-02 01:56:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:56:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.0.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sd2cj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sd2cj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n54rl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.50,PodIP:172.30.0.209,StartTime:2023-03-02 01:56:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:56:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://830cded9d32096a2c46a559de045b00e5bd889987cd59926634193c07d91f38a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.0.209,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:56:20.523: INFO: Pod "webserver-deployment-55df494869-x7d8x" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-x7d8x webserver-deployment-55df494869- deployment-5808  a2fb8ffe-75d6-45de-adf0-84f33c8df09a 132638 0 2023-03-02 01:56:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:c1043b6212073fcd99e33e148afd6addac962f2347b81d161c349a32a156fac8 cni.projectcalico.org/podIP:172.30.54.251/32 cni.projectcalico.org/podIPs:172.30.54.251/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.54.251"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.54.251"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 3f9b2627-fa47-4c43-8621-f2cdc18eb40a 0xc005707927 0xc005707928}] []  [{kube-controller-manager Update v1 2023-03-02 01:56:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3f9b2627-fa47-4c43-8621-f2cdc18eb40a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-03-02 01:56:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:56:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:56:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.54.251\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zh5g6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zh5g6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.41,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n54rl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.41,PodIP:172.30.54.251,StartTime:2023-03-02 01:56:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:56:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://e20793f6871f70a51bc5d12625301b7e14d57ab779d62992aace16acf49b1322,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.54.251,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:56:20.524: INFO: Pod "webserver-deployment-55df494869-xk2zc" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-xk2zc webserver-deployment-55df494869- deployment-5808  d1d41b08-c67f-4871-b071-1be8da8fee07 132641 0 2023-03-02 01:56:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:1a11406fa1d64213ed2cdc6026715df58470a9551e62524d4023c01d319a8509 cni.projectcalico.org/podIP:172.30.88.198/32 cni.projectcalico.org/podIPs:172.30.88.198/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.198"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.198"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 3f9b2627-fa47-4c43-8621-f2cdc18eb40a 0xc005707bd7 0xc005707bd8}] []  [{kube-controller-manager Update v1 2023-03-02 01:56:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3f9b2627-fa47-4c43-8621-f2cdc18eb40a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-03-02 01:56:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:56:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:56:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.88.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f4lhw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f4lhw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.39,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n54rl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.39,PodIP:172.30.88.198,StartTime:2023-03-02 01:56:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:56:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://ef19db27d66544b36644e52cc291943794c3ab453cfbc49bf1f9390b9f9e5332,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.88.198,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:56:20.524: INFO: Pod "webserver-deployment-55df494869-z64ng" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-z64ng webserver-deployment-55df494869- deployment-5808  1f2870a6-6ba5-4e05-9fd4-2f156faeee76 132669 0 2023-03-02 01:56:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:f9265b3a98442f5992b28e246dd2f4308b2583a45c0c9eff50180ea05a2e1f83 cni.projectcalico.org/podIP:172.30.0.206/32 cni.projectcalico.org/podIPs:172.30.0.206/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.0.206"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.0.206"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 3f9b2627-fa47-4c43-8621-f2cdc18eb40a 0xc005707e77 0xc005707e78}] []  [{kube-controller-manager Update v1 2023-03-02 01:56:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3f9b2627-fa47-4c43-8621-f2cdc18eb40a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-03-02 01:56:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:56:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:56:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.0.206\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q6d9z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q6d9z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n54rl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.50,PodIP:172.30.0.206,StartTime:2023-03-02 01:56:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:56:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://ac1caf4018de6e0fb4349f5f82324dea6c6f2e5a8c19d99ed20141b19c6c7236,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.0.206,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:56:20.524: INFO: Pod "webserver-deployment-55df494869-zsz8l" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-zsz8l webserver-deployment-55df494869- deployment-5808  79b5a82a-88f4-42c1-98c3-93c1c97b43a6 132698 0 2023-03-02 01:56:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:87c8ae07f45c937a6346d04dcdeea186791a5ca2bfa249296d7ef1f138b7c611 cni.projectcalico.org/podIP:172.30.0.210/32 cni.projectcalico.org/podIPs:172.30.0.210/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.0.210"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.0.210"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-55df494869 3f9b2627-fa47-4c43-8621-f2cdc18eb40a 0xc0022a0467 0xc0022a0468}] []  [{kube-controller-manager Update v1 2023-03-02 01:56:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3f9b2627-fa47-4c43-8621-f2cdc18eb40a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-03-02 01:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:56:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.0.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c8cm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c8cm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n54rl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.50,PodIP:172.30.0.210,StartTime:2023-03-02 01:56:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:56:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://b00add0f10630b29991df805145618e48971d072c367fde210dbbc8760dfba67,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.0.210,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:56:20.525: INFO: Pod "webserver-deployment-57ccb67bb8-57c76" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-57c76 webserver-deployment-57ccb67bb8- deployment-5808  10a0b589-98bb-4692-bb63-cac88e55ff86 132823 0 2023-03-02 01:56:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:80e5cd8b1361050408ba7e81b9092afcfa5937eacdedadaa0ed5317e013a688b cni.projectcalico.org/podIP:172.30.54.206/32 cni.projectcalico.org/podIPs:172.30.54.206/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.54.206"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.54.206"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a6fef589-99a1-4e5f-9d15-12336df22891 0xc0022a0ff7 0xc0022a0ff8}] []  [{kube-controller-manager Update v1 2023-03-02 01:56:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6fef589-99a1-4e5f-9d15-12336df22891\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 01:56:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-03-02 01:56:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:56:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p7h2g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p7h2g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.41,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n54rl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.41,PodIP:,StartTime:2023-03-02 01:56:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:56:20.525: INFO: Pod "webserver-deployment-57ccb67bb8-5hthz" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-5hthz webserver-deployment-57ccb67bb8- deployment-5808  b8d8a5f1-5442-4bdb-b3c8-3f4f23d2ee57 132862 0 2023-03-02 01:56:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a6fef589-99a1-4e5f-9d15-12336df22891 0xc0022a1627 0xc0022a1628}] []  [{kube-controller-manager Update v1 2023-03-02 01:56:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6fef589-99a1-4e5f-9d15-12336df22891\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ptrfk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ptrfk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n54rl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:56:20.525: INFO: Pod "webserver-deployment-57ccb67bb8-9wp7m" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-9wp7m webserver-deployment-57ccb67bb8- deployment-5808  2becd427-4781-4221-a58c-1d4fabb16017 132836 0 2023-03-02 01:56:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:cd419c1b77cced15415c8212aa04f0aff459a21929427680291c4854c8657b84 cni.projectcalico.org/podIP:172.30.54.254/32 cni.projectcalico.org/podIPs:172.30.54.254/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.54.254"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.54.254"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a6fef589-99a1-4e5f-9d15-12336df22891 0xc0022a1807 0xc0022a1808}] []  [{kube-controller-manager Update v1 2023-03-02 01:56:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6fef589-99a1-4e5f-9d15-12336df22891\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 01:56:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-03-02 01:56:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:56:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7cpgj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7cpgj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.41,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n54rl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.41,PodIP:,StartTime:2023-03-02 01:56:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:56:20.526: INFO: Pod "webserver-deployment-57ccb67bb8-fpps8" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-fpps8 webserver-deployment-57ccb67bb8- deployment-5808  ab095b58-e334-417f-ae45-5fa49975c139 132846 0 2023-03-02 01:56:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:89b42bf2d5f6bbaa9418308289b1801d0239e57c200f861b45026143af74013a cni.projectcalico.org/podIP:172.30.88.244/32 cni.projectcalico.org/podIPs:172.30.88.244/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.244"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.244"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a6fef589-99a1-4e5f-9d15-12336df22891 0xc0022a1aa7 0xc0022a1aa8}] []  [{kube-controller-manager Update v1 2023-03-02 01:56:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6fef589-99a1-4e5f-9d15-12336df22891\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 01:56:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-03-02 01:56:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:56:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2scvq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2scvq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.39,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n54rl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.39,PodIP:,StartTime:2023-03-02 01:56:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:56:20.526: INFO: Pod "webserver-deployment-57ccb67bb8-kq5xk" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-kq5xk webserver-deployment-57ccb67bb8- deployment-5808  c78160d5-fe75-426a-b1b1-c7d89b033a09 132821 0 2023-03-02 01:56:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:cb0a92dfd732ec63e7e619f578654f0c872309460fb543c02853f58ca3505c6c cni.projectcalico.org/podIP:172.30.0.255/32 cni.projectcalico.org/podIPs:172.30.0.255/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.0.255"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.0.255"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a6fef589-99a1-4e5f-9d15-12336df22891 0xc0022a1d47 0xc0022a1d48}] []  [{kube-controller-manager Update v1 2023-03-02 01:56:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6fef589-99a1-4e5f-9d15-12336df22891\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 01:56:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-03-02 01:56:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:56:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f7xvd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f7xvd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n54rl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.50,PodIP:,StartTime:2023-03-02 01:56:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar  2 01:56:20.532: INFO: Pod "webserver-deployment-57ccb67bb8-rn8xk" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-rn8xk webserver-deployment-57ccb67bb8- deployment-5808  18f4c86f-fb18-4030-af48-1444d52d4cc1 132837 0 2023-03-02 01:56:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:cb683f4f7bff013b788d16b1e258f19fc199b317218fb4fb0ad04505a02cc101 cni.projectcalico.org/podIP:172.30.88.227/32 cni.projectcalico.org/podIPs:172.30.88.227/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.227"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.227"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a6fef589-99a1-4e5f-9d15-12336df22891 0xc00331fe97 0xc00331fe98}] []  [{kube-controller-manager Update v1 2023-03-02 01:56:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6fef589-99a1-4e5f-9d15-12336df22891\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 01:56:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-03-02 01:56:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:56:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nkcvv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nkcvv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.39,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-n54rl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:56:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.39,PodIP:,StartTime:2023-03-02 01:56:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Mar  2 01:56:20.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5808" for this suite.

• [SLOW TEST:8.981 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":356,"completed":257,"skipped":5068,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:56:20.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating cluster-info
Mar  2 01:56:20.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-329 cluster-info'
Mar  2 01:56:21.133: INFO: stderr: ""
Mar  2 01:56:21.133: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar  2 01:56:21.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-329" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":356,"completed":258,"skipped":5079,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:56:21.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Mar  2 01:56:26.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9814" for this suite.

• [SLOW TEST:5.207 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":356,"completed":259,"skipped":5081,"failed":0}
SSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:56:26.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Mar  2 01:56:26.728: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:56:28.742: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:56:30.760: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Mar  2 01:56:30.833: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Mar  2 01:56:32.851: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar  2 01:56:32.864: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4481 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:56:32.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:56:32.866: INFO: ExecWithOptions: Clientset creation
Mar  2 01:56:32.866: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4481/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  2 01:56:33.088: INFO: Exec stderr: ""
Mar  2 01:56:33.088: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4481 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:56:33.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:56:33.089: INFO: ExecWithOptions: Clientset creation
Mar  2 01:56:33.090: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4481/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  2 01:56:33.332: INFO: Exec stderr: ""
Mar  2 01:56:33.332: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4481 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:56:33.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:56:33.335: INFO: ExecWithOptions: Clientset creation
Mar  2 01:56:33.335: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4481/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  2 01:56:33.627: INFO: Exec stderr: ""
Mar  2 01:56:33.628: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4481 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:56:33.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:56:33.630: INFO: ExecWithOptions: Clientset creation
Mar  2 01:56:33.630: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4481/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  2 01:56:34.530: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar  2 01:56:34.530: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4481 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:56:34.530: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:56:34.532: INFO: ExecWithOptions: Clientset creation
Mar  2 01:56:34.532: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4481/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar  2 01:56:34.893: INFO: Exec stderr: ""
Mar  2 01:56:34.893: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4481 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:56:34.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:56:34.910: INFO: ExecWithOptions: Clientset creation
Mar  2 01:56:34.910: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4481/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar  2 01:56:35.166: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar  2 01:56:35.166: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4481 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:56:35.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:56:35.171: INFO: ExecWithOptions: Clientset creation
Mar  2 01:56:35.171: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4481/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  2 01:56:35.367: INFO: Exec stderr: ""
Mar  2 01:56:35.367: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4481 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:56:35.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:56:35.369: INFO: ExecWithOptions: Clientset creation
Mar  2 01:56:35.370: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4481/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar  2 01:56:35.589: INFO: Exec stderr: ""
Mar  2 01:56:35.589: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4481 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:56:35.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:56:35.591: INFO: ExecWithOptions: Clientset creation
Mar  2 01:56:35.591: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4481/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  2 01:56:35.811: INFO: Exec stderr: ""
Mar  2 01:56:35.811: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4481 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar  2 01:56:35.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
Mar  2 01:56:35.814: INFO: ExecWithOptions: Clientset creation
Mar  2 01:56:35.814: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4481/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar  2 01:56:35.996: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:188
Mar  2 01:56:35.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4481" for this suite.

• [SLOW TEST:9.601 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":260,"skipped":5089,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:56:36.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-3a9aaf82-42b1-4890-b7c4-0c0ebcbd8317
STEP: Creating a pod to test consume secrets
Mar  2 01:56:36.373: INFO: Waiting up to 5m0s for pod "pod-secrets-f4db29db-c208-44ad-b0a3-41229bb0f43f" in namespace "secrets-290" to be "Succeeded or Failed"
Mar  2 01:56:36.416: INFO: Pod "pod-secrets-f4db29db-c208-44ad-b0a3-41229bb0f43f": Phase="Pending", Reason="", readiness=false. Elapsed: 43.336133ms
Mar  2 01:56:38.430: INFO: Pod "pod-secrets-f4db29db-c208-44ad-b0a3-41229bb0f43f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057248891s
Mar  2 01:56:40.442: INFO: Pod "pod-secrets-f4db29db-c208-44ad-b0a3-41229bb0f43f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069229076s
Mar  2 01:56:42.481: INFO: Pod "pod-secrets-f4db29db-c208-44ad-b0a3-41229bb0f43f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.108898518s
STEP: Saw pod success
Mar  2 01:56:42.482: INFO: Pod "pod-secrets-f4db29db-c208-44ad-b0a3-41229bb0f43f" satisfied condition "Succeeded or Failed"
Mar  2 01:56:42.517: INFO: Trying to get logs from node 10.123.244.39 pod pod-secrets-f4db29db-c208-44ad-b0a3-41229bb0f43f container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 01:56:42.708: INFO: Waiting for pod pod-secrets-f4db29db-c208-44ad-b0a3-41229bb0f43f to disappear
Mar  2 01:56:42.722: INFO: Pod pod-secrets-f4db29db-c208-44ad-b0a3-41229bb0f43f no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Mar  2 01:56:42.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-290" for this suite.

• [SLOW TEST:6.718 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":261,"skipped":5101,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:56:42.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar  2 01:56:54.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7405" for this suite.

• [SLOW TEST:11.448 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":356,"completed":262,"skipped":5108,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:56:54.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:56:55.977: INFO: Checking APIGroup: apiregistration.k8s.io
Mar  2 01:56:55.981: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar  2 01:56:55.981: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Mar  2 01:56:55.981: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar  2 01:56:55.981: INFO: Checking APIGroup: apps
Mar  2 01:56:55.985: INFO: PreferredVersion.GroupVersion: apps/v1
Mar  2 01:56:55.985: INFO: Versions found [{apps/v1 v1}]
Mar  2 01:56:55.985: INFO: apps/v1 matches apps/v1
Mar  2 01:56:55.985: INFO: Checking APIGroup: events.k8s.io
Mar  2 01:56:55.989: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar  2 01:56:55.989: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Mar  2 01:56:55.989: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar  2 01:56:55.989: INFO: Checking APIGroup: authentication.k8s.io
Mar  2 01:56:55.993: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar  2 01:56:55.993: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Mar  2 01:56:55.993: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar  2 01:56:55.993: INFO: Checking APIGroup: authorization.k8s.io
Mar  2 01:56:55.997: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar  2 01:56:55.997: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Mar  2 01:56:55.997: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar  2 01:56:55.997: INFO: Checking APIGroup: autoscaling
Mar  2 01:56:56.002: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Mar  2 01:56:56.002: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Mar  2 01:56:56.002: INFO: autoscaling/v2 matches autoscaling/v2
Mar  2 01:56:56.002: INFO: Checking APIGroup: batch
Mar  2 01:56:56.006: INFO: PreferredVersion.GroupVersion: batch/v1
Mar  2 01:56:56.006: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Mar  2 01:56:56.006: INFO: batch/v1 matches batch/v1
Mar  2 01:56:56.006: INFO: Checking APIGroup: certificates.k8s.io
Mar  2 01:56:56.011: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar  2 01:56:56.011: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Mar  2 01:56:56.011: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar  2 01:56:56.011: INFO: Checking APIGroup: networking.k8s.io
Mar  2 01:56:56.015: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar  2 01:56:56.015: INFO: Versions found [{networking.k8s.io/v1 v1}]
Mar  2 01:56:56.016: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar  2 01:56:56.016: INFO: Checking APIGroup: policy
Mar  2 01:56:56.020: INFO: PreferredVersion.GroupVersion: policy/v1
Mar  2 01:56:56.020: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Mar  2 01:56:56.020: INFO: policy/v1 matches policy/v1
Mar  2 01:56:56.020: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar  2 01:56:56.025: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar  2 01:56:56.026: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Mar  2 01:56:56.026: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar  2 01:56:56.026: INFO: Checking APIGroup: storage.k8s.io
Mar  2 01:56:56.030: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar  2 01:56:56.030: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar  2 01:56:56.030: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar  2 01:56:56.030: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar  2 01:56:56.033: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar  2 01:56:56.034: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Mar  2 01:56:56.034: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar  2 01:56:56.034: INFO: Checking APIGroup: apiextensions.k8s.io
Mar  2 01:56:56.037: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar  2 01:56:56.037: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Mar  2 01:56:56.037: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar  2 01:56:56.037: INFO: Checking APIGroup: scheduling.k8s.io
Mar  2 01:56:56.041: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar  2 01:56:56.041: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Mar  2 01:56:56.041: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar  2 01:56:56.041: INFO: Checking APIGroup: coordination.k8s.io
Mar  2 01:56:56.046: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar  2 01:56:56.046: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Mar  2 01:56:56.046: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar  2 01:56:56.046: INFO: Checking APIGroup: node.k8s.io
Mar  2 01:56:56.050: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar  2 01:56:56.050: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Mar  2 01:56:56.050: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar  2 01:56:56.050: INFO: Checking APIGroup: discovery.k8s.io
Mar  2 01:56:56.054: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Mar  2 01:56:56.054: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Mar  2 01:56:56.054: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Mar  2 01:56:56.054: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar  2 01:56:56.058: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Mar  2 01:56:56.058: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Mar  2 01:56:56.058: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Mar  2 01:56:56.058: INFO: Checking APIGroup: apps.openshift.io
Mar  2 01:56:56.061: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Mar  2 01:56:56.061: INFO: Versions found [{apps.openshift.io/v1 v1}]
Mar  2 01:56:56.061: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Mar  2 01:56:56.061: INFO: Checking APIGroup: authorization.openshift.io
Mar  2 01:56:56.065: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Mar  2 01:56:56.065: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Mar  2 01:56:56.065: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Mar  2 01:56:56.065: INFO: Checking APIGroup: build.openshift.io
Mar  2 01:56:56.070: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Mar  2 01:56:56.070: INFO: Versions found [{build.openshift.io/v1 v1}]
Mar  2 01:56:56.070: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Mar  2 01:56:56.070: INFO: Checking APIGroup: image.openshift.io
Mar  2 01:56:56.074: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Mar  2 01:56:56.074: INFO: Versions found [{image.openshift.io/v1 v1}]
Mar  2 01:56:56.074: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Mar  2 01:56:56.074: INFO: Checking APIGroup: oauth.openshift.io
Mar  2 01:56:56.078: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Mar  2 01:56:56.078: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Mar  2 01:56:56.078: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Mar  2 01:56:56.078: INFO: Checking APIGroup: project.openshift.io
Mar  2 01:56:56.084: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Mar  2 01:56:56.084: INFO: Versions found [{project.openshift.io/v1 v1}]
Mar  2 01:56:56.084: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Mar  2 01:56:56.084: INFO: Checking APIGroup: quota.openshift.io
Mar  2 01:56:56.088: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Mar  2 01:56:56.088: INFO: Versions found [{quota.openshift.io/v1 v1}]
Mar  2 01:56:56.088: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Mar  2 01:56:56.088: INFO: Checking APIGroup: route.openshift.io
Mar  2 01:56:56.092: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Mar  2 01:56:56.093: INFO: Versions found [{route.openshift.io/v1 v1}]
Mar  2 01:56:56.093: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Mar  2 01:56:56.093: INFO: Checking APIGroup: security.openshift.io
Mar  2 01:56:56.097: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Mar  2 01:56:56.097: INFO: Versions found [{security.openshift.io/v1 v1}]
Mar  2 01:56:56.097: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Mar  2 01:56:56.097: INFO: Checking APIGroup: template.openshift.io
Mar  2 01:56:56.101: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Mar  2 01:56:56.101: INFO: Versions found [{template.openshift.io/v1 v1}]
Mar  2 01:56:56.101: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Mar  2 01:56:56.101: INFO: Checking APIGroup: user.openshift.io
Mar  2 01:56:56.106: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Mar  2 01:56:56.106: INFO: Versions found [{user.openshift.io/v1 v1}]
Mar  2 01:56:56.106: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Mar  2 01:56:56.106: INFO: Checking APIGroup: packages.operators.coreos.com
Mar  2 01:56:56.111: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Mar  2 01:56:56.111: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Mar  2 01:56:56.111: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Mar  2 01:56:56.111: INFO: Checking APIGroup: config.openshift.io
Mar  2 01:56:56.115: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Mar  2 01:56:56.115: INFO: Versions found [{config.openshift.io/v1 v1}]
Mar  2 01:56:56.115: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Mar  2 01:56:56.115: INFO: Checking APIGroup: operator.openshift.io
Mar  2 01:56:56.118: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Mar  2 01:56:56.118: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Mar  2 01:56:56.118: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Mar  2 01:56:56.118: INFO: Checking APIGroup: apiserver.openshift.io
Mar  2 01:56:56.122: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
Mar  2 01:56:56.122: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
Mar  2 01:56:56.122: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
Mar  2 01:56:56.122: INFO: Checking APIGroup: cloudcredential.openshift.io
Mar  2 01:56:56.127: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Mar  2 01:56:56.127: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Mar  2 01:56:56.127: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Mar  2 01:56:56.127: INFO: Checking APIGroup: console.openshift.io
Mar  2 01:56:56.131: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Mar  2 01:56:56.131: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Mar  2 01:56:56.131: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Mar  2 01:56:56.131: INFO: Checking APIGroup: crd.projectcalico.org
Mar  2 01:56:56.135: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Mar  2 01:56:56.135: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Mar  2 01:56:56.135: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Mar  2 01:56:56.135: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Mar  2 01:56:56.138: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Mar  2 01:56:56.138: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Mar  2 01:56:56.138: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Mar  2 01:56:56.138: INFO: Checking APIGroup: ingress.operator.openshift.io
Mar  2 01:56:56.142: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Mar  2 01:56:56.142: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Mar  2 01:56:56.142: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Mar  2 01:56:56.142: INFO: Checking APIGroup: k8s.cni.cncf.io
Mar  2 01:56:56.146: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Mar  2 01:56:56.146: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Mar  2 01:56:56.146: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Mar  2 01:56:56.146: INFO: Checking APIGroup: machineconfiguration.openshift.io
Mar  2 01:56:56.150: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Mar  2 01:56:56.150: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Mar  2 01:56:56.150: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Mar  2 01:56:56.150: INFO: Checking APIGroup: monitoring.coreos.com
Mar  2 01:56:56.153: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Mar  2 01:56:56.154: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Mar  2 01:56:56.154: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Mar  2 01:56:56.154: INFO: Checking APIGroup: network.operator.openshift.io
Mar  2 01:56:56.157: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Mar  2 01:56:56.157: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Mar  2 01:56:56.157: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Mar  2 01:56:56.157: INFO: Checking APIGroup: operator.tigera.io
Mar  2 01:56:56.161: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Mar  2 01:56:56.161: INFO: Versions found [{operator.tigera.io/v1 v1}]
Mar  2 01:56:56.161: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Mar  2 01:56:56.161: INFO: Checking APIGroup: operators.coreos.com
Mar  2 01:56:56.165: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
Mar  2 01:56:56.165: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Mar  2 01:56:56.165: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
Mar  2 01:56:56.165: INFO: Checking APIGroup: performance.openshift.io
Mar  2 01:56:56.169: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
Mar  2 01:56:56.169: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
Mar  2 01:56:56.169: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
Mar  2 01:56:56.169: INFO: Checking APIGroup: samples.operator.openshift.io
Mar  2 01:56:56.173: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Mar  2 01:56:56.173: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Mar  2 01:56:56.173: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Mar  2 01:56:56.173: INFO: Checking APIGroup: security.internal.openshift.io
Mar  2 01:56:56.177: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Mar  2 01:56:56.177: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Mar  2 01:56:56.177: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Mar  2 01:56:56.177: INFO: Checking APIGroup: snapshot.storage.k8s.io
Mar  2 01:56:56.180: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Mar  2 01:56:56.180: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Mar  2 01:56:56.180: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Mar  2 01:56:56.180: INFO: Checking APIGroup: tuned.openshift.io
Mar  2 01:56:56.184: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Mar  2 01:56:56.184: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Mar  2 01:56:56.184: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Mar  2 01:56:56.184: INFO: Checking APIGroup: controlplane.operator.openshift.io
Mar  2 01:56:56.190: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Mar  2 01:56:56.190: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Mar  2 01:56:56.190: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Mar  2 01:56:56.190: INFO: Checking APIGroup: ibm.com
Mar  2 01:56:56.194: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Mar  2 01:56:56.194: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Mar  2 01:56:56.194: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Mar  2 01:56:56.194: INFO: Checking APIGroup: migration.k8s.io
Mar  2 01:56:56.198: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Mar  2 01:56:56.198: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Mar  2 01:56:56.198: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Mar  2 01:56:56.198: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Mar  2 01:56:56.222: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Mar  2 01:56:56.222: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Mar  2 01:56:56.222: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Mar  2 01:56:56.222: INFO: Checking APIGroup: helm.openshift.io
Mar  2 01:56:56.272: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Mar  2 01:56:56.272: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Mar  2 01:56:56.272: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Mar  2 01:56:56.272: INFO: Checking APIGroup: metrics.k8s.io
Mar  2 01:56:56.322: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Mar  2 01:56:56.322: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Mar  2 01:56:56.322: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:188
Mar  2 01:56:56.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-2822" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":356,"completed":263,"skipped":5116,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:56:56.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:56:58.701: INFO: Deleting pod "var-expansion-8e9eec45-e46e-423f-94bb-f1e485cedc5f" in namespace "var-expansion-3087"
Mar  2 01:56:58.719: INFO: Wait up to 5m0s for pod "var-expansion-8e9eec45-e46e-423f-94bb-f1e485cedc5f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Mar  2 01:57:02.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3087" for this suite.

• [SLOW TEST:6.349 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":356,"completed":264,"skipped":5120,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:57:02.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Mar  2 01:57:02.986: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Mar  2 01:57:08.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3450" for this suite.

• [SLOW TEST:5.670 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":356,"completed":265,"skipped":5133,"failed":0}
SSSS
------------------------------
[sig-apps] Job 
  should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:57:08.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensure pods equal to paralellism count is attached to the job
W0302 01:57:08.766767      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: patching /status
STEP: updating /status
STEP: get /status
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Mar  2 01:57:12.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2968" for this suite.
•{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","total":356,"completed":266,"skipped":5137,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:57:12.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on node default medium
Mar  2 01:57:13.120: INFO: Waiting up to 5m0s for pod "pod-1b0b2b21-7f76-4388-8601-418894a656c9" in namespace "emptydir-2509" to be "Succeeded or Failed"
Mar  2 01:57:13.146: INFO: Pod "pod-1b0b2b21-7f76-4388-8601-418894a656c9": Phase="Pending", Reason="", readiness=false. Elapsed: 25.031992ms
Mar  2 01:57:15.178: INFO: Pod "pod-1b0b2b21-7f76-4388-8601-418894a656c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05751262s
Mar  2 01:57:17.192: INFO: Pod "pod-1b0b2b21-7f76-4388-8601-418894a656c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071377133s
Mar  2 01:57:19.208: INFO: Pod "pod-1b0b2b21-7f76-4388-8601-418894a656c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.087268686s
STEP: Saw pod success
Mar  2 01:57:19.208: INFO: Pod "pod-1b0b2b21-7f76-4388-8601-418894a656c9" satisfied condition "Succeeded or Failed"
Mar  2 01:57:19.218: INFO: Trying to get logs from node 10.123.244.39 pod pod-1b0b2b21-7f76-4388-8601-418894a656c9 container test-container: <nil>
STEP: delete the pod
Mar  2 01:57:19.272: INFO: Waiting for pod pod-1b0b2b21-7f76-4388-8601-418894a656c9 to disappear
Mar  2 01:57:19.282: INFO: Pod pod-1b0b2b21-7f76-4388-8601-418894a656c9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar  2 01:57:19.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2509" for this suite.

• [SLOW TEST:6.424 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":267,"skipped":5156,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:57:19.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Mar  2 01:57:19.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-41" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":268,"skipped":5182,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:57:19.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  2 01:57:19.683: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 01:58:19.957: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Mar  2 01:58:20.115: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar  2 01:58:20.160: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar  2 01:58:20.251: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar  2 01:58:20.289: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar  2 01:58:20.374: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar  2 01:58:20.424: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Mar  2 01:58:38.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6318" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:79.441 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":356,"completed":269,"skipped":5212,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:58:38.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Mar  2 01:58:39.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3242" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":356,"completed":270,"skipped":5213,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:58:39.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar  2 01:58:39.478: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 01:59:39.795: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:59:39.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 01:59:39.993: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Mar  2 01:59:40.004: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:188
Mar  2 01:59:40.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8861" for this suite.
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Mar  2 01:59:40.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3626" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:60.951 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":356,"completed":271,"skipped":5238,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:59:40.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Mar  2 01:59:40.435: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 01:59:40.435: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 01:59:40.467: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 01:59:40.467: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 01:59:40.538: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 01:59:40.539: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 01:59:40.866: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 01:59:40.866: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar  2 01:59:42.020: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  2 01:59:42.020: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar  2 01:59:42.355: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Mar  2 01:59:42.384: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Mar  2 01:59:42.390: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 0
Mar  2 01:59:42.390: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 0
Mar  2 01:59:42.390: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 0
Mar  2 01:59:42.390: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 0
Mar  2 01:59:42.391: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 0
Mar  2 01:59:42.391: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 0
Mar  2 01:59:42.391: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 0
Mar  2 01:59:42.393: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 0
Mar  2 01:59:42.393: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 1
Mar  2 01:59:42.393: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 1
Mar  2 01:59:42.393: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2
Mar  2 01:59:42.393: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2
Mar  2 01:59:42.394: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2
Mar  2 01:59:42.394: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2
Mar  2 01:59:42.414: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2
Mar  2 01:59:42.414: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2
Mar  2 01:59:42.451: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2
Mar  2 01:59:42.451: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2
Mar  2 01:59:42.490: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 1
Mar  2 01:59:42.490: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 1
Mar  2 01:59:42.560: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 1
Mar  2 01:59:42.560: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 1
Mar  2 01:59:44.028: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2
Mar  2 01:59:44.028: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2
Mar  2 01:59:44.105: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 1
STEP: listing Deployments
Mar  2 01:59:44.138: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Mar  2 01:59:44.182: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Mar  2 01:59:44.204: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 01:59:44.238: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 01:59:44.373: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 01:59:44.492: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 01:59:46.059: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 01:59:46.105: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 01:59:46.153: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 01:59:46.153: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 01:59:46.218: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar  2 01:59:49.705: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Mar  2 01:59:49.866: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 1
Mar  2 01:59:49.867: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 1
Mar  2 01:59:49.867: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 1
Mar  2 01:59:49.867: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 1
Mar  2 01:59:49.868: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2
Mar  2 01:59:49.868: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2
Mar  2 01:59:49.868: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2
Mar  2 01:59:49.869: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2
Mar  2 01:59:49.869: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 2
Mar  2 01:59:49.869: INFO: observed Deployment test-deployment in namespace deployment-9163 with ReadyReplicas 3
STEP: deleting the Deployment
Mar  2 01:59:49.907: INFO: observed event type MODIFIED
Mar  2 01:59:49.907: INFO: observed event type MODIFIED
Mar  2 01:59:49.907: INFO: observed event type MODIFIED
Mar  2 01:59:49.907: INFO: observed event type MODIFIED
Mar  2 01:59:49.907: INFO: observed event type MODIFIED
Mar  2 01:59:49.909: INFO: observed event type MODIFIED
Mar  2 01:59:49.909: INFO: observed event type MODIFIED
Mar  2 01:59:49.909: INFO: observed event type MODIFIED
Mar  2 01:59:49.910: INFO: observed event type MODIFIED
Mar  2 01:59:49.910: INFO: observed event type MODIFIED
Mar  2 01:59:49.910: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 01:59:49.919: INFO: Log out all the ReplicaSets if there is no deployment created
Mar  2 01:59:49.940: INFO: ReplicaSet "test-deployment-6b48c869b6":
&ReplicaSet{ObjectMeta:{test-deployment-6b48c869b6  deployment-9163  45803819-ce3b-4351-b46d-3fc97c2ba731 135678 3 2023-03-02 01:59:40 +0000 UTC <nil> <nil> map[pod-template-hash:6b48c869b6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 48790707-a816-4f8f-b3b7-14c5c9df65a1 0xc0043d8827 0xc0043d8828}] []  [{kube-controller-manager Update apps/v1 2023-03-02 01:59:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48790707-a816-4f8f-b3b7-14c5c9df65a1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:59:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6b48c869b6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6b48c869b6 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043d88b0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Mar  2 01:59:49.958: INFO: ReplicaSet "test-deployment-74c6dd549b":
&ReplicaSet{ObjectMeta:{test-deployment-74c6dd549b  deployment-9163  bdb2c91e-3038-4262-953a-8ba002edbcea 135841 2 2023-03-02 01:59:44 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 48790707-a816-4f8f-b3b7-14c5c9df65a1 0xc0043d8917 0xc0043d8918}] []  [{kube-controller-manager Update apps/v1 2023-03-02 01:59:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"48790707-a816-4f8f-b3b7-14c5c9df65a1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 01:59:45 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 74c6dd549b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043d89a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Mar  2 01:59:49.972: INFO: pod: "test-deployment-74c6dd549b-4j9rj":
&Pod{ObjectMeta:{test-deployment-74c6dd549b-4j9rj test-deployment-74c6dd549b- deployment-9163  f4192877-664b-49b9-9ed9-7ef7e8cb7a49 135840 0 2023-03-02 01:59:46 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[cni.projectcalico.org/containerID:9bd7fea667b982a34e6624e398d5c1aa198eba53568705849bde3d985418bc59 cni.projectcalico.org/podIP:172.30.0.228/32 cni.projectcalico.org/podIPs:172.30.0.228/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.0.228"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.0.228"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-74c6dd549b bdb2c91e-3038-4262-953a-8ba002edbcea 0xc0043d8cb7 0xc0043d8cb8}] []  [{kube-controller-manager Update v1 2023-03-02 01:59:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bdb2c91e-3038-4262-953a-8ba002edbcea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-03-02 01:59:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 01:59:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 01:59:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.0.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6jlrg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6jlrg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c61,c50,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-jt7vm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:59:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:59:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:59:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:59:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.50,PodIP:172.30.0.228,StartTime:2023-03-02 01:59:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:59:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://1976c57282b17a7ff186659123031beca4c126aa32aa931a26c61d299c39e4b9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.0.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar  2 01:59:49.973: INFO: pod: "test-deployment-74c6dd549b-l7k9l":
&Pod{ObjectMeta:{test-deployment-74c6dd549b-l7k9l test-deployment-74c6dd549b- deployment-9163  3477bcb6-5638-41d9-a940-b90f03a7b205 135743 0 2023-03-02 01:59:44 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[cni.projectcalico.org/containerID:835a4f47053ca18912c5e23223e1dc0cda942b3db53eed12a687cbfe4de646dd cni.projectcalico.org/podIP:172.30.88.250/32 cni.projectcalico.org/podIPs:172.30.88.250/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.250"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.250"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-74c6dd549b bdb2c91e-3038-4262-953a-8ba002edbcea 0xc0043d8f57 0xc0043d8f58}] []  [{kube-controller-manager Update v1 2023-03-02 01:59:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bdb2c91e-3038-4262-953a-8ba002edbcea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-03-02 01:59:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-03-02 01:59:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.88.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-03-02 01:59:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d65ph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d65ph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.39,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c61,c50,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-jt7vm,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:59:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:59:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:59:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 01:59:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.39,PodIP:172.30.88.250,StartTime:2023-03-02 01:59:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 01:59:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://c527f3a89210c20b189287998ed02761e7ce9bc7efcfdf1224a137b14caa093e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.88.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Mar  2 01:59:49.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9163" for this suite.

• [SLOW TEST:9.702 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":356,"completed":272,"skipped":5290,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 01:59:50.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-fd7bdbba-ccc0-4c24-b632-8919d26fd8ae in namespace container-probe-2881
Mar  2 01:59:52.184: INFO: Started pod busybox-fd7bdbba-ccc0-4c24-b632-8919d26fd8ae in namespace container-probe-2881
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 01:59:52.205: INFO: Initial restart count of pod busybox-fd7bdbba-ccc0-4c24-b632-8919d26fd8ae is 0
Mar  2 02:00:43.090: INFO: Restart count of pod container-probe-2881/busybox-fd7bdbba-ccc0-4c24-b632-8919d26fd8ae is now 1 (50.865569407s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Mar  2 02:00:43.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2881" for this suite.

• [SLOW TEST:53.169 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":273,"skipped":5300,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:00:43.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar  2 02:00:43.456: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ade124a-a9d5-4822-a890-886cead5bd06" in namespace "downward-api-5649" to be "Succeeded or Failed"
W0302 02:00:43.456542      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 02:00:43.471: INFO: Pod "downwardapi-volume-7ade124a-a9d5-4822-a890-886cead5bd06": Phase="Pending", Reason="", readiness=false. Elapsed: 14.486684ms
Mar  2 02:00:45.514: INFO: Pod "downwardapi-volume-7ade124a-a9d5-4822-a890-886cead5bd06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057556854s
Mar  2 02:00:47.539: INFO: Pod "downwardapi-volume-7ade124a-a9d5-4822-a890-886cead5bd06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.082720506s
STEP: Saw pod success
Mar  2 02:00:47.539: INFO: Pod "downwardapi-volume-7ade124a-a9d5-4822-a890-886cead5bd06" satisfied condition "Succeeded or Failed"
Mar  2 02:00:47.552: INFO: Trying to get logs from node 10.123.244.39 pod downwardapi-volume-7ade124a-a9d5-4822-a890-886cead5bd06 container client-container: <nil>
STEP: delete the pod
Mar  2 02:00:47.830: INFO: Waiting for pod downwardapi-volume-7ade124a-a9d5-4822-a890-886cead5bd06 to disappear
Mar  2 02:00:47.846: INFO: Pod downwardapi-volume-7ade124a-a9d5-4822-a890-886cead5bd06 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar  2 02:00:47.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5649" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":274,"skipped":5366,"failed":0}
SSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:00:47.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Mar  2 02:00:48.044: INFO: Waiting up to 5m0s for pod "security-context-256ad364-769d-424a-8891-360cc87b33b7" in namespace "security-context-9576" to be "Succeeded or Failed"
Mar  2 02:00:48.055: INFO: Pod "security-context-256ad364-769d-424a-8891-360cc87b33b7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.580422ms
Mar  2 02:00:50.067: INFO: Pod "security-context-256ad364-769d-424a-8891-360cc87b33b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022736619s
Mar  2 02:00:52.084: INFO: Pod "security-context-256ad364-769d-424a-8891-360cc87b33b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040003766s
Mar  2 02:00:54.098: INFO: Pod "security-context-256ad364-769d-424a-8891-360cc87b33b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053969188s
STEP: Saw pod success
Mar  2 02:00:54.098: INFO: Pod "security-context-256ad364-769d-424a-8891-360cc87b33b7" satisfied condition "Succeeded or Failed"
Mar  2 02:00:54.107: INFO: Trying to get logs from node 10.123.244.39 pod security-context-256ad364-769d-424a-8891-360cc87b33b7 container test-container: <nil>
STEP: delete the pod
Mar  2 02:00:54.154: INFO: Waiting for pod security-context-256ad364-769d-424a-8891-360cc87b33b7 to disappear
Mar  2 02:00:54.162: INFO: Pod security-context-256ad364-769d-424a-8891-360cc87b33b7 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Mar  2 02:00:54.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-9576" for this suite.

• [SLOW TEST:6.335 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":275,"skipped":5372,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:00:54.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Mar  2 02:01:00.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5329" for this suite.

• [SLOW TEST:6.293 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":356,"completed":276,"skipped":5385,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:01:00.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1574
[It] should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Mar  2 02:01:00.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-7038 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar  2 02:01:00.921: INFO: stderr: ""
Mar  2 02:01:00.921: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar  2 02:01:05.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-7038 get pod e2e-test-httpd-pod -o json'
Mar  2 02:01:06.168: INFO: stderr: ""
Mar  2 02:01:06.169: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"71894422dd2a5bd8edcbb2c435fc3af8940422a02d7e1c0f8fd411340f76269e\",\n            \"cni.projectcalico.org/podIP\": \"172.30.88.231/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.88.231/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.88.231\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.88.231\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-03-02T02:01:00Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7038\",\n        \"resourceVersion\": \"136700\",\n        \"uid\": \"987acb85-5e8b-47fe-846e-86d7074a600f\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-76jzk\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-6q472\"\n            }\n        ],\n        \"nodeName\": \"10.123.244.39\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c62,c14\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-76jzk\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T02:01:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T02:01:02Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T02:01:02Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-02T02:01:00Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://a96f4a8390808ff01d4cac36a829fab37727b2aff78a8f6bff0e73bb4c7538eb\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-02T02:01:02Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.123.244.39\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.88.231\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.88.231\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-02T02:01:00Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar  2 02:01:06.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-7038 replace -f -'
Mar  2 02:01:07.381: INFO: stderr: ""
Mar  2 02:01:07.381: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1578
Mar  2 02:01:07.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-7038 delete pods e2e-test-httpd-pod'
Mar  2 02:01:09.417: INFO: stderr: ""
Mar  2 02:01:09.417: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar  2 02:01:09.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7038" for this suite.

• [SLOW TEST:8.936 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1571
    should update a single-container pod's image  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":356,"completed":277,"skipped":5395,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:01:09.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar  2 02:01:09.737: INFO: Waiting up to 5m0s for pod "pod-cccb49ca-e960-4e8d-9125-b32d114f60b8" in namespace "emptydir-5062" to be "Succeeded or Failed"
Mar  2 02:01:09.747: INFO: Pod "pod-cccb49ca-e960-4e8d-9125-b32d114f60b8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.777492ms
Mar  2 02:01:11.759: INFO: Pod "pod-cccb49ca-e960-4e8d-9125-b32d114f60b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02187825s
Mar  2 02:01:13.780: INFO: Pod "pod-cccb49ca-e960-4e8d-9125-b32d114f60b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043348542s
STEP: Saw pod success
Mar  2 02:01:13.780: INFO: Pod "pod-cccb49ca-e960-4e8d-9125-b32d114f60b8" satisfied condition "Succeeded or Failed"
Mar  2 02:01:13.791: INFO: Trying to get logs from node 10.123.244.39 pod pod-cccb49ca-e960-4e8d-9125-b32d114f60b8 container test-container: <nil>
STEP: delete the pod
Mar  2 02:01:13.906: INFO: Waiting for pod pod-cccb49ca-e960-4e8d-9125-b32d114f60b8 to disappear
Mar  2 02:01:13.918: INFO: Pod pod-cccb49ca-e960-4e8d-9125-b32d114f60b8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar  2 02:01:13.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5062" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":278,"skipped":5408,"failed":0}
SSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:01:14.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in volume subpath
Mar  2 02:01:14.290: INFO: Waiting up to 5m0s for pod "var-expansion-2d0f9fa2-8c22-4b38-bc9a-773b78d07f4c" in namespace "var-expansion-459" to be "Succeeded or Failed"
Mar  2 02:01:14.300: INFO: Pod "var-expansion-2d0f9fa2-8c22-4b38-bc9a-773b78d07f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.66688ms
Mar  2 02:01:16.313: INFO: Pod "var-expansion-2d0f9fa2-8c22-4b38-bc9a-773b78d07f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022844728s
Mar  2 02:01:18.333: INFO: Pod "var-expansion-2d0f9fa2-8c22-4b38-bc9a-773b78d07f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043142231s
Mar  2 02:01:20.346: INFO: Pod "var-expansion-2d0f9fa2-8c22-4b38-bc9a-773b78d07f4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056144508s
STEP: Saw pod success
Mar  2 02:01:20.347: INFO: Pod "var-expansion-2d0f9fa2-8c22-4b38-bc9a-773b78d07f4c" satisfied condition "Succeeded or Failed"
Mar  2 02:01:20.359: INFO: Trying to get logs from node 10.123.244.39 pod var-expansion-2d0f9fa2-8c22-4b38-bc9a-773b78d07f4c container dapi-container: <nil>
STEP: delete the pod
Mar  2 02:01:20.422: INFO: Waiting for pod var-expansion-2d0f9fa2-8c22-4b38-bc9a-773b78d07f4c to disappear
Mar  2 02:01:20.431: INFO: Pod var-expansion-2d0f9fa2-8c22-4b38-bc9a-773b78d07f4c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Mar  2 02:01:20.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-459" for this suite.

• [SLOW TEST:6.449 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":356,"completed":279,"skipped":5414,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:01:20.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar  2 02:01:20.604: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a79aad9-2cf6-4ac0-9186-db44ca4c5151" in namespace "projected-663" to be "Succeeded or Failed"
Mar  2 02:01:20.617: INFO: Pod "downwardapi-volume-1a79aad9-2cf6-4ac0-9186-db44ca4c5151": Phase="Pending", Reason="", readiness=false. Elapsed: 12.602339ms
Mar  2 02:01:22.637: INFO: Pod "downwardapi-volume-1a79aad9-2cf6-4ac0-9186-db44ca4c5151": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032863801s
Mar  2 02:01:24.650: INFO: Pod "downwardapi-volume-1a79aad9-2cf6-4ac0-9186-db44ca4c5151": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045333405s
STEP: Saw pod success
Mar  2 02:01:24.650: INFO: Pod "downwardapi-volume-1a79aad9-2cf6-4ac0-9186-db44ca4c5151" satisfied condition "Succeeded or Failed"
Mar  2 02:01:24.672: INFO: Trying to get logs from node 10.123.244.39 pod downwardapi-volume-1a79aad9-2cf6-4ac0-9186-db44ca4c5151 container client-container: <nil>
STEP: delete the pod
Mar  2 02:01:24.749: INFO: Waiting for pod downwardapi-volume-1a79aad9-2cf6-4ac0-9186-db44ca4c5151 to disappear
Mar  2 02:01:24.758: INFO: Pod downwardapi-volume-1a79aad9-2cf6-4ac0-9186-db44ca4c5151 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar  2 02:01:24.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-663" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":280,"skipped":5422,"failed":0}
SSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:01:24.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
W0302 02:01:24.984935      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar  2 02:01:29.558: INFO: Successfully updated pod "adopt-release-gfc6n"
STEP: Checking that the Job readopts the Pod
Mar  2 02:01:29.558: INFO: Waiting up to 15m0s for pod "adopt-release-gfc6n" in namespace "job-9463" to be "adopted"
Mar  2 02:01:29.569: INFO: Pod "adopt-release-gfc6n": Phase="Running", Reason="", readiness=true. Elapsed: 10.320669ms
Mar  2 02:01:31.587: INFO: Pod "adopt-release-gfc6n": Phase="Running", Reason="", readiness=true. Elapsed: 2.028308484s
Mar  2 02:01:31.587: INFO: Pod "adopt-release-gfc6n" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar  2 02:01:32.164: INFO: Successfully updated pod "adopt-release-gfc6n"
STEP: Checking that the Job releases the Pod
Mar  2 02:01:32.165: INFO: Waiting up to 15m0s for pod "adopt-release-gfc6n" in namespace "job-9463" to be "released"
Mar  2 02:01:32.174: INFO: Pod "adopt-release-gfc6n": Phase="Running", Reason="", readiness=true. Elapsed: 9.044067ms
Mar  2 02:01:34.187: INFO: Pod "adopt-release-gfc6n": Phase="Running", Reason="", readiness=true. Elapsed: 2.022424889s
Mar  2 02:01:34.187: INFO: Pod "adopt-release-gfc6n" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Mar  2 02:01:34.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9463" for this suite.

• [SLOW TEST:9.430 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":356,"completed":281,"skipped":5427,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:01:34.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar  2 02:01:34.411: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5a022f2-587a-48dc-a942-31f826897a81" in namespace "downward-api-4716" to be "Succeeded or Failed"
Mar  2 02:01:34.439: INFO: Pod "downwardapi-volume-f5a022f2-587a-48dc-a942-31f826897a81": Phase="Pending", Reason="", readiness=false. Elapsed: 27.937456ms
Mar  2 02:01:36.451: INFO: Pod "downwardapi-volume-f5a022f2-587a-48dc-a942-31f826897a81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040126882s
Mar  2 02:01:38.464: INFO: Pod "downwardapi-volume-f5a022f2-587a-48dc-a942-31f826897a81": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053230465s
Mar  2 02:01:40.500: INFO: Pod "downwardapi-volume-f5a022f2-587a-48dc-a942-31f826897a81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.089245437s
STEP: Saw pod success
Mar  2 02:01:40.500: INFO: Pod "downwardapi-volume-f5a022f2-587a-48dc-a942-31f826897a81" satisfied condition "Succeeded or Failed"
Mar  2 02:01:40.510: INFO: Trying to get logs from node 10.123.244.39 pod downwardapi-volume-f5a022f2-587a-48dc-a942-31f826897a81 container client-container: <nil>
STEP: delete the pod
Mar  2 02:01:40.560: INFO: Waiting for pod downwardapi-volume-f5a022f2-587a-48dc-a942-31f826897a81 to disappear
Mar  2 02:01:40.572: INFO: Pod downwardapi-volume-f5a022f2-587a-48dc-a942-31f826897a81 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar  2 02:01:40.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4716" for this suite.

• [SLOW TEST:6.414 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":282,"skipped":5433,"failed":0}
SSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:01:40.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar  2 02:01:42.268: INFO: starting watch
STEP: patching
STEP: updating
Mar  2 02:01:42.313: INFO: waiting for watch events with expected annotations
Mar  2 02:01:42.313: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 02:01:42.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-6226" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":356,"completed":283,"skipped":5439,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:01:42.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar  2 02:01:42.810: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de910149-39b9-4d21-8293-3d2abbcf8887" in namespace "downward-api-9621" to be "Succeeded or Failed"
Mar  2 02:01:42.819: INFO: Pod "downwardapi-volume-de910149-39b9-4d21-8293-3d2abbcf8887": Phase="Pending", Reason="", readiness=false. Elapsed: 9.22655ms
Mar  2 02:01:44.891: INFO: Pod "downwardapi-volume-de910149-39b9-4d21-8293-3d2abbcf8887": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080628621s
Mar  2 02:01:46.904: INFO: Pod "downwardapi-volume-de910149-39b9-4d21-8293-3d2abbcf8887": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.093965936s
STEP: Saw pod success
Mar  2 02:01:46.904: INFO: Pod "downwardapi-volume-de910149-39b9-4d21-8293-3d2abbcf8887" satisfied condition "Succeeded or Failed"
Mar  2 02:01:46.913: INFO: Trying to get logs from node 10.123.244.39 pod downwardapi-volume-de910149-39b9-4d21-8293-3d2abbcf8887 container client-container: <nil>
STEP: delete the pod
Mar  2 02:01:46.988: INFO: Waiting for pod downwardapi-volume-de910149-39b9-4d21-8293-3d2abbcf8887 to disappear
Mar  2 02:01:46.998: INFO: Pod downwardapi-volume-de910149-39b9-4d21-8293-3d2abbcf8887 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar  2 02:01:46.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9621" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":284,"skipped":5457,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:01:47.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override arguments
Mar  2 02:01:47.189: INFO: Waiting up to 5m0s for pod "client-containers-4048777d-b74d-4c17-83ad-4db41a77631d" in namespace "containers-2383" to be "Succeeded or Failed"
Mar  2 02:01:47.204: INFO: Pod "client-containers-4048777d-b74d-4c17-83ad-4db41a77631d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.822816ms
Mar  2 02:01:49.217: INFO: Pod "client-containers-4048777d-b74d-4c17-83ad-4db41a77631d": Phase="Running", Reason="", readiness=true. Elapsed: 2.02824788s
Mar  2 02:01:51.230: INFO: Pod "client-containers-4048777d-b74d-4c17-83ad-4db41a77631d": Phase="Running", Reason="", readiness=false. Elapsed: 4.040840507s
Mar  2 02:01:53.253: INFO: Pod "client-containers-4048777d-b74d-4c17-83ad-4db41a77631d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064493704s
STEP: Saw pod success
Mar  2 02:01:53.254: INFO: Pod "client-containers-4048777d-b74d-4c17-83ad-4db41a77631d" satisfied condition "Succeeded or Failed"
Mar  2 02:01:53.262: INFO: Trying to get logs from node 10.123.244.39 pod client-containers-4048777d-b74d-4c17-83ad-4db41a77631d container agnhost-container: <nil>
STEP: delete the pod
Mar  2 02:01:53.361: INFO: Waiting for pod client-containers-4048777d-b74d-4c17-83ad-4db41a77631d to disappear
Mar  2 02:01:53.370: INFO: Pod client-containers-4048777d-b74d-4c17-83ad-4db41a77631d no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Mar  2 02:01:53.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2383" for this suite.

• [SLOW TEST:6.388 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","total":356,"completed":285,"skipped":5474,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:01:53.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3287
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Mar  2 02:01:53.601: INFO: Found 0 stateful pods, waiting for 3
Mar  2 02:02:03.617: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 02:02:03.618: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 02:02:03.618: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Mar  2 02:02:03.687: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar  2 02:02:13.998: INFO: Updating stateful set ss2
Mar  2 02:02:14.100: INFO: Waiting for Pod statefulset-3287/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Mar  2 02:02:24.289: INFO: Found 1 stateful pods, waiting for 3
Mar  2 02:02:34.310: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 02:02:34.310: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 02:02:34.310: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar  2 02:02:34.383: INFO: Updating stateful set ss2
Mar  2 02:02:34.405: INFO: Waiting for Pod statefulset-3287/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Mar  2 02:02:44.505: INFO: Updating stateful set ss2
Mar  2 02:02:44.560: INFO: Waiting for StatefulSet statefulset-3287/ss2 to complete update
Mar  2 02:02:44.560: INFO: Waiting for Pod statefulset-3287/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 02:02:54.633: INFO: Deleting all statefulset in ns statefulset-3287
Mar  2 02:02:54.650: INFO: Scaling statefulset ss2 to 0
Mar  2 02:03:04.754: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 02:03:04.764: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Mar  2 02:03:04.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3287" for this suite.

• [SLOW TEST:71.573 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":356,"completed":286,"skipped":5478,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:03:04.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar  2 02:03:05.240: INFO: The status of Pod pod-update-activedeadlineseconds-2354cbdc-ca83-4d87-b0c6-b66612172368 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:03:07.253: INFO: The status of Pod pod-update-activedeadlineseconds-2354cbdc-ca83-4d87-b0c6-b66612172368 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar  2 02:03:07.850: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2354cbdc-ca83-4d87-b0c6-b66612172368"
Mar  2 02:03:07.850: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2354cbdc-ca83-4d87-b0c6-b66612172368" in namespace "pods-962" to be "terminated due to deadline exceeded"
Mar  2 02:03:07.859: INFO: Pod "pod-update-activedeadlineseconds-2354cbdc-ca83-4d87-b0c6-b66612172368": Phase="Running", Reason="", readiness=true. Elapsed: 9.115146ms
Mar  2 02:03:09.873: INFO: Pod "pod-update-activedeadlineseconds-2354cbdc-ca83-4d87-b0c6-b66612172368": Phase="Running", Reason="", readiness=true. Elapsed: 2.023308048s
Mar  2 02:03:11.894: INFO: Pod "pod-update-activedeadlineseconds-2354cbdc-ca83-4d87-b0c6-b66612172368": Phase="Running", Reason="", readiness=false. Elapsed: 4.044527066s
Mar  2 02:03:13.936: INFO: Pod "pod-update-activedeadlineseconds-2354cbdc-ca83-4d87-b0c6-b66612172368": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.086428492s
Mar  2 02:03:13.936: INFO: Pod "pod-update-activedeadlineseconds-2354cbdc-ca83-4d87-b0c6-b66612172368" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Mar  2 02:03:13.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-962" for this suite.

• [SLOW TEST:9.052 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":356,"completed":287,"skipped":5490,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:03:14.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 02:03:15.405: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 02:03:18.508: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 02:03:18.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 02:03:21.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6788" for this suite.
STEP: Destroying namespace "webhook-6788-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:8.394 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":356,"completed":288,"skipped":5490,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:03:22.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-504.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-504.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-504.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-504.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 02:03:27.935: INFO: DNS probes using dns-test-ace93dc0-602c-42ca-ae72-55a4d65a9ebe succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-504.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-504.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-504.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-504.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 02:03:32.373: INFO: DNS probes using dns-test-171940ca-89d8-40a7-94cf-f3e5cf3981af succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-504.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-504.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-504.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-504.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 02:03:36.724: INFO: DNS probes using dns-test-b8fe0223-42b2-40be-810e-6352091cd2f8 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Mar  2 02:03:36.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-504" for this suite.

• [SLOW TEST:13.921 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":356,"completed":289,"skipped":5494,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:03:36.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  2 02:03:37.005: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 02:03:37.157: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 02:03:37.202: INFO: 
Logging pods the apiserver thinks is on node 10.123.244.39 before test
Mar  2 02:03:37.243: INFO: calico-node-vkllz from calico-system started at 2023-03-01 22:01:04 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.243: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 02:03:37.243: INFO: calico-typha-db9579c55-nnjdl from calico-system started at 2023-03-01 22:01:04 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.243: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 02:03:37.243: INFO: ibm-keepalived-watcher-hf2lb from kube-system started at 2023-03-01 22:00:56 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.243: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 02:03:37.243: INFO: ibm-master-proxy-static-10.123.244.39 from kube-system started at 2023-03-01 22:00:52 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.243: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 02:03:37.243: INFO: 	Container pause ready: true, restart count 0
Mar  2 02:03:37.243: INFO: ibmcloud-block-storage-driver-qqdnc from kube-system started at 2023-03-01 22:01:08 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.243: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 02:03:37.243: INFO: tuned-7g6rx from openshift-cluster-node-tuning-operator started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.243: INFO: 	Container tuned ready: true, restart count 0
Mar  2 02:03:37.243: INFO: dns-default-rsb27 from openshift-dns started at 2023-03-02 00:51:04 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.243: INFO: 	Container dns ready: true, restart count 0
Mar  2 02:03:37.243: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.243: INFO: node-resolver-678rp from openshift-dns started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.243: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 02:03:37.244: INFO: node-ca-2dknm from openshift-image-registry started at 2023-03-01 22:02:13 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.244: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 02:03:37.244: INFO: ingress-canary-kfdpg from openshift-ingress-canary started at 2023-03-02 00:51:05 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.244: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 02:03:37.244: INFO: openshift-kube-proxy-tzkhx from openshift-kube-proxy started at 2023-03-01 22:00:56 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.244: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 02:03:37.244: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.244: INFO: redhat-marketplace-ffkpx from openshift-marketplace started at 2023-03-02 00:52:46 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.244: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:03:37.244: INFO: node-exporter-9hccw from openshift-monitoring started at 2023-03-01 22:06:02 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.244: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.244: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 02:03:37.244: INFO: multus-additional-cni-plugins-h44x5 from openshift-multus started at 2023-03-01 22:00:56 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.244: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 02:03:37.244: INFO: multus-admission-controller-mtp8j from openshift-multus started at 2023-03-02 00:51:09 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.244: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.244: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 02:03:37.244: INFO: multus-tcchz from openshift-multus started at 2023-03-01 22:00:56 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.244: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 02:03:37.244: INFO: network-metrics-daemon-865xx from openshift-multus started at 2023-03-01 22:00:56 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.245: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.245: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 02:03:37.245: INFO: network-check-target-2gwsp from openshift-network-diagnostics started at 2023-03-01 22:00:56 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.245: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 02:03:37.245: INFO: collect-profiles-27962010-s8z59 from openshift-operator-lifecycle-manager started at 2023-03-02 01:30:00 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.245: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 02:03:37.245: INFO: collect-profiles-27962025-tqwrx from openshift-operator-lifecycle-manager started at 2023-03-02 01:45:00 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.245: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 02:03:37.245: INFO: collect-profiles-27962040-slsnz from openshift-operator-lifecycle-manager started at 2023-03-02 02:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.245: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 02:03:37.245: INFO: sonobuoy-systemd-logs-daemon-set-f72afd02755d449d-vlqt6 from sonobuoy started at 2023-03-02 00:42:02 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.245: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:03:37.245: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 02:03:37.245: INFO: 
Logging pods the apiserver thinks is on node 10.123.244.41 before test
Mar  2 02:03:37.319: INFO: calico-kube-controllers-589f57b7f-tm9tz from calico-system started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  2 02:03:37.319: INFO: calico-node-6flhq from calico-system started at 2023-03-01 22:01:04 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 02:03:37.319: INFO: managed-storage-validation-webhooks-85f57b66cf-79986 from ibm-odf-validation-webhook started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Mar  2 02:03:37.319: INFO: managed-storage-validation-webhooks-85f57b66cf-mqr5f from ibm-odf-validation-webhook started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 02:03:37.319: INFO: managed-storage-validation-webhooks-85f57b66cf-pwdx9 from ibm-odf-validation-webhook started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 02:03:37.319: INFO: ibm-cloud-provider-ip-149-81-197-212-d64d8f476-bks7g from ibm-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container ibm-cloud-provider-ip-149-81-197-212 ready: true, restart count 0
Mar  2 02:03:37.319: INFO: ibm-file-plugin-5fdd985647-mldf5 from kube-system started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar  2 02:03:37.319: INFO: ibm-keepalived-watcher-mjldf from kube-system started at 2023-03-01 22:00:13 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 02:03:37.319: INFO: ibm-master-proxy-static-10.123.244.41 from kube-system started at 2023-03-01 22:00:11 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 02:03:37.319: INFO: 	Container pause ready: true, restart count 0
Mar  2 02:03:37.319: INFO: ibm-storage-metrics-agent-6b696986d4-jwmxc from kube-system started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Mar  2 02:03:37.319: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 02:03:37.319: INFO: ibm-storage-watcher-64987648df-vl847 from kube-system started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar  2 02:03:37.319: INFO: ibmcloud-block-storage-driver-h5vmt from kube-system started at 2023-03-01 22:00:19 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 02:03:37.319: INFO: ibmcloud-block-storage-plugin-7cc7c95d6d-tw99v from kube-system started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar  2 02:03:37.319: INFO: cluster-node-tuning-operator-74fbdd5d47-scz8r from openshift-cluster-node-tuning-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Mar  2 02:03:37.319: INFO: tuned-bqzdt from openshift-cluster-node-tuning-operator started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container tuned ready: true, restart count 0
Mar  2 02:03:37.319: INFO: cluster-samples-operator-86c59f694d-klcrd from openshift-cluster-samples-operator started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Mar  2 02:03:37.319: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Mar  2 02:03:37.319: INFO: cluster-storage-operator-5b746f999f-vv6t5 from openshift-cluster-storage-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Mar  2 02:03:37.319: INFO: csi-snapshot-controller-8576577866-4lrgg from openshift-cluster-storage-operator started at 2023-03-01 22:01:48 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 02:03:37.319: INFO: csi-snapshot-controller-8576577866-8cdgr from openshift-cluster-storage-operator started at 2023-03-01 22:01:48 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.319: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 02:03:37.320: INFO: csi-snapshot-controller-operator-75849b8ccf-nmpqq from openshift-cluster-storage-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Mar  2 02:03:37.320: INFO: csi-snapshot-webhook-786787f645-wxmqk from openshift-cluster-storage-operator started at 2023-03-01 22:01:45 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container webhook ready: true, restart count 0
Mar  2 02:03:37.320: INFO: csi-snapshot-webhook-786787f645-zbq67 from openshift-cluster-storage-operator started at 2023-03-01 22:01:45 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container webhook ready: true, restart count 0
Mar  2 02:03:37.320: INFO: console-operator-67666f4bd-45tdk from openshift-console-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container console-operator ready: true, restart count 1
Mar  2 02:03:37.320: INFO: console-5549dcfdd9-gf5fk from openshift-console started at 2023-03-01 22:08:36 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container console ready: true, restart count 0
Mar  2 02:03:37.320: INFO: downloads-5b4f566bc5-hlvxl from openshift-console started at 2023-03-01 22:02:08 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container download-server ready: true, restart count 0
Mar  2 02:03:37.320: INFO: dns-operator-6b74679c6d-xqlfl from openshift-dns-operator started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container dns-operator ready: true, restart count 0
Mar  2 02:03:37.320: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.320: INFO: dns-default-c4wx7 from openshift-dns started at 2023-03-01 22:05:24 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container dns ready: true, restart count 0
Mar  2 02:03:37.320: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.320: INFO: node-resolver-dgx88 from openshift-dns started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 02:03:37.320: INFO: cluster-image-registry-operator-5846b9d966-sj4bv from openshift-image-registry started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Mar  2 02:03:37.320: INFO: node-ca-9vk8c from openshift-image-registry started at 2023-03-01 22:02:13 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 02:03:37.320: INFO: ingress-canary-5gtz6 from openshift-ingress-canary started at 2023-03-01 22:02:18 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 02:03:37.320: INFO: ingress-operator-5fdf7c4bb-8jgbn from openshift-ingress-operator started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container ingress-operator ready: true, restart count 0
Mar  2 02:03:37.320: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.320: INFO: router-default-555588874-8ffhk from openshift-ingress started at 2023-03-01 22:27:19 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container router ready: true, restart count 0
Mar  2 02:03:37.320: INFO: insights-operator-5449cdb4b9-7xt8p from openshift-insights started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container insights-operator ready: true, restart count 1
Mar  2 02:03:37.320: INFO: openshift-kube-proxy-j4ff5 from openshift-kube-proxy started at 2023-03-01 22:00:51 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 02:03:37.320: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.320: INFO: kube-storage-version-migrator-operator-66dd7b9865-9lrlp from openshift-kube-storage-version-migrator-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Mar  2 02:03:37.320: INFO: migrator-767b585794-kkngg from openshift-kube-storage-version-migrator started at 2023-03-01 22:02:08 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container migrator ready: true, restart count 0
Mar  2 02:03:37.320: INFO: marketplace-operator-d98c65969-9hhnp from openshift-marketplace started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container marketplace-operator ready: true, restart count 0
Mar  2 02:03:37.320: INFO: redhat-operators-mxdgk from openshift-marketplace started at 2023-03-02 00:50:46 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:03:37.320: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-03-01 22:06:09 +0000 UTC (6 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 02:03:37.320: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 02:03:37.320: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:03:37.320: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.320: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Mar  2 02:03:37.320: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:03:37.320: INFO: cluster-monitoring-operator-8584844f6f-8bkqg from openshift-monitoring started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Mar  2 02:03:37.320: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.320: INFO: node-exporter-z8kbd from openshift-monitoring started at 2023-03-01 22:06:02 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.320: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 02:03:37.320: INFO: prometheus-adapter-646cdcc6f7-vb8tx from openshift-monitoring started at 2023-03-01 22:07:14 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 02:03:37.320: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-03-01 22:06:25 +0000 UTC (6 container statuses recorded)
Mar  2 02:03:37.320: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:03:37.321: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.321: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 02:03:37.321: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 02:03:37.321: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 02:03:37.321: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 02:03:37.321: INFO: prometheus-operator-admission-webhook-78c85948cd-7dxdr from openshift-monitoring started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Mar  2 02:03:37.321: INFO: telemeter-client-7b8cf9859c-5jxpn from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (3 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.321: INFO: 	Container reload ready: true, restart count 0
Mar  2 02:03:37.321: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 02:03:37.321: INFO: thanos-querier-697f97f554-wxl4x from openshift-monitoring started at 2023-03-01 22:06:15 +0000 UTC (6 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.321: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Mar  2 02:03:37.321: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 02:03:37.321: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 02:03:37.321: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:03:37.321: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 02:03:37.321: INFO: multus-additional-cni-plugins-mj5tl from openshift-multus started at 2023-03-01 22:00:46 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 02:03:37.321: INFO: multus-admission-controller-tbdnc from openshift-multus started at 2023-03-01 22:05:24 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.321: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 02:03:37.321: INFO: multus-dktpl from openshift-multus started at 2023-03-01 22:00:46 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 02:03:37.321: INFO: network-metrics-daemon-zl9qv from openshift-multus started at 2023-03-01 22:00:47 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.321: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 02:03:37.321: INFO: network-check-source-59cbfb554c-7kpmg from openshift-network-diagnostics started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container check-endpoints ready: true, restart count 0
Mar  2 02:03:37.321: INFO: network-check-target-nvxbw from openshift-network-diagnostics started at 2023-03-01 22:00:54 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 02:03:37.321: INFO: network-operator-7b666665c4-c8w4p from openshift-network-operator started at 2023-03-01 22:00:23 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container network-operator ready: true, restart count 1
Mar  2 02:03:37.321: INFO: catalog-operator-5b55d75455-w899d from openshift-operator-lifecycle-manager started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container catalog-operator ready: true, restart count 0
Mar  2 02:03:37.321: INFO: olm-operator-d785dddf9-v54mf from openshift-operator-lifecycle-manager started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container olm-operator ready: true, restart count 0
Mar  2 02:03:37.321: INFO: package-server-manager-5c8469f7c6-prjpw from openshift-operator-lifecycle-manager started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container package-server-manager ready: true, restart count 0
Mar  2 02:03:37.321: INFO: packageserver-bb46bbdb7-dr4v9 from openshift-operator-lifecycle-manager started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 02:03:37.321: INFO: metrics-5d9985b7b6-zpwc2 from openshift-roks-metrics started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container metrics ready: true, restart count 2
Mar  2 02:03:37.321: INFO: push-gateway-6fcf467c7d-8fxh8 from openshift-roks-metrics started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container push-gateway ready: true, restart count 0
Mar  2 02:03:37.321: INFO: service-ca-operator-865b774c95-b82zk from openshift-service-ca-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container service-ca-operator ready: true, restart count 1
Mar  2 02:03:37.321: INFO: service-ca-5995968f9b-25cgt from openshift-service-ca started at 2023-03-01 22:01:47 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container service-ca-controller ready: true, restart count 0
Mar  2 02:03:37.321: INFO: sonobuoy-systemd-logs-daemon-set-f72afd02755d449d-kddl5 from sonobuoy started at 2023-03-02 00:42:02 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:03:37.321: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 02:03:37.321: INFO: tigera-operator-f58c87f48-c9x9k from tigera-operator started at 2023-03-01 22:00:23 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.321: INFO: 	Container tigera-operator ready: true, restart count 1
Mar  2 02:03:37.321: INFO: 
Logging pods the apiserver thinks is on node 10.123.244.50 before test
Mar  2 02:03:37.365: INFO: calico-node-drfb9 from calico-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.365: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 02:03:37.366: INFO: calico-typha-db9579c55-7mslq from calico-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 02:03:37.366: INFO: ibm-cloud-provider-ip-149-81-197-212-d64d8f476-rq24c from ibm-system started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container ibm-cloud-provider-ip-149-81-197-212 ready: true, restart count 0
Mar  2 02:03:37.366: INFO: ibm-keepalived-watcher-bmgn9 from kube-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 02:03:37.366: INFO: ibm-master-proxy-static-10.123.244.50 from kube-system started at 2023-03-01 22:03:27 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container pause ready: true, restart count 0
Mar  2 02:03:37.366: INFO: ibmcloud-block-storage-driver-qm9pz from kube-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 02:03:37.366: INFO: vpn-5bbdb546b8-9w74v from kube-system started at 2023-03-01 22:19:58 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container vpn ready: true, restart count 0
Mar  2 02:03:37.366: INFO: tuned-6k6k4 from openshift-cluster-node-tuning-operator started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container tuned ready: true, restart count 0
Mar  2 02:03:37.366: INFO: console-5549dcfdd9-w5zs4 from openshift-console started at 2023-03-01 22:08:10 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container console ready: true, restart count 0
Mar  2 02:03:37.366: INFO: downloads-5b4f566bc5-wpgn5 from openshift-console started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container download-server ready: true, restart count 1
Mar  2 02:03:37.366: INFO: dns-default-q4vfw from openshift-dns started at 2023-03-01 22:08:04 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container dns ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.366: INFO: node-resolver-r6ktd from openshift-dns started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 02:03:37.366: INFO: image-pruner-27961920-bg4sx from openshift-image-registry started at 2023-03-02 00:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container image-pruner ready: false, restart count 0
Mar  2 02:03:37.366: INFO: image-registry-86657c9d77-wtb4h from openshift-image-registry started at 2023-03-01 22:48:17 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container registry ready: true, restart count 0
Mar  2 02:03:37.366: INFO: node-ca-cjkk7 from openshift-image-registry started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 02:03:37.366: INFO: registry-pvc-permissions-6jq4m from openshift-image-registry started at 2023-03-01 22:48:17 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container pvc-permissions ready: false, restart count 0
Mar  2 02:03:37.366: INFO: ingress-canary-nkd4d from openshift-ingress-canary started at 2023-03-01 22:08:04 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 02:03:37.366: INFO: router-default-555588874-w7r9r from openshift-ingress started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container router ready: true, restart count 0
Mar  2 02:03:37.366: INFO: openshift-kube-proxy-hnz52 from openshift-kube-proxy started at 2023-03-01 22:05:24 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.366: INFO: certified-operators-rdltn from openshift-marketplace started at 2023-03-02 00:50:44 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:03:37.366: INFO: community-operators-zrr7v from openshift-marketplace started at 2023-03-02 00:50:44 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:03:37.366: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-03-02 00:50:47 +0000 UTC (6 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:03:37.366: INFO: kube-state-metrics-84464bb775-hgv8b from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (3 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 02:03:37.366: INFO: node-exporter-mlwcg from openshift-monitoring started at 2023-03-01 22:06:02 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 02:03:37.366: INFO: openshift-state-metrics-6486d6b674-g7xp7 from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (3 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar  2 02:03:37.366: INFO: prometheus-adapter-646cdcc6f7-9m9ls from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 02:03:37.366: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-03-02 00:50:46 +0000 UTC (6 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 02:03:37.366: INFO: prometheus-operator-5d8ddb4f-cs9vq from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.366: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.366: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 02:03:37.367: INFO: prometheus-operator-admission-webhook-78c85948cd-kxbf8 from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.367: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Mar  2 02:03:37.367: INFO: thanos-querier-697f97f554-49n2l from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (6 container statuses recorded)
Mar  2 02:03:37.367: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.367: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Mar  2 02:03:37.367: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 02:03:37.367: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 02:03:37.367: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:03:37.367: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 02:03:37.367: INFO: multus-78s9d from openshift-multus started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.367: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 02:03:37.367: INFO: multus-additional-cni-plugins-x67r8 from openshift-multus started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.367: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 02:03:37.367: INFO: multus-admission-controller-8d7qw from openshift-multus started at 2023-03-01 22:08:04 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.367: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.367: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 02:03:37.367: INFO: network-metrics-daemon-9m6hn from openshift-multus started at 2023-03-01 22:05:25 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.367: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:03:37.367: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 02:03:37.367: INFO: network-check-target-ckmqs from openshift-network-diagnostics started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.367: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 02:03:37.367: INFO: packageserver-bb46bbdb7-2kq2k from openshift-operator-lifecycle-manager started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.367: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 02:03:37.367: INFO: sonobuoy from sonobuoy started at 2023-03-02 00:41:53 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.367: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 02:03:37.367: INFO: sonobuoy-e2e-job-370728edd32948a1 from sonobuoy started at 2023-03-02 00:42:02 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.367: INFO: 	Container e2e ready: true, restart count 0
Mar  2 02:03:37.367: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:03:37.367: INFO: sonobuoy-systemd-logs-daemon-set-f72afd02755d449d-pjpm6 from sonobuoy started at 2023-03-02 00:42:02 +0000 UTC (2 container statuses recorded)
Mar  2 02:03:37.367: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:03:37.367: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 02:03:37.367: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-03-01 22:20:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:03:37.367: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-02a82222-4cb6-4a5d-b1c8-dcf833491e0a 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-02a82222-4cb6-4a5d-b1c8-dcf833491e0a off the node 10.123.244.39
STEP: verifying the node doesn't have the label kubernetes.io/e2e-02a82222-4cb6-4a5d-b1c8-dcf833491e0a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Mar  2 02:03:41.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2548" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":356,"completed":290,"skipped":5499,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:03:41.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override command
Mar  2 02:03:41.974: INFO: Waiting up to 5m0s for pod "client-containers-9481f856-b84d-4f18-bccf-75620289a893" in namespace "containers-7760" to be "Succeeded or Failed"
Mar  2 02:03:41.986: INFO: Pod "client-containers-9481f856-b84d-4f18-bccf-75620289a893": Phase="Pending", Reason="", readiness=false. Elapsed: 12.642375ms
Mar  2 02:03:44.006: INFO: Pod "client-containers-9481f856-b84d-4f18-bccf-75620289a893": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032006418s
Mar  2 02:03:46.018: INFO: Pod "client-containers-9481f856-b84d-4f18-bccf-75620289a893": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044422239s
Mar  2 02:03:48.036: INFO: Pod "client-containers-9481f856-b84d-4f18-bccf-75620289a893": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062792466s
STEP: Saw pod success
Mar  2 02:03:48.036: INFO: Pod "client-containers-9481f856-b84d-4f18-bccf-75620289a893" satisfied condition "Succeeded or Failed"
Mar  2 02:03:48.047: INFO: Trying to get logs from node 10.123.244.39 pod client-containers-9481f856-b84d-4f18-bccf-75620289a893 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 02:03:48.120: INFO: Waiting for pod client-containers-9481f856-b84d-4f18-bccf-75620289a893 to disappear
Mar  2 02:03:48.128: INFO: Pod client-containers-9481f856-b84d-4f18-bccf-75620289a893 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Mar  2 02:03:48.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7760" for this suite.

• [SLOW TEST:6.329 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","total":356,"completed":291,"skipped":5509,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:03:48.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 02:03:49.697: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 02:03:52.848: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 02:03:53.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2639" for this suite.
STEP: Destroying namespace "webhook-2639-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.480 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":356,"completed":292,"skipped":5530,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:03:53.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 02:03:53.822: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-8a2630ac-10ed-4f62-8f29-44afb51aa9f9" in namespace "security-context-test-4283" to be "Succeeded or Failed"
Mar  2 02:03:53.864: INFO: Pod "alpine-nnp-false-8a2630ac-10ed-4f62-8f29-44afb51aa9f9": Phase="Pending", Reason="", readiness=false. Elapsed: 42.128293ms
Mar  2 02:03:55.877: INFO: Pod "alpine-nnp-false-8a2630ac-10ed-4f62-8f29-44afb51aa9f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05524513s
Mar  2 02:03:57.889: INFO: Pod "alpine-nnp-false-8a2630ac-10ed-4f62-8f29-44afb51aa9f9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067168803s
Mar  2 02:03:59.919: INFO: Pod "alpine-nnp-false-8a2630ac-10ed-4f62-8f29-44afb51aa9f9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.09673838s
Mar  2 02:04:01.931: INFO: Pod "alpine-nnp-false-8a2630ac-10ed-4f62-8f29-44afb51aa9f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.109357012s
Mar  2 02:04:01.931: INFO: Pod "alpine-nnp-false-8a2630ac-10ed-4f62-8f29-44afb51aa9f9" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Mar  2 02:04:01.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4283" for this suite.

• [SLOW TEST:8.338 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:298
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":293,"skipped":5572,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:04:01.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1412
STEP: creating an pod
Mar  2 02:04:02.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-2623 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.39 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar  2 02:04:02.393: INFO: stderr: ""
Mar  2 02:04:02.393: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for log generator to start.
Mar  2 02:04:02.393: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar  2 02:04:02.393: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2623" to be "running and ready, or succeeded"
Mar  2 02:04:02.406: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 13.324032ms
Mar  2 02:04:04.419: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.025778832s
Mar  2 02:04:04.419: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar  2 02:04:04.419: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar  2 02:04:04.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-2623 logs logs-generator logs-generator'
Mar  2 02:04:05.310: INFO: stderr: ""
Mar  2 02:04:05.310: INFO: stdout: "I0302 02:04:03.584321       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/rrw 492\nI0302 02:04:03.784934       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/qp9 476\nI0302 02:04:03.985343       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/7kc 295\nI0302 02:04:04.184756       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/2xdn 300\nI0302 02:04:04.385163       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/w6z5 393\nI0302 02:04:04.584517       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/vxqk 517\nI0302 02:04:04.784854       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/42zp 223\nI0302 02:04:04.985296       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/pcp 353\nI0302 02:04:05.184690       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/zx2 204\n"
STEP: limiting log lines
Mar  2 02:04:05.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-2623 logs logs-generator logs-generator --tail=1'
Mar  2 02:04:06.020: INFO: stderr: ""
Mar  2 02:04:06.020: INFO: stdout: "I0302 02:04:05.784729       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/87hd 452\n"
Mar  2 02:04:06.020: INFO: got output "I0302 02:04:05.784729       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/87hd 452\n"
STEP: limiting log bytes
Mar  2 02:04:06.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-2623 logs logs-generator logs-generator --limit-bytes=1'
Mar  2 02:04:06.518: INFO: stderr: ""
Mar  2 02:04:06.518: INFO: stdout: "I"
Mar  2 02:04:06.518: INFO: got output "I"
STEP: exposing timestamps
Mar  2 02:04:06.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-2623 logs logs-generator logs-generator --tail=1 --timestamps'
Mar  2 02:04:06.874: INFO: stderr: ""
Mar  2 02:04:06.874: INFO: stdout: "2023-03-01T20:04:06.784698975-06:00 I0302 02:04:06.784640       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/vg5 332\n"
Mar  2 02:04:06.874: INFO: got output "2023-03-01T20:04:06.784698975-06:00 I0302 02:04:06.784640       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/vg5 332\n"
STEP: restricting to a time range
Mar  2 02:04:09.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-2623 logs logs-generator logs-generator --since=1s'
Mar  2 02:04:09.627: INFO: stderr: ""
Mar  2 02:04:09.627: INFO: stdout: "I0302 02:04:08.784737       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/b225 592\nI0302 02:04:08.985214       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/j8s 355\nI0302 02:04:09.184546       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/7p4 500\nI0302 02:04:09.384900       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/hzb 499\nI0302 02:04:09.585369       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/ns/pods/j2bj 556\n"
Mar  2 02:04:09.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-2623 logs logs-generator logs-generator --since=24h'
Mar  2 02:04:09.860: INFO: stderr: ""
Mar  2 02:04:09.860: INFO: stdout: "I0302 02:04:03.584321       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/rrw 492\nI0302 02:04:03.784934       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/qp9 476\nI0302 02:04:03.985343       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/7kc 295\nI0302 02:04:04.184756       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/2xdn 300\nI0302 02:04:04.385163       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/w6z5 393\nI0302 02:04:04.584517       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/vxqk 517\nI0302 02:04:04.784854       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/42zp 223\nI0302 02:04:04.985296       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/pcp 353\nI0302 02:04:05.184690       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/zx2 204\nI0302 02:04:05.384601       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/6p6 330\nI0302 02:04:05.585246       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/f6bg 261\nI0302 02:04:05.784729       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/87hd 452\nI0302 02:04:05.985230       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/sqz7 526\nI0302 02:04:06.184538       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/xv8k 327\nI0302 02:04:06.384908       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/897z 406\nI0302 02:04:06.585261       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/b45c 282\nI0302 02:04:06.784640       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/vg5 332\nI0302 02:04:06.985052       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/2t9p 272\nI0302 02:04:07.184363       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/vcbt 234\nI0302 02:04:07.384608       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/9x8s 477\nI0302 02:04:07.585030       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/xxbm 252\nI0302 02:04:07.784340       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/5dgd 367\nI0302 02:04:07.984985       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/5sr 537\nI0302 02:04:08.185529       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/s6cn 475\nI0302 02:04:08.384942       1 logs_generator.go:76] 24 GET /api/v1/namespaces/kube-system/pods/z42h 372\nI0302 02:04:08.584459       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/cr6p 340\nI0302 02:04:08.784737       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/b225 592\nI0302 02:04:08.985214       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/ns/pods/j8s 355\nI0302 02:04:09.184546       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/7p4 500\nI0302 02:04:09.384900       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/hzb 499\nI0302 02:04:09.585369       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/ns/pods/j2bj 556\nI0302 02:04:09.784758       1 logs_generator.go:76] 31 GET /api/v1/namespaces/kube-system/pods/klg 208\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1417
Mar  2 02:04:09.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-2623 delete pod logs-generator'
Mar  2 02:04:11.375: INFO: stderr: ""
Mar  2 02:04:11.375: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar  2 02:04:11.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2623" for this suite.

• [SLOW TEST:9.427 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1409
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":356,"completed":294,"skipped":5584,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:04:11.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 02:04:11.731: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:04:11.731: INFO: Node 10.123.244.39 is running 0 daemon pod, expected 1
Mar  2 02:04:12.775: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:04:12.775: INFO: Node 10.123.244.39 is running 0 daemon pod, expected 1
Mar  2 02:04:13.767: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar  2 02:04:13.767: INFO: Node 10.123.244.41 is running 0 daemon pod, expected 1
Mar  2 02:04:14.881: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 02:04:14.882: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar  2 02:04:15.325: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 02:04:15.325: INFO: Node 10.123.244.41 is running 0 daemon pod, expected 1
Mar  2 02:04:16.364: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 02:04:16.364: INFO: Node 10.123.244.41 is running 0 daemon pod, expected 1
Mar  2 02:04:17.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 02:04:17.377: INFO: Node 10.123.244.41 is running 0 daemon pod, expected 1
Mar  2 02:04:18.352: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 02:04:18.352: INFO: Node 10.123.244.41 is running 0 daemon pod, expected 1
Mar  2 02:04:19.356: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 02:04:19.356: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3386, will wait for the garbage collector to delete the pods
Mar  2 02:04:19.447: INFO: Deleting DaemonSet.extensions daemon-set took: 16.24687ms
Mar  2 02:04:19.548: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.191445ms
Mar  2 02:04:22.573: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:04:22.573: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  2 02:04:22.591: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"140275"},"items":null}

Mar  2 02:04:22.605: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"140275"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Mar  2 02:04:22.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3386" for this suite.

• [SLOW TEST:11.300 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":356,"completed":295,"skipped":5605,"failed":0}
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:04:22.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 02:04:22.840: INFO: Creating pod...
Mar  2 02:04:24.922: INFO: Creating service...
Mar  2 02:04:24.981: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2545/pods/agnhost/proxy?method=DELETE
Mar  2 02:04:25.033: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  2 02:04:25.033: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2545/pods/agnhost/proxy?method=OPTIONS
Mar  2 02:04:25.098: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  2 02:04:25.098: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2545/pods/agnhost/proxy?method=PATCH
Mar  2 02:04:25.126: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  2 02:04:25.126: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2545/pods/agnhost/proxy?method=POST
Mar  2 02:04:25.156: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  2 02:04:25.156: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2545/pods/agnhost/proxy?method=PUT
Mar  2 02:04:25.181: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar  2 02:04:25.181: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2545/services/e2e-proxy-test-service/proxy?method=DELETE
Mar  2 02:04:25.203: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar  2 02:04:25.203: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2545/services/e2e-proxy-test-service/proxy?method=OPTIONS
Mar  2 02:04:25.228: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar  2 02:04:25.228: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2545/services/e2e-proxy-test-service/proxy?method=PATCH
Mar  2 02:04:25.247: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar  2 02:04:25.247: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2545/services/e2e-proxy-test-service/proxy?method=POST
Mar  2 02:04:25.273: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar  2 02:04:25.273: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2545/services/e2e-proxy-test-service/proxy?method=PUT
Mar  2 02:04:25.293: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar  2 02:04:25.293: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2545/pods/agnhost/proxy?method=GET
Mar  2 02:04:25.301: INFO: http.Client request:GET StatusCode:301
Mar  2 02:04:25.301: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2545/services/e2e-proxy-test-service/proxy?method=GET
Mar  2 02:04:25.316: INFO: http.Client request:GET StatusCode:301
Mar  2 02:04:25.316: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2545/pods/agnhost/proxy?method=HEAD
Mar  2 02:04:25.325: INFO: http.Client request:HEAD StatusCode:301
Mar  2 02:04:25.325: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2545/services/e2e-proxy-test-service/proxy?method=HEAD
Mar  2 02:04:25.341: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Mar  2 02:04:25.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2545" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","total":356,"completed":296,"skipped":5605,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:04:25.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Mar  2 02:04:33.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3890" for this suite.
STEP: Destroying namespace "nsdeletetest-4813" for this suite.
Mar  2 02:04:34.067: INFO: Namespace nsdeletetest-4813 was already deleted
STEP: Destroying namespace "nsdeletetest-6874" for this suite.

• [SLOW TEST:8.706 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":356,"completed":297,"skipped":5679,"failed":0}
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:04:34.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 02:04:34.243: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
W0302 02:04:34.265399      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 02:04:34.297: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  2 02:04:39.312: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar  2 02:04:39.312: INFO: Creating deployment "test-rolling-update-deployment"
Mar  2 02:04:39.328: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar  2 02:04:39.351: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar  2 02:04:41.375: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar  2 02:04:41.386: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 4, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 4, 39, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 4, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 4, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67c8f74c6c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 02:04:43.400: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 02:04:43.432: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9318  9423f3c1-d118-4749-9f79-2c19e736f322 140734 1 2023-03-02 02:04:39 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2023-03-02 02:04:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 02:04:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001a62648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-02 02:04:39 +0000 UTC,LastTransitionTime:2023-03-02 02:04:39 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67c8f74c6c" has successfully progressed.,LastUpdateTime:2023-03-02 02:04:41 +0000 UTC,LastTransitionTime:2023-03-02 02:04:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar  2 02:04:43.446: INFO: New ReplicaSet "test-rolling-update-deployment-67c8f74c6c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67c8f74c6c  deployment-9318  ed103108-a3f3-4af5-ab77-da4b3491d1f2 140724 1 2023-03-02 02:04:39 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 9423f3c1-d118-4749-9f79-2c19e736f322 0xc002fcc8b7 0xc002fcc8b8}] []  [{kube-controller-manager Update apps/v1 2023-03-02 02:04:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9423f3c1-d118-4749-9f79-2c19e736f322\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 02:04:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67c8f74c6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002fcc968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar  2 02:04:43.446: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar  2 02:04:43.446: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9318  a214d444-527b-47e9-b8a8-5be408a5b800 140733 2 2023-03-02 02:04:34 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 9423f3c1-d118-4749-9f79-2c19e736f322 0xc002fcc787 0xc002fcc788}] []  [{e2e.test Update apps/v1 2023-03-02 02:04:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 02:04:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9423f3c1-d118-4749-9f79-2c19e736f322\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-02 02:04:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002fcc848 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 02:04:43.456: INFO: Pod "test-rolling-update-deployment-67c8f74c6c-q8xsc" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67c8f74c6c-q8xsc test-rolling-update-deployment-67c8f74c6c- deployment-9318  e083d959-2e79-4883-a2dc-ca7686ced1fd 140723 0 2023-03-02 02:04:39 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[cni.projectcalico.org/containerID:aa8983127477abd3846929193c98d54ea9cec6174c5d2ac624bc41bf8bbd0f82 cni.projectcalico.org/podIP:172.30.88.246/32 cni.projectcalico.org/podIPs:172.30.88.246/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.246"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.88.246"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-67c8f74c6c ed103108-a3f3-4af5-ab77-da4b3491d1f2 0xc002fcce17 0xc002fcce18}] []  [{kube-controller-manager Update v1 2023-03-02 02:04:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed103108-a3f3-4af5-ab77-da4b3491d1f2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-03-02 02:04:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-03-02 02:04:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-03-02 02:04:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.88.246\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bn4zv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bn4zv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.39,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-h92sv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:04:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:04:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:04:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:04:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.39,PodIP:172.30.88.246,StartTime:2023-03-02 02:04:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-02 02:04:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:cri-o://5a1fe1eb33cc3d75149dcd3c5e21b597c497461226e7f48b4baaf1e8215809bb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.88.246,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Mar  2 02:04:43.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9318" for this suite.

• [SLOW TEST:9.404 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":298,"skipped":5679,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:04:43.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9175
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9175
STEP: creating replication controller externalsvc in namespace services-9175
I0302 02:04:43.736342      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9175, replica count: 2
I0302 02:04:46.787882      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar  2 02:04:46.871: INFO: Creating new exec pod
Mar  2 02:04:48.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-9175 exec execpod4dsv2 -- /bin/sh -x -c nslookup nodeport-service.services-9175.svc.cluster.local'
Mar  2 02:04:49.348: INFO: stderr: "+ nslookup nodeport-service.services-9175.svc.cluster.local\n"
Mar  2 02:04:49.349: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-9175.svc.cluster.local\tcanonical name = externalsvc.services-9175.svc.cluster.local.\nName:\texternalsvc.services-9175.svc.cluster.local\nAddress: 172.21.234.175\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9175, will wait for the garbage collector to delete the pods
Mar  2 02:04:49.442: INFO: Deleting ReplicationController externalsvc took: 29.510642ms
Mar  2 02:04:49.543: INFO: Terminating ReplicationController externalsvc pods took: 100.687377ms
Mar  2 02:04:52.504: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar  2 02:04:52.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9175" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:9.124 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":356,"completed":299,"skipped":5697,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:04:52.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Mar  2 02:04:59.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9454" for this suite.

• [SLOW TEST:7.195 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":356,"completed":300,"skipped":5726,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:04:59.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1391
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 02:04:59.978: INFO: Found 0 stateful pods, waiting for 1
Mar  2 02:05:10.000: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Mar  2 02:05:10.064: INFO: Found 1 stateful pods, waiting for 2
Mar  2 02:05:20.079: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 02:05:20.079: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 02:05:20.145: INFO: Deleting all statefulset in ns statefulset-1391
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Mar  2 02:05:20.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1391" for this suite.

• [SLOW TEST:20.402 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":356,"completed":301,"skipped":5755,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:05:20.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-0e6de7f0-39c5-4e08-ab6d-cbc766025670
STEP: Creating a pod to test consume configMaps
Mar  2 02:05:20.453: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-48020f7a-254b-49ba-a8c8-86da2b44c6f4" in namespace "projected-1091" to be "Succeeded or Failed"
Mar  2 02:05:20.464: INFO: Pod "pod-projected-configmaps-48020f7a-254b-49ba-a8c8-86da2b44c6f4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.877501ms
Mar  2 02:05:22.514: INFO: Pod "pod-projected-configmaps-48020f7a-254b-49ba-a8c8-86da2b44c6f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060849125s
Mar  2 02:05:24.531: INFO: Pod "pod-projected-configmaps-48020f7a-254b-49ba-a8c8-86da2b44c6f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077415331s
Mar  2 02:05:26.544: INFO: Pod "pod-projected-configmaps-48020f7a-254b-49ba-a8c8-86da2b44c6f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.090657853s
STEP: Saw pod success
Mar  2 02:05:26.545: INFO: Pod "pod-projected-configmaps-48020f7a-254b-49ba-a8c8-86da2b44c6f4" satisfied condition "Succeeded or Failed"
Mar  2 02:05:26.556: INFO: Trying to get logs from node 10.123.244.39 pod pod-projected-configmaps-48020f7a-254b-49ba-a8c8-86da2b44c6f4 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 02:05:26.655: INFO: Waiting for pod pod-projected-configmaps-48020f7a-254b-49ba-a8c8-86da2b44c6f4 to disappear
Mar  2 02:05:26.665: INFO: Pod pod-projected-configmaps-48020f7a-254b-49ba-a8c8-86da2b44c6f4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Mar  2 02:05:26.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1091" for this suite.

• [SLOW TEST:6.537 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":302,"skipped":5770,"failed":0}
SSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:05:26.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Mar  2 02:07:01.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7435" for this suite.

• [SLOW TEST:94.320 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":356,"completed":303,"skipped":5777,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:07:01.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Mar  2 02:07:01.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 create -f -'
Mar  2 02:07:02.092: INFO: stderr: ""
Mar  2 02:07:02.092: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 02:07:02.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 02:07:02.320: INFO: stderr: ""
Mar  2 02:07:02.320: INFO: stdout: "update-demo-nautilus-294kr update-demo-nautilus-m2gl9 "
Mar  2 02:07:02.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods update-demo-nautilus-294kr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:07:02.566: INFO: stderr: ""
Mar  2 02:07:02.566: INFO: stdout: ""
Mar  2 02:07:02.566: INFO: update-demo-nautilus-294kr is created but not running
Mar  2 02:07:07.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 02:07:07.789: INFO: stderr: ""
Mar  2 02:07:07.790: INFO: stdout: "update-demo-nautilus-294kr update-demo-nautilus-m2gl9 "
Mar  2 02:07:07.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods update-demo-nautilus-294kr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:07:08.040: INFO: stderr: ""
Mar  2 02:07:08.040: INFO: stdout: "true"
Mar  2 02:07:08.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods update-demo-nautilus-294kr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 02:07:08.245: INFO: stderr: ""
Mar  2 02:07:08.245: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar  2 02:07:08.245: INFO: validating pod update-demo-nautilus-294kr
Mar  2 02:07:08.297: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 02:07:08.297: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 02:07:08.297: INFO: update-demo-nautilus-294kr is verified up and running
Mar  2 02:07:08.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods update-demo-nautilus-m2gl9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:07:08.512: INFO: stderr: ""
Mar  2 02:07:08.512: INFO: stdout: "true"
Mar  2 02:07:08.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods update-demo-nautilus-m2gl9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 02:07:08.795: INFO: stderr: ""
Mar  2 02:07:08.796: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar  2 02:07:08.796: INFO: validating pod update-demo-nautilus-m2gl9
Mar  2 02:07:08.867: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 02:07:08.867: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 02:07:08.867: INFO: update-demo-nautilus-m2gl9 is verified up and running
STEP: scaling down the replication controller
Mar  2 02:07:08.876: INFO: scanned /root for discovery docs: <nil>
Mar  2 02:07:08.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar  2 02:07:09.354: INFO: stderr: ""
Mar  2 02:07:09.355: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 02:07:09.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 02:07:09.702: INFO: stderr: ""
Mar  2 02:07:09.702: INFO: stdout: "update-demo-nautilus-294kr update-demo-nautilus-m2gl9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar  2 02:07:14.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 02:07:15.110: INFO: stderr: ""
Mar  2 02:07:15.110: INFO: stdout: "update-demo-nautilus-294kr "
Mar  2 02:07:15.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods update-demo-nautilus-294kr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:07:15.670: INFO: stderr: ""
Mar  2 02:07:15.670: INFO: stdout: "true"
Mar  2 02:07:15.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods update-demo-nautilus-294kr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 02:07:15.964: INFO: stderr: ""
Mar  2 02:07:15.964: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar  2 02:07:15.964: INFO: validating pod update-demo-nautilus-294kr
Mar  2 02:07:16.080: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 02:07:16.080: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 02:07:16.080: INFO: update-demo-nautilus-294kr is verified up and running
STEP: scaling up the replication controller
Mar  2 02:07:16.099: INFO: scanned /root for discovery docs: <nil>
Mar  2 02:07:16.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar  2 02:07:17.525: INFO: stderr: ""
Mar  2 02:07:17.525: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar  2 02:07:17.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 02:07:17.799: INFO: stderr: ""
Mar  2 02:07:17.799: INFO: stdout: "update-demo-nautilus-294kr update-demo-nautilus-4895w "
Mar  2 02:07:17.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods update-demo-nautilus-294kr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:07:17.905: INFO: stderr: ""
Mar  2 02:07:17.905: INFO: stdout: "true"
Mar  2 02:07:17.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods update-demo-nautilus-294kr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 02:07:18.020: INFO: stderr: ""
Mar  2 02:07:18.020: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar  2 02:07:18.020: INFO: validating pod update-demo-nautilus-294kr
Mar  2 02:07:18.041: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 02:07:18.041: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 02:07:18.041: INFO: update-demo-nautilus-294kr is verified up and running
Mar  2 02:07:18.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods update-demo-nautilus-4895w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:07:18.154: INFO: stderr: ""
Mar  2 02:07:18.154: INFO: stdout: ""
Mar  2 02:07:18.154: INFO: update-demo-nautilus-4895w is created but not running
Mar  2 02:07:23.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar  2 02:07:23.413: INFO: stderr: ""
Mar  2 02:07:23.413: INFO: stdout: "update-demo-nautilus-294kr update-demo-nautilus-4895w "
Mar  2 02:07:23.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods update-demo-nautilus-294kr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:07:23.658: INFO: stderr: ""
Mar  2 02:07:23.658: INFO: stdout: "true"
Mar  2 02:07:23.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods update-demo-nautilus-294kr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 02:07:23.830: INFO: stderr: ""
Mar  2 02:07:23.830: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar  2 02:07:23.830: INFO: validating pod update-demo-nautilus-294kr
Mar  2 02:07:23.859: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 02:07:23.859: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 02:07:23.859: INFO: update-demo-nautilus-294kr is verified up and running
Mar  2 02:07:23.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods update-demo-nautilus-4895w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar  2 02:07:24.017: INFO: stderr: ""
Mar  2 02:07:24.017: INFO: stdout: "true"
Mar  2 02:07:24.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods update-demo-nautilus-4895w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar  2 02:07:24.290: INFO: stderr: ""
Mar  2 02:07:24.290: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Mar  2 02:07:24.290: INFO: validating pod update-demo-nautilus-4895w
Mar  2 02:07:24.313: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar  2 02:07:24.313: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar  2 02:07:24.313: INFO: update-demo-nautilus-4895w is verified up and running
STEP: using delete to clean up resources
Mar  2 02:07:24.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 delete --grace-period=0 --force -f -'
Mar  2 02:07:24.471: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar  2 02:07:24.472: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar  2 02:07:24.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get rc,svc -l name=update-demo --no-headers'
Mar  2 02:07:24.740: INFO: stderr: "No resources found in kubectl-225 namespace.\n"
Mar  2 02:07:24.741: INFO: stdout: ""
Mar  2 02:07:24.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-225 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar  2 02:07:25.118: INFO: stderr: ""
Mar  2 02:07:25.118: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar  2 02:07:25.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-225" for this suite.

• [SLOW TEST:24.093 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should scale a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":356,"completed":304,"skipped":5778,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:07:25.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Mar  2 02:07:25.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2146" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":356,"completed":305,"skipped":5820,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:07:25.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Mar  2 02:07:25.987: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7e67f079-d742-4b99-aeb6-eead7ba954c3" in namespace "projected-2826" to be "Succeeded or Failed"
Mar  2 02:07:25.997: INFO: Pod "downwardapi-volume-7e67f079-d742-4b99-aeb6-eead7ba954c3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.657848ms
Mar  2 02:07:28.010: INFO: Pod "downwardapi-volume-7e67f079-d742-4b99-aeb6-eead7ba954c3": Phase="Running", Reason="", readiness=true. Elapsed: 2.022876416s
Mar  2 02:07:30.024: INFO: Pod "downwardapi-volume-7e67f079-d742-4b99-aeb6-eead7ba954c3": Phase="Running", Reason="", readiness=false. Elapsed: 4.03708151s
Mar  2 02:07:32.036: INFO: Pod "downwardapi-volume-7e67f079-d742-4b99-aeb6-eead7ba954c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049360032s
STEP: Saw pod success
Mar  2 02:07:32.036: INFO: Pod "downwardapi-volume-7e67f079-d742-4b99-aeb6-eead7ba954c3" satisfied condition "Succeeded or Failed"
Mar  2 02:07:32.048: INFO: Trying to get logs from node 10.123.244.39 pod downwardapi-volume-7e67f079-d742-4b99-aeb6-eead7ba954c3 container client-container: <nil>
STEP: delete the pod
Mar  2 02:07:32.161: INFO: Waiting for pod downwardapi-volume-7e67f079-d742-4b99-aeb6-eead7ba954c3 to disappear
Mar  2 02:07:32.175: INFO: Pod downwardapi-volume-7e67f079-d742-4b99-aeb6-eead7ba954c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar  2 02:07:32.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2826" for this suite.

• [SLOW TEST:6.424 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":306,"skipped":5827,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:07:32.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Mar  2 02:07:32.301: INFO: Waiting up to 1m0s for all nodes to be ready
Mar  2 02:08:32.549: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 02:08:32.580: INFO: Starting informer...
STEP: Starting pod...
Mar  2 02:08:32.840: INFO: Pod is running on 10.123.244.39. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Mar  2 02:08:32.888: INFO: Pod wasn't evicted. Proceeding
Mar  2 02:08:32.888: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Mar  2 02:09:47.970: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:188
Mar  2 02:09:47.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6974" for this suite.

• [SLOW TEST:135.799 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":356,"completed":307,"skipped":5852,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:09:48.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-projected-mbmb
STEP: Creating a pod to test atomic-volume-subpath
Mar  2 02:09:48.283: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-mbmb" in namespace "subpath-9616" to be "Succeeded or Failed"
Mar  2 02:09:48.293: INFO: Pod "pod-subpath-test-projected-mbmb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.980093ms
Mar  2 02:09:50.304: INFO: Pod "pod-subpath-test-projected-mbmb": Phase="Running", Reason="", readiness=true. Elapsed: 2.021627902s
Mar  2 02:09:52.317: INFO: Pod "pod-subpath-test-projected-mbmb": Phase="Running", Reason="", readiness=true. Elapsed: 4.034164318s
Mar  2 02:09:54.397: INFO: Pod "pod-subpath-test-projected-mbmb": Phase="Running", Reason="", readiness=true. Elapsed: 6.114016449s
Mar  2 02:09:56.413: INFO: Pod "pod-subpath-test-projected-mbmb": Phase="Running", Reason="", readiness=true. Elapsed: 8.130187737s
Mar  2 02:09:58.430: INFO: Pod "pod-subpath-test-projected-mbmb": Phase="Running", Reason="", readiness=true. Elapsed: 10.147221907s
Mar  2 02:10:00.444: INFO: Pod "pod-subpath-test-projected-mbmb": Phase="Running", Reason="", readiness=true. Elapsed: 12.161385393s
Mar  2 02:10:02.460: INFO: Pod "pod-subpath-test-projected-mbmb": Phase="Running", Reason="", readiness=true. Elapsed: 14.176923409s
Mar  2 02:10:04.476: INFO: Pod "pod-subpath-test-projected-mbmb": Phase="Running", Reason="", readiness=true. Elapsed: 16.193010445s
Mar  2 02:10:06.493: INFO: Pod "pod-subpath-test-projected-mbmb": Phase="Running", Reason="", readiness=true. Elapsed: 18.209884128s
Mar  2 02:10:08.521: INFO: Pod "pod-subpath-test-projected-mbmb": Phase="Running", Reason="", readiness=true. Elapsed: 20.238059842s
Mar  2 02:10:10.535: INFO: Pod "pod-subpath-test-projected-mbmb": Phase="Running", Reason="", readiness=false. Elapsed: 22.251794907s
Mar  2 02:10:12.574: INFO: Pod "pod-subpath-test-projected-mbmb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.291443706s
STEP: Saw pod success
Mar  2 02:10:12.575: INFO: Pod "pod-subpath-test-projected-mbmb" satisfied condition "Succeeded or Failed"
Mar  2 02:10:12.584: INFO: Trying to get logs from node 10.123.244.39 pod pod-subpath-test-projected-mbmb container test-container-subpath-projected-mbmb: <nil>
STEP: delete the pod
Mar  2 02:10:12.711: INFO: Waiting for pod pod-subpath-test-projected-mbmb to disappear
Mar  2 02:10:12.741: INFO: Pod pod-subpath-test-projected-mbmb no longer exists
STEP: Deleting pod pod-subpath-test-projected-mbmb
Mar  2 02:10:12.741: INFO: Deleting pod "pod-subpath-test-projected-mbmb" in namespace "subpath-9616"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Mar  2 02:10:12.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9616" for this suite.

• [SLOW TEST:24.778 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","total":356,"completed":308,"skipped":5858,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:10:12.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-343b7638-fc30-44ca-b4ce-9ccbb150df8f
STEP: Creating a pod to test consume configMaps
Mar  2 02:10:13.084: INFO: Waiting up to 5m0s for pod "pod-configmaps-903a90db-caf8-4a7c-9892-2af2a0df1dcc" in namespace "configmap-1396" to be "Succeeded or Failed"
Mar  2 02:10:13.093: INFO: Pod "pod-configmaps-903a90db-caf8-4a7c-9892-2af2a0df1dcc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.353111ms
Mar  2 02:10:15.111: INFO: Pod "pod-configmaps-903a90db-caf8-4a7c-9892-2af2a0df1dcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027138526s
Mar  2 02:10:17.129: INFO: Pod "pod-configmaps-903a90db-caf8-4a7c-9892-2af2a0df1dcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045154683s
STEP: Saw pod success
Mar  2 02:10:17.129: INFO: Pod "pod-configmaps-903a90db-caf8-4a7c-9892-2af2a0df1dcc" satisfied condition "Succeeded or Failed"
Mar  2 02:10:17.139: INFO: Trying to get logs from node 10.123.244.39 pod pod-configmaps-903a90db-caf8-4a7c-9892-2af2a0df1dcc container agnhost-container: <nil>
STEP: delete the pod
Mar  2 02:10:17.191: INFO: Waiting for pod pod-configmaps-903a90db-caf8-4a7c-9892-2af2a0df1dcc to disappear
Mar  2 02:10:17.200: INFO: Pod pod-configmaps-903a90db-caf8-4a7c-9892-2af2a0df1dcc no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar  2 02:10:17.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1396" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":309,"skipped":5879,"failed":0}
SSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:10:17.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 02:10:17.378: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-0d11592c-3a9b-4002-ae81-b9dad90d061a" in namespace "security-context-test-1624" to be "Succeeded or Failed"
Mar  2 02:10:17.410: INFO: Pod "busybox-privileged-false-0d11592c-3a9b-4002-ae81-b9dad90d061a": Phase="Pending", Reason="", readiness=false. Elapsed: 32.558813ms
Mar  2 02:10:19.430: INFO: Pod "busybox-privileged-false-0d11592c-3a9b-4002-ae81-b9dad90d061a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052499687s
Mar  2 02:10:21.445: INFO: Pod "busybox-privileged-false-0d11592c-3a9b-4002-ae81-b9dad90d061a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067068852s
Mar  2 02:10:21.445: INFO: Pod "busybox-privileged-false-0d11592c-3a9b-4002-ae81-b9dad90d061a" satisfied condition "Succeeded or Failed"
Mar  2 02:10:21.466: INFO: Got logs for pod "busybox-privileged-false-0d11592c-3a9b-4002-ae81-b9dad90d061a": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Mar  2 02:10:21.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1624" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":310,"skipped":5882,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:10:21.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-e39982f4-dbf5-4a56-bae9-cfb0ad449da4
STEP: Creating a pod to test consume secrets
Mar  2 02:10:21.715: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ea55e1ea-fc0d-44cf-aabb-dedba7eb2662" in namespace "projected-6871" to be "Succeeded or Failed"
Mar  2 02:10:21.725: INFO: Pod "pod-projected-secrets-ea55e1ea-fc0d-44cf-aabb-dedba7eb2662": Phase="Pending", Reason="", readiness=false. Elapsed: 9.904388ms
Mar  2 02:10:23.744: INFO: Pod "pod-projected-secrets-ea55e1ea-fc0d-44cf-aabb-dedba7eb2662": Phase="Running", Reason="", readiness=true. Elapsed: 2.02919427s
Mar  2 02:10:25.760: INFO: Pod "pod-projected-secrets-ea55e1ea-fc0d-44cf-aabb-dedba7eb2662": Phase="Running", Reason="", readiness=false. Elapsed: 4.044572607s
Mar  2 02:10:27.773: INFO: Pod "pod-projected-secrets-ea55e1ea-fc0d-44cf-aabb-dedba7eb2662": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057931553s
STEP: Saw pod success
Mar  2 02:10:27.773: INFO: Pod "pod-projected-secrets-ea55e1ea-fc0d-44cf-aabb-dedba7eb2662" satisfied condition "Succeeded or Failed"
Mar  2 02:10:27.800: INFO: Trying to get logs from node 10.123.244.39 pod pod-projected-secrets-ea55e1ea-fc0d-44cf-aabb-dedba7eb2662 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar  2 02:10:27.870: INFO: Waiting for pod pod-projected-secrets-ea55e1ea-fc0d-44cf-aabb-dedba7eb2662 to disappear
Mar  2 02:10:27.879: INFO: Pod pod-projected-secrets-ea55e1ea-fc0d-44cf-aabb-dedba7eb2662 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Mar  2 02:10:27.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6871" for this suite.

• [SLOW TEST:6.414 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":311,"skipped":5923,"failed":0}
S
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:10:27.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar  2 02:10:28.117: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar  2 02:10:28.134: INFO: starting watch
STEP: patching
STEP: updating
Mar  2 02:10:28.175: INFO: waiting for watch events with expected annotations
Mar  2 02:10:28.176: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:188
Mar  2 02:10:28.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-735" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":356,"completed":312,"skipped":5924,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:10:28.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6429
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating stateful set ss in namespace statefulset-6429
W0302 02:10:28.471684      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6429
Mar  2 02:10:28.482: INFO: Found 0 stateful pods, waiting for 1
Mar  2 02:10:38.502: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar  2 02:10:38.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-6429 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 02:10:39.020: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 02:10:39.020: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 02:10:39.020: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 02:10:39.030: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar  2 02:10:49.042: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 02:10:49.042: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 02:10:49.083: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar  2 02:10:49.083: INFO: ss-0  10.123.244.39  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:28 +0000 UTC  }]
Mar  2 02:10:49.083: INFO: 
Mar  2 02:10:49.083: INFO: StatefulSet ss has not reached scale 3, at 1
Mar  2 02:10:50.130: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987385298s
Mar  2 02:10:51.240: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.942670569s
Mar  2 02:10:52.549: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.828700863s
Mar  2 02:10:53.580: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.521593628s
Mar  2 02:10:54.598: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.489692632s
Mar  2 02:10:55.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.475078044s
Mar  2 02:10:56.626: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.460686132s
Mar  2 02:10:57.639: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.446544355s
Mar  2 02:10:58.657: INFO: Verifying statefulset ss doesn't scale past 3 for another 433.837903ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6429
Mar  2 02:10:59.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-6429 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 02:11:00.165: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar  2 02:11:00.165: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 02:11:00.165: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 02:11:00.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-6429 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 02:11:00.598: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  2 02:11:00.598: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 02:11:00.598: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 02:11:00.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-6429 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar  2 02:11:00.899: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar  2 02:11:00.899: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar  2 02:11:00.899: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar  2 02:11:00.911: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 02:11:00.911: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar  2 02:11:00.911: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar  2 02:11:00.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-6429 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 02:11:01.287: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 02:11:01.287: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 02:11:01.287: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 02:11:01.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-6429 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 02:11:01.820: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 02:11:01.820: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 02:11:01.820: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 02:11:01.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=statefulset-6429 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar  2 02:11:02.314: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar  2 02:11:02.314: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar  2 02:11:02.314: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar  2 02:11:02.314: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 02:11:02.324: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar  2 02:11:12.352: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 02:11:12.352: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 02:11:12.352: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar  2 02:11:12.390: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar  2 02:11:12.390: INFO: ss-0  10.123.244.39  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:11:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:11:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:28 +0000 UTC  }]
Mar  2 02:11:12.390: INFO: ss-1  10.123.244.50  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:11:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:11:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:49 +0000 UTC  }]
Mar  2 02:11:12.390: INFO: ss-2  10.123.244.41  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:11:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:11:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:49 +0000 UTC  }]
Mar  2 02:11:12.390: INFO: 
Mar  2 02:11:12.390: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 02:11:13.403: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar  2 02:11:13.403: INFO: ss-0  10.123.244.39  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:11:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:11:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:28 +0000 UTC  }]
Mar  2 02:11:13.403: INFO: ss-1  10.123.244.50  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:11:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:11:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:49 +0000 UTC  }]
Mar  2 02:11:13.403: INFO: ss-2  10.123.244.41  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:11:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:11:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:49 +0000 UTC  }]
Mar  2 02:11:13.403: INFO: 
Mar  2 02:11:13.403: INFO: StatefulSet ss has not reached scale 0, at 3
Mar  2 02:11:14.455: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar  2 02:11:14.455: INFO: ss-1  10.123.244.50  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:11:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:11:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:49 +0000 UTC  }]
Mar  2 02:11:14.455: INFO: 
Mar  2 02:11:14.455: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  2 02:11:15.496: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar  2 02:11:15.496: INFO: ss-1  10.123.244.50  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:11:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:11:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:10:49 +0000 UTC  }]
Mar  2 02:11:15.496: INFO: 
Mar  2 02:11:15.496: INFO: StatefulSet ss has not reached scale 0, at 1
Mar  2 02:11:16.717: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.730171398s
Mar  2 02:11:17.756: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.63850793s
Mar  2 02:11:18.772: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.623676905s
Mar  2 02:11:19.792: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.606447068s
Mar  2 02:11:20.805: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.586758742s
Mar  2 02:11:21.820: INFO: Verifying statefulset ss doesn't scale past 0 for another 573.577147ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6429
Mar  2 02:11:22.834: INFO: Scaling statefulset ss to 0
Mar  2 02:11:22.904: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar  2 02:11:22.920: INFO: Deleting all statefulset in ns statefulset-6429
Mar  2 02:11:22.929: INFO: Scaling statefulset ss to 0
Mar  2 02:11:23.033: INFO: Waiting for statefulset status.replicas updated to 0
Mar  2 02:11:23.048: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Mar  2 02:11:23.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6429" for this suite.

• [SLOW TEST:54.841 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":356,"completed":313,"skipped":5936,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:11:23.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8776
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8776
STEP: creating replication controller externalsvc in namespace services-8776
I0302 02:11:23.767620      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8776, replica count: 2
I0302 02:11:26.818056      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar  2 02:11:26.896: INFO: Creating new exec pod
Mar  2 02:11:28.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-8776 exec execpodvs68p -- /bin/sh -x -c nslookup clusterip-service.services-8776.svc.cluster.local'
Mar  2 02:11:30.504: INFO: stderr: "+ nslookup clusterip-service.services-8776.svc.cluster.local\n"
Mar  2 02:11:30.504: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-8776.svc.cluster.local\tcanonical name = externalsvc.services-8776.svc.cluster.local.\nName:\texternalsvc.services-8776.svc.cluster.local\nAddress: 172.21.37.122\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8776, will wait for the garbage collector to delete the pods
Mar  2 02:11:30.601: INFO: Deleting ReplicationController externalsvc took: 24.721588ms
Mar  2 02:11:30.702: INFO: Terminating ReplicationController externalsvc pods took: 100.408458ms
Mar  2 02:11:33.592: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar  2 02:11:33.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8776" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:10.548 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":356,"completed":314,"skipped":5963,"failed":0}
SSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:11:33.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Mar  2 02:17:02.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9144" for this suite.

• [SLOW TEST:328.394 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":356,"completed":315,"skipped":5968,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:17:02.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a pod template
W0302 02:17:02.333777      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Replace a pod template
Mar  2 02:17:02.377: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Mar  2 02:17:02.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2201" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","total":356,"completed":316,"skipped":5981,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:17:02.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-f23022bc-4490-4f6f-a4ad-f18dd2f8d84e
STEP: Creating a pod to test consume configMaps
Mar  2 02:17:02.831: INFO: Waiting up to 5m0s for pod "pod-configmaps-10993612-4e85-4155-9381-f14764ffc0a8" in namespace "configmap-4124" to be "Succeeded or Failed"
Mar  2 02:17:02.840: INFO: Pod "pod-configmaps-10993612-4e85-4155-9381-f14764ffc0a8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.703974ms
Mar  2 02:17:04.913: INFO: Pod "pod-configmaps-10993612-4e85-4155-9381-f14764ffc0a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081926149s
Mar  2 02:17:06.929: INFO: Pod "pod-configmaps-10993612-4e85-4155-9381-f14764ffc0a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0979908s
STEP: Saw pod success
Mar  2 02:17:06.929: INFO: Pod "pod-configmaps-10993612-4e85-4155-9381-f14764ffc0a8" satisfied condition "Succeeded or Failed"
Mar  2 02:17:06.940: INFO: Trying to get logs from node 10.123.244.39 pod pod-configmaps-10993612-4e85-4155-9381-f14764ffc0a8 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 02:17:07.021: INFO: Waiting for pod pod-configmaps-10993612-4e85-4155-9381-f14764ffc0a8 to disappear
Mar  2 02:17:07.037: INFO: Pod pod-configmaps-10993612-4e85-4155-9381-f14764ffc0a8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar  2 02:17:07.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4124" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":317,"skipped":5993,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:17:07.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 02:17:09.007: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 02:17:12.073: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar  2 02:17:12.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 02:17:12.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1761" for this suite.
STEP: Destroying namespace "webhook-1761-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.379 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":356,"completed":318,"skipped":6003,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:17:12.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test service account token: 
Mar  2 02:17:12.848: INFO: Waiting up to 5m0s for pod "test-pod-f94d553d-ef1b-4b42-91f9-4fd405dd45c8" in namespace "svcaccounts-4112" to be "Succeeded or Failed"
Mar  2 02:17:12.900: INFO: Pod "test-pod-f94d553d-ef1b-4b42-91f9-4fd405dd45c8": Phase="Pending", Reason="", readiness=false. Elapsed: 52.107076ms
Mar  2 02:17:14.920: INFO: Pod "test-pod-f94d553d-ef1b-4b42-91f9-4fd405dd45c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072030345s
Mar  2 02:17:16.932: INFO: Pod "test-pod-f94d553d-ef1b-4b42-91f9-4fd405dd45c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.084065559s
STEP: Saw pod success
Mar  2 02:17:16.932: INFO: Pod "test-pod-f94d553d-ef1b-4b42-91f9-4fd405dd45c8" satisfied condition "Succeeded or Failed"
Mar  2 02:17:16.941: INFO: Trying to get logs from node 10.123.244.39 pod test-pod-f94d553d-ef1b-4b42-91f9-4fd405dd45c8 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 02:17:17.207: INFO: Waiting for pod test-pod-f94d553d-ef1b-4b42-91f9-4fd405dd45c8 to disappear
Mar  2 02:17:17.218: INFO: Pod test-pod-f94d553d-ef1b-4b42-91f9-4fd405dd45c8 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Mar  2 02:17:17.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4112" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":356,"completed":319,"skipped":6011,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:17:17.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Mar  2 02:17:17.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 02:18:53.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7675" for this suite.

• [SLOW TEST:95.996 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":356,"completed":320,"skipped":6012,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:18:53.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 02:18:53.561: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar  2 02:18:58.577: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Mar  2 02:18:58.628: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Mar  2 02:18:58.656: INFO: observed ReplicaSet test-rs in namespace replicaset-9005 with ReadyReplicas 1, AvailableReplicas 1
Mar  2 02:18:58.731: INFO: observed ReplicaSet test-rs in namespace replicaset-9005 with ReadyReplicas 1, AvailableReplicas 1
Mar  2 02:18:58.816: INFO: observed ReplicaSet test-rs in namespace replicaset-9005 with ReadyReplicas 1, AvailableReplicas 1
Mar  2 02:18:58.848: INFO: observed ReplicaSet test-rs in namespace replicaset-9005 with ReadyReplicas 1, AvailableReplicas 1
Mar  2 02:19:00.644: INFO: observed ReplicaSet test-rs in namespace replicaset-9005 with ReadyReplicas 2, AvailableReplicas 2
Mar  2 02:19:01.313: INFO: observed Replicaset test-rs in namespace replicaset-9005 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Mar  2 02:19:01.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9005" for this suite.

• [SLOW TEST:8.116 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":356,"completed":321,"skipped":6014,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:19:01.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 02:19:01.566: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-36530456-e6cd-42a5-b88d-b627b2d1d7e7
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Mar  2 02:19:05.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9707" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":322,"skipped":6016,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:19:05.950: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-8077
Mar  2 02:19:06.250: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:19:08.281: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar  2 02:19:08.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-8077 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar  2 02:19:09.093: INFO: rc: 7
Mar  2 02:19:09.137: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar  2 02:19:09.153: INFO: Pod kube-proxy-mode-detector no longer exists
Mar  2 02:19:09.153: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-8077 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-8077
STEP: creating replication controller affinity-clusterip-timeout in namespace services-8077
I0302 02:19:09.270441      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-8077, replica count: 3
I0302 02:19:12.324743      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 02:19:15.324953      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 02:19:15.381: INFO: Creating new exec pod
Mar  2 02:19:18.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-8077 exec execpod-affinitywdcqw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Mar  2 02:19:18.776: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar  2 02:19:18.776: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 02:19:18.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-8077 exec execpod-affinitywdcqw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.211.195 80'
Mar  2 02:19:19.216: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.211.195 80\nConnection to 172.21.211.195 80 port [tcp/http] succeeded!\n"
Mar  2 02:19:19.216: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 02:19:19.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-8077 exec execpod-affinitywdcqw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.211.195:80/ ; done'
Mar  2 02:19:19.699: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.211.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.211.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.211.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.211.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.211.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.211.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.211.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.211.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.211.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.211.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.211.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.211.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.211.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.211.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.211.195:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.211.195:80/\n"
Mar  2 02:19:19.699: INFO: stdout: "\naffinity-clusterip-timeout-s4hgd\naffinity-clusterip-timeout-s4hgd\naffinity-clusterip-timeout-s4hgd\naffinity-clusterip-timeout-s4hgd\naffinity-clusterip-timeout-s4hgd\naffinity-clusterip-timeout-s4hgd\naffinity-clusterip-timeout-s4hgd\naffinity-clusterip-timeout-s4hgd\naffinity-clusterip-timeout-s4hgd\naffinity-clusterip-timeout-s4hgd\naffinity-clusterip-timeout-s4hgd\naffinity-clusterip-timeout-s4hgd\naffinity-clusterip-timeout-s4hgd\naffinity-clusterip-timeout-s4hgd\naffinity-clusterip-timeout-s4hgd\naffinity-clusterip-timeout-s4hgd"
Mar  2 02:19:19.699: INFO: Received response from host: affinity-clusterip-timeout-s4hgd
Mar  2 02:19:19.699: INFO: Received response from host: affinity-clusterip-timeout-s4hgd
Mar  2 02:19:19.699: INFO: Received response from host: affinity-clusterip-timeout-s4hgd
Mar  2 02:19:19.699: INFO: Received response from host: affinity-clusterip-timeout-s4hgd
Mar  2 02:19:19.699: INFO: Received response from host: affinity-clusterip-timeout-s4hgd
Mar  2 02:19:19.699: INFO: Received response from host: affinity-clusterip-timeout-s4hgd
Mar  2 02:19:19.699: INFO: Received response from host: affinity-clusterip-timeout-s4hgd
Mar  2 02:19:19.699: INFO: Received response from host: affinity-clusterip-timeout-s4hgd
Mar  2 02:19:19.699: INFO: Received response from host: affinity-clusterip-timeout-s4hgd
Mar  2 02:19:19.699: INFO: Received response from host: affinity-clusterip-timeout-s4hgd
Mar  2 02:19:19.699: INFO: Received response from host: affinity-clusterip-timeout-s4hgd
Mar  2 02:19:19.699: INFO: Received response from host: affinity-clusterip-timeout-s4hgd
Mar  2 02:19:19.700: INFO: Received response from host: affinity-clusterip-timeout-s4hgd
Mar  2 02:19:19.700: INFO: Received response from host: affinity-clusterip-timeout-s4hgd
Mar  2 02:19:19.700: INFO: Received response from host: affinity-clusterip-timeout-s4hgd
Mar  2 02:19:19.700: INFO: Received response from host: affinity-clusterip-timeout-s4hgd
Mar  2 02:19:19.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-8077 exec execpod-affinitywdcqw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.211.195:80/'
Mar  2 02:19:20.119: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.211.195:80/\n"
Mar  2 02:19:20.119: INFO: stdout: "affinity-clusterip-timeout-s4hgd"
Mar  2 02:19:40.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-8077 exec execpod-affinitywdcqw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.211.195:80/'
Mar  2 02:19:40.481: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.211.195:80/\n"
Mar  2 02:19:40.481: INFO: stdout: "affinity-clusterip-timeout-mz4m2"
Mar  2 02:19:40.481: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-8077, will wait for the garbage collector to delete the pods
Mar  2 02:19:40.616: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 24.18382ms
Mar  2 02:19:40.722: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 106.231574ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar  2 02:19:44.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8077" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:38.393 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":323,"skipped":6020,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:19:44.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar  2 02:19:44.544: INFO: Waiting up to 5m0s for pod "pod-1aeba4dd-ef7c-4f1f-a180-1bcf57128135" in namespace "emptydir-8344" to be "Succeeded or Failed"
Mar  2 02:19:44.559: INFO: Pod "pod-1aeba4dd-ef7c-4f1f-a180-1bcf57128135": Phase="Pending", Reason="", readiness=false. Elapsed: 12.294431ms
Mar  2 02:19:46.583: INFO: Pod "pod-1aeba4dd-ef7c-4f1f-a180-1bcf57128135": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036377503s
Mar  2 02:19:48.600: INFO: Pod "pod-1aeba4dd-ef7c-4f1f-a180-1bcf57128135": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052845742s
STEP: Saw pod success
Mar  2 02:19:48.600: INFO: Pod "pod-1aeba4dd-ef7c-4f1f-a180-1bcf57128135" satisfied condition "Succeeded or Failed"
Mar  2 02:19:48.618: INFO: Trying to get logs from node 10.123.244.39 pod pod-1aeba4dd-ef7c-4f1f-a180-1bcf57128135 container test-container: <nil>
STEP: delete the pod
Mar  2 02:19:48.725: INFO: Waiting for pod pod-1aeba4dd-ef7c-4f1f-a180-1bcf57128135 to disappear
Mar  2 02:19:48.735: INFO: Pod pod-1aeba4dd-ef7c-4f1f-a180-1bcf57128135 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar  2 02:19:48.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8344" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":324,"skipped":6026,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:19:48.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 02:19:49.027: INFO: The status of Pod server-envvars-2172bf31-31be-4101-a565-75764769c9f7 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:19:51.090: INFO: The status of Pod server-envvars-2172bf31-31be-4101-a565-75764769c9f7 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:19:53.068: INFO: The status of Pod server-envvars-2172bf31-31be-4101-a565-75764769c9f7 is Running (Ready = true)
Mar  2 02:19:53.206: INFO: Waiting up to 5m0s for pod "client-envvars-5cdf6bea-24c5-41e1-b254-8193194167a2" in namespace "pods-5823" to be "Succeeded or Failed"
Mar  2 02:19:53.221: INFO: Pod "client-envvars-5cdf6bea-24c5-41e1-b254-8193194167a2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.492375ms
Mar  2 02:19:55.238: INFO: Pod "client-envvars-5cdf6bea-24c5-41e1-b254-8193194167a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031121211s
Mar  2 02:19:57.252: INFO: Pod "client-envvars-5cdf6bea-24c5-41e1-b254-8193194167a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045087883s
Mar  2 02:19:59.264: INFO: Pod "client-envvars-5cdf6bea-24c5-41e1-b254-8193194167a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057530247s
STEP: Saw pod success
Mar  2 02:19:59.264: INFO: Pod "client-envvars-5cdf6bea-24c5-41e1-b254-8193194167a2" satisfied condition "Succeeded or Failed"
Mar  2 02:19:59.273: INFO: Trying to get logs from node 10.123.244.39 pod client-envvars-5cdf6bea-24c5-41e1-b254-8193194167a2 container env3cont: <nil>
STEP: delete the pod
Mar  2 02:19:59.418: INFO: Waiting for pod client-envvars-5cdf6bea-24c5-41e1-b254-8193194167a2 to disappear
Mar  2 02:19:59.430: INFO: Pod client-envvars-5cdf6bea-24c5-41e1-b254-8193194167a2 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Mar  2 02:19:59.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5823" for this suite.

• [SLOW TEST:10.674 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":356,"completed":325,"skipped":6040,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:19:59.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar  2 02:19:59.634: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8868  3faf0ff5-d821-4d74-9f74-616251e8b63a 149032 0 2023-03-02 02:19:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-03-02 02:19:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 02:19:59.635: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8868  3faf0ff5-d821-4d74-9f74-616251e8b63a 149035 0 2023-03-02 02:19:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-03-02 02:19:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 02:19:59.635: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8868  3faf0ff5-d821-4d74-9f74-616251e8b63a 149038 0 2023-03-02 02:19:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-03-02 02:19:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar  2 02:20:09.763: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8868  3faf0ff5-d821-4d74-9f74-616251e8b63a 149153 0 2023-03-02 02:19:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-03-02 02:19:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 02:20:09.763: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8868  3faf0ff5-d821-4d74-9f74-616251e8b63a 149155 0 2023-03-02 02:19:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-03-02 02:19:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar  2 02:20:09.764: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8868  3faf0ff5-d821-4d74-9f74-616251e8b63a 149156 0 2023-03-02 02:19:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-03-02 02:19:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Mar  2 02:20:09.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8868" for this suite.

• [SLOW TEST:10.324 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":356,"completed":326,"skipped":6055,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:20:09.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 02:20:09.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-6023 create -f -'
Mar  2 02:20:12.525: INFO: stderr: ""
Mar  2 02:20:12.525: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar  2 02:20:12.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-6023 create -f -'
Mar  2 02:20:14.402: INFO: stderr: ""
Mar  2 02:20:14.403: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar  2 02:20:15.423: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 02:20:15.423: INFO: Found 1 / 1
Mar  2 02:20:15.423: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar  2 02:20:15.432: INFO: Selector matched 1 pods for map[app:agnhost]
Mar  2 02:20:15.432: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar  2 02:20:15.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-6023 describe pod agnhost-primary-d4tmw'
Mar  2 02:20:15.755: INFO: stderr: ""
Mar  2 02:20:15.758: INFO: stdout: "Name:         agnhost-primary-d4tmw\nNamespace:    kubectl-6023\nPriority:     0\nNode:         10.123.244.39/10.123.244.39\nStart Time:   Thu, 02 Mar 2023 02:20:12 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: 40fd166d03c94f88a9c4a04b52043b6283d5e3f56d55450dab17cb029963f0a5\n              cni.projectcalico.org/podIP: 172.30.88.249/32\n              cni.projectcalico.org/podIPs: 172.30.88.249/32\n              k8s.v1.cni.cncf.io/network-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"172.30.88.249\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"172.30.88.249\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              openshift.io/scc: anyuid\nStatus:       Running\nIP:           172.30.88.249\nIPs:\n  IP:           172.30.88.249\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://2f00d9abc6170a0b8b821a5cb70c1112fcc9ed83f02e8742317dd5cba2c2d8ef\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 02 Mar 2023 02:20:13 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-95hnb (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-95hnb:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       3s    default-scheduler  Successfully assigned kubectl-6023/agnhost-primary-d4tmw to 10.123.244.39 by kube-scheduler-749c95db5d-t4qhz\n  Normal  AddedInterface  2s    multus             Add eth0 [172.30.88.249/32] from k8s-pod-network\n  Normal  Pulled          2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.39\" already present on machine\n  Normal  Created         2s    kubelet            Created container agnhost-primary\n  Normal  Started         2s    kubelet            Started container agnhost-primary\n"
Mar  2 02:20:15.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-6023 describe rc agnhost-primary'
Mar  2 02:20:16.130: INFO: stderr: ""
Mar  2 02:20:16.130: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6023\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-d4tmw\n"
Mar  2 02:20:16.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-6023 describe service agnhost-primary'
Mar  2 02:20:16.450: INFO: stderr: ""
Mar  2 02:20:16.450: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6023\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.147.51\nIPs:               172.21.147.51\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.88.249:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar  2 02:20:16.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-6023 describe node 10.123.244.39'
Mar  2 02:20:16.853: INFO: stderr: ""
Mar  2 02:20:16.853: INFO: stdout: "Name:               10.123.244.39\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-de\n                    failure-domain.beta.kubernetes.io/zone=fra05\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=149.81.124.183\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.123.244.39\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_8_64\n                    ibm-cloud.kubernetes.io/region=eu-de\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cfvsb2if0qgvn6bhmphg-kubee2epvgd-default-00000353\n                    ibm-cloud.kubernetes.io/worker-pool-id=cfvsb2if0qgvn6bhmphg-f5c45f4\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.11.28_1543_openshift\n                    ibm-cloud.kubernetes.io/zone=fra05\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.123.244.39\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2723042\n                    publicVLAN=2723040\n                    topology.kubernetes.io/region=eu-de\n                    topology.kubernetes.io/zone=fra05\nAnnotations:        projectcalico.org/IPv4Address: 10.123.244.39/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.88.192\nCreationTimestamp:  Wed, 01 Mar 2023 22:00:53 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.123.244.39\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 02 Mar 2023 02:20:10 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 01 Mar 2023 22:01:55 +0000   Wed, 01 Mar 2023 22:01:55 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 02 Mar 2023 02:20:09 +0000   Wed, 01 Mar 2023 22:00:53 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 02 Mar 2023 02:20:09 +0000   Wed, 01 Mar 2023 22:00:53 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 02 Mar 2023 02:20:09 +0000   Wed, 01 Mar 2023 22:00:53 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 02 Mar 2023 02:20:09 +0000   Wed, 01 Mar 2023 22:02:15 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.123.244.39\n  ExternalIP:  149.81.124.183\n  Hostname:    10.123.244.39\nCapacity:\n  cpu:                  4\n  ephemeral-storage:    102609848Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               16386528Ki\n  pods:                 110\nAllocatable:\n  cpu:                  3910m\n  ephemeral-storage:    93913280025\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               13597152Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                             2dd79f668cc140fda925cfa5ca2629bb\n  System UUID:                            dcc78f0b-b65e-884b-7015-bf56737fba93\n  Boot ID:                                be75f782-9038-408b-8b33-b02fb126006a\n  Kernel Version:                         4.18.0-425.13.1.el8_7.x86_64\n  OS Image:                               Red Hat Enterprise Linux 8.7 (Ootpa)\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.24.4-5.rhaos4.11.git57d7127.el8\n  Kubelet Version:                        v1.24.6+deccab3\n  Kube-Proxy Version:                     v1.24.6+deccab3\nPodCIDR:                                  172.30.1.0/24\nPodCIDRs:                                 172.30.1.0/24\nProviderID:                               ibm://fee034388aa6435883a1f720010ab3a2///cfvsb2if0qgvn6bhmphg/kube-cfvsb2if0qgvn6bhmphg-kubee2epvgd-default-00000353\nNon-terminated Pods:                      (20 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                           calico-node-vkllz                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         4h19m\n  calico-system                           calico-typha-db9579c55-nnjdl                               250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         4h19m\n  kube-system                             ibm-keepalived-watcher-hf2lb                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         4h19m\n  kube-system                             ibm-master-proxy-static-10.123.244.39                      26m (0%)      300m (7%)   32001024 (0%)    512M (3%)      4h18m\n  kube-system                             ibmcloud-block-storage-driver-qqdnc                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     4h19m\n  kubectl-6023                            agnhost-primary-d4tmw                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  openshift-cluster-node-tuning-operator  tuned-7g6rx                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h14m\n  openshift-dns                           dns-default-t6ghn                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         11m\n  openshift-dns                           node-resolver-678rp                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         4h14m\n  openshift-image-registry                node-ca-2dknm                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         4h18m\n  openshift-ingress-canary                ingress-canary-s4hkm                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         11m\n  openshift-kube-proxy                    openshift-kube-proxy-tzkhx                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         4h19m\n  openshift-marketplace                   redhat-marketplace-d44mn                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         11m\n  openshift-monitoring                    node-exporter-9hccw                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         4h14m\n  openshift-multus                        multus-additional-cni-plugins-h44x5                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         4h19m\n  openshift-multus                        multus-admission-controller-qctqt                          20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         11m\n  openshift-multus                        multus-tcchz                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         4h19m\n  openshift-multus                        network-metrics-daemon-865xx                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         4h19m\n  openshift-network-diagnostics           network-check-target-2gwsp                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         4h19m\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-f72afd02755d449d-vlqt6    0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests        Limits\n  --------             --------        ------\n  cpu                  875m (22%)      600m (15%)\n  memory               1135123Ki (8%)  826572800 (5%)\n  ephemeral-storage    0 (0%)          0 (0%)\n  hugepages-1Gi        0 (0%)          0 (0%)\n  hugepages-2Mi        0 (0%)          0 (0%)\n  example.com/fakecpu  0               0\nEvents:                <none>\n"
Mar  2 02:20:16.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=kubectl-6023 describe namespace kubectl-6023'
Mar  2 02:20:17.173: INFO: stderr: ""
Mar  2 02:20:17.173: INFO: stdout: "Name:         kubectl-6023\nLabels:       e2e-framework=kubectl\n              e2e-run=07dcbd30-74e4-46f6-993c-0f84563800fa\n              kubernetes.io/metadata.name=kubectl-6023\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c66,c40\n              openshift.io/sa.scc.supplemental-groups: 1004370000/10000\n              openshift.io/sa.scc.uid-range: 1004370000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Mar  2 02:20:17.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6023" for this suite.

• [SLOW TEST:7.442 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1110
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":356,"completed":327,"skipped":6085,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:20:17.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Mar  2 02:20:17.478: INFO: The status of Pod labelsupdatec734ea7c-1b09-4367-8018-a0421d7e6b66 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:20:19.500: INFO: The status of Pod labelsupdatec734ea7c-1b09-4367-8018-a0421d7e6b66 is Running (Ready = true)
Mar  2 02:20:20.097: INFO: Successfully updated pod "labelsupdatec734ea7c-1b09-4367-8018-a0421d7e6b66"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Mar  2 02:20:22.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5462" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":328,"skipped":6120,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:20:22.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar  2 02:20:22.460: INFO: Waiting up to 5m0s for pod "pod-ea3b0dcf-762b-435f-b12f-36b030f56d87" in namespace "emptydir-7245" to be "Succeeded or Failed"
Mar  2 02:20:22.471: INFO: Pod "pod-ea3b0dcf-762b-435f-b12f-36b030f56d87": Phase="Pending", Reason="", readiness=false. Elapsed: 9.881581ms
Mar  2 02:20:24.485: INFO: Pod "pod-ea3b0dcf-762b-435f-b12f-36b030f56d87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024004445s
Mar  2 02:20:26.500: INFO: Pod "pod-ea3b0dcf-762b-435f-b12f-36b030f56d87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039142135s
STEP: Saw pod success
Mar  2 02:20:26.500: INFO: Pod "pod-ea3b0dcf-762b-435f-b12f-36b030f56d87" satisfied condition "Succeeded or Failed"
Mar  2 02:20:26.510: INFO: Trying to get logs from node 10.123.244.39 pod pod-ea3b0dcf-762b-435f-b12f-36b030f56d87 container test-container: <nil>
STEP: delete the pod
Mar  2 02:20:26.800: INFO: Waiting for pod pod-ea3b0dcf-762b-435f-b12f-36b030f56d87 to disappear
Mar  2 02:20:26.811: INFO: Pod pod-ea3b0dcf-762b-435f-b12f-36b030f56d87 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Mar  2 02:20:26.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7245" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":329,"skipped":6137,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:20:26.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-4bbb6e99-f418-4601-aafc-cc63725acccd in namespace container-probe-4183
Mar  2 02:20:31.085: INFO: Started pod busybox-4bbb6e99-f418-4601-aafc-cc63725acccd in namespace container-probe-4183
STEP: checking the pod's current state and verifying that restartCount is present
Mar  2 02:20:31.098: INFO: Initial restart count of pod busybox-4bbb6e99-f418-4601-aafc-cc63725acccd is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Mar  2 02:24:31.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4183" for this suite.

• [SLOW TEST:244.911 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":330,"skipped":6157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:24:31.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar  2 02:24:31.925: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar  2 02:24:32.004: INFO: Waiting for terminating namespaces to be deleted...
Mar  2 02:24:32.052: INFO: 
Logging pods the apiserver thinks is on node 10.123.244.39 before test
Mar  2 02:24:32.103: INFO: calico-node-vkllz from calico-system started at 2023-03-01 22:01:04 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.103: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 02:24:32.103: INFO: calico-typha-db9579c55-nnjdl from calico-system started at 2023-03-01 22:01:04 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.103: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 02:24:32.103: INFO: ibm-keepalived-watcher-hf2lb from kube-system started at 2023-03-01 22:00:56 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.103: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 02:24:32.103: INFO: ibm-master-proxy-static-10.123.244.39 from kube-system started at 2023-03-01 22:00:52 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.103: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 02:24:32.103: INFO: 	Container pause ready: true, restart count 0
Mar  2 02:24:32.103: INFO: ibmcloud-block-storage-driver-qqdnc from kube-system started at 2023-03-01 22:01:08 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.103: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 02:24:32.103: INFO: tuned-7g6rx from openshift-cluster-node-tuning-operator started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.103: INFO: 	Container tuned ready: true, restart count 0
Mar  2 02:24:32.103: INFO: dns-default-t6ghn from openshift-dns started at 2023-03-02 02:08:54 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.103: INFO: 	Container dns ready: true, restart count 0
Mar  2 02:24:32.103: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.103: INFO: node-resolver-678rp from openshift-dns started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.104: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 02:24:32.104: INFO: node-ca-2dknm from openshift-image-registry started at 2023-03-01 22:02:13 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.104: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 02:24:32.104: INFO: ingress-canary-s4hkm from openshift-ingress-canary started at 2023-03-02 02:08:34 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.104: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 02:24:32.104: INFO: openshift-kube-proxy-tzkhx from openshift-kube-proxy started at 2023-03-01 22:00:56 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.104: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 02:24:32.104: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.104: INFO: redhat-marketplace-d44mn from openshift-marketplace started at 2023-03-02 02:08:35 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.104: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:24:32.104: INFO: node-exporter-9hccw from openshift-monitoring started at 2023-03-01 22:06:02 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.104: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.104: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 02:24:32.104: INFO: multus-additional-cni-plugins-h44x5 from openshift-multus started at 2023-03-01 22:00:56 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.105: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 02:24:32.105: INFO: multus-admission-controller-qctqt from openshift-multus started at 2023-03-02 02:09:04 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.105: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.105: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 02:24:32.105: INFO: multus-tcchz from openshift-multus started at 2023-03-01 22:00:56 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.105: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 02:24:32.105: INFO: network-metrics-daemon-865xx from openshift-multus started at 2023-03-01 22:00:56 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.105: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.105: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 02:24:32.105: INFO: network-check-target-2gwsp from openshift-network-diagnostics started at 2023-03-01 22:00:56 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.105: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 02:24:32.105: INFO: collect-profiles-27962055-7ng8c from openshift-operator-lifecycle-manager started at 2023-03-02 02:15:00 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.105: INFO: 	Container collect-profiles ready: false, restart count 0
Mar  2 02:24:32.105: INFO: sonobuoy-systemd-logs-daemon-set-f72afd02755d449d-vlqt6 from sonobuoy started at 2023-03-02 00:42:02 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.105: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:24:32.105: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 02:24:32.105: INFO: 
Logging pods the apiserver thinks is on node 10.123.244.41 before test
Mar  2 02:24:32.177: INFO: calico-kube-controllers-589f57b7f-tm9tz from calico-system started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.177: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar  2 02:24:32.177: INFO: calico-node-6flhq from calico-system started at 2023-03-01 22:01:04 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.177: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 02:24:32.177: INFO: managed-storage-validation-webhooks-85f57b66cf-79986 from ibm-odf-validation-webhook started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.177: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Mar  2 02:24:32.177: INFO: managed-storage-validation-webhooks-85f57b66cf-mqr5f from ibm-odf-validation-webhook started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.177: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 02:24:32.177: INFO: managed-storage-validation-webhooks-85f57b66cf-pwdx9 from ibm-odf-validation-webhook started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.177: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Mar  2 02:24:32.177: INFO: ibm-cloud-provider-ip-149-81-197-212-d64d8f476-bks7g from ibm-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.177: INFO: 	Container ibm-cloud-provider-ip-149-81-197-212 ready: true, restart count 0
Mar  2 02:24:32.177: INFO: ibm-file-plugin-5fdd985647-mldf5 from kube-system started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.177: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar  2 02:24:32.177: INFO: ibm-keepalived-watcher-mjldf from kube-system started at 2023-03-01 22:00:13 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.177: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 02:24:32.177: INFO: ibm-master-proxy-static-10.123.244.41 from kube-system started at 2023-03-01 22:00:11 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.177: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 02:24:32.177: INFO: 	Container pause ready: true, restart count 0
Mar  2 02:24:32.178: INFO: ibm-storage-metrics-agent-6b696986d4-jwmxc from kube-system started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.178: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Mar  2 02:24:32.178: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Mar  2 02:24:32.178: INFO: ibm-storage-watcher-64987648df-vl847 from kube-system started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.178: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar  2 02:24:32.178: INFO: ibmcloud-block-storage-driver-h5vmt from kube-system started at 2023-03-01 22:00:19 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.178: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 02:24:32.178: INFO: ibmcloud-block-storage-plugin-7cc7c95d6d-tw99v from kube-system started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.178: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar  2 02:24:32.178: INFO: cluster-node-tuning-operator-74fbdd5d47-scz8r from openshift-cluster-node-tuning-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.178: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Mar  2 02:24:32.178: INFO: tuned-bqzdt from openshift-cluster-node-tuning-operator started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.178: INFO: 	Container tuned ready: true, restart count 0
Mar  2 02:24:32.178: INFO: cluster-samples-operator-86c59f694d-klcrd from openshift-cluster-samples-operator started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.178: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Mar  2 02:24:32.178: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Mar  2 02:24:32.178: INFO: cluster-storage-operator-5b746f999f-vv6t5 from openshift-cluster-storage-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.178: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Mar  2 02:24:32.178: INFO: csi-snapshot-controller-8576577866-4lrgg from openshift-cluster-storage-operator started at 2023-03-01 22:01:48 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.178: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 02:24:32.178: INFO: csi-snapshot-controller-8576577866-8cdgr from openshift-cluster-storage-operator started at 2023-03-01 22:01:48 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.178: INFO: 	Container snapshot-controller ready: true, restart count 0
Mar  2 02:24:32.179: INFO: csi-snapshot-controller-operator-75849b8ccf-nmpqq from openshift-cluster-storage-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.179: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Mar  2 02:24:32.179: INFO: csi-snapshot-webhook-786787f645-wxmqk from openshift-cluster-storage-operator started at 2023-03-01 22:01:45 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.179: INFO: 	Container webhook ready: true, restart count 0
Mar  2 02:24:32.179: INFO: csi-snapshot-webhook-786787f645-zbq67 from openshift-cluster-storage-operator started at 2023-03-01 22:01:45 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.179: INFO: 	Container webhook ready: true, restart count 0
Mar  2 02:24:32.179: INFO: console-operator-67666f4bd-45tdk from openshift-console-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.179: INFO: 	Container console-operator ready: true, restart count 1
Mar  2 02:24:32.179: INFO: console-5549dcfdd9-gf5fk from openshift-console started at 2023-03-01 22:08:36 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.179: INFO: 	Container console ready: true, restart count 0
Mar  2 02:24:32.179: INFO: downloads-5b4f566bc5-hlvxl from openshift-console started at 2023-03-01 22:02:08 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.179: INFO: 	Container download-server ready: true, restart count 0
Mar  2 02:24:32.179: INFO: dns-operator-6b74679c6d-xqlfl from openshift-dns-operator started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.179: INFO: 	Container dns-operator ready: true, restart count 0
Mar  2 02:24:32.179: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.179: INFO: dns-default-c4wx7 from openshift-dns started at 2023-03-01 22:05:24 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.179: INFO: 	Container dns ready: true, restart count 0
Mar  2 02:24:32.179: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.179: INFO: node-resolver-dgx88 from openshift-dns started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.179: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 02:24:32.179: INFO: cluster-image-registry-operator-5846b9d966-sj4bv from openshift-image-registry started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.179: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Mar  2 02:24:32.179: INFO: node-ca-9vk8c from openshift-image-registry started at 2023-03-01 22:02:13 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.179: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 02:24:32.179: INFO: ingress-canary-5gtz6 from openshift-ingress-canary started at 2023-03-01 22:02:18 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.180: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 02:24:32.180: INFO: ingress-operator-5fdf7c4bb-8jgbn from openshift-ingress-operator started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.180: INFO: 	Container ingress-operator ready: true, restart count 0
Mar  2 02:24:32.180: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.180: INFO: router-default-555588874-8ffhk from openshift-ingress started at 2023-03-01 22:27:19 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.180: INFO: 	Container router ready: true, restart count 0
Mar  2 02:24:32.180: INFO: insights-operator-5449cdb4b9-7xt8p from openshift-insights started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.180: INFO: 	Container insights-operator ready: true, restart count 1
Mar  2 02:24:32.180: INFO: openshift-kube-proxy-j4ff5 from openshift-kube-proxy started at 2023-03-01 22:00:51 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.180: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 02:24:32.180: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.180: INFO: kube-storage-version-migrator-operator-66dd7b9865-9lrlp from openshift-kube-storage-version-migrator-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.180: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Mar  2 02:24:32.180: INFO: migrator-767b585794-kkngg from openshift-kube-storage-version-migrator started at 2023-03-01 22:02:08 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.180: INFO: 	Container migrator ready: true, restart count 0
Mar  2 02:24:32.180: INFO: marketplace-operator-d98c65969-9hhnp from openshift-marketplace started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.181: INFO: 	Container marketplace-operator ready: true, restart count 0
Mar  2 02:24:32.181: INFO: redhat-operators-mxdgk from openshift-marketplace started at 2023-03-02 00:50:46 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.181: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:24:32.181: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-03-01 22:06:09 +0000 UTC (6 container statuses recorded)
Mar  2 02:24:32.181: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 02:24:32.181: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 02:24:32.181: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:24:32.181: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.181: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Mar  2 02:24:32.181: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:24:32.181: INFO: cluster-monitoring-operator-8584844f6f-8bkqg from openshift-monitoring started at 2023-03-01 22:01:24 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.181: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Mar  2 02:24:32.181: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.181: INFO: node-exporter-z8kbd from openshift-monitoring started at 2023-03-01 22:06:02 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.181: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.182: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 02:24:32.182: INFO: prometheus-adapter-646cdcc6f7-vb8tx from openshift-monitoring started at 2023-03-01 22:07:14 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.182: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 02:24:32.182: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-03-01 22:06:25 +0000 UTC (6 container statuses recorded)
Mar  2 02:24:32.182: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:24:32.182: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.182: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 02:24:32.182: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 02:24:32.182: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 02:24:32.182: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 02:24:32.182: INFO: prometheus-operator-admission-webhook-78c85948cd-7dxdr from openshift-monitoring started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.182: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Mar  2 02:24:32.182: INFO: telemeter-client-7b8cf9859c-5jxpn from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (3 container statuses recorded)
Mar  2 02:24:32.182: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.182: INFO: 	Container reload ready: true, restart count 0
Mar  2 02:24:32.182: INFO: 	Container telemeter-client ready: true, restart count 0
Mar  2 02:24:32.183: INFO: thanos-querier-697f97f554-wxl4x from openshift-monitoring started at 2023-03-01 22:06:15 +0000 UTC (6 container statuses recorded)
Mar  2 02:24:32.183: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.183: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Mar  2 02:24:32.183: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 02:24:32.183: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 02:24:32.183: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:24:32.183: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 02:24:32.183: INFO: multus-additional-cni-plugins-mj5tl from openshift-multus started at 2023-03-01 22:00:46 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.183: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 02:24:32.183: INFO: multus-admission-controller-tbdnc from openshift-multus started at 2023-03-01 22:05:24 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.183: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.183: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 02:24:32.183: INFO: multus-dktpl from openshift-multus started at 2023-03-01 22:00:46 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.183: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 02:24:32.183: INFO: network-metrics-daemon-zl9qv from openshift-multus started at 2023-03-01 22:00:47 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.183: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.183: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 02:24:32.183: INFO: network-check-source-59cbfb554c-7kpmg from openshift-network-diagnostics started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.183: INFO: 	Container check-endpoints ready: true, restart count 0
Mar  2 02:24:32.184: INFO: network-check-target-nvxbw from openshift-network-diagnostics started at 2023-03-01 22:00:54 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.184: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 02:24:32.184: INFO: network-operator-7b666665c4-c8w4p from openshift-network-operator started at 2023-03-01 22:00:23 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.184: INFO: 	Container network-operator ready: true, restart count 1
Mar  2 02:24:32.184: INFO: catalog-operator-5b55d75455-w899d from openshift-operator-lifecycle-manager started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.184: INFO: 	Container catalog-operator ready: true, restart count 0
Mar  2 02:24:32.184: INFO: olm-operator-d785dddf9-v54mf from openshift-operator-lifecycle-manager started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.184: INFO: 	Container olm-operator ready: true, restart count 0
Mar  2 02:24:32.184: INFO: package-server-manager-5c8469f7c6-prjpw from openshift-operator-lifecycle-manager started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.184: INFO: 	Container package-server-manager ready: true, restart count 0
Mar  2 02:24:32.184: INFO: packageserver-bb46bbdb7-dr4v9 from openshift-operator-lifecycle-manager started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.184: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 02:24:32.184: INFO: metrics-5d9985b7b6-zpwc2 from openshift-roks-metrics started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.184: INFO: 	Container metrics ready: true, restart count 2
Mar  2 02:24:32.184: INFO: push-gateway-6fcf467c7d-8fxh8 from openshift-roks-metrics started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.184: INFO: 	Container push-gateway ready: true, restart count 0
Mar  2 02:24:32.184: INFO: service-ca-operator-865b774c95-b82zk from openshift-service-ca-operator started at 2023-03-01 22:01:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.184: INFO: 	Container service-ca-operator ready: true, restart count 1
Mar  2 02:24:32.184: INFO: service-ca-5995968f9b-25cgt from openshift-service-ca started at 2023-03-01 22:01:47 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.184: INFO: 	Container service-ca-controller ready: true, restart count 0
Mar  2 02:24:32.184: INFO: sonobuoy-systemd-logs-daemon-set-f72afd02755d449d-kddl5 from sonobuoy started at 2023-03-02 00:42:02 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.184: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:24:32.185: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 02:24:32.185: INFO: tigera-operator-f58c87f48-c9x9k from tigera-operator started at 2023-03-01 22:00:23 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.185: INFO: 	Container tigera-operator ready: true, restart count 1
Mar  2 02:24:32.185: INFO: 
Logging pods the apiserver thinks is on node 10.123.244.50 before test
Mar  2 02:24:32.245: INFO: calico-node-drfb9 from calico-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.245: INFO: 	Container calico-node ready: true, restart count 0
Mar  2 02:24:32.245: INFO: calico-typha-db9579c55-7mslq from calico-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.245: INFO: 	Container calico-typha ready: true, restart count 0
Mar  2 02:24:32.245: INFO: ibm-cloud-provider-ip-149-81-197-212-d64d8f476-rq24c from ibm-system started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.245: INFO: 	Container ibm-cloud-provider-ip-149-81-197-212 ready: true, restart count 0
Mar  2 02:24:32.245: INFO: ibm-keepalived-watcher-bmgn9 from kube-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.245: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar  2 02:24:32.245: INFO: ibm-master-proxy-static-10.123.244.50 from kube-system started at 2023-03-01 22:03:27 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.245: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar  2 02:24:32.245: INFO: 	Container pause ready: true, restart count 0
Mar  2 02:24:32.245: INFO: ibmcloud-block-storage-driver-qm9pz from kube-system started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.245: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar  2 02:24:32.245: INFO: vpn-5bbdb546b8-9w74v from kube-system started at 2023-03-01 22:19:58 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.245: INFO: 	Container vpn ready: true, restart count 0
Mar  2 02:24:32.245: INFO: tuned-6k6k4 from openshift-cluster-node-tuning-operator started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.245: INFO: 	Container tuned ready: true, restart count 0
Mar  2 02:24:32.245: INFO: console-5549dcfdd9-w5zs4 from openshift-console started at 2023-03-01 22:08:10 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.245: INFO: 	Container console ready: true, restart count 0
Mar  2 02:24:32.245: INFO: downloads-5b4f566bc5-wpgn5 from openshift-console started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.245: INFO: 	Container download-server ready: true, restart count 1
Mar  2 02:24:32.245: INFO: dns-default-q4vfw from openshift-dns started at 2023-03-01 22:08:04 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.246: INFO: 	Container dns ready: true, restart count 0
Mar  2 02:24:32.246: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.246: INFO: node-resolver-r6ktd from openshift-dns started at 2023-03-01 22:05:24 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.246: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar  2 02:24:32.246: INFO: image-pruner-27961920-bg4sx from openshift-image-registry started at 2023-03-02 00:00:00 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.246: INFO: 	Container image-pruner ready: false, restart count 0
Mar  2 02:24:32.246: INFO: image-registry-86657c9d77-wtb4h from openshift-image-registry started at 2023-03-01 22:48:17 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.246: INFO: 	Container registry ready: true, restart count 0
Mar  2 02:24:32.246: INFO: node-ca-cjkk7 from openshift-image-registry started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.246: INFO: 	Container node-ca ready: true, restart count 0
Mar  2 02:24:32.246: INFO: registry-pvc-permissions-6jq4m from openshift-image-registry started at 2023-03-01 22:48:17 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.246: INFO: 	Container pvc-permissions ready: false, restart count 0
Mar  2 02:24:32.246: INFO: ingress-canary-nkd4d from openshift-ingress-canary started at 2023-03-01 22:08:04 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.246: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Mar  2 02:24:32.246: INFO: router-default-555588874-w7r9r from openshift-ingress started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.246: INFO: 	Container router ready: true, restart count 0
Mar  2 02:24:32.246: INFO: openshift-kube-proxy-hnz52 from openshift-kube-proxy started at 2023-03-01 22:05:24 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.246: INFO: 	Container kube-proxy ready: true, restart count 0
Mar  2 02:24:32.246: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.246: INFO: certified-operators-rdltn from openshift-marketplace started at 2023-03-02 00:50:44 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.246: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:24:32.246: INFO: community-operators-zrr7v from openshift-marketplace started at 2023-03-02 00:50:44 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.246: INFO: 	Container registry-server ready: true, restart count 0
Mar  2 02:24:32.246: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-03-02 00:50:47 +0000 UTC (6 container statuses recorded)
Mar  2 02:24:32.246: INFO: 	Container alertmanager ready: true, restart count 0
Mar  2 02:24:32.246: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar  2 02:24:32.246: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:24:32.246: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.246: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Mar  2 02:24:32.246: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:24:32.246: INFO: kube-state-metrics-84464bb775-hgv8b from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (3 container statuses recorded)
Mar  2 02:24:32.246: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 02:24:32.246: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 02:24:32.246: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar  2 02:24:32.247: INFO: node-exporter-mlwcg from openshift-monitoring started at 2023-03-01 22:06:02 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.247: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.247: INFO: 	Container node-exporter ready: true, restart count 0
Mar  2 02:24:32.247: INFO: openshift-state-metrics-6486d6b674-g7xp7 from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (3 container statuses recorded)
Mar  2 02:24:32.247: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar  2 02:24:32.247: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar  2 02:24:32.247: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar  2 02:24:32.247: INFO: prometheus-adapter-646cdcc6f7-9m9ls from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.247: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar  2 02:24:32.247: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-03-02 00:50:46 +0000 UTC (6 container statuses recorded)
Mar  2 02:24:32.247: INFO: 	Container config-reloader ready: true, restart count 0
Mar  2 02:24:32.247: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.247: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Mar  2 02:24:32.247: INFO: 	Container prometheus ready: true, restart count 0
Mar  2 02:24:32.247: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar  2 02:24:32.247: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar  2 02:24:32.247: INFO: prometheus-operator-5d8ddb4f-cs9vq from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.247: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.247: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar  2 02:24:32.247: INFO: prometheus-operator-admission-webhook-78c85948cd-kxbf8 from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.247: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Mar  2 02:24:32.247: INFO: thanos-querier-697f97f554-49n2l from openshift-monitoring started at 2023-03-02 00:50:38 +0000 UTC (6 container statuses recorded)
Mar  2 02:24:32.247: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.247: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Mar  2 02:24:32.247: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Mar  2 02:24:32.247: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar  2 02:24:32.247: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar  2 02:24:32.247: INFO: 	Container thanos-query ready: true, restart count 0
Mar  2 02:24:32.247: INFO: multus-78s9d from openshift-multus started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.247: INFO: 	Container kube-multus ready: true, restart count 0
Mar  2 02:24:32.247: INFO: multus-additional-cni-plugins-x67r8 from openshift-multus started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.247: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Mar  2 02:24:32.247: INFO: multus-admission-controller-8d7qw from openshift-multus started at 2023-03-01 22:08:04 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.247: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.247: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar  2 02:24:32.247: INFO: network-metrics-daemon-9m6hn from openshift-multus started at 2023-03-01 22:05:25 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.247: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar  2 02:24:32.247: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Mar  2 02:24:32.247: INFO: network-check-target-ckmqs from openshift-network-diagnostics started at 2023-03-01 22:05:25 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.248: INFO: 	Container network-check-target-container ready: true, restart count 0
Mar  2 02:24:32.248: INFO: packageserver-bb46bbdb7-2kq2k from openshift-operator-lifecycle-manager started at 2023-03-02 00:50:38 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.248: INFO: 	Container packageserver ready: true, restart count 0
Mar  2 02:24:32.248: INFO: sonobuoy from sonobuoy started at 2023-03-02 00:41:53 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.248: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar  2 02:24:32.248: INFO: sonobuoy-e2e-job-370728edd32948a1 from sonobuoy started at 2023-03-02 00:42:02 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.248: INFO: 	Container e2e ready: true, restart count 0
Mar  2 02:24:32.248: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:24:32.248: INFO: sonobuoy-systemd-logs-daemon-set-f72afd02755d449d-pjpm6 from sonobuoy started at 2023-03-02 00:42:02 +0000 UTC (2 container statuses recorded)
Mar  2 02:24:32.248: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar  2 02:24:32.248: INFO: 	Container systemd-logs ready: true, restart count 0
Mar  2 02:24:32.248: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-03-01 22:20:41 +0000 UTC (1 container statuses recorded)
Mar  2 02:24:32.248: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-cc1f47dd-943d-44f3-8d1f-dcda785e2bfa 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.123.244.39 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-cc1f47dd-943d-44f3-8d1f-dcda785e2bfa off the node 10.123.244.39
STEP: verifying the node doesn't have the label kubernetes.io/e2e-cc1f47dd-943d-44f3-8d1f-dcda785e2bfa
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Mar  2 02:29:36.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2803" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:304.998 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":356,"completed":331,"skipped":6225,"failed":0}
[sig-network] Services 
  should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:29:36.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a collection of services
Mar  2 02:29:36.900: INFO: Creating e2e-svc-a-s4r8z
Mar  2 02:29:36.964: INFO: Creating e2e-svc-b-56lbq
Mar  2 02:29:37.090: INFO: Creating e2e-svc-c-mflnx
STEP: deleting service collection
Mar  2 02:29:37.338: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar  2 02:29:37.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1314" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":356,"completed":332,"skipped":6225,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:29:37.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-84b950e3-900e-426b-a622-81a0998006da
STEP: Creating a pod to test consume configMaps
Mar  2 02:29:37.724: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ba3e99c0-d0d0-42cc-b453-4210a7436135" in namespace "projected-8949" to be "Succeeded or Failed"
Mar  2 02:29:37.739: INFO: Pod "pod-projected-configmaps-ba3e99c0-d0d0-42cc-b453-4210a7436135": Phase="Pending", Reason="", readiness=false. Elapsed: 15.04578ms
Mar  2 02:29:39.752: INFO: Pod "pod-projected-configmaps-ba3e99c0-d0d0-42cc-b453-4210a7436135": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027918493s
Mar  2 02:29:41.767: INFO: Pod "pod-projected-configmaps-ba3e99c0-d0d0-42cc-b453-4210a7436135": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042530768s
STEP: Saw pod success
Mar  2 02:29:41.767: INFO: Pod "pod-projected-configmaps-ba3e99c0-d0d0-42cc-b453-4210a7436135" satisfied condition "Succeeded or Failed"
Mar  2 02:29:41.793: INFO: Trying to get logs from node 10.123.244.39 pod pod-projected-configmaps-ba3e99c0-d0d0-42cc-b453-4210a7436135 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 02:29:41.899: INFO: Waiting for pod pod-projected-configmaps-ba3e99c0-d0d0-42cc-b453-4210a7436135 to disappear
Mar  2 02:29:41.909: INFO: Pod pod-projected-configmaps-ba3e99c0-d0d0-42cc-b453-4210a7436135 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Mar  2 02:29:41.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8949" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":333,"skipped":6236,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:29:41.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-b7d742a5-aabf-4fe3-8898-44f2a3f7b9d6
STEP: Creating a pod to test consume configMaps
Mar  2 02:29:42.232: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e7ffc030-4311-44ad-8758-00ec6fb493b1" in namespace "projected-1857" to be "Succeeded or Failed"
Mar  2 02:29:42.244: INFO: Pod "pod-projected-configmaps-e7ffc030-4311-44ad-8758-00ec6fb493b1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.839607ms
Mar  2 02:29:44.267: INFO: Pod "pod-projected-configmaps-e7ffc030-4311-44ad-8758-00ec6fb493b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035336021s
Mar  2 02:29:46.282: INFO: Pod "pod-projected-configmaps-e7ffc030-4311-44ad-8758-00ec6fb493b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049710823s
Mar  2 02:29:48.294: INFO: Pod "pod-projected-configmaps-e7ffc030-4311-44ad-8758-00ec6fb493b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062259077s
STEP: Saw pod success
Mar  2 02:29:48.294: INFO: Pod "pod-projected-configmaps-e7ffc030-4311-44ad-8758-00ec6fb493b1" satisfied condition "Succeeded or Failed"
Mar  2 02:29:48.306: INFO: Trying to get logs from node 10.123.244.39 pod pod-projected-configmaps-e7ffc030-4311-44ad-8758-00ec6fb493b1 container agnhost-container: <nil>
STEP: delete the pod
Mar  2 02:29:48.354: INFO: Waiting for pod pod-projected-configmaps-e7ffc030-4311-44ad-8758-00ec6fb493b1 to disappear
Mar  2 02:29:48.375: INFO: Pod pod-projected-configmaps-e7ffc030-4311-44ad-8758-00ec6fb493b1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Mar  2 02:29:48.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1857" for this suite.

• [SLOW TEST:6.458 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":334,"skipped":6236,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:29:48.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 02:29:48.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8039
I0302 02:29:48.644034      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8039, replica count: 1
I0302 02:29:49.698099      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 02:29:50.700324      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 02:29:50.851: INFO: Created: latency-svc-l8vdb
Mar  2 02:29:50.895: INFO: Got endpoints: latency-svc-l8vdb [93.010823ms]
Mar  2 02:29:50.943: INFO: Created: latency-svc-7rzwb
Mar  2 02:29:50.960: INFO: Got endpoints: latency-svc-7rzwb [65.265161ms]
Mar  2 02:29:50.980: INFO: Created: latency-svc-z6lrc
Mar  2 02:29:50.982: INFO: Created: latency-svc-k8n9f
Mar  2 02:29:50.998: INFO: Got endpoints: latency-svc-z6lrc [102.681522ms]
Mar  2 02:29:50.999: INFO: Got endpoints: latency-svc-k8n9f [103.676778ms]
Mar  2 02:29:51.399: INFO: Created: latency-svc-fctck
Mar  2 02:29:51.399: INFO: Created: latency-svc-j56sr
Mar  2 02:29:51.399: INFO: Created: latency-svc-fqnl7
Mar  2 02:29:51.399: INFO: Created: latency-svc-22dmh
Mar  2 02:29:51.399: INFO: Created: latency-svc-v757d
Mar  2 02:29:51.399: INFO: Created: latency-svc-7wzsn
Mar  2 02:29:51.399: INFO: Created: latency-svc-ssqkw
Mar  2 02:29:51.400: INFO: Created: latency-svc-2ft9h
Mar  2 02:29:51.400: INFO: Created: latency-svc-kpg6j
Mar  2 02:29:51.402: INFO: Created: latency-svc-h2nr7
Mar  2 02:29:51.402: INFO: Created: latency-svc-wt2cd
Mar  2 02:29:51.402: INFO: Created: latency-svc-d6d2g
Mar  2 02:29:51.404: INFO: Got endpoints: latency-svc-ssqkw [405.431607ms]
Mar  2 02:29:51.406: INFO: Created: latency-svc-glrtj
Mar  2 02:29:51.408: INFO: Created: latency-svc-dp7gj
Mar  2 02:29:51.411: INFO: Created: latency-svc-5hcpx
Mar  2 02:29:51.424: INFO: Got endpoints: latency-svc-v757d [527.614789ms]
Mar  2 02:29:51.425: INFO: Got endpoints: latency-svc-22dmh [528.209332ms]
Mar  2 02:29:51.426: INFO: Got endpoints: latency-svc-7wzsn [426.467812ms]
Mar  2 02:29:51.426: INFO: Got endpoints: latency-svc-j56sr [529.246254ms]
Mar  2 02:29:51.426: INFO: Got endpoints: latency-svc-fctck [529.38425ms]
Mar  2 02:29:51.444: INFO: Got endpoints: latency-svc-fqnl7 [547.131496ms]
Mar  2 02:29:51.446: INFO: Got endpoints: latency-svc-kpg6j [550.407423ms]
Mar  2 02:29:51.453: INFO: Got endpoints: latency-svc-glrtj [556.849457ms]
Mar  2 02:29:51.453: INFO: Got endpoints: latency-svc-5hcpx [557.087335ms]
Mar  2 02:29:51.471: INFO: Got endpoints: latency-svc-2ft9h [575.009407ms]
Mar  2 02:29:51.471: INFO: Got endpoints: latency-svc-dp7gj [575.462523ms]
Mar  2 02:29:51.471: INFO: Got endpoints: latency-svc-h2nr7 [575.436066ms]
Mar  2 02:29:51.513: INFO: Got endpoints: latency-svc-wt2cd [552.746046ms]
Mar  2 02:29:51.517: INFO: Got endpoints: latency-svc-d6d2g [620.937958ms]
Mar  2 02:29:51.544: INFO: Created: latency-svc-9t2b8
Mar  2 02:29:51.564: INFO: Got endpoints: latency-svc-9t2b8 [160.132196ms]
Mar  2 02:29:51.590: INFO: Created: latency-svc-5mkfx
Mar  2 02:29:51.612: INFO: Got endpoints: latency-svc-5mkfx [187.651801ms]
Mar  2 02:29:51.659: INFO: Created: latency-svc-llkc9
Mar  2 02:29:51.676: INFO: Created: latency-svc-rqn5q
Mar  2 02:29:51.684: INFO: Got endpoints: latency-svc-llkc9 [259.768164ms]
Mar  2 02:29:51.692: INFO: Got endpoints: latency-svc-rqn5q [266.193023ms]
Mar  2 02:29:51.712: INFO: Created: latency-svc-4gr48
Mar  2 02:29:51.730: INFO: Got endpoints: latency-svc-4gr48 [303.7091ms]
Mar  2 02:29:51.755: INFO: Created: latency-svc-ps8qv
Mar  2 02:29:51.765: INFO: Created: latency-svc-whk2c
Mar  2 02:29:51.775: INFO: Got endpoints: latency-svc-ps8qv [349.241057ms]
Mar  2 02:29:51.798: INFO: Got endpoints: latency-svc-whk2c [354.455835ms]
Mar  2 02:29:51.812: INFO: Created: latency-svc-kxbh2
Mar  2 02:29:51.838: INFO: Got endpoints: latency-svc-kxbh2 [384.82363ms]
Mar  2 02:29:51.878: INFO: Created: latency-svc-brh8s
Mar  2 02:29:51.878: INFO: Got endpoints: latency-svc-brh8s [432.156654ms]
Mar  2 02:29:51.929: INFO: Created: latency-svc-cttpp
Mar  2 02:29:51.930: INFO: Created: latency-svc-jw2gp
Mar  2 02:29:51.930: INFO: Got endpoints: latency-svc-jw2gp [476.906439ms]
Mar  2 02:29:51.948: INFO: Got endpoints: latency-svc-cttpp [476.968204ms]
Mar  2 02:29:51.949: INFO: Created: latency-svc-lltgq
Mar  2 02:29:51.953: INFO: Got endpoints: latency-svc-lltgq [481.683513ms]
Mar  2 02:29:51.983: INFO: Created: latency-svc-qc2pg
Mar  2 02:29:52.010: INFO: Created: latency-svc-d9fjx
Mar  2 02:29:52.011: INFO: Got endpoints: latency-svc-qc2pg [539.728851ms]
Mar  2 02:29:52.017: INFO: Got endpoints: latency-svc-d9fjx [504.1186ms]
Mar  2 02:29:52.030: INFO: Created: latency-svc-ghcvm
Mar  2 02:29:52.043: INFO: Got endpoints: latency-svc-ghcvm [525.717218ms]
Mar  2 02:29:52.085: INFO: Created: latency-svc-txpwc
Mar  2 02:29:52.112: INFO: Got endpoints: latency-svc-txpwc [547.625287ms]
Mar  2 02:29:52.135: INFO: Created: latency-svc-5g4xd
Mar  2 02:29:52.147: INFO: Got endpoints: latency-svc-5g4xd [534.57315ms]
Mar  2 02:29:52.153: INFO: Created: latency-svc-82pvv
Mar  2 02:29:52.173: INFO: Got endpoints: latency-svc-82pvv [488.615982ms]
Mar  2 02:29:52.225: INFO: Created: latency-svc-h4dnn
Mar  2 02:29:52.265: INFO: Got endpoints: latency-svc-h4dnn [573.423242ms]
Mar  2 02:29:52.283: INFO: Created: latency-svc-jrltd
Mar  2 02:29:52.298: INFO: Created: latency-svc-2v9jl
Mar  2 02:29:52.299: INFO: Got endpoints: latency-svc-jrltd [569.506174ms]
Mar  2 02:29:52.313: INFO: Got endpoints: latency-svc-2v9jl [538.050181ms]
Mar  2 02:29:52.317: INFO: Created: latency-svc-8b5q2
Mar  2 02:29:52.339: INFO: Got endpoints: latency-svc-8b5q2 [540.511542ms]
Mar  2 02:29:52.355: INFO: Created: latency-svc-6grdn
Mar  2 02:29:52.370: INFO: Got endpoints: latency-svc-6grdn [531.78084ms]
Mar  2 02:29:52.390: INFO: Created: latency-svc-76rnt
Mar  2 02:29:52.391: INFO: Got endpoints: latency-svc-76rnt [513.036502ms]
Mar  2 02:29:52.455: INFO: Created: latency-svc-65hcb
Mar  2 02:29:52.537: INFO: Created: latency-svc-rz8fd
Mar  2 02:29:52.545: INFO: Created: latency-svc-4wqjv
Mar  2 02:29:52.546: INFO: Created: latency-svc-qz8k2
Mar  2 02:29:52.547: INFO: Created: latency-svc-pbflb
Mar  2 02:29:52.560: INFO: Got endpoints: latency-svc-rz8fd [611.210903ms]
Mar  2 02:29:52.561: INFO: Got endpoints: latency-svc-65hcb [630.385579ms]
Mar  2 02:29:52.561: INFO: Got endpoints: latency-svc-qz8k2 [549.626606ms]
Mar  2 02:29:52.562: INFO: Got endpoints: latency-svc-4wqjv [609.546218ms]
Mar  2 02:29:52.588: INFO: Got endpoints: latency-svc-pbflb [570.974491ms]
Mar  2 02:29:52.607: INFO: Created: latency-svc-khncg
Mar  2 02:29:52.624: INFO: Got endpoints: latency-svc-khncg [580.445996ms]
Mar  2 02:29:52.710: INFO: Created: latency-svc-9t849
Mar  2 02:29:52.710: INFO: Got endpoints: latency-svc-9t849 [598.282846ms]
Mar  2 02:29:52.752: INFO: Created: latency-svc-vxcwv
Mar  2 02:29:52.756: INFO: Got endpoints: latency-svc-vxcwv [608.805281ms]
Mar  2 02:29:52.775: INFO: Created: latency-svc-s2j88
Mar  2 02:29:52.790: INFO: Got endpoints: latency-svc-s2j88 [617.093597ms]
Mar  2 02:29:52.793: INFO: Created: latency-svc-z5zm2
Mar  2 02:29:52.815: INFO: Created: latency-svc-7kn52
Mar  2 02:29:52.818: INFO: Got endpoints: latency-svc-z5zm2 [552.418111ms]
Mar  2 02:29:52.836: INFO: Got endpoints: latency-svc-7kn52 [536.772679ms]
Mar  2 02:29:53.257: INFO: Created: latency-svc-2n2wf
Mar  2 02:29:53.259: INFO: Created: latency-svc-gcxhc
Mar  2 02:29:53.260: INFO: Created: latency-svc-nf2hf
Mar  2 02:29:53.260: INFO: Created: latency-svc-wlprh
Mar  2 02:29:53.260: INFO: Created: latency-svc-g76fc
Mar  2 02:29:53.261: INFO: Created: latency-svc-zbd67
Mar  2 02:29:53.261: INFO: Created: latency-svc-4bsc8
Mar  2 02:29:53.262: INFO: Created: latency-svc-r8hgk
Mar  2 02:29:53.262: INFO: Created: latency-svc-xgpqn
Mar  2 02:29:53.262: INFO: Created: latency-svc-bs5r2
Mar  2 02:29:53.263: INFO: Created: latency-svc-tm5tj
Mar  2 02:29:53.263: INFO: Created: latency-svc-td5hn
Mar  2 02:29:53.263: INFO: Created: latency-svc-p6d2g
Mar  2 02:29:53.271: INFO: Created: latency-svc-gzblx
Mar  2 02:29:53.274: INFO: Got endpoints: latency-svc-2n2wf [650.274956ms]
Mar  2 02:29:53.281: INFO: Created: latency-svc-qpqw2
Mar  2 02:29:53.282: INFO: Got endpoints: latency-svc-p6d2g [526.027967ms]
Mar  2 02:29:53.286: INFO: Got endpoints: latency-svc-zbd67 [946.789354ms]
Mar  2 02:29:53.286: INFO: Got endpoints: latency-svc-4bsc8 [723.298208ms]
Mar  2 02:29:53.287: INFO: Got endpoints: latency-svc-wlprh [726.942079ms]
Mar  2 02:29:53.302: INFO: Got endpoints: latency-svc-td5hn [465.607288ms]
Mar  2 02:29:53.301: INFO: Got endpoints: latency-svc-gcxhc [483.293256ms]
Mar  2 02:29:53.305: INFO: Got endpoints: latency-svc-bs5r2 [594.749529ms]
Mar  2 02:29:53.308: INFO: Got endpoints: latency-svc-tm5tj [995.062046ms]
Mar  2 02:29:53.309: INFO: Got endpoints: latency-svc-nf2hf [518.5536ms]
Mar  2 02:29:53.364: INFO: Created: latency-svc-cd85j
Mar  2 02:29:53.364: INFO: Got endpoints: latency-svc-cd85j [89.956801ms]
Mar  2 02:29:53.365: INFO: Got endpoints: latency-svc-g76fc [994.879536ms]
Mar  2 02:29:53.366: INFO: Got endpoints: latency-svc-xgpqn [777.682827ms]
Mar  2 02:29:53.367: INFO: Got endpoints: latency-svc-qpqw2 [805.91682ms]
Mar  2 02:29:53.367: INFO: Got endpoints: latency-svc-r8hgk [975.634017ms]
Mar  2 02:29:53.367: INFO: Got endpoints: latency-svc-gzblx [806.083424ms]
Mar  2 02:29:53.368: INFO: Created: latency-svc-9lrkf
Mar  2 02:29:53.388: INFO: Got endpoints: latency-svc-9lrkf [105.156918ms]
Mar  2 02:29:53.400: INFO: Created: latency-svc-lj7kl
Mar  2 02:29:53.416: INFO: Got endpoints: latency-svc-lj7kl [129.889456ms]
Mar  2 02:29:53.417: INFO: Created: latency-svc-bh9f7
Mar  2 02:29:53.445: INFO: Got endpoints: latency-svc-bh9f7 [79.526262ms]
Mar  2 02:29:53.461: INFO: Created: latency-svc-xmxms
Mar  2 02:29:53.475: INFO: Got endpoints: latency-svc-xmxms [108.513154ms]
Mar  2 02:29:53.867: INFO: Created: latency-svc-pclz7
Mar  2 02:29:53.868: INFO: Got endpoints: latency-svc-pclz7 [558.619664ms]
Mar  2 02:29:53.873: INFO: Created: latency-svc-kqlvq
Mar  2 02:29:53.874: INFO: Created: latency-svc-mddl5
Mar  2 02:29:53.876: INFO: Created: latency-svc-vgx8x
Mar  2 02:29:53.877: INFO: Created: latency-svc-7459z
Mar  2 02:29:53.877: INFO: Created: latency-svc-k9dvr
Mar  2 02:29:53.878: INFO: Created: latency-svc-ffkgl
Mar  2 02:29:53.878: INFO: Created: latency-svc-qwdns
Mar  2 02:29:53.878: INFO: Got endpoints: latency-svc-qwdns [569.390713ms]
Mar  2 02:29:53.879: INFO: Created: latency-svc-mf95k
Mar  2 02:29:53.879: INFO: Got endpoints: latency-svc-mf95k [577.157857ms]
Mar  2 02:29:53.880: INFO: Created: latency-svc-9vrrm
Mar  2 02:29:53.881: INFO: Got endpoints: latency-svc-9vrrm [516.636607ms]
Mar  2 02:29:53.882: INFO: Created: latency-svc-467lf
Mar  2 02:29:53.882: INFO: Created: latency-svc-99d9d
Mar  2 02:29:53.883: INFO: Created: latency-svc-cxf7w
Mar  2 02:29:53.889: INFO: Got endpoints: latency-svc-99d9d [414.117777ms]
Mar  2 02:29:53.890: INFO: Got endpoints: latency-svc-ffkgl [587.693811ms]
Mar  2 02:29:53.890: INFO: Got endpoints: latency-svc-7459z [602.453009ms]
Mar  2 02:29:53.893: INFO: Got endpoints: latency-svc-467lf [523.285202ms]
Mar  2 02:29:53.910: INFO: Got endpoints: latency-svc-cxf7w [490.248298ms]
Mar  2 02:29:53.912: INFO: Created: latency-svc-zt922
Mar  2 02:29:53.912: INFO: Got endpoints: latency-svc-zt922 [543.523623ms]
Mar  2 02:29:53.915: INFO: Created: latency-svc-qg6k8
Mar  2 02:29:53.915: INFO: Got endpoints: latency-svc-qg6k8 [470.096694ms]
Mar  2 02:29:53.916: INFO: Got endpoints: latency-svc-kqlvq [528.693128ms]
Mar  2 02:29:53.916: INFO: Got endpoints: latency-svc-k9dvr [630.215258ms]
Mar  2 02:29:53.917: INFO: Got endpoints: latency-svc-mddl5 [612.009693ms]
Mar  2 02:29:53.930: INFO: Got endpoints: latency-svc-vgx8x [562.344689ms]
Mar  2 02:29:53.988: INFO: Created: latency-svc-zw7sp
Mar  2 02:29:54.009: INFO: Created: latency-svc-cdmj7
Mar  2 02:29:54.009: INFO: Got endpoints: latency-svc-zw7sp [130.163103ms]
Mar  2 02:29:54.026: INFO: Got endpoints: latency-svc-cdmj7 [157.972851ms]
Mar  2 02:29:54.050: INFO: Created: latency-svc-qftdm
Mar  2 02:29:54.091: INFO: Got endpoints: latency-svc-qftdm [209.822255ms]
Mar  2 02:29:54.092: INFO: Created: latency-svc-kmwq6
Mar  2 02:29:54.127: INFO: Created: latency-svc-tbqdj
Mar  2 02:29:54.138: INFO: Got endpoints: latency-svc-kmwq6 [259.60387ms]
Mar  2 02:29:54.149: INFO: Got endpoints: latency-svc-tbqdj [259.347299ms]
Mar  2 02:29:54.159: INFO: Created: latency-svc-xsjv2
Mar  2 02:29:54.210: INFO: Got endpoints: latency-svc-xsjv2 [319.807197ms]
Mar  2 02:29:54.259: INFO: Created: latency-svc-zgw7t
Mar  2 02:29:54.276: INFO: Got endpoints: latency-svc-zgw7t [387.067944ms]
Mar  2 02:29:54.332: INFO: Created: latency-svc-mzmmd
Mar  2 02:29:54.340: INFO: Created: latency-svc-tk7n4
Mar  2 02:29:54.347: INFO: Got endpoints: latency-svc-mzmmd [454.270885ms]
Mar  2 02:29:54.362: INFO: Got endpoints: latency-svc-tk7n4 [446.947435ms]
Mar  2 02:29:54.372: INFO: Created: latency-svc-kq7k7
Mar  2 02:29:54.392: INFO: Created: latency-svc-wczrn
Mar  2 02:29:54.397: INFO: Got endpoints: latency-svc-kq7k7 [486.943413ms]
Mar  2 02:29:54.419: INFO: Got endpoints: latency-svc-wczrn [502.024582ms]
Mar  2 02:29:54.420: INFO: Created: latency-svc-9gbck
Mar  2 02:29:54.463: INFO: Got endpoints: latency-svc-9gbck [550.24536ms]
Mar  2 02:29:54.465: INFO: Created: latency-svc-k6sf7
Mar  2 02:29:54.465: INFO: Got endpoints: latency-svc-k6sf7 [548.44289ms]
Mar  2 02:29:54.474: INFO: Created: latency-svc-gd6v9
Mar  2 02:29:54.505: INFO: Got endpoints: latency-svc-gd6v9 [588.060199ms]
Mar  2 02:29:54.553: INFO: Created: latency-svc-l68j6
Mar  2 02:29:54.553: INFO: Got endpoints: latency-svc-l68j6 [622.32453ms]
Mar  2 02:29:54.574: INFO: Created: latency-svc-lthnf
Mar  2 02:29:54.586: INFO: Got endpoints: latency-svc-lthnf [576.787218ms]
Mar  2 02:29:54.587: INFO: Created: latency-svc-jrznv
Mar  2 02:29:54.609: INFO: Got endpoints: latency-svc-jrznv [582.525126ms]
Mar  2 02:29:54.610: INFO: Created: latency-svc-7b8nk
Mar  2 02:29:54.625: INFO: Got endpoints: latency-svc-7b8nk [533.582963ms]
Mar  2 02:29:54.632: INFO: Created: latency-svc-xldkz
Mar  2 02:29:54.682: INFO: Created: latency-svc-fzt8z
Mar  2 02:29:54.683: INFO: Created: latency-svc-7sprp
Mar  2 02:29:54.683: INFO: Got endpoints: latency-svc-fzt8z [533.610648ms]
Mar  2 02:29:54.684: INFO: Got endpoints: latency-svc-xldkz [546.150093ms]
Mar  2 02:29:54.721: INFO: Got endpoints: latency-svc-7sprp [510.901664ms]
Mar  2 02:29:54.722: INFO: Created: latency-svc-q58gt
Mar  2 02:29:54.770: INFO: Created: latency-svc-7lbz6
Mar  2 02:29:54.781: INFO: Got endpoints: latency-svc-7lbz6 [433.864329ms]
Mar  2 02:29:54.778: INFO: Got endpoints: latency-svc-q58gt [501.289278ms]
Mar  2 02:29:54.780: INFO: Created: latency-svc-9bgdb
Mar  2 02:29:54.782: INFO: Got endpoints: latency-svc-9bgdb [419.331871ms]
Mar  2 02:29:54.798: INFO: Created: latency-svc-rp2tz
Mar  2 02:29:54.824: INFO: Got endpoints: latency-svc-rp2tz [425.934577ms]
Mar  2 02:29:54.830: INFO: Created: latency-svc-kd4kc
Mar  2 02:29:54.843: INFO: Got endpoints: latency-svc-kd4kc [424.308295ms]
Mar  2 02:29:54.871: INFO: Created: latency-svc-rnvj7
Mar  2 02:29:54.889: INFO: Got endpoints: latency-svc-rnvj7 [425.391523ms]
Mar  2 02:29:54.892: INFO: Created: latency-svc-2qgx7
Mar  2 02:29:54.913: INFO: Got endpoints: latency-svc-2qgx7 [446.310366ms]
Mar  2 02:29:54.953: INFO: Created: latency-svc-66mpr
Mar  2 02:29:54.955: INFO: Got endpoints: latency-svc-66mpr [450.042151ms]
Mar  2 02:29:54.960: INFO: Created: latency-svc-5vq4c
Mar  2 02:29:55.005: INFO: Got endpoints: latency-svc-5vq4c [452.01198ms]
Mar  2 02:29:55.006: INFO: Created: latency-svc-shtgz
Mar  2 02:29:55.024: INFO: Got endpoints: latency-svc-shtgz [437.961458ms]
Mar  2 02:29:55.035: INFO: Created: latency-svc-xhxx4
Mar  2 02:29:55.054: INFO: Got endpoints: latency-svc-xhxx4 [444.347863ms]
Mar  2 02:29:55.100: INFO: Created: latency-svc-ddw89
Mar  2 02:29:55.125: INFO: Got endpoints: latency-svc-ddw89 [500.289812ms]
Mar  2 02:29:55.134: INFO: Created: latency-svc-qg9x6
Mar  2 02:29:55.143: INFO: Got endpoints: latency-svc-qg9x6 [459.98518ms]
Mar  2 02:29:55.154: INFO: Created: latency-svc-nttpj
Mar  2 02:29:55.174: INFO: Got endpoints: latency-svc-nttpj [489.821686ms]
Mar  2 02:29:55.248: INFO: Created: latency-svc-vnbg2
Mar  2 02:29:55.248: INFO: Got endpoints: latency-svc-vnbg2 [465.97332ms]
Mar  2 02:29:55.271: INFO: Created: latency-svc-w8b9b
Mar  2 02:29:55.283: INFO: Got endpoints: latency-svc-w8b9b [500.912575ms]
Mar  2 02:29:55.313: INFO: Created: latency-svc-2f9m2
Mar  2 02:29:55.325: INFO: Got endpoints: latency-svc-2f9m2 [544.296172ms]
Mar  2 02:29:55.340: INFO: Created: latency-svc-rkvrm
Mar  2 02:29:55.355: INFO: Created: latency-svc-vhbsd
Mar  2 02:29:55.355: INFO: Got endpoints: latency-svc-vhbsd [633.930789ms]
Mar  2 02:29:55.413: INFO: Created: latency-svc-k8mxd
Mar  2 02:29:55.414: INFO: Got endpoints: latency-svc-k8mxd [524.59087ms]
Mar  2 02:29:55.414: INFO: Got endpoints: latency-svc-rkvrm [590.751598ms]
Mar  2 02:29:55.428: INFO: Created: latency-svc-ktkc9
Mar  2 02:29:55.447: INFO: Created: latency-svc-t6pp9
Mar  2 02:29:55.456: INFO: Got endpoints: latency-svc-ktkc9 [543.230935ms]
Mar  2 02:29:55.466: INFO: Got endpoints: latency-svc-t6pp9 [511.115215ms]
Mar  2 02:29:55.470: INFO: Created: latency-svc-kzpfw
Mar  2 02:29:55.470: INFO: Got endpoints: latency-svc-kzpfw [626.292469ms]
Mar  2 02:29:55.478: INFO: Created: latency-svc-9zn94
Mar  2 02:29:55.525: INFO: Created: latency-svc-tcpf5
Mar  2 02:29:55.526: INFO: Got endpoints: latency-svc-tcpf5 [500.954489ms]
Mar  2 02:29:55.526: INFO: Got endpoints: latency-svc-9zn94 [521.241962ms]
Mar  2 02:29:55.828: INFO: Created: latency-svc-zmq6q
Mar  2 02:29:55.829: INFO: Created: latency-svc-w4fxp
Mar  2 02:29:55.829: INFO: Created: latency-svc-wj8vh
Mar  2 02:29:55.830: INFO: Created: latency-svc-6rkcz
Mar  2 02:29:55.830: INFO: Created: latency-svc-29hfx
Mar  2 02:29:55.831: INFO: Created: latency-svc-x7m79
Mar  2 02:29:55.832: INFO: Created: latency-svc-9r8jm
Mar  2 02:29:55.852: INFO: Got endpoints: latency-svc-9r8jm [678.077338ms]
Mar  2 02:29:55.853: INFO: Created: latency-svc-8bvxx
Mar  2 02:29:55.854: INFO: Created: latency-svc-tvgmv
Mar  2 02:29:55.854: INFO: Created: latency-svc-rgptq
Mar  2 02:29:55.854: INFO: Created: latency-svc-zffwd
Mar  2 02:29:55.855: INFO: Created: latency-svc-gqp56
Mar  2 02:29:55.855: INFO: Created: latency-svc-qfsvt
Mar  2 02:29:55.855: INFO: Created: latency-svc-ldk8q
Mar  2 02:29:55.856: INFO: Created: latency-svc-k4wnc
Mar  2 02:29:55.863: INFO: Got endpoints: latency-svc-wj8vh [579.925076ms]
Mar  2 02:29:55.863: INFO: Got endpoints: latency-svc-6rkcz [614.681682ms]
Mar  2 02:29:55.888: INFO: Got endpoints: latency-svc-x7m79 [417.84523ms]
Mar  2 02:29:55.888: INFO: Got endpoints: latency-svc-w4fxp [562.70628ms]
Mar  2 02:29:55.889: INFO: Got endpoints: latency-svc-zmq6q [533.2429ms]
Mar  2 02:29:55.889: INFO: Got endpoints: latency-svc-29hfx [432.514991ms]
Mar  2 02:29:55.896: INFO: Got endpoints: latency-svc-8bvxx [481.546837ms]
Mar  2 02:29:55.912: INFO: Got endpoints: latency-svc-tvgmv [445.798002ms]
Mar  2 02:29:55.913: INFO: Got endpoints: latency-svc-qfsvt [788.00491ms]
Mar  2 02:29:55.917: INFO: Got endpoints: latency-svc-rgptq [862.749966ms]
Mar  2 02:29:55.958: INFO: Created: latency-svc-zdswl
Mar  2 02:29:55.958: INFO: Got endpoints: latency-svc-gqp56 [544.492628ms]
Mar  2 02:29:55.959: INFO: Got endpoints: latency-svc-k4wnc [433.198781ms]
Mar  2 02:29:56.031: INFO: Got endpoints: latency-svc-ldk8q [887.337649ms]
Mar  2 02:29:56.031: INFO: Got endpoints: latency-svc-zffwd [505.613336ms]
Mar  2 02:29:56.047: INFO: Got endpoints: latency-svc-zdswl [194.587246ms]
Mar  2 02:29:56.081: INFO: Created: latency-svc-wrr48
Mar  2 02:29:56.101: INFO: Got endpoints: latency-svc-wrr48 [237.669858ms]
Mar  2 02:29:56.120: INFO: Created: latency-svc-7hmn8
Mar  2 02:29:56.134: INFO: Got endpoints: latency-svc-7hmn8 [270.887306ms]
Mar  2 02:29:56.140: INFO: Created: latency-svc-frxh7
Mar  2 02:29:56.161: INFO: Got endpoints: latency-svc-frxh7 [272.867292ms]
Mar  2 02:29:56.228: INFO: Created: latency-svc-c2z7p
Mar  2 02:29:56.228: INFO: Got endpoints: latency-svc-c2z7p [340.727631ms]
Mar  2 02:29:56.231: INFO: Created: latency-svc-r6f2n
Mar  2 02:29:56.247: INFO: Created: latency-svc-gbs66
Mar  2 02:29:56.257: INFO: Got endpoints: latency-svc-r6f2n [367.952851ms]
Mar  2 02:29:56.270: INFO: Created: latency-svc-b7q4v
Mar  2 02:29:56.282: INFO: Got endpoints: latency-svc-gbs66 [392.83195ms]
Mar  2 02:29:56.296: INFO: Created: latency-svc-xtf69
Mar  2 02:29:56.297: INFO: Got endpoints: latency-svc-b7q4v [400.436195ms]
Mar  2 02:29:56.313: INFO: Got endpoints: latency-svc-xtf69 [400.6386ms]
Mar  2 02:29:56.315: INFO: Created: latency-svc-4bv9f
Mar  2 02:29:56.357: INFO: Got endpoints: latency-svc-4bv9f [443.637458ms]
Mar  2 02:29:56.358: INFO: Created: latency-svc-58jds
Mar  2 02:29:56.361: INFO: Got endpoints: latency-svc-58jds [444.075949ms]
Mar  2 02:29:56.372: INFO: Created: latency-svc-nvgfm
Mar  2 02:29:56.386: INFO: Got endpoints: latency-svc-nvgfm [427.483041ms]
Mar  2 02:29:56.400: INFO: Created: latency-svc-7z44v
Mar  2 02:29:56.417: INFO: Got endpoints: latency-svc-7z44v [457.723897ms]
Mar  2 02:29:56.445: INFO: Created: latency-svc-47ljf
Mar  2 02:29:56.446: INFO: Created: latency-svc-pfcvd
Mar  2 02:29:56.466: INFO: Got endpoints: latency-svc-47ljf [435.514008ms]
Mar  2 02:29:56.468: INFO: Created: latency-svc-cxkfj
Mar  2 02:29:56.498: INFO: Created: latency-svc-7t9nv
Mar  2 02:29:56.499: INFO: Got endpoints: latency-svc-cxkfj [451.847427ms]
Mar  2 02:29:56.500: INFO: Got endpoints: latency-svc-pfcvd [468.375596ms]
Mar  2 02:29:56.508: INFO: Created: latency-svc-7n8ss
Mar  2 02:29:56.510: INFO: Got endpoints: latency-svc-7t9nv [409.069804ms]
Mar  2 02:29:56.528: INFO: Created: latency-svc-jnmbl
Mar  2 02:29:56.551: INFO: Got endpoints: latency-svc-7n8ss [416.921092ms]
Mar  2 02:29:56.552: INFO: Created: latency-svc-9sgw9
Mar  2 02:29:56.569: INFO: Got endpoints: latency-svc-jnmbl [408.149845ms]
Mar  2 02:29:56.572: INFO: Got endpoints: latency-svc-9sgw9 [342.821695ms]
Mar  2 02:29:56.579: INFO: Created: latency-svc-97m49
Mar  2 02:29:56.600: INFO: Got endpoints: latency-svc-97m49 [343.387405ms]
Mar  2 02:29:56.601: INFO: Created: latency-svc-fxz2j
Mar  2 02:29:56.614: INFO: Got endpoints: latency-svc-fxz2j [331.457152ms]
Mar  2 02:29:56.642: INFO: Created: latency-svc-mr4h6
Mar  2 02:29:56.643: INFO: Got endpoints: latency-svc-mr4h6 [345.778247ms]
Mar  2 02:29:56.655: INFO: Created: latency-svc-pz9wn
Mar  2 02:29:56.672: INFO: Got endpoints: latency-svc-pz9wn [359.425281ms]
Mar  2 02:29:56.673: INFO: Created: latency-svc-qrjd5
Mar  2 02:29:56.686: INFO: Created: latency-svc-r67jv
Mar  2 02:29:56.705: INFO: Created: latency-svc-jlkch
Mar  2 02:29:56.710: INFO: Got endpoints: latency-svc-qrjd5 [353.249743ms]
Mar  2 02:29:56.712: INFO: Got endpoints: latency-svc-r67jv [350.769214ms]
Mar  2 02:29:56.717: INFO: Created: latency-svc-gpq5w
Mar  2 02:29:56.721: INFO: Got endpoints: latency-svc-jlkch [334.881257ms]
Mar  2 02:29:56.754: INFO: Created: latency-svc-j9ntp
Mar  2 02:29:56.758: INFO: Got endpoints: latency-svc-j9ntp [291.806087ms]
Mar  2 02:29:56.759: INFO: Got endpoints: latency-svc-gpq5w [341.85213ms]
Mar  2 02:29:56.764: INFO: Created: latency-svc-6fv99
Mar  2 02:29:56.785: INFO: Created: latency-svc-7xskc
Mar  2 02:29:56.787: INFO: Got endpoints: latency-svc-6fv99 [286.985702ms]
Mar  2 02:29:56.793: INFO: Got endpoints: latency-svc-7xskc [294.146985ms]
Mar  2 02:29:56.803: INFO: Created: latency-svc-8rf2n
Mar  2 02:29:56.818: INFO: Got endpoints: latency-svc-8rf2n [307.16913ms]
Mar  2 02:29:56.819: INFO: Created: latency-svc-tkdf8
Mar  2 02:29:56.833: INFO: Created: latency-svc-bv5rh
Mar  2 02:29:56.839: INFO: Got endpoints: latency-svc-tkdf8 [288.316329ms]
Mar  2 02:29:56.848: INFO: Got endpoints: latency-svc-bv5rh [278.689727ms]
Mar  2 02:29:56.849: INFO: Created: latency-svc-9p9sv
Mar  2 02:29:56.857: INFO: Got endpoints: latency-svc-9p9sv [284.61806ms]
Mar  2 02:29:56.868: INFO: Created: latency-svc-2rjms
Mar  2 02:29:56.877: INFO: Created: latency-svc-rrrw6
Mar  2 02:29:56.883: INFO: Got endpoints: latency-svc-2rjms [282.63681ms]
Mar  2 02:29:56.897: INFO: Created: latency-svc-d99jc
Mar  2 02:29:56.897: INFO: Got endpoints: latency-svc-rrrw6 [283.458587ms]
Mar  2 02:29:56.918: INFO: Created: latency-svc-9nzb4
Mar  2 02:29:56.919: INFO: Got endpoints: latency-svc-d99jc [276.670675ms]
Mar  2 02:29:56.927: INFO: Created: latency-svc-794bw
Mar  2 02:29:56.933: INFO: Got endpoints: latency-svc-9nzb4 [260.299157ms]
Mar  2 02:29:56.943: INFO: Created: latency-svc-nrxbf
Mar  2 02:29:56.946: INFO: Got endpoints: latency-svc-794bw [235.500742ms]
Mar  2 02:29:56.958: INFO: Got endpoints: latency-svc-nrxbf [246.142603ms]
Mar  2 02:29:56.965: INFO: Created: latency-svc-hzk4k
Mar  2 02:29:56.984: INFO: Created: latency-svc-95hbr
Mar  2 02:29:56.985: INFO: Got endpoints: latency-svc-hzk4k [264.182225ms]
Mar  2 02:29:57.051: INFO: Got endpoints: latency-svc-95hbr [292.328505ms]
Mar  2 02:29:57.051: INFO: Created: latency-svc-b9x79
Mar  2 02:29:57.065: INFO: Got endpoints: latency-svc-b9x79 [305.578884ms]
Mar  2 02:29:57.067: INFO: Created: latency-svc-dvkz6
Mar  2 02:29:57.068: INFO: Got endpoints: latency-svc-dvkz6 [281.068921ms]
Mar  2 02:29:57.068: INFO: Created: latency-svc-6zsdl
Mar  2 02:29:57.073: INFO: Got endpoints: latency-svc-6zsdl [279.840477ms]
Mar  2 02:29:57.074: INFO: Created: latency-svc-rng8b
Mar  2 02:29:57.083: INFO: Got endpoints: latency-svc-rng8b [265.215809ms]
Mar  2 02:29:57.102: INFO: Created: latency-svc-p8l7w
Mar  2 02:29:57.128: INFO: Created: latency-svc-x42m7
Mar  2 02:29:57.129: INFO: Created: latency-svc-m959w
Mar  2 02:29:57.131: INFO: Got endpoints: latency-svc-x42m7 [282.138413ms]
Mar  2 02:29:57.131: INFO: Got endpoints: latency-svc-p8l7w [291.34262ms]
Mar  2 02:29:57.135: INFO: Got endpoints: latency-svc-m959w [277.906673ms]
Mar  2 02:29:57.176: INFO: Created: latency-svc-xwrcs
Mar  2 02:29:57.186: INFO: Got endpoints: latency-svc-xwrcs [303.338654ms]
Mar  2 02:29:57.187: INFO: Latencies: [65.265161ms 79.526262ms 89.956801ms 102.681522ms 103.676778ms 105.156918ms 108.513154ms 129.889456ms 130.163103ms 157.972851ms 160.132196ms 187.651801ms 194.587246ms 209.822255ms 235.500742ms 237.669858ms 246.142603ms 259.347299ms 259.60387ms 259.768164ms 260.299157ms 264.182225ms 265.215809ms 266.193023ms 270.887306ms 272.867292ms 276.670675ms 277.906673ms 278.689727ms 279.840477ms 281.068921ms 282.138413ms 282.63681ms 283.458587ms 284.61806ms 286.985702ms 288.316329ms 291.34262ms 291.806087ms 292.328505ms 294.146985ms 303.338654ms 303.7091ms 305.578884ms 307.16913ms 319.807197ms 331.457152ms 334.881257ms 340.727631ms 341.85213ms 342.821695ms 343.387405ms 345.778247ms 349.241057ms 350.769214ms 353.249743ms 354.455835ms 359.425281ms 367.952851ms 384.82363ms 387.067944ms 392.83195ms 400.436195ms 400.6386ms 405.431607ms 408.149845ms 409.069804ms 414.117777ms 416.921092ms 417.84523ms 419.331871ms 424.308295ms 425.391523ms 425.934577ms 426.467812ms 427.483041ms 432.156654ms 432.514991ms 433.198781ms 433.864329ms 435.514008ms 437.961458ms 443.637458ms 444.075949ms 444.347863ms 445.798002ms 446.310366ms 446.947435ms 450.042151ms 451.847427ms 452.01198ms 454.270885ms 457.723897ms 459.98518ms 465.607288ms 465.97332ms 468.375596ms 470.096694ms 476.906439ms 476.968204ms 481.546837ms 481.683513ms 483.293256ms 486.943413ms 488.615982ms 489.821686ms 490.248298ms 500.289812ms 500.912575ms 500.954489ms 501.289278ms 502.024582ms 504.1186ms 505.613336ms 510.901664ms 511.115215ms 513.036502ms 516.636607ms 518.5536ms 521.241962ms 523.285202ms 524.59087ms 525.717218ms 526.027967ms 527.614789ms 528.209332ms 528.693128ms 529.246254ms 529.38425ms 531.78084ms 533.2429ms 533.582963ms 533.610648ms 534.57315ms 536.772679ms 538.050181ms 539.728851ms 540.511542ms 543.230935ms 543.523623ms 544.296172ms 544.492628ms 546.150093ms 547.131496ms 547.625287ms 548.44289ms 549.626606ms 550.24536ms 550.407423ms 552.418111ms 552.746046ms 556.849457ms 557.087335ms 558.619664ms 562.344689ms 562.70628ms 569.390713ms 569.506174ms 570.974491ms 573.423242ms 575.009407ms 575.436066ms 575.462523ms 576.787218ms 577.157857ms 579.925076ms 580.445996ms 582.525126ms 587.693811ms 588.060199ms 590.751598ms 594.749529ms 598.282846ms 602.453009ms 608.805281ms 609.546218ms 611.210903ms 612.009693ms 614.681682ms 617.093597ms 620.937958ms 622.32453ms 626.292469ms 630.215258ms 630.385579ms 633.930789ms 650.274956ms 678.077338ms 723.298208ms 726.942079ms 777.682827ms 788.00491ms 805.91682ms 806.083424ms 862.749966ms 887.337649ms 946.789354ms 975.634017ms 994.879536ms 995.062046ms]
Mar  2 02:29:57.187: INFO: 50 %ile: 481.546837ms
Mar  2 02:29:57.187: INFO: 90 %ile: 620.937958ms
Mar  2 02:29:57.187: INFO: 99 %ile: 994.879536ms
Mar  2 02:29:57.187: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:188
Mar  2 02:29:57.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8039" for this suite.

• [SLOW TEST:8.885 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":356,"completed":335,"skipped":6250,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:29:57.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 02:29:57.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Mar  2 02:30:18.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-8902 --namespace=crd-publish-openapi-8902 create -f -'
Mar  2 02:30:20.557: INFO: stderr: ""
Mar  2 02:30:20.558: INFO: stdout: "e2e-test-crd-publish-openapi-5161-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  2 02:30:20.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-8902 --namespace=crd-publish-openapi-8902 delete e2e-test-crd-publish-openapi-5161-crds test-cr'
Mar  2 02:30:20.805: INFO: stderr: ""
Mar  2 02:30:20.805: INFO: stdout: "e2e-test-crd-publish-openapi-5161-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar  2 02:30:20.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-8902 --namespace=crd-publish-openapi-8902 apply -f -'
Mar  2 02:30:24.501: INFO: stderr: ""
Mar  2 02:30:24.502: INFO: stdout: "e2e-test-crd-publish-openapi-5161-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar  2 02:30:24.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-8902 --namespace=crd-publish-openapi-8902 delete e2e-test-crd-publish-openapi-5161-crds test-cr'
Mar  2 02:30:24.949: INFO: stderr: ""
Mar  2 02:30:24.949: INFO: stdout: "e2e-test-crd-publish-openapi-5161-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar  2 02:30:24.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=crd-publish-openapi-8902 explain e2e-test-crd-publish-openapi-5161-crds'
Mar  2 02:30:27.530: INFO: stderr: ""
Mar  2 02:30:27.530: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5161-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 02:30:45.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8902" for this suite.

• [SLOW TEST:48.107 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":356,"completed":336,"skipped":6252,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:30:45.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a suspended cronjob
W0302 02:30:45.554265      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Mar  2 02:35:45.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4948" for this suite.

• [SLOW TEST:300.247 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":356,"completed":337,"skipped":6270,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:35:45.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Mar  2 02:35:49.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7810" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":356,"completed":338,"skipped":6288,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:35:49.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-5797
STEP: creating service affinity-clusterip in namespace services-5797
STEP: creating replication controller affinity-clusterip in namespace services-5797
I0302 02:35:50.183964      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-5797, replica count: 3
I0302 02:35:53.240986      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0302 02:35:56.242588      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 02:35:56.341: INFO: Creating new exec pod
Mar  2 02:35:59.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-5797 exec execpod-affinityzfdkl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Mar  2 02:36:00.107: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar  2 02:36:00.107: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 02:36:00.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-5797 exec execpod-affinityzfdkl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.130.26 80'
Mar  2 02:36:00.456: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.130.26 80\nConnection to 172.21.130.26 80 port [tcp/http] succeeded!\n"
Mar  2 02:36:00.456: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar  2 02:36:00.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-5797 exec execpod-affinityzfdkl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.130.26:80/ ; done'
Mar  2 02:36:00.929: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.130.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.130.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.130.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.130.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.130.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.130.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.130.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.130.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.130.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.130.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.130.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.130.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.130.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.130.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.130.26:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.130.26:80/\n"
Mar  2 02:36:00.930: INFO: stdout: "\naffinity-clusterip-djl84\naffinity-clusterip-djl84\naffinity-clusterip-djl84\naffinity-clusterip-djl84\naffinity-clusterip-djl84\naffinity-clusterip-djl84\naffinity-clusterip-djl84\naffinity-clusterip-djl84\naffinity-clusterip-djl84\naffinity-clusterip-djl84\naffinity-clusterip-djl84\naffinity-clusterip-djl84\naffinity-clusterip-djl84\naffinity-clusterip-djl84\naffinity-clusterip-djl84\naffinity-clusterip-djl84"
Mar  2 02:36:00.930: INFO: Received response from host: affinity-clusterip-djl84
Mar  2 02:36:00.930: INFO: Received response from host: affinity-clusterip-djl84
Mar  2 02:36:00.930: INFO: Received response from host: affinity-clusterip-djl84
Mar  2 02:36:00.930: INFO: Received response from host: affinity-clusterip-djl84
Mar  2 02:36:00.930: INFO: Received response from host: affinity-clusterip-djl84
Mar  2 02:36:00.930: INFO: Received response from host: affinity-clusterip-djl84
Mar  2 02:36:00.930: INFO: Received response from host: affinity-clusterip-djl84
Mar  2 02:36:00.930: INFO: Received response from host: affinity-clusterip-djl84
Mar  2 02:36:00.930: INFO: Received response from host: affinity-clusterip-djl84
Mar  2 02:36:00.930: INFO: Received response from host: affinity-clusterip-djl84
Mar  2 02:36:00.930: INFO: Received response from host: affinity-clusterip-djl84
Mar  2 02:36:00.930: INFO: Received response from host: affinity-clusterip-djl84
Mar  2 02:36:00.930: INFO: Received response from host: affinity-clusterip-djl84
Mar  2 02:36:00.930: INFO: Received response from host: affinity-clusterip-djl84
Mar  2 02:36:00.930: INFO: Received response from host: affinity-clusterip-djl84
Mar  2 02:36:00.930: INFO: Received response from host: affinity-clusterip-djl84
Mar  2 02:36:00.930: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-5797, will wait for the garbage collector to delete the pods
Mar  2 02:36:01.056: INFO: Deleting ReplicationController affinity-clusterip took: 18.967458ms
Mar  2 02:36:01.157: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.688599ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar  2 02:36:05.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5797" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:15.769 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":339,"skipped":6296,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:36:05.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2862
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-2862
I0302 02:36:06.098515      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2862, replica count: 2
Mar  2 02:36:09.151: INFO: Creating new exec pod
I0302 02:36:09.151456      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar  2 02:36:12.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-2862 exec execpodztpx4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar  2 02:36:12.679: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar  2 02:36:12.679: INFO: stdout: "externalname-service-785j2"
Mar  2 02:36:12.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-2862 exec execpodztpx4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.157.15 80'
Mar  2 02:36:12.980: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.157.15 80\nConnection to 172.21.157.15 80 port [tcp/http] succeeded!\n"
Mar  2 02:36:12.980: INFO: stdout: ""
Mar  2 02:36:13.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-2862 exec execpodztpx4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.157.15 80'
Mar  2 02:36:14.503: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.157.15 80\nConnection to 172.21.157.15 80 port [tcp/http] succeeded!\n"
Mar  2 02:36:14.503: INFO: stdout: "externalname-service-97j2m"
Mar  2 02:36:14.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-2862 exec execpodztpx4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.123.244.50 32111'
Mar  2 02:36:14.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.123.244.50 32111\nConnection to 10.123.244.50 32111 port [tcp/*] succeeded!\n"
Mar  2 02:36:14.890: INFO: stdout: ""
Mar  2 02:36:15.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-2862 exec execpodztpx4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.123.244.50 32111'
Mar  2 02:36:16.248: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.123.244.50 32111\nConnection to 10.123.244.50 32111 port [tcp/*] succeeded!\n"
Mar  2 02:36:16.248: INFO: stdout: ""
Mar  2 02:36:16.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-2862 exec execpodztpx4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.123.244.50 32111'
Mar  2 02:36:17.247: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.123.244.50 32111\nConnection to 10.123.244.50 32111 port [tcp/*] succeeded!\n"
Mar  2 02:36:17.247: INFO: stdout: "externalname-service-97j2m"
Mar  2 02:36:17.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3094301754 --namespace=services-2862 exec execpodztpx4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.123.244.41 32111'
Mar  2 02:36:17.639: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.123.244.41 32111\nConnection to 10.123.244.41 32111 port [tcp/*] succeeded!\n"
Mar  2 02:36:17.639: INFO: stdout: "externalname-service-97j2m"
Mar  2 02:36:17.639: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Mar  2 02:36:17.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2862" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:12.069 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":356,"completed":340,"skipped":6297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:36:17.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Mar  2 02:36:18.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3828" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","total":356,"completed":341,"skipped":6343,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:36:18.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Mar  2 02:36:18.436: INFO: observed Pod pod-test in namespace pods-9651 in phase Pending with labels: map[test-pod-static:true] & conditions []
Mar  2 02:36:18.443: INFO: observed Pod pod-test in namespace pods-9651 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:36:18 +0000 UTC  }]
Mar  2 02:36:18.483: INFO: observed Pod pod-test in namespace pods-9651 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:36:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:36:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:36:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:36:18 +0000 UTC  }]
Mar  2 02:36:19.174: INFO: observed Pod pod-test in namespace pods-9651 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:36:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:36:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:36:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:36:18 +0000 UTC  }]
Mar  2 02:36:19.230: INFO: observed Pod pod-test in namespace pods-9651 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:36:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:36:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:36:18 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:36:18 +0000 UTC  }]
Mar  2 02:36:20.664: INFO: Found Pod pod-test in namespace pods-9651 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:36:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:36:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:36:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-02 02:36:18 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Mar  2 02:36:20.911: INFO: observed event type MODIFIED
Mar  2 02:36:22.671: INFO: observed event type MODIFIED
Mar  2 02:36:22.978: INFO: observed event type MODIFIED
Mar  2 02:36:23.679: INFO: observed event type MODIFIED
Mar  2 02:36:23.993: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Mar  2 02:36:24.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9651" for this suite.

• [SLOW TEST:5.959 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":356,"completed":342,"skipped":6360,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:36:24.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar  2 02:36:29.613: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Mar  2 02:36:29.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1456" for this suite.

• [SLOW TEST:5.586 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":343,"skipped":6375,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:36:29.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 02:36:29.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 02:36:37.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4181" for this suite.

• [SLOW TEST:7.499 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":356,"completed":344,"skipped":6378,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:36:37.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 02:36:37.327: INFO: Creating deployment "test-recreate-deployment"
W0302 02:36:37.379708      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Mar  2 02:36:37.380: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar  2 02:36:37.437: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar  2 02:36:39.500: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar  2 02:36:39.526: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 36, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 36, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 36, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 36, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-845d658455\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar  2 02:36:41.549: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar  2 02:36:41.612: INFO: Updating deployment test-recreate-deployment
Mar  2 02:36:41.612: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar  2 02:36:41.914: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2435  c6b40dd1-da6f-4577-ae1c-51d8f4a8c51e 157790 2 2023-03-02 02:36:37 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-03-02 02:36:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 02:36:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006c0d728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-02 02:36:41 +0000 UTC,LastTransitionTime:2023-03-02 02:36:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cd8586fc7" is progressing.,LastUpdateTime:2023-03-02 02:36:41 +0000 UTC,LastTransitionTime:2023-03-02 02:36:37 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar  2 02:36:41.929: INFO: New ReplicaSet "test-recreate-deployment-cd8586fc7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cd8586fc7  deployment-2435  0e083eed-3f95-47c1-937d-9f6df811ce9d 157788 1 2023-03-02 02:36:41 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment c6b40dd1-da6f-4577-ae1c-51d8f4a8c51e 0xc0048390b0 0xc0048390b1}] []  [{kube-controller-manager Update apps/v1 2023-03-02 02:36:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6b40dd1-da6f-4577-ae1c-51d8f4a8c51e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 02:36:41 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cd8586fc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004839148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 02:36:41.929: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar  2 02:36:41.930: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-845d658455  deployment-2435  26ebd23b-57f3-47d2-bf12-1457b16423bb 157778 2 2023-03-02 02:36:37 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:845d658455] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment c6b40dd1-da6f-4577-ae1c-51d8f4a8c51e 0xc004838f97 0xc004838f98}] []  [{kube-controller-manager Update apps/v1 2023-03-02 02:36:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c6b40dd1-da6f-4577-ae1c-51d8f4a8c51e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-02 02:36:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 845d658455,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:845d658455] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004839048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar  2 02:36:41.943: INFO: Pod "test-recreate-deployment-cd8586fc7-xvn8v" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cd8586fc7-xvn8v test-recreate-deployment-cd8586fc7- deployment-2435  59eb3d87-530a-4dee-96d9-d71299c4776d 157789 0 2023-03-02 02:36:41 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-cd8586fc7 0e083eed-3f95-47c1-937d-9f6df811ce9d 0xc0048395f7 0xc0048395f8}] []  [{kube-controller-manager Update v1 2023-03-02 02:36:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e083eed-3f95-47c1-937d-9f6df811ce9d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-02 02:36:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bfzrf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bfzrf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.123.244.39,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c67,c64,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gm85b,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:36:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:36:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:36:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-02 02:36:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.123.244.39,PodIP:,StartTime:2023-03-02 02:36:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Mar  2 02:36:41.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2435" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":345,"skipped":6391,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:36:41.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 02:36:42.315: INFO: Create a RollingUpdate DaemonSet
Mar  2 02:36:42.335: INFO: Check that daemon pods launch on every node of the cluster
Mar  2 02:36:42.373: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:36:42.373: INFO: Node 10.123.244.39 is running 0 daemon pod, expected 1
Mar  2 02:36:43.423: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:36:43.423: INFO: Node 10.123.244.39 is running 0 daemon pod, expected 1
Mar  2 02:36:44.455: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:36:44.455: INFO: Node 10.123.244.39 is running 0 daemon pod, expected 1
Mar  2 02:36:45.416: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 02:36:45.416: INFO: Node 10.123.244.50 is running 0 daemon pod, expected 1
Mar  2 02:36:46.424: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 02:36:46.424: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Mar  2 02:36:46.424: INFO: Update the DaemonSet to trigger a rollout
Mar  2 02:36:46.503: INFO: Updating DaemonSet daemon-set
Mar  2 02:36:49.594: INFO: Roll back the DaemonSet before rollout is complete
Mar  2 02:36:49.646: INFO: Updating DaemonSet daemon-set
Mar  2 02:36:49.646: INFO: Make sure DaemonSet rollback is complete
Mar  2 02:36:49.665: INFO: Wrong image for pod: daemon-set-t8g96. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Mar  2 02:36:49.665: INFO: Pod daemon-set-t8g96 is not available
Mar  2 02:36:52.712: INFO: Pod daemon-set-pgpzh is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5327, will wait for the garbage collector to delete the pods
Mar  2 02:36:52.940: INFO: Deleting DaemonSet.extensions daemon-set took: 35.260741ms
Mar  2 02:36:53.041: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.94538ms
Mar  2 02:36:56.678: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:36:56.678: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  2 02:36:56.692: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"158081"},"items":null}

Mar  2 02:36:56.718: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"158081"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Mar  2 02:36:56.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5327" for this suite.

• [SLOW TEST:14.856 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":356,"completed":346,"skipped":6407,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:36:56.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Mar  2 02:36:57.135: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Mar  2 02:36:57.282: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Mar  2 02:36:57.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1961" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":356,"completed":347,"skipped":6450,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:36:57.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 02:36:59.259: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 02:37:02.345: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 02:37:02.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8698" for this suite.
STEP: Destroying namespace "webhook-8698-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:5.424 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":356,"completed":348,"skipped":6491,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:37:02.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-3f5a39c4-f3a7-4bea-a98e-39e7d6fbb9a8
STEP: Creating a pod to test consume secrets
Mar  2 02:37:03.182: INFO: Waiting up to 5m0s for pod "pod-secrets-1b68fc8a-0ef5-4ed8-bb88-096e52683fd1" in namespace "secrets-5690" to be "Succeeded or Failed"
Mar  2 02:37:03.192: INFO: Pod "pod-secrets-1b68fc8a-0ef5-4ed8-bb88-096e52683fd1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.507877ms
Mar  2 02:37:05.210: INFO: Pod "pod-secrets-1b68fc8a-0ef5-4ed8-bb88-096e52683fd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027372551s
Mar  2 02:37:07.256: INFO: Pod "pod-secrets-1b68fc8a-0ef5-4ed8-bb88-096e52683fd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074017775s
STEP: Saw pod success
Mar  2 02:37:07.257: INFO: Pod "pod-secrets-1b68fc8a-0ef5-4ed8-bb88-096e52683fd1" satisfied condition "Succeeded or Failed"
Mar  2 02:37:07.269: INFO: Trying to get logs from node 10.123.244.39 pod pod-secrets-1b68fc8a-0ef5-4ed8-bb88-096e52683fd1 container secret-volume-test: <nil>
STEP: delete the pod
Mar  2 02:37:07.402: INFO: Waiting for pod pod-secrets-1b68fc8a-0ef5-4ed8-bb88-096e52683fd1 to disappear
Mar  2 02:37:07.413: INFO: Pod pod-secrets-1b68fc8a-0ef5-4ed8-bb88-096e52683fd1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Mar  2 02:37:07.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5690" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":349,"skipped":6542,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:37:07.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Mar  2 02:37:07.667: INFO: Waiting up to 5m0s for pod "downward-api-5d07f9f7-45bd-40fb-87c7-5129f028de3a" in namespace "downward-api-1230" to be "Succeeded or Failed"
Mar  2 02:37:07.678: INFO: Pod "downward-api-5d07f9f7-45bd-40fb-87c7-5129f028de3a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.935525ms
Mar  2 02:37:09.701: INFO: Pod "downward-api-5d07f9f7-45bd-40fb-87c7-5129f028de3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033889457s
Mar  2 02:37:11.716: INFO: Pod "downward-api-5d07f9f7-45bd-40fb-87c7-5129f028de3a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048890393s
Mar  2 02:37:13.728: INFO: Pod "downward-api-5d07f9f7-45bd-40fb-87c7-5129f028de3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.061200544s
STEP: Saw pod success
Mar  2 02:37:13.728: INFO: Pod "downward-api-5d07f9f7-45bd-40fb-87c7-5129f028de3a" satisfied condition "Succeeded or Failed"
Mar  2 02:37:13.737: INFO: Trying to get logs from node 10.123.244.39 pod downward-api-5d07f9f7-45bd-40fb-87c7-5129f028de3a container dapi-container: <nil>
STEP: delete the pod
Mar  2 02:37:13.812: INFO: Waiting for pod downward-api-5d07f9f7-45bd-40fb-87c7-5129f028de3a to disappear
Mar  2 02:37:13.833: INFO: Pod downward-api-5d07f9f7-45bd-40fb-87c7-5129f028de3a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Mar  2 02:37:13.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1230" for this suite.

• [SLOW TEST:6.435 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":356,"completed":350,"skipped":6549,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:37:13.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 02:37:14.113: INFO: The status of Pod busybox-scheduling-53d590d9-2f62-4aa9-aa98-6ec6cdb513c5 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:37:16.127: INFO: The status of Pod busybox-scheduling-53d590d9-2f62-4aa9-aa98-6ec6cdb513c5 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Mar  2 02:37:16.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1880" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":356,"completed":351,"skipped":6568,"failed":0}

------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:37:16.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's args
Mar  2 02:37:16.370: INFO: Waiting up to 5m0s for pod "var-expansion-582c804d-babe-4bcd-974f-377922760105" in namespace "var-expansion-8662" to be "Succeeded or Failed"
Mar  2 02:37:16.380: INFO: Pod "var-expansion-582c804d-babe-4bcd-974f-377922760105": Phase="Pending", Reason="", readiness=false. Elapsed: 10.121608ms
Mar  2 02:37:18.392: INFO: Pod "var-expansion-582c804d-babe-4bcd-974f-377922760105": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022366829s
Mar  2 02:37:20.407: INFO: Pod "var-expansion-582c804d-babe-4bcd-974f-377922760105": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036726445s
STEP: Saw pod success
Mar  2 02:37:20.407: INFO: Pod "var-expansion-582c804d-babe-4bcd-974f-377922760105" satisfied condition "Succeeded or Failed"
Mar  2 02:37:20.418: INFO: Trying to get logs from node 10.123.244.39 pod var-expansion-582c804d-babe-4bcd-974f-377922760105 container dapi-container: <nil>
STEP: delete the pod
Mar  2 02:37:20.476: INFO: Waiting for pod var-expansion-582c804d-babe-4bcd-974f-377922760105 to disappear
Mar  2 02:37:20.499: INFO: Pod var-expansion-582c804d-babe-4bcd-974f-377922760105 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Mar  2 02:37:20.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8662" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":356,"completed":352,"skipped":6568,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:37:20.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar  2 02:37:21.753: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar  2 02:37:23.812: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 2, 2, 37, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 37, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 2, 2, 37, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 2, 2, 37, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar  2 02:37:26.975: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Mar  2 02:37:27.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-777" for this suite.
STEP: Destroying namespace "webhook-777-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.331 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":356,"completed":353,"skipped":6579,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:37:27.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
Mar  2 02:37:28.244: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar  2 02:37:28.359: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:37:28.359: INFO: Node 10.123.244.39 is running 0 daemon pod, expected 1
Mar  2 02:37:29.403: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:37:29.403: INFO: Node 10.123.244.39 is running 0 daemon pod, expected 1
Mar  2 02:37:30.490: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 02:37:30.490: INFO: Node 10.123.244.50 is running 0 daemon pod, expected 1
Mar  2 02:37:31.410: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 02:37:31.410: INFO: Node 10.123.244.50 is running 0 daemon pod, expected 1
Mar  2 02:37:32.398: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 02:37:32.398: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar  2 02:37:32.553: INFO: Wrong image for pod: daemon-set-5t98t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 02:37:32.554: INFO: Wrong image for pod: daemon-set-qgctt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 02:37:33.591: INFO: Wrong image for pod: daemon-set-5t98t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 02:37:33.591: INFO: Wrong image for pod: daemon-set-qgctt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 02:37:34.588: INFO: Wrong image for pod: daemon-set-5t98t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 02:37:34.589: INFO: Wrong image for pod: daemon-set-qgctt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 02:37:35.601: INFO: Wrong image for pod: daemon-set-5t98t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 02:37:35.601: INFO: Pod daemon-set-gx27d is not available
Mar  2 02:37:35.601: INFO: Wrong image for pod: daemon-set-qgctt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 02:37:36.600: INFO: Wrong image for pod: daemon-set-5t98t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 02:37:36.600: INFO: Pod daemon-set-gx27d is not available
Mar  2 02:37:36.600: INFO: Wrong image for pod: daemon-set-qgctt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 02:37:37.631: INFO: Wrong image for pod: daemon-set-5t98t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 02:37:38.597: INFO: Wrong image for pod: daemon-set-5t98t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 02:37:39.587: INFO: Wrong image for pod: daemon-set-5t98t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 02:37:40.599: INFO: Wrong image for pod: daemon-set-5t98t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 02:37:40.599: INFO: Pod daemon-set-fsqkt is not available
Mar  2 02:37:41.592: INFO: Wrong image for pod: daemon-set-5t98t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 02:37:41.592: INFO: Pod daemon-set-fsqkt is not available
Mar  2 02:37:42.592: INFO: Wrong image for pod: daemon-set-5t98t. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Mar  2 02:37:42.592: INFO: Pod daemon-set-fsqkt is not available
Mar  2 02:37:44.595: INFO: Pod daemon-set-vpjpt is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Mar  2 02:37:44.651: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 02:37:44.651: INFO: Node 10.123.244.39 is running 0 daemon pod, expected 1
Mar  2 02:37:45.682: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar  2 02:37:45.682: INFO: Node 10.123.244.39 is running 0 daemon pod, expected 1
Mar  2 02:37:46.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar  2 02:37:46.687: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9542, will wait for the garbage collector to delete the pods
Mar  2 02:37:46.880: INFO: Deleting DaemonSet.extensions daemon-set took: 24.901443ms
Mar  2 02:37:47.181: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.914889ms
Mar  2 02:37:49.197: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar  2 02:37:49.197: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar  2 02:37:49.210: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"159312"},"items":null}

Mar  2 02:37:49.226: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"159312"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Mar  2 02:37:49.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9542" for this suite.

• [SLOW TEST:21.499 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":356,"completed":354,"skipped":6591,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:37:49.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Mar  2 02:37:49.656: INFO: The status of Pod annotationupdate03282158-c26a-446b-bbec-700d24604a25 is Pending, waiting for it to be Running (with Ready = true)
Mar  2 02:37:51.669: INFO: The status of Pod annotationupdate03282158-c26a-446b-bbec-700d24604a25 is Running (Ready = true)
Mar  2 02:37:52.307: INFO: Successfully updated pod "annotationupdate03282158-c26a-446b-bbec-700d24604a25"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Mar  2 02:37:56.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6002" for this suite.

• [SLOW TEST:7.032 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":355,"skipped":6593,"failed":0}
[sig-network] DNS 
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Mar  2 02:37:56.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3094301754
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9788.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9788.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9788.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9788.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar  2 02:38:00.829: INFO: DNS probes using dns-9788/dns-test-5baaaeb3-55bc-4a27-8c53-3bdf1131625b succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Mar  2 02:38:00.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9788" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","total":356,"completed":356,"skipped":6593,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSMar  2 02:38:00.949: INFO: Running AfterSuite actions on all nodes
Mar  2 02:38:00.949: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func19.2
Mar  2 02:38:00.949: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Mar  2 02:38:00.949: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Mar  2 02:38:00.949: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Mar  2 02:38:00.949: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Mar  2 02:38:00.949: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Mar  2 02:38:00.949: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Mar  2 02:38:00.949: INFO: Running AfterSuite actions on node 1
Mar  2 02:38:00.949: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":356,"completed":356,"skipped":6617,"failed":0}

Ran 356 of 6973 Specs in 6921.705 seconds
SUCCESS! -- 356 Passed | 0 Failed | 0 Pending | 6617 Skipped
PASS

Ginkgo ran 1 suite in 1h55m27.532244279s
Test Suite Passed
